<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20250908-20250914" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20250908-20250914 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20250908-20250914"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20250908-20250914 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-09-08"><meta data-rh="true" property="og:description" content="2025-09-08"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20250908-20250914"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250908-20250914" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250908-20250914" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20250908-20250914","item":"https://darkknight996.github.io/daily/20250908-20250914"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.78e1c784.js" defer="defer"></script>
<script src="/assets/js/main.be5f6e7c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20250908-20250914</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20250908-20250914</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-08">2025-09-08<a href="#2025-09-08" class="hash-link" aria-label="Direct link to 2025-09-08" title="Direct link to 2025-09-08" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Research on fault diagnosis and root cause analysis based on full stack
observability</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [fault diagnosis, root cause analysis, causal discovery, multi-modal fusion, observability data]</li>
<li class=""><strong>authors:</strong> Jian Hou</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12231v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12231v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes KylinRCA framework that integrates temporal causal discovery and cross-modal graph learning for fault diagnosis. It achieves global root cause localization and generates auditable evidence chains using mask-based explanation methods. The framework provides an effective solution for fault diagnosis under full-stack observability by combining dynamic causal analysis with multi-modal data fusion.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Several Performance Bounds on Decentralized Online Optimization are
Highly Conservative and Potentially Misleading</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [decentralized optimization, performance analysis, algorithm tuning, worst-case analysis, online optimization]</li>
<li class=""><strong>authors:</strong> Erwan Meunier, Julien M. Hendrickx</li>
<li class=""><strong>institution:</strong> Université catholique de Louvain</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.06466v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.06466v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses the Performance Estimation Problem approach to analyze decentralized online optimization algorithms, revealing that existing performance bounds are highly conservative and can lead to poor algorithm selection. The authors demonstrate how step-size tuning can improve classical methods, achieving up to 20% reduction in worst-case performance regret. Their analysis suggests some algorithms may not benefit from inter-agent communications for significant periods in worst-case scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World
Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [speculative decoding, dynamic adaptation, KLD stability, large-batch serving, post-hoc signals]</li>
<li class=""><strong>authors:</strong> Mingyu Yang, Jae-Young Choi, Kihyo Moon, Minsung Jang, Eunjoo Jeon</li>
<li class=""><strong>institution:</strong> Samsung SDS</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01083v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01083v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DSDE proposes a training-free framework using KLD variance as a stability signal to dynamically adjust speculation length during LLM inference. The method achieves competitive latency and superior robustness across diverse workloads, particularly in low-acceptance-rate scenarios. This demonstrates the value of post-hoc signals for building more adaptive and robust LLM inference systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [MaaS, SLO-aware orchestration, heterogeneous model instances, parallelism strategies, continuous batching]</li>
<li class=""><strong>authors:</strong> Mo Xuan, Zhang yue, Wu Weigang</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.06362v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.06362v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MaaSO introduces a novel orchestrator for Model-as-a-Service platforms that optimizes heterogeneous LLM instance configurations through profiling, placement optimization, and SLO-aware request distribution. The system leverages different parallelism strategies and batch sizes to better meet diverse latency requirements. Experimental results show MaaSO improves SLO satisfaction by 15-30% and reduces latency by 40-60% compared to existing approaches while lowering orchestration overhead.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-09">2025-09-09<a href="#2025-09-09" class="hash-link" aria-label="Direct link to 2025-09-09" title="Direct link to 2025-09-09" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for
Efficient MoE LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Mixture of Experts, expert scheduling, memory optimization, prefetching, cache management, CUDA pipeline]</li>
<li class=""><strong>authors:</strong> Yuning Zhang, Grant Pinkert, Nan Yang, Yanli Li, Dong Yuan</li>
<li class=""><strong>institution:</strong> The University of Sydney</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07379v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07379v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DuoServe-MoE proposes a dual-phase inference system that separates prefill and decode stages with tailored expert scheduling strategies. It uses a two-stream CUDA pipeline for prefill and a lightweight predictor for decode-stage expert prefetching. Experiments show 1.42-7.54× latency improvements while maintaining only 15% peak memory usage of full model size.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Towards Scalable Proteomics: Opportunistic SMC Samplers on HTCondor</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Sequential Monte Carlo, proteomics, HTCondor, opportunistic computing, Bayesian inference, distributed computing]</li>
<li class=""><strong>authors:</strong> Matthew Carter, Lee Devlin, Alexander Philips, Edward Pyzer-Knapp, Paul Spirakis, Simon Maskell</li>
<li class=""><strong>institution:</strong> University of Liverpool</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.08020v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.08020v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an opportunistic computing framework using HTCondor to deploy Sequential Monte Carlo samplers for large-scale proteomics inference. The proposed Coordinator-Manager-Follower architecture reduces synchronization overhead and enables scalable Bayesian inference using idle university computing resources. Results show the framework achieves accurate inference with weak scaling, generating more samples under fixed time constraints as resources increase.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Astra: A Multi-Agent System for GPU Kernel Performance Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU kernel optimization, multi-agent LLM system, CUDA code generation, performance optimization, SGLang framework]</li>
<li class=""><strong>authors:</strong> Anjiang Wei, Tianran Sun, Yogesh Seenichamy, Hang Song, Anne Ouyang, Azalia Mirhoseini, Ke Wang, Alex Aiken</li>
<li class=""><strong>institution:</strong> Stanford University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07506v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07506v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Astra introduces a multi-agent LLM system that collaboratively optimizes GPU kernels through iterative code generation, testing, profiling, and planning. Starting from existing CUDA implementations in SGLang, it achieves an average 1.32x speedup using zero-shot prompting. The system autonomously applies optimizations like loop transformations and memory access improvements, demonstrating LLMs&#x27; potential for high-performance kernel optimization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Accelerating Frontier MoE Training with 3D Integrated Optics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [3D integrated optics, photonic interconnects, mixture of experts, scale-up domain, high-performance computing]</li>
<li class=""><strong>authors:</strong> Mikhail Bernadskiy, Peter Carson, Thomas Graham, Taylor Groves, Ho John Lee, Eric Yeh</li>
<li class=""><strong>institution:</strong> Lightmatter</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15893v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15893v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using 3D-stacked photonic interconnects to overcome electrical interconnect limitations in large-scale AI training. The method enables connecting thousands of GPUs across multiple racks with 8X scale-up capability. Results show 2.7X faster training time for trillion-parameter MoE models through improved bandwidth and multi-dimensional parallelism.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with
FaaS-hosted MCP Services</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Agentic AI, Workflow Patterns, FaaS, MCP, Model Context Protocol, Cloud Computing]</li>
<li class=""><strong>authors:</strong> Shiva Sai Krishna Anand Tokal, Vaibhav Jha, Anand Eswaran, Praveen Jayachandran, Yogesh Simmhan</li>
<li class=""><strong>institution:</strong> Indian Institute of Science, Bangalore and IBM India Research Lab, Bangalore</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07595v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07595v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AgentX introduces a novel agentic workflow pattern with stage designer, planner, and executor agents that outperforms state-of-the-art patterns like ReAct and Magentic One. It leverages Model Context Protocol tools deployed via FaaS for improved performance. Empirical evaluation shows competitive success rates with better latency and cost efficiency in practical applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] MoE-Compression: How the Compression Error of Experts Affects the
Inference Accuracy of MoE Model?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Mixture of Experts, model compression, error-bounded lossy compression, inference accuracy, expert layers]</li>
<li class=""><strong>authors:</strong> Songkai Ma, Zhaorui Zhang, Sheng Di, Benben Liu, Xiaodong Yu, Xiaoyi Lu, Dan Wang</li>
<li class=""><strong>institution:</strong> Hong Kong Polytechnic University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07727v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07727v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using error-bounded lossy compression algorithms to compress non-activated experts in MoE models for efficient inference under GPU memory constraints. Experiments show compression errors in shallow layers cause minimal accuracy degradation, while middle-layer errors significantly impair performance, and deep-layer errors can sometimes improve accuracy. The study provides a comprehensive analysis of how compression-induced errors affect MoE inference accuracy across different expert layers.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-10">2025-09-10<a href="#2025-09-10" class="hash-link" aria-label="Direct link to 2025-09-10" title="Direct link to 2025-09-10" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Design and Implementation of Code Completion System Based on LLM and
CodeBERT Hybrid Subsystem</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [code completion, hybrid model, CodeBERT, GPT-3.5, code generation]</li>
<li class=""><strong>authors:</strong> Bingbing Zhang, Ziyu Lin, Yingxin Su</li>
<li class=""><strong>institution:</strong> Xiamen Institute of Technology, Google LLC, University of California Davis</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.08215v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.08215v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid model combining CodeBERT and GPT-3.5 for code completion tasks, leveraging CodeBERT&#x27;s semantic understanding and GPT-3.5&#x27;s generation capabilities. The hybrid approach demonstrates superior performance in accuracy, code quality, and efficiency compared to benchmarks. Robustness testing confirms the model&#x27;s reliability and stability in practical applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Optimizing the Variant Calling Pipeline Execution on Human Genomes Using
GPU-Enabled Machines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [variant calling pipeline, GPU optimization, machine learning prediction, job shop scheduling, genome analysis]</li>
<li class=""><strong>authors:</strong> Ajay Kumar, Praveen Rao, Peter Sanders</li>
<li class=""><strong>institution:</strong> The University of Missouri, Karlsruhe Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.09058v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.09058v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a machine learning-based approach to optimize variant calling pipeline execution on GPU-enabled machines. The method uses ML to predict execution times of pipeline stages and generates optimal execution plans inspired by flexible job shop scheduling. Experimental results show the approach achieved 2X speedup over greedy methods and effectively predicted execution times using genomic sequence features.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and
Dynamic Parallelism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [heterogeneous GPU clusters, fine-grained parallelism, dynamic load balancing, attention mechanism optimization]</li>
<li class=""><strong>authors:</strong> Zizhao Mo, Jianxiong Liao, Huanle Xu, Zhi Zhou, Chengzhong Xu</li>
<li class=""><strong>institution:</strong> University of Macau, Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.08309v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.08309v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hetis introduces fine-grained and dynamic parallelism for LLM serving in heterogeneous GPU clusters, selectively parallelizing compute-intensive operations and distributing attention computations at head granularity. The system employs an online load dispatching policy to balance network latency, computational load, and memory intensity. Evaluation shows Hetis improves throughput by up to 2.25× and reduces latency by 1.49× compared to existing systems.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-11">2025-09-11<a href="#2025-09-11" class="hash-link" aria-label="Direct link to 2025-09-11" title="Direct link to 2025-09-11" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] TrEnv: Transparently Share Serverless Execution Environments Across
Different Functions and Nodes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [serverless computing, execution environment sharing, memory optimization, cold start reduction, LLM agents]</li>
<li class=""><strong>authors:</strong> Jialiang Huang, Teng Ma, Zheng Liu, Sixing Lin, Kang Chen, Jinlei Jiang, Xia Liao, Yingdi Shan, Yongwei Wu, Ning Zhang, Mengting Lu, Tao Ma, Haifeng Gong, Mingxing Zhang</li>
<li class=""><strong>institution:</strong> Tsinghua University, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.09525v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.09525v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TrEnv introduces a serverless platform that transparently shares execution environments across functions and nodes using repurposable sandboxes and memory templates. It reduces startup latency and memory usage for both container-based and VM-based environments. The system achieves up to 7× lower P99 latency and 61% memory savings compared to state-of-the-art systems like E2B for LLM agent workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Coherence-Aware Task Graph Modeling for Realistic Application</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [cache coherence, task graph modeling, multicore systems, runtime behavior, system performance]</li>
<li class=""><strong>authors:</strong> Guochu Xiong, Xiangzhong Luo, Weichen Liu</li>
<li class=""><strong>institution:</strong> Nanyang Technological University, Southeast University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.09094v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.09094v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CoTAM proposes a coherence-aware task graph modeling framework that constructs unified task graphs reflecting runtime behavior by analyzing cache coherence impacts through decoupling and learned weighting schemes. The framework outperforms implicit methods and bridges the gap between dynamic workload behavior and existing designs. Experimental results demonstrate the importance of incorporating cache coherence into task graph modeling for accurate system-level analysis.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-12">2025-09-12<a href="#2025-09-12" class="hash-link" aria-label="Direct link to 2025-09-12" title="Direct link to 2025-09-12" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards
Autonomous Science</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Agentic AI, Scientific Workflows, Autonomous Science, Swarm Intelligence, Workflow Evolution]</li>
<li class=""><strong>authors:</strong> Woong Shin, Renan Souza, Daniel Rosendo, Frédéric Suter, Feiyi Wang, Prasanna Balaprakash, Rafael Ferreira da Silva</li>
<li class=""><strong>institution:</strong> Oak Ridge National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.09915v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.09915v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a conceptual framework for evolving scientific workflows along intelligence and composition dimensions, transitioning from static systems to intelligent swarm-based approaches. It presents an architectural blueprint for autonomous science laboratories that integrate AI agents as ecosystem components. This framework aims to enable 100x acceleration in scientific discovery through distributed, autonomous workflow coordination.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] SynergAI: Edge-to-Cloud Synergy for Architecture-Driven High-Performance
Orchestration for AI Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [AI inference, edge computing, cloud computing, performance optimization, Kubernetes]</li>
<li class=""><strong>authors:</strong> Foteini Stathopoulou, Aggelos Ferikoglou, Manolis Katsaragakis, Dimosthenis Masouros, Sotirios Xydis, Dimitrios Soudris</li>
<li class=""><strong>institution:</strong> National Technical University of Athens</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12252v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12252v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SynergAI introduces an architecture-aware framework for AI inference serving across edge-to-cloud infrastructures using offline and online scheduling policies. It dynamically allocates workloads across heterogeneous hardware to minimize QoS violations. The system achieves 2.4x fewer QoS violations compared to state-of-the-art solutions when implemented in a Kubernetes ecosystem.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] MinatoLoader: Accelerating Machine Learning Training Through Efficient
Data Preprocessing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [data loader, data preprocessing, GPU utilization, batch construction, PyTorch]</li>
<li class=""><strong>authors:</strong> Rahma Nouaji, Stella Bitchebe, Ricardo Macedo, Oana Balmau</li>
<li class=""><strong>institution:</strong> McGill University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.10712v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.10712v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MinatoLoader introduces a new data loading approach that prioritizes fast-to-preprocess samples and processes slower samples in parallel to reduce GPU idleness. It achieves up to 7.5× training speedup and improves GPU utilization from 46.4% to 90.45% while maintaining model accuracy across various workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Coordinated Reinforcement Learning Prefetching Architecture for
Multicore Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [reinforcement learning, hardware prefetching, multicore systems, coordinated learning, memory optimization]</li>
<li class=""><strong>authors:</strong> Mohammed Humaid Siddiqui, Fernando Guzman, Yufei Wu, Ruishu Ann</li>
<li class=""><strong>institution:</strong> Fairleigh Dickinson University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.10719v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.10719v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes CRL-Pythia, a coordinated reinforcement learning prefetcher that enables cross-core information sharing and cooperative prefetching decisions in multicore systems. This approach significantly reduces redundant prefetch requests and improves learning convergence across cores. Experimental results show CRL-Pythia outperforms single Pythia configurations with approximately 12% IPC improvement for bandwidth-constrained workloads while maintaining moderate hardware overhead.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-13">2025-09-13<a href="#2025-09-13" class="hash-link" aria-label="Direct link to 2025-09-13" title="Direct link to 2025-09-13" translate="no">​</a></h2>
<ul>
<li class=""><strong>[arXiv2509] FastTrack: GPU-Accelerated Tracking for Visual SLAM</strong>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [GPU acceleration, visual SLAM, stereo feature matching, local map tracking, CUDA implementation, real-time performance]</li>
<li class=""><strong>authors:</strong> Kimia Khabiri, Parsa Hosseininejad, Shishir Gopinath, Karthik Dantu, Steven Y. Ko</li>
<li class=""><strong>institution:</strong> University at Buffalo (based on author affiliations: 1 - University at Buffalo, 2 - University of Texas at Austin)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.10757v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.10757v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents FastTrack, a GPU-accelerated tracking system for visual-inertial SLAM that uses CUDA to speed up stereo feature matching and local map tracking. The implementation within ORB-SLAM3 demonstrates up to 2.8× performance improvement on both desktop and embedded platforms. Evaluation using EuRoC and TUM-VI datasets confirms the effectiveness of GPU acceleration for real-time SLAM applications.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-14">2025-09-14<a href="#2025-09-14" class="hash-link" aria-label="Direct link to 2025-09-14" title="Direct link to 2025-09-14" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Parallel/Distributed Tabu Search for Scheduling Microprocessor Tasks in
Hybrid Flowshop</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [tabu search, hybrid flow shop, multiprocessor tasks, distributed computing, parallel computing]</li>
<li class=""><strong>authors:</strong> Adam Janiak, Damian Kowalczyk, Maciej Lichtenstein</li>
<li class=""><strong>institution:</strong> Wrocław University of Science and Technology (Politechnika Wrocławska)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11396v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11396v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a parallel/distributed tabu search algorithm for scheduling microprocessor tasks in hybrid flow shops to minimize makespan. The algorithm effectively utilizes parallel and distributed mechanisms for neighborhood evaluation and balances heterogeneous network environments. The approach demonstrates efficient performance for solving this NP-hard scheduling problem.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] GFS: A Preemption-aware Scheduling Framework for GPU Clusters with
Predictive Spot Instance Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU scheduling, spot instances, preemption management, resource allocation, cluster management]</li>
<li class=""><strong>authors:</strong> Jiaang Duan, Shenglin Xu, Shiyou Qian, Dingyu Yang, Kangjin Wang, Chenzhi Liao, Yinghao Yu, Qin Hua, Hanwen Hu, Qi Wang, Wenchao Wu, Dongqing Bao, Tianyu Lu, Jian Cao, Guangtao Xue, Guodong Yang, Liping Zhang, Gang Chen</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Zhejiang University, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11134v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11134v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GFS introduces a preemption-aware scheduling framework with predictive spot instance management for GPU clusters. It uses demand forecasting, dynamic quota allocation, and preemptive scheduling policies to balance high-priority and low-priority tasks. The framework reduces eviction rates by 33.0%, cuts queuing delays by 44.1%, and improves GPU allocation rates by up to 22.8% in production clusters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Chameleon: Taming Dynamic Operator Sequences for Memory-Intensive LLM
Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [memory optimization, swap-based training, dynamic operator sequences, eager mode]</li>
<li class=""><strong>authors:</strong> Zibo Wang, Yuhang Zhou, Zhibin Wang, Shipeng Li, Xinjing Huang, Chendong Cai, Bingxu Mu, Yuqing Sun, Zhiheng Hu, Bin She, Shu You, Guanghuan Fang, Rong Gu, Wanchun Dou, Guihai Chen, Chen Tian</li>
<li class=""><strong>institution:</strong> Nanjing University, Huawei Technologies Co., Ltd</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11076v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11076v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Chameleon introduces a lightweight online profiler and optimized swap policy execution to handle dynamic operator sequences in LLM training. It enables training models up to 4x larger than hardware memory capacity while reducing profiling overhead by 84.25%. The system achieves up to 38.94% performance improvement compared to recomputation or high-degree parallelism methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing
for Energy-Efficient LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [energy efficiency, frequency scaling, request routing, prefill-decode disaggregation, SLO-aware serving]</li>
<li class=""><strong>authors:</strong> Jiahuan Yu, Aryan Taneja, Junfeng Lin, Minjia Zhang</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04827v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04827v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VoltanaLLM introduces a control-theoretic approach combining feedback-driven frequency control and state-space routing for LLM serving. It dynamically adjusts GPU frequencies for prefill/decode phases and optimizes request routing to minimize energy consumption. The system achieves up to 36.3% energy savings while maintaining near-perfect SLO attainment rates.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-24T13:11:58.000Z" itemprop="dateModified">Oct 24, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20250901-20250907"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20250901-20250907</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20250915-20250921"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20250915-20250921</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-09-08" class="table-of-contents__link toc-highlight">2025-09-08</a></li><li><a href="#2025-09-09" class="table-of-contents__link toc-highlight">2025-09-09</a></li><li><a href="#2025-09-10" class="table-of-contents__link toc-highlight">2025-09-10</a></li><li><a href="#2025-09-11" class="table-of-contents__link toc-highlight">2025-09-11</a></li><li><a href="#2025-09-12" class="table-of-contents__link toc-highlight">2025-09-12</a></li><li><a href="#2025-09-13" class="table-of-contents__link toc-highlight">2025-09-13</a></li><li><a href="#2025-09-14" class="table-of-contents__link toc-highlight">2025-09-14</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>