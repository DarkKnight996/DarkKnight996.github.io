<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20250908-20250914" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20250908-20250914 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20250908-20250914"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20250908-20250914 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-09-08"><meta data-rh="true" property="og:description" content="2025-09-08"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20250908-20250914"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250908-20250914" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250908-20250914" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20250908-20250914","item":"https://darkknight996.github.io/daily/20250908-20250914"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.2b56a955.js" defer="defer"></script>
<script src="/assets/js/main.41473612.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20250908-20250914</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20250908-20250914</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-08">2025-09-08<a href="#2025-09-08" class="hash-link" aria-label="Direct link to 2025-09-08" title="Direct link to 2025-09-08" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Research on fault diagnosis and root cause analysis based on full stack
observability</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [root cause analysis, causal discovery, multi-modal fusion, fault diagnosis, observability data]</li>
<li class=""><strong>authors:</strong> Jian Hou</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12231v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12231v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes KylinRCA framework that integrates temporal causal discovery and cross-modal graph learning for fault diagnosis. It achieves global root cause localization and generates auditable evidence chains using mask-based explanation methods. The framework provides an effective solution for fault diagnosis under full-stack observability by combining dynamic causal analysis with multi-modal data fusion.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World
Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [speculative decoding, dynamic adaptation, KLD stability, large-batch serving]</li>
<li class=""><strong>authors:</strong> Mingyu Yang, Jae-Young Choi, Kihyo Moon, Minsung Jang, Eunjoo Jeon</li>
<li class=""><strong>institution:</strong> Samsung SDS</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01083v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01083v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DSDE, a training-free framework that uses Kullback-Leibler divergence variance as a stability signal to dynamically adjust speculation length in speculative decoding. This approach achieves competitive latency with existing baselines and demonstrates superior robustness across diverse workloads, particularly in challenging low-acceptance-rate scenarios. The results validate post-hoc signals as valuable components for building more robust LLM inference systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [MaaS, SLO-aware orchestration, heterogeneous model instances, parallelism strategies, continuous batching]</li>
<li class=""><strong>authors:</strong> Mo Xuan, Zhang yue, Wu Weigang</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.06362v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.06362v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MaaSO proposes a three-module orchestrator for Model-as-a-Service platforms, including a profiler for instance performance characterization, a placer for configuration optimization, and a distributor for SLO-aware request routing. The system improves SLO satisfaction ratio by 15-30% and reduces response latency by 40-60% compared to existing approaches while lowering orchestration overhead. It effectively manages heterogeneous LLM instances with different parallelism strategies to meet diverse service requirements.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-09">2025-09-09<a href="#2025-09-09" class="hash-link" aria-label="Direct link to 2025-09-09" title="Direct link to 2025-09-09" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Towards Scalable Proteomics: Opportunistic SMC Samplers on HTCondor</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Sequential Monte Carlo, Bayesian inference, proteomics, high-performance computing, opportunistic computing]</li>
<li class=""><strong>authors:</strong> Matthew Carter, Lee Devlin, Alexander Philips, Edward Pyzer-Knapp, Paul Spirakis, Simon Maskell</li>
<li class=""><strong>institution:</strong> University of Liverpool, Xyme</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.08020v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.08020v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an opportunistic computing framework for Sequential Monte Carlo samplers using HTCondor to leverage idle compute resources at the University of Liverpool. The proposed Coordinator-Manager-Follower architecture reduces synchronization overhead and enables scalable Bayesian inference for proteomics without dedicated HPC infrastructure. Results show the framework achieves accurate inference with weak scaling, generating more samples under fixed time constraints as resources increase.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with
FaaS-hosted MCP Services</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Agentic AI, Workflow Patterns, FaaS, MCP, Multi-step Tasks]</li>
<li class=""><strong>authors:</strong> Shiva Sai Krishna Anand Tokal, Vaibhav Jha, Anand Eswaran, Praveen Jayachandran, Yogesh Simmhan</li>
<li class=""><strong>institution:</strong> Indian Institute of Science, IBM India Research Lab</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07595v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07595v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AgentX, a novel agentic workflow pattern composed of stage designer, planner, and executor agents that outperforms state-of-the-art patterns. It leverages Model Context Protocol tools and deploys MCP servers as cloud Functions as a Service. Empirical evaluation shows competitive success rates with insights into latency and cost trade-offs for practical applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for
Efficient MoE LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Mixture of Experts, expert scheduling, memory optimization, prefetching, cache management]</li>
<li class=""><strong>authors:</strong> Yuning Zhang, Grant Pinkert, Nan Yang, Yanli Li, Dong Yuan</li>
<li class=""><strong>institution:</strong> The University of Sydney</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07379v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07379v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DuoServe-MoE proposes a dual-phase inference system that separates prefill and decode stages with specialized expert scheduling strategies. It uses CUDA pipeline overlapping for prefill and lightweight prediction for decode stage expert prefetching. The system achieves 1.42-7.54× latency improvement while maintaining only 15% peak memory usage of full model size.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] DREAMS: Decentralized Resource Allocation and Service Management across
the Compute Continuum Using Service Affinity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [decentralized resource allocation, microservice placement, compute continuum, Raft consensus, fault tolerance]</li>
<li class=""><strong>authors:</strong> Hai Dinh-Tuan, Tien Hung Nguyen, Sanjeet Raj Pandey</li>
<li class=""><strong>institution:</strong> Technische Universität Berlin</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07497v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07497v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DREAMS proposes a decentralized framework using autonomous agents with Raft-based consensus and cost-benefit voting for microservice placement across compute continuum domains. The system achieves globally optimized service placements while maintaining high fault tolerance and privacy. Evaluations show key coordination operations scale sub-linearly with domain numbers, confirming efficiency and scalability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Astra: A Multi-Agent System for GPU Kernel Performance Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU kernel optimization, multi-agent LLM system, CUDA code generation, performance optimization]</li>
<li class=""><strong>authors:</strong> Anjiang Wei, Tianran Sun, Yogesh Seenichamy, Hang Song, Anne Ouyang, Azalia Mirhoseini, Ke Wang, Alex Aiken</li>
<li class=""><strong>institution:</strong> Stanford University, Shanghai Jiao Tong University, Nanjing University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07506v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07506v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Astra introduces the first LLM-based multi-agent system for GPU kernel optimization that starts from existing CUDA implementations and uses specialized agents for iterative code generation, testing, and profiling. The system achieves an average 1.32x speedup on SGLang kernels through autonomous optimization techniques like loop transformations and memory access improvements. This work demonstrates multi-agent LLM systems as a promising paradigm for automating GPU kernel optimization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] MoE-Compression: How the Compression Error of Experts Affects the
Inference Accuracy of MoE Model?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [model compression, Mixture of Experts, error analysis, lossy compression]</li>
<li class=""><strong>authors:</strong> Songkai Ma, Zhaorui Zhang, Sheng Di, Benben Liu, Xiaodong Yu, Xiaoyi Lu, Dan Wang</li>
<li class=""><strong>institution:</strong> Hong Kong Polytechnic University, Argonne National Laboratory, The University of Hong Kong, Stevens Institute of Technology, University of California Merced</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07727v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07727v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using error-bounded lossy compression algorithms to compress non-activated experts in MoE models for efficient inference. Experiments show shallow layer experts tolerate compression errors well, middle layers are highly sensitive to errors, while deep layer errors can sometimes improve accuracy. The study provides a comprehensive error sensitivity analysis across different expert layers in MoE models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Accelerating Frontier MoE Training with 3D Integrated Optics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [3D integrated photonics, mixture of experts, scale-up networks, high-bandwidth interconnects, GPU clusters]</li>
<li class=""><strong>authors:</strong> Mikhail Bernadskiy, Peter Carson, Thomas Graham, Taylor Groves, Ho John Lee, Eric Yeh</li>
<li class=""><strong>institution:</strong> Lightmatter</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15893v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15893v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using 3D integrated photonics to overcome copper interconnect limitations in large-scale AI training. The method enables 8X scale-up capability and 2.7X faster training for trillion-parameter MoE models by creating high-bandwidth, low-latency optical connections across multiple data center racks. The photonic approach significantly reduces communication bottlenecks and unlocks unprecedented model scaling potential.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-10">2025-09-10<a href="#2025-09-10" class="hash-link" aria-label="Direct link to 2025-09-10" title="Direct link to 2025-09-10" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and
Dynamic Parallelism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [heterogeneous GPU clusters, fine-grained parallelism, dynamic parallelism, attention computation, load dispatching]</li>
<li class=""><strong>authors:</strong> Zizhao Mo, Jianxiong Liao, Huanle Xu, Zhi Zhou, Chengzhong Xu</li>
<li class=""><strong>institution:</strong> University of Macau, Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.08309v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.08309v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hetis introduces a fine-grained and dynamic parallelism approach for LLM serving in heterogeneous GPU clusters, selectively parallelizing compute-intensive operations and distributing attention computations at head granularity. It employs an online load dispatching policy to balance network latency, computational load, and memory intensity. Evaluation shows Hetis improves throughput by up to 2.25× and reduces latency by 1.49× compared to existing systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Optimizing the Variant Calling Pipeline Execution on Human Genomes Using
GPU-Enabled Machines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [variant calling, GPU optimization, machine learning, job shop scheduling, genomic data processing]</li>
<li class=""><strong>authors:</strong> Ajay Kumar, Praveen Rao, Peter Sanders</li>
<li class=""><strong>institution:</strong> The University of Missouri, Karlsruhe Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.09058v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.09058v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a machine learning-based approach to optimize variant calling pipeline execution on GPU-enabled machines. The method uses ML to predict execution times of pipeline stages and generates optimal execution plans inspired by flexible job shop scheduling. Evaluation showed 2X speedup over greedy approaches and 1.6X speedup over dynamic resource-based approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Design and Implementation of Code Completion System Based on LLM and
CodeBERT Hybrid Subsystem</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [code completion, hybrid model, CodeBERT, GPT-3.5, code generation, deep learning]</li>
<li class=""><strong>authors:</strong> Bingbing Zhang, Ziyu Lin, Yingxin Su</li>
<li class=""><strong>institution:</strong> Xiamen Institute of Technology, Google LLC, University of California Davis</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.08215v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.08215v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid model combining CodeBERT and GPT-3.5 for code completion tasks, leveraging CodeBERT&#x27;s context understanding and GPT-3.5&#x27;s generation capabilities. The hybrid approach demonstrated superior performance in accuracy, code quality, and efficiency compared to benchmarks. Robustness testing confirmed the model&#x27;s reliability and stability for practical software development applications.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-11">2025-09-11<a href="#2025-09-11" class="hash-link" aria-label="Direct link to 2025-09-11" title="Direct link to 2025-09-11" translate="no">​</a></h2>
<ul>
<li class=""><strong>[arXiv2509] TrEnv: Transparently Share Serverless Execution Environments Across
Different Functions and Nodes</strong>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [serverless computing, LLM agents, execution environment sharing, memory optimization, cold start reduction]</li>
<li class=""><strong>authors:</strong> Jialiang Huang, Teng Ma, Zheng Liu, Sixing Lin, Kang Chen, Jinlei Jiang, Xia Liao, Yingdi Shan, Yongwei Wu, Ning Zhang, Mengting Lu, Tao Ma, Haifeng Gong, Mingxing Zhang</li>
<li class=""><strong>institution:</strong> Tsinghua University, Alibaba Group, Zhejiang University, Intel</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.09525v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.09525v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TrEnv introduces a serverless platform that transparently shares execution environments across functions and nodes using repurposable sandboxes and memory templates. It reduces startup latency by up to 7x and memory usage by 48% for container-based settings, and achieves 58% lower P99 latency with 61% memory savings for VM-based agents compared to state-of-the-art systems. The platform is specifically optimized for LLM agent workloads with unpredictable invocation patterns.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-12">2025-09-12<a href="#2025-09-12" class="hash-link" aria-label="Direct link to 2025-09-12" title="Direct link to 2025-09-12" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards
Autonomous Science</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [autonomous science, scientific workflows, AI agents, swarm intelligence, workflow evolution]</li>
<li class=""><strong>authors:</strong> Woong Shin, Renan Souza, Daniel Rosendo, Frédéric Suter, Feiyi Wang, Prasanna Balaprakash, Rafael Ferreira da Silva</li>
<li class=""><strong>institution:</strong> Oak Ridge National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.09915v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.09915v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a conceptual framework where scientific workflows evolve along intelligence and composition dimensions, transitioning from static systems to intelligent swarm-based architectures. It presents an architectural blueprint for autonomous science laboratories that integrate AI agents as ecosystem components. The approach aims to achieve 100x acceleration in scientific discovery by enabling fully autonomous, distributed scientific workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] SynergAI: Edge-to-Cloud Synergy for Architecture-Driven High-Performance
Orchestration for AI Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [edge-to-cloud computing, AI inference, performance-aware scheduling, Kubernetes, QoS optimization]</li>
<li class=""><strong>authors:</strong> Foteini Stathopoulou, Aggelos Ferikoglou, Manolis Katsaragakis, Dimosthenis Masouros, Sotirios Xydis, Dimitrios Soudris</li>
<li class=""><strong>institution:</strong> National Technical University of Athens</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12252v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12252v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SynergAI introduces an architecture-aware framework for AI inference serving across heterogeneous edge-to-cloud infrastructures, combining offline and online decision-making policies for intelligent workload scheduling. The system dynamically allocates workloads across diverse hardware architectures to minimize QoS violations. Evaluation shows it achieves 2.4x fewer QoS violations compared to state-of-the-art solutions through optimized architecture-aware deployments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] MinatoLoader: Accelerating Machine Learning Training Through Efficient
Data Preprocessing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [data loader, data preprocessing, GPU utilization, batch construction, PyTorch, training acceleration]</li>
<li class=""><strong>authors:</strong> Rahma Nouaji, Stella Bitchebe, Ricardo Macedo, Oana Balmau</li>
<li class=""><strong>institution:</strong> McGill University, INESC TEC, University of Minho</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.10712v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.10712v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MinatoLoader introduces a novel data loading approach that prioritizes fast-to-preprocess samples and processes slower samples in parallel to address GPU idleness caused by preprocessing variability. The system achieves up to 7.5× training speedup and increases GPU utilization from 46.4% to 90.45% while maintaining model accuracy across various workloads. This demonstrates significant improvements over existing data loaders like PyTorch DataLoader and DALI in single-server multi-GPU setups.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Coordinated Reinforcement Learning Prefetching Architecture for
Multicore Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [reinforcement learning, hardware prefetching, multicore systems, coordinated learning, memory bandwidth optimization]</li>
<li class=""><strong>authors:</strong> Mohammed Humaid Siddiqui, Fernando Guzman, Yufei Wu, Ruishu Ann</li>
<li class=""><strong>institution:</strong> Fairleigh Dickinson University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.10719v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.10719v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes CRL-Pythia, a coordinated reinforcement learning prefetcher that enables cross-core information sharing and cooperative prefetching decisions in multicore systems. This approach significantly reduces redundant prefetch requests and improves learning convergence across cores. Experimental results show CRL-Pythia achieves approximately 12% IPC improvement for bandwidth-constrained workloads while maintaining moderate hardware overhead.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-13">2025-09-13<a href="#2025-09-13" class="hash-link" aria-label="Direct link to 2025-09-13" title="Direct link to 2025-09-13" translate="no">​</a></h2>
<ul>
<li class=""><strong>[arXiv2509] FastTrack: GPU-Accelerated Tracking for Visual SLAM</strong>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [GPU acceleration, visual SLAM, stereo feature matching, local map tracking, CUDA implementation, real-time performance]</li>
<li class=""><strong>authors:</strong> Kimia Khabiri, Parsa Hosseininejad, Shishir Gopinath, Karthik Dantu, Steven Y. Ko</li>
<li class=""><strong>institution:</strong> University at Buffalo (based on author affiliations: 1 - University at Buffalo, 2 - University of Texas at Austin)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.10757v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.10757v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents FastTrack, a GPU-accelerated tracking system for visual-inertial SLAM that uses CUDA to speed up stereo feature matching and local map tracking. The implementation within ORB-SLAM3 demonstrates up to 2.8× performance improvement on both desktop and embedded platforms. Evaluation using EuRoC and TUM-VI datasets confirms the effectiveness of GPU acceleration for real-time SLAM applications.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-14">2025-09-14<a href="#2025-09-14" class="hash-link" aria-label="Direct link to 2025-09-14" title="Direct link to 2025-09-14" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Parallel/Distributed Tabu Search for Scheduling Microprocessor Tasks in
Hybrid Flowshop</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [tabu search, hybrid flow shop, distributed computing, multiprocessor tasks, makespan minimization]</li>
<li class=""><strong>authors:</strong> Adam Janiak, Damian Kowalczyk, Maciej Lichtenstein</li>
<li class=""><strong>institution:</strong> Wrocław University of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11396v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11396v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a parallel/distributed tabu search algorithm for scheduling multiprocessor tasks in hybrid flow shop environments to minimize makespan. The proposed method effectively balances computational load across heterogeneous networks while evaluating neighborhood solutions. The algorithm demonstrates efficient performance for solving this NP-hard scheduling problem through distributed computing mechanisms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Chameleon: Taming Dynamic Operator Sequences for Memory-Intensive LLM
Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [memory optimization, swap mechanism, dynamic operator sequences, eager mode, online profiling]</li>
<li class=""><strong>authors:</strong> Zibo Wang, Yuhang Zhou, Zhibin Wang, Shipeng Li, Xinjing Huang, Chendong Cai, Bingxu Mu, Yuqing Sun, Zhiheng Hu, Bin She, Shu You, Guanghuan Fang, Rong Gu, Wanchun Dou, Guihai Chen, Chen Tian</li>
<li class=""><strong>institution:</strong> Nanjing University, Huawei Technologies Co., Ltd</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11076v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11076v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Chameleon introduces a lightweight online profiler to handle dynamic operator sequences in Eager Mode and generates effective swap policies for memory optimization during LLM training. It reduces profiling overhead by 84.25% and enables training models up to 4x larger than hardware memory. The system improves performance by up to 38.94% compared to traditional methods like recomputation or high-degree parallelism.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] GFS: A Preemption-aware Scheduling Framework for GPU Clusters with
Predictive Spot Instance Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU scheduling, preemption-aware, spot instances, resource management, predictive allocation]</li>
<li class=""><strong>authors:</strong> Jiaang Duan, Shenglin Xu, Shiyou Qian, Dingyu Yang, Kangjin Wang, Chenzhi Liao, Yinghao Yu, Qin Hua, Hanwen Hu, Qi Wang, Wenchao Wu, Dongqing Bao, Tianyu Lu, Jian Cao, Guangtao Xue, Guodong Yang, Liping Zhang, Gang Chen</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Zhejiang University, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11134v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11134v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GFS introduces a preemption-aware scheduling framework with predictive spot instance management for GPU clusters. It uses demand forecasting, dynamic quota allocation, and preemptive scheduling to prioritize high-priority tasks while minimizing impact on low-priority ones. Results show 33.0% lower eviction rates, 44.1% reduced queuing delays, and significant cost savings in production clusters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing
for Energy-Efficient LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [energy efficiency, frequency scaling, request routing, SLO-aware serving, prefill-decode disaggregation]</li>
<li class=""><strong>authors:</strong> Jiahuan Yu, Aryan Taneja, Junfeng Lin, Minjia Zhang</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04827v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04827v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VoltanaLLM introduces a control-theoretic approach combining feedback-driven frequency control and state-space routing in prefill/decode disaggregated architectures. It dynamically adjusts GPU frequencies and routes requests to minimize energy consumption while maintaining latency SLOs. The system achieves up to 36.3% energy savings with near-perfect SLO attainment across multiple LLMs and real-world datasets.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-28T04:11:23.000Z" itemprop="dateModified">Oct 28, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20250901-20250907"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20250901-20250907</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20250915-20250921"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20250915-20250921</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-09-08" class="table-of-contents__link toc-highlight">2025-09-08</a></li><li><a href="#2025-09-09" class="table-of-contents__link toc-highlight">2025-09-09</a></li><li><a href="#2025-09-10" class="table-of-contents__link toc-highlight">2025-09-10</a></li><li><a href="#2025-09-11" class="table-of-contents__link toc-highlight">2025-09-11</a></li><li><a href="#2025-09-12" class="table-of-contents__link toc-highlight">2025-09-12</a></li><li><a href="#2025-09-13" class="table-of-contents__link toc-highlight">2025-09-13</a></li><li><a href="#2025-09-14" class="table-of-contents__link toc-highlight">2025-09-14</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>