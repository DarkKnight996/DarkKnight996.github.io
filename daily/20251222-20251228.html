<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251222-20251228" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251222-20251228 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20251222-20251228"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251222-20251228 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-12-22"><meta data-rh="true" property="og:description" content="2025-12-22"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20251222-20251228"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251222-20251228" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251222-20251228" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20251222-20251228","item":"https://darkknight996.github.io/daily/20251222-20251228"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.68aa077b.js" defer="defer"></script>
<script src="/assets/js/main.0c05f739.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260105-20260111"><span title="20260105-20260111" class="linkLabel_WmDU">20260105-20260111</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260112-20260118"><span title="20260112-20260118" class="linkLabel_WmDU">20260112-20260118</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260119-20260125"><span title="20260119-20260125" class="linkLabel_WmDU">20260119-20260125</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260126-20260201"><span title="20260126-20260201" class="linkLabel_WmDU">20260126-20260201</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260202-20260208"><span title="20260202-20260208" class="linkLabel_WmDU">20260202-20260208</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260209-20260215"><span title="20260209-20260215" class="linkLabel_WmDU">20260209-20260215</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260216-20260222"><span title="20260216-20260222" class="linkLabel_WmDU">20260216-20260222</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251222-20251228</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251222-20251228</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-22">2025-12-22<a href="#2025-12-22" class="hash-link" aria-label="Direct link to 2025-12-22" title="Direct link to 2025-12-22" translate="no">​</a></h2>
<p><strong>cs.DC total: 13</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251222] Sedna: Sharding transactions in multiple concurrent proposer blockchains</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain consensus], [verifiable rateless coding, transaction sharding, multi-proposer consensus, until-decode privacy]</li>
<li class=""><strong>authors:</strong> Alejandro Ranchal-Pedrosa, Benjamin Marsh, Lefteris Kokoris-Kogias, Alberto Sonnino</li>
<li class=""><strong>institution:</strong> Sei Labs, Mysten Labs, University of Portsmouth, University College London</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17045" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17045</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Sedna is a user-facing protocol that uses verifiable, rateless coding to shard transactions across multiple concurrent proposers in a blockchain, replacing naive replication. It guarantees liveness and until-decode privacy, reducing MEV exposure and approaching the information-theoretic lower bound for bandwidth overhead, yielding a 2–3x efficiency improvement. The protocol requires no consensus modifications, enabling incremental deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [dimensionality reduction, Byzantine-robust aggregation, privacy-preserving federated learning, secure multi-party computation, adaptive tuning]</li>
<li class=""><strong>authors:</strong> Baolei Zhang, Minghong Fang, Zhuqing Liu, Biao Yi, Peizhao Zhou, Yuan Wang, Tong Li, Zheli Liu</li>
<li class=""><strong>institution:</strong> Nankai University, University of Louisville, University of North Texas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17254" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17254</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ABBR, a practical framework for federated learning that integrates privacy-preserving and Byzantine-robust defenses. It uses dimensionality reduction to speed up secure computations for model filtering and an adaptive tuning strategy to mitigate the impact of undetected malicious models. The framework demonstrates significantly faster performance with minimal overhead while maintaining strong resilience against attacks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Dion2: A Simple Method to Shrink Matrix in Muon</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [Muon optimizer, orthonormalization, matrix shrinking, sampling, Newton-Schulz iterations, FSDP]</li>
<li class=""><strong>authors:</strong> Kwangjun Ahn, Noah Amsel, John Langford</li>
<li class=""><strong>institution:</strong> Microsoft Research, AI Frontiers, NYU</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16928" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16928</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Dion2, a simple method to improve the scalability of the Muon optimizer by reducing the computational cost of its orthonormalization step. It works by sampling a fraction of rows or columns at each iteration for orthonormalization, making the update sparse. The method maintains update quality close to full Muon while significantly reducing time per step, as demonstrated in large-scale model training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Democratizing Scalable Cloud Applications: Transactional Stateful Functions on Streaming Dataflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems, cloud computing], [streaming dataflow, stateful functions, serializable transactions, fault-tolerance, serverless, Apache Flink, Stateflow, Styx]</li>
<li class=""><strong>authors:</strong> Kyriakos Psarakis</li>
<li class=""><strong>institution:</strong> Unknown (Institution not provided in the given text)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17429" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17429</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This thesis proposes using the streaming dataflow execution model to simplify building scalable cloud applications. It introduces Stateflow, a high-level programming model, and Styx, a distributed engine that provides deterministic, serializable transactions with strong fault tolerance. The work concludes that this approach democratizes development by improving programmability and performance over existing systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Scalable Distributed Vector Search via Accuracy Preserving Index Construction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [SPIRE, hierarchical vector index, partition granularity, accuracy-preserving recursive construction, approximate nearest neighbor search, distributed index]</li>
<li class=""><strong>authors:</strong> Yuming Xu, Qianxi Zhang, Qi Chen, Baotong Lu, Menghao Li, Philip Adams, Mingqin Li, Zengzhong Li, Jing Liu, Cheng Li, Fan Yang</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China, Microsoft Research, Shopify, Microsoft AI and Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17264" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17264</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SPIRE, a scalable distributed vector index system designed for Approximate Nearest Neighbor Search (ANNS). Its core method involves identifying a balanced partition granularity to avoid read-cost explosion and using an accuracy-preserving recursive construction to build a multi-level index. The main conclusion is that SPIRE achieves high scalability and up to 9.64x higher throughput than state-of-the-art systems in experiments with billions of vectors.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] The HEAL Data Platform</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [data platform], [Gen3 platform, federated system, cloud-based, FAIR principles, mesh architecture, persistent identifiers, metadata services, APIs]</li>
<li class=""><strong>authors:</strong> Brienna M. Larrick, L. Philip Schumm, Mingfei Shao, Craig Barnes, Anthony Juehne, Hara Prasad Juvvla, Michael B. Kranz, Michael Lukowski, Clint Malson, Jessica N. Mazerik, Christopher G. Meyer, Jawad Qureshi, Erin Spaniol, Andrea Tentner, Alexander VanTol, Peter Vassilatos, Sara Volk de Garcia, Robert L. Grossman</li>
<li class=""><strong>institution:</strong> University of Chicago</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17506" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17506</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents the HEAL Data Platform, a cloud-based federated system built on the open-source Gen3 platform to serve as a single point of search, discovery, and analysis for data from the NIH HEAL Initiative. It interoperates with multiple data repositories using framework services for authentication, metadata, and persistent identifiers. The platform maximizes data value by ensuring data are Findable, Accessible, Interoperable, and Reusable (FAIR), facilitating secondary analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [Logit-Aware Activation Budgeting, Phase-Multiplexed Scheduler, Head-Centric Sparse Attention, parallel decoding, memory footprint optimization]</li>
<li class=""><strong>authors:</strong> Jiakun Fan, Yanglin Zhang, Xiangchen Li, Dimitrios S. Nikolopoulos</li>
<li class=""><strong>institution:</strong> Virginia Tech</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17077" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17077</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces dLLM-Serve, a serving system that addresses the memory footprint crisis in Diffusion LLMs by co-optimizing memory, scheduling, and generation quality. It proposes techniques like Logit-Aware Activation Budgeting and a Phase-Multiplexed Scheduler to manage resource oscillation and improve efficiency. The system significantly improves throughput and reduces latency across different hardware, establishing a blueprint for scalable dLLM inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [real-time systems], [fixed-priority scheduling, EDF, DAG task models, events executor, LIFO queue]</li>
<li class=""><strong>authors:</strong> Oren Bell, Harun Teper, Mario Günzel, Chris Gill, Jian-Jia Chen</li>
<li class=""><strong>institution:</strong> Washington University in St Louis, TU Dortmund University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16926" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16926</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel scheduling approach for ROS2 applications by using an events executor to implement fixed-job-level-priority schedulers for arbitrary Directed Acyclic Graph (DAG) tasks on uniprocessor systems. The method abstracts ROS2 applications as forests of trees and maps them to traditional real-time DAG models, requiring a special LIFO-ordered events queue. The authors conclude that their implementation generates schedules equivalent to a conventional fixed-priority DAG scheduler, helping to bridge the gap between real-time systems theory and ROS2 scheduling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [C++, MPI, OpenMP, parallel programming, code generation, scalability, ChatGPT, Claude, LLaMA]</li>
<li class=""><strong>authors:</strong> Patrick Diehl, Noujoud Nader, Deepti Gupta</li>
<li class=""><strong>institution:</strong> Los Alamos National Laboratory, Louisiana State University, Texas A&amp;M University-Central Texas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17023" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17023</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically evaluates large language models (LLMs) like ChatGPT, Claude, and LLaMA on generating correct and scalable parallel C++ code using MPI and OpenMP for Mandelbrot set computation. The method involves compiling and executing the generated programs to assess correctness, robustness, and performance. The main conclusion is that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance in this HPC code generation task.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [adaptive graph pruning, Spatio-Temporal Graph Neural Networks (ST-GNNs), Federated Learning (FL), Gossip Learning, Sudden Event Prediction Accuracy (SEPA), online semi-decentralized training]</li>
<li class=""><strong>authors:</strong> Ivan Kralj, Lodovico Giaretta, Gordan Ježić, Ivana Podnar Žarko, Šarūnas Girdzijauskas</li>
<li class=""><strong>institution:</strong> University of Zagreb, RISE Research Institutes of Sweden, KTH Royal Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17352" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17352</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an adaptive graph pruning algorithm for Spatio-Temporal Graph Neural Networks (ST-GNNs) to reduce communication overhead in online semi-decentralized traffic prediction systems. It also introduces a novel event-focused metric called SEPA to better evaluate responsiveness to sudden traffic changes. The experiments demonstrate that the method significantly lowers communication costs without compromising prediction accuracy or responsiveness to critical traffic events.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Torrent: A Distributed DMA for Efficient and Flexible Point-to-Multipoint Data Movement</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [distributed dma, chainwrite, network-on-chip, point-to-multipoint, scheduling algorithms]</li>
<li class=""><strong>authors:</strong> Yunhao Deng, Fanchen Kong, Xiaoling Yi, Ryan Antonio, Marian Verhelst</li>
<li class=""><strong>institution:</strong> KU Leuven</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17589" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17589</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Torrent, a distributed DMA architecture that performs efficient point-to-multipoint data transfers by forming logical data chains (Chainwrite) over a standard Network-on-Chip without modifying its hardware or protocol. It uses scheduling algorithms to optimize chain order and demonstrates significant performance improvements, achieving up to 7.88x speedup over unicast with minimal area and power overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [GPU-internal scheduling, resource sharing, collaborative multi-GPU video decoding, logically decoupled execution, inter-stage blocking elimination]</li>
<li class=""><strong>authors:</strong> Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng</li>
<li class=""><strong>institution:</strong> Wuhan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17574" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17574</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FlashCodec and UnifiedServe, a framework that optimizes multi-stage MLLM inference by accelerating video decoding and enabling resource sharing between vision and LLM stages. This approach reduces latency and increases throughput by eliminating inter-stage blocking and improving GPU utilization. The system achieves significantly higher throughput and can serve more requests compared to existing state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251222] Asymptotic behaviour of galactic small-scale dynamos at modest magnetic Prandtl number</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [astrophysics], [Pencil Code, Astaroth, GPU acceleration, magnetohydrodynamics (MHD), supernova-driven dynamo, magnetic Prandtl number]</li>
<li class=""><strong>authors:</strong> Frederick A. Gent, Mordecai-Mark Mac Low, Maarit J. Korpi-Lagg, Touko Puro, Matthias Reinhardt</li>
<li class=""><strong>institution:</strong> Nordita, KTH Royal Institute of Technology and Stockholm University, Aalto University, Newcastle University, American Museum of Natural History</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17885" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17885</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper uses high-resolution GPU-accelerated simulations with the Pencil Code and Astaroth to model a supernova-driven galactic dynamo. The main finding is that the strength of the turbulent magnetic field from the small-scale dynamo reaches an asymptotic limit at a modest magnetic Prandtl number of only a few hundred, which is far below physical interstellar values. This asymptotic behavior allows the model&#x27;s characteristics to be incorporated into larger-scale galactic simulations.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 22</strong></p>
<ul>
<li class="">[arXiv251222] GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning <a href="https://arxiv.org/pdf/2512.17034" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency <a href="https://arxiv.org/pdf/2512.17213" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation <a href="https://arxiv.org/pdf/2512.17194" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making <a href="https://arxiv.org/pdf/2512.17091" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors <a href="https://arxiv.org/pdf/2512.17180" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2512.17444" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Value Under Ignorance in Universal Artificial Intelligence <a href="https://arxiv.org/pdf/2512.17086" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Reinforcement Learning for Self-Improving Agent with Skill Library <a href="https://arxiv.org/pdf/2512.17102" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Understanding Generalization in Role-Playing Models via Information Theory <a href="https://arxiv.org/pdf/2512.17270" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] A Theoretical Analysis of State Similarity Between Markov Decision Processes <a href="https://arxiv.org/pdf/2512.17265" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows <a href="https://arxiv.org/pdf/2512.16969" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation <a href="https://arxiv.org/pdf/2512.17308" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs <a href="https://arxiv.org/pdf/2512.17008" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering <a href="https://arxiv.org/pdf/2512.17043" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Learning Safe Autonomous Driving Policies Using Predictive Safety Representations <a href="https://arxiv.org/pdf/2512.17586" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] SCOPE: Sequential Causal Optimization of Process Interventions <a href="https://arxiv.org/pdf/2512.17629" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Trust-Region Adaptive Policy Optimization <a href="https://arxiv.org/pdf/2512.17636" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] About Time: Model-free Reinforcement Learning with Timed Reward Machines <a href="https://arxiv.org/pdf/2512.17637" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes <a href="https://arxiv.org/pdf/2512.17846" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning <a href="https://arxiv.org/pdf/2512.17853" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy <a href="https://arxiv.org/pdf/2512.17899" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] HydroGym: A Reinforcement Learning Platform for Fluid Dynamics <a href="https://arxiv.org/pdf/2512.17534" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 8</strong></p>
<ul>
<li class="">[arXiv251222] AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators <a href="https://arxiv.org/pdf/2512.17267" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection <a href="https://arxiv.org/pdf/2512.17281" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding <a href="https://arxiv.org/pdf/2512.17461" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making <a href="https://arxiv.org/pdf/2512.17091" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction <a href="https://arxiv.org/pdf/2512.17137" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics <a href="https://arxiv.org/pdf/2512.17273" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge <a href="https://arxiv.org/pdf/2512.17299" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251222] Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design <a href="https://arxiv.org/pdf/2512.17659" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-23">2025-12-23<a href="#2025-12-23" class="hash-link" aria-label="Direct link to 2025-12-23" title="Direct link to 2025-12-23" translate="no">​</a></h2>
<p><strong>cs.DC total: 21</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251223] Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [Byzantine fault tolerance, gossip protocol, consensus algorithms, cryptographic signatures, SHA-256, timestamp validation]</li>
<li class=""><strong>authors:</strong> Nihir Chadderwala</li>
<li class=""><strong>institution:</strong> Independent researcher (based on email domain)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17913" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17913</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Byzantine fault-tolerant multi-agent system for healthcare that uses a gossip protocol and cryptographic validation to securely propagate medical messages. The system achieves consensus with up to 33% faulty nodes and maintains 100% accuracy in experiments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [FPGA, mobile GPU, digital twin, model recovery, hardware acceleration, edge AI]</li>
<li class=""><strong>authors:</strong> Bin Xu, Ayan Banerjee, Midhat Urooj, Sandeep K.S. Gupta</li>
<li class=""><strong>institution:</strong> Arizona State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17941" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17941</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a digital twin learning framework accelerated on reconfigurable hardware (FPGA) and compares it to a mobile GPU implementation for edge AI. The FPGA implementation achieves superior performance-per-watt, reduced DRAM footprint, and faster runtime compared to both mobile GPU and cloud GPU baselines, demonstrating its efficiency for mission-critical healthcare applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Fast Online Digital Twinning on FPGA for Mission Critical Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [FPGA acceleration, digital twinning, model recovery, GRU, dense layers, hardware acceleration, edge computing]</li>
<li class=""><strong>authors:</strong> Bin Xu, Ayan Banerjee, Sandeep K. S. Gupta</li>
<li class=""><strong>institution:</strong> Arizona State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17942" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17942</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an FPGA-accelerated framework for fast online digital twinning, offloading key neural components like GRUs and dense layers to reconfigurable hardware for parallel execution. It demonstrates that this approach enables real-time, low-latency operation for mission-critical applications, such as collision avoidance, achieving responsiveness significantly faster than human reaction times.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [quantum reinforcement learning, parameterized quantum circuits, policy gradient, deep q-learning, quantum cloud computing, resource allocation]</li>
<li class=""><strong>authors:</strong> Irwindeep Singh, Sukhpal Singh Gill, Jinzhao Sun, Jan Mol</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Jodhpur, Queen Mary University of London</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17918" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17918</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes QAISim, a Python toolkit for simulating quantum artificial intelligence models to manage resource allocation in quantum cloud environments. It uses quantum reinforcement learning with parameterized quantum circuits to optimize resource allocation for IoT applications. The approach reduces model complexity compared to classical methods, requiring fewer trainable variables.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [CXL shared memory, KV cache, disaggregated serving, two-tier synchronization, RDMA, Dynamo framework]</li>
<li class=""><strong>authors:</strong> Dongha Yoon, Younghoon Min, Hoshik Kim, Sam H. Noh, Jongryool Kim</li>
<li class=""><strong>institution:</strong> Virginia Tech, SK Hynix America</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18194" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18194</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TraCT is a rack-scale LLM serving system that uses CXL shared memory as a substrate for transferring and caching KV tensors, eliminating the network bottleneck of traditional RDMA-based approaches. It addresses challenges like synchronization and consistency on non-coherent CXL memory through software mechanisms like a two-tier inter-node synchronization scheme. The system significantly reduces latency and improves throughput compared to existing baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Constrained Cuts, Flows, and Lattice-Linearity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [graph theory, combinatorial optimization], [lattice-linearity, distributive lattices, min-cuts, max-flow, parallel algorithms, poset slicing]</li>
<li class=""><strong>authors:</strong> Robert Streit, Vijay K. Garg</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18141" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18141</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces methods using lattice-linear predicates and parallel advancement to efficiently compute min-cuts under additional constraints, representing them via distributive lattices. It shows that while constrained min-cut problems are generally NP-hard, exact algorithms with improved complexity over exhaustive search can be achieved using techniques like poset slicing. The work also contributes new concepts like k-transition predicates and strong advancement for better parallel complexity analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [attention-based gradient importance predictor, differentiated parameter compression, knapsack-based optimization, hierarchical cloud-edge coordination, residual-based error compensation]</li>
<li class=""><strong>authors:</strong> Yi Yang, Ziyu Lin, Liesheng Wei</li>
<li class=""><strong>institution:</strong> Sichuan Agricultural University, Google LLC, College of Information Technology, ShangHai Ocean University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18127" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18127</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ACE-Sync is an adaptive cloud-edge synchronization framework that reduces communication overhead in distributed training by dynamically selecting and compressing parameter groups using an attention-based importance predictor and a knapsack-based optimization strategy. It maintains model accuracy through residual error compensation and device clustering. Experiments show it reduces communication cost by 60% while preserving competitive model accuracy, demonstrating efficiency for large-scale distributed training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [pipeline parallelism, asynchronous execution, message-queue decoupling, graph compilation, mixed-precision quantization, kernel fusion, silence-detection, Transformer]</li>
<li class=""><strong>authors:</strong> Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas</li>
<li class=""><strong>institution:</strong> Yildiz Technical University, Aktif Investment Bank Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18318" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18318</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an asynchronous pipeline-parallel Transformer framework for real-time multilingual lip synchronization, which decouples translation, speech, and lip-sync modules using message queues and optimizes them with graph compilation and quantization. The method reduces end-to-end latency by up to 3.1x compared to sequential approaches while maintaining accuracy. It is designed for resource-constrained AIoT communication systems like telemedicine and multilingual kiosks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [graph algorithms], [GPU, vertex cover, branch-and-reduce, component-aware branching, load balancing, non-tail-recursive parallelization]</li>
<li class=""><strong>authors:</strong> Hussein Amro, Basel Fakhri, Amer E. Mouawad, Izzat El Hajj</li>
<li class=""><strong>institution:</strong> American University of Beirut, University of Waterloo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18334" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18334</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a new GPU algorithm for the vertex cover problem that improves scalability by detecting when a graph splits into independent components and processing them separately to avoid redundant work. It overcomes the load balancing challenge of non-tail-recursive branching by delegating solution aggregation to the last descendant of each branch. The method significantly outperforms the prior state-of-the-art GPU solution, reducing runtime from hours to seconds on large graphs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [KV-cache reuse, Activated LoRA (aLoRA), vLLM, base-aligned block hashing, activation-aware masking]</li>
<li class=""><strong>authors:</strong> Allison Li, Kristjan Greenewald, Thomas Parnell, Navid Azizan</li>
<li class=""><strong>institution:</strong> Massachusetts Institute of Technology, IBM Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.17910" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.17910</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a method for efficient multi-adapter LLM serving by enabling cross-model KV-cache reuse between a base model and its LoRA adapters via a technique called Activated LoRA (aLoRA). The approach, integrated into the vLLM framework, uses base-aligned block hashing and activation-aware masking to avoid recomputation when switching adapters. The evaluation shows significant latency and time-to-first-token improvements, demonstrating a practical bridge between parameter-efficient adaptation and high-performance inference serving.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Snowveil: A Framework for Decentralised Preference Discovery</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [decentralised governance], [gossip-based protocol, potential function, submartingale theory, Constrained Hybrid Borda (CHB), axiomatic analysis, scalability simulation]</li>
<li class=""><strong>authors:</strong> Grammateia Kotsialou</li>
<li class=""><strong>institution:</strong> King’s College London</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18444" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18444</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Snowveil, a gossip-based protocol for decentralised preference discovery, where voters iteratively sample random peers to converge on a collective outcome. It proposes a novel aggregation rule (CHB) and uses potential functions and submartingale theory to prove convergence to a stable winner in finite time. The framework is shown to be scalable and offers a formal toolkit for decentralised governance in large systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] A Real-Time Digital Twin for Adaptive Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing scheduling], [digital twin, discrete-event simulation, adaptive scheduling, PBS scheduler, what-if evaluation]</li>
<li class=""><strong>authors:</strong> Yihe Zhang, Yash Kurkure, Yiheng Tao, Michael E. Papka, Zhiling Lan</li>
<li class=""><strong>institution:</strong> University of Illinois Chicago, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18894" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18894</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents SchedTwin, a real-time digital twin that uses a high-fidelity discrete-event simulator to perform rapid what-if evaluations of multiple scheduling policies and dynamically selects the best one for adaptive HPC scheduling. The method is integrated with the PBS scheduler, and preliminary results show it consistently outperforms static policies while maintaining low overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Multi-Player Multi-Armed Bandit (MP-MAB), Kernel Density Estimation (KDE), QoS-aware load balancing, decentralized proxy, adaptive exploration]</li>
<li class=""><strong>authors:</strong> Ivan Čilić, Ivana Podnar Žarko, Pantelis Frangoudis, Schahram Dustdar</li>
<li class=""><strong>institution:</strong> University of Zagreb, TU Wien</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18915" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18915</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes QEdgeProxy, a decentralized load balancer for the Computing Continuum that formulates the load balancing problem as a Multi-Player Multi-Armed Bandit with heterogeneous rewards, using Kernel Density Estimation to estimate QoS success probabilities. It demonstrates that this approach significantly outperforms baseline methods in per-client QoS satisfaction and effectively adapts to dynamic changes like load surges.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Timely Parameter Updating in Over-the-Air Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [over-the-air computation, federated learning, gradient selection, FAIR-k, parameter staleness, convergence analysis]</li>
<li class=""><strong>authors:</strong> Jiaqi Zhu, Zhongyuan Zhao, Xiao Li, Ruihao Du, Shi Jin, Howard H.Yang</li>
<li class=""><strong>institution:</strong> Zhejiang University, Beijing University of Posts and Telecommunications, Southeast University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19103" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19103</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes the FAIR-k algorithm for Over-the-Air Federated Learning, which selects a subset of gradients for transmission by balancing update freshness and gradient magnitude. The analysis shows that FAIR-k accelerates convergence and improves communication efficiency by mitigating the dimension-waveform disparity and the effects of data heterogeneity and channel noise.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [Similar Prompts Searching (SPS), Main Model Pre-allocation (MMP), Lagrangian duality, Longest Processing Time (LPT), heterogeneous inference, expert offloading]</li>
<li class=""><strong>authors:</strong> Wentao Liu, Yuhao Hu, Ruiting Zhou, Baochun Li, Ne Wang</li>
<li class=""><strong>institution:</strong> Southeast University, University of Toronto, The Hong Kong Polytechnic University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18674" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18674</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Remoe, a heterogeneous MoE inference system for serverless computing that assigns non-expert modules to GPUs and expert modules to CPUs, offloading infrequently activated experts to separate functions. It uses techniques like SPS for activation prediction and MMP for memory management to optimize cost and latency. The system reduces inference cost by up to 57% and cold start latency by 47% compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [decentralized federated learning, evidential deep learning, Dirichlet-based evidential models, trust-aware aggregation, peer compatibility scoring, model personalization]</li>
<li class=""><strong>authors:</strong> Murtaza Rangwala, Richard O. Sinnott, Rajkumar Buyya</li>
<li class=""><strong>institution:</strong> The University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19131" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19131</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents Murmura, a framework for decentralized federated learning that uses evidential deep learning to measure epistemic uncertainty and compute peer compatibility scores for trust-aware model aggregation. This enables nodes to personalize their models by selectively collaborating with compatible peers while excluding those with mismatched data distributions. The method significantly reduces performance degradation in non-IID settings and achieves faster convergence compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [length-aware scheduling, dynamic programming, runtime range refinement, decentralized load balancing, multi-instance serving, continuous batching]</li>
<li class=""><strong>authors:</strong> Yitao Yuan, Chenqi Zhao, Bohan Zhao, Zane Cao, Yongchao He, Wenfei Wu</li>
<li class=""><strong>institution:</strong> Peking University, ScitiX AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19179" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19179</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents L4, a runtime system for LLM serving that dynamically reschedules requests across multiple instances to reduce length heterogeneity within batches. It partitions instances into length-specialized groups forming a pipeline and uses a dynamic programming algorithm for optimal partitioning. The evaluation shows L4 significantly reduces latency and improves throughput compared to state-of-the-art multi-instance schedulers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [bounded lag synchronous, alltoallv, distributed inference, embedding tables, PyTorch Distributed]</li>
<li class=""><strong>authors:</strong> Kiril Dichev, Filip Pawlowski, Albert-Jan Yzelman</li>
<li class=""><strong>institution:</strong> Huawei Technologies Switzerland AG</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19342" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19342</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a bounded lag synchronous (BLS) version of the alltoallv collective operation to reduce synchronization overhead in distributed deep learning recommender model inference. This method allows slower processes to lag behind within a bounded number of iterations, preserving accuracy while improving performance. The authors demonstrate that BLS significantly improves latency and throughput for unbalanced inference runs, effectively masking process delays.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] Simulations between Strongly Sublinear MPC and Node-Capacitated Clique</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [MPC, Node-Capacitated Clique, simulation, round-preserving, strongly sublinear regime]</li>
<li class=""><strong>authors:</strong> Philipp Schneider, Julian Werthmann</li>
<li class=""><strong>institution:</strong> CISPA Helmholtz Center for Information Security, Paderborn University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19326" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19326</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper studies round-preserving simulations between the strongly sublinear Massively Parallel Computation (MPC) model and the Node-Capacitated Clique (NCC) model under matched total resources. It provides techniques for efficient, constant-overhead simulations between the models and also proves impossibility results that show the limitations of such simulations are inherent.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [performance modeling, hybrid parallelism, congestion-aware routing, ZeRO/FDSP sharding, Astra-Sim, DeepFlow, Chakra execution traces]</li>
<li class=""><strong>authors:</strong> George Karfakis, Faraz Tahmasebi, Binglu Chen, Lime Yao, Saptarshi Mitra, Tianyue Pan, Hyoukjun Kwon, Puneet Gupta</li>
<li class=""><strong>institution:</strong> University of California, Los Angeles</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19606" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19606</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RAPID-LLM is a performance modeling framework that combines a DeepFlow-based frontend to generate hardware-aware execution traces from an abstract LLM specification with an extended Astra-Sim backend to simulate these traces on explicit network topologies. It accurately predicts LLM training and inference performance on GPU clusters and enables exploration of hybrid parallelism, network faults, and hardware design variants.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251223] EuroHPC SPACE CoE: Redesigning Scalable Parallel Astrophysical Codes for Exascale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [astrophysics simulation], [exascale computing, parallel code redesign, high-performance data analysis, machine learning, visualization, software repositories, data sharing]</li>
<li class=""><strong>authors:</strong> Nitin Shukla, Alessandro Romeo, Caterina Caravita, Lubomir Riha, Ondrej Vysocky, Petr Strakos, Milan Jaros, João Barbosa, Radim Vavrik, Andrea Mignone, Marco Rossazza, Stefano Truzzi, Vittoria Berta, Iacopo Colonnelli, Doriana Medić, Elisabetta Boella, Daniele Gregori, Eva Sciacca, Luca Tornatore, Giuliano Taffoni, Pranab J. Deka, Fabio Bacchini, Rostislav-Paul Wilhelm, Georgios Doulis, Khalil Pierre, Luciano Rezzolla, Tine Colman, Benoît Commerçon, Othman Bouizi, Matthieu Kuhn, Erwan Raffin, Marc Sergent, Robert Wissing, Guillermo Marin, Klaus Dolag, Geray S. Karademir, Gino Perna, Marisa Zanotti, Sebastian Trujillo-Gomez</li>
<li class=""><strong>institution:</strong> CINECA, IT4Innovations, University of Turin, E4 COMPUTER ENGINEERING SpA, INAF, KU Leuven, Goethe-Universität, CRAL, CNRS, ENS Lyon, Eviden, University of Oslo, Barcelona Supercomputing Center, ENGINSOFT SpA, Ludwig-Maximilians-Universität, Heidelberg Institute for Theoretical Studies</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.18883" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.18883</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The SPACE Centre of Excellence aims to re-engineer key astrophysical simulation codes to tackle the architectural complexity of exascale systems by adopting innovative programming paradigms and software solutions. It concludes that this collaborative effort, which also addresses high-performance data analysis using machine learning, is essential for enabling and promoting the use of exascale and post-exascale computing capabilities in Astrophysics and Cosmology.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 40</strong></p>
<ul>
<li class="">[arXiv251223] Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications <a href="https://arxiv.org/pdf/2512.18135" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems <a href="https://arxiv.org/pdf/2512.18317" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India <a href="https://arxiv.org/pdf/2512.18014" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] On Swarm Leader Identification using Probing Policies <a href="https://arxiv.org/pdf/2512.18146" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis <a href="https://arxiv.org/pdf/2512.17979" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework <a href="https://arxiv.org/pdf/2512.18189" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Stable and Efficient Single-Rollout RL for Multimodal Reasoning <a href="https://arxiv.org/pdf/2512.18215" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction <a href="https://arxiv.org/pdf/2512.17943" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Monitoring Monitorability <a href="https://arxiv.org/pdf/2512.18311" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings <a href="https://arxiv.org/pdf/2512.18309" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning <a href="https://arxiv.org/pdf/2512.17912" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC) <a href="https://arxiv.org/pdf/2512.18333" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism <a href="https://arxiv.org/pdf/2512.18336" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] On the Universality of Transformer Architectures; How Much Attention Is Enough? <a href="https://arxiv.org/pdf/2512.18445" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies <a href="https://arxiv.org/pdf/2512.18540" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Toward Training Superintelligent Software Agents through Self-Play SWE-RL <a href="https://arxiv.org/pdf/2512.18552" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V <a href="https://arxiv.org/pdf/2512.18564" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning <a href="https://arxiv.org/pdf/2512.18604" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback <a href="https://arxiv.org/pdf/2512.18622" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction <a href="https://arxiv.org/pdf/2512.18623" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments <a href="https://arxiv.org/pdf/2512.18670" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models <a href="https://arxiv.org/pdf/2512.18730" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search <a href="https://arxiv.org/pdf/2512.18745" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning <a href="https://arxiv.org/pdf/2512.18763" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning <a href="https://arxiv.org/pdf/2512.18857" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection <a href="https://arxiv.org/pdf/2512.18956" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation <a href="https://arxiv.org/pdf/2512.18957" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management <a href="https://arxiv.org/pdf/2512.19001" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving <a href="https://arxiv.org/pdf/2512.19093" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] First-Order Representation Languages for Goal-Conditioned RL <a href="https://arxiv.org/pdf/2512.19355" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation <a href="https://arxiv.org/pdf/2512.19361" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Learning General Policies with Policy Gradient Methods <a href="https://arxiv.org/pdf/2512.19366" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning <a href="https://arxiv.org/pdf/2512.19516" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal <a href="https://arxiv.org/pdf/2512.19554" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller <a href="https://arxiv.org/pdf/2512.19576" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies <a href="https://arxiv.org/pdf/2512.19673" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight <a href="https://arxiv.org/pdf/2512.19691" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods <a href="https://arxiv.org/pdf/2512.17929" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics <a href="https://arxiv.org/pdf/2512.18892" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions <a href="https://arxiv.org/pdf/2512.19104" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 12</strong></p>
<ul>
<li class="">[arXiv251223] Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization <a href="https://arxiv.org/pdf/2512.17958" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout <a href="https://arxiv.org/pdf/2512.18034" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] SoK: Understanding (New) Security Issues Across AI4Code Use Cases <a href="https://arxiv.org/pdf/2512.18456" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning <a href="https://arxiv.org/pdf/2512.18604" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments <a href="https://arxiv.org/pdf/2512.18670" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Generative Modeling through Spectral Analysis of Koopman Operator <a href="https://arxiv.org/pdf/2512.18837" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Merging of Kolmogorov-Arnold networks trained on disjoint datasets <a href="https://arxiv.org/pdf/2512.18921" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models <a href="https://arxiv.org/pdf/2512.19004" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] Phase-space entropy at acquisition reflects downstream learnability <a href="https://arxiv.org/pdf/2512.19223" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] An Agentic Framework for Autonomous Materials Computation <a href="https://arxiv.org/pdf/2512.19458" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models <a href="https://arxiv.org/pdf/2512.19526" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251223] The Subject of Emergent Misalignment in Superintelligence: An Anthropological, Cognitive Neuropsychological, Machine-Learning, and Ontological Perspective <a href="https://arxiv.org/pdf/2512.17989" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-24">2025-12-24<a href="#2025-12-24" class="hash-link" aria-label="Direct link to 2025-12-24" title="Direct link to 2025-12-24" translate="no">​</a></h2>
<p><strong>cs.DC total: 15</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251224] Holoscope: Open and Lightweight Distributed Telescope &amp; Honeypot Platform</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cybersecurity observatory], [K3s, WireGuard, Infrastructure-as-Code, network telescope, honeypot]</li>
<li class=""><strong>authors:</strong> Andrea Sordello, Marco Mellia, Idilio Drago, Rodolfo Valentim, Francesco Musumeci, Massimo Tornatore, Federico Cerutti, Martino Trevisan, Alessio Botta, Willen Borges Coelho</li>
<li class=""><strong>institution:</strong> Politecnico di Torino, Universidade Federal do Rio de Janeiro</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19842" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19842</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Holoscope is a lightweight, cloud-native platform built on K3s and WireGuard that simplifies the deployment and management of distributed network telescope and honeypot sensors. It enables secure, automated, and resilient operation across multiple institutions and networks, providing unified visibility into large-scale Internet attacks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] An Adaptive Distributed Stencil Abstraction for GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [stencil computation, GPU, distributed abstraction, Charm++, resource elasticity, NumPy-like syntax]</li>
<li class=""><strong>authors:</strong> Aditya Bhosale, Laxmikant Kale</li>
<li class=""><strong>institution:</strong> University of Illinois at Urbana-Champaign</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19851" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19851</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents an adaptive distributed abstraction for stencil computations on multi-node GPUs, built using the CharmTyles framework with a NumPy-like syntax to ease porting from prototype to production. It demonstrates resource elasticity by dynamically rescaling applications across nodes and shows significant performance improvements over specialized stencil DSLs and generalized NumPy replacements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Scaling Point-based Differentiable Rendering for Large-scale Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [point-based differentiable rendering, distributed training, data locality optimization, communication reduction, unified API]</li>
<li class=""><strong>authors:</strong> Hexu Zhao, Xiaoteng Liu, Xiwen Min, Jianhao Huang, Youming Deng, Yanfei Li, Ang Li, Jinyang Li, Aurojit Panda</li>
<li class=""><strong>institution:</strong> New York University, Cornell University, Pacific Northwest National Laboratory, University of Washington</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20017" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20017</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Gaian, a general distributed training system for Point-based Differentiable Rendering (PBDR) that addresses scalability issues. It provides a unified API to support various PBDR methods and optimizes data locality to reduce communication overhead. The system significantly improves training throughput and reduces communication by up to 91% across multiple datasets and GPU scales.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [knowledge distillation, collaborative learning, federated learning, memory mechanism, heterogeneity]</li>
<li class=""><strong>authors:</strong> Pengchao Han, Xi Huang, Yi Fang, Guojun Han</li>
<li class=""><strong>institution:</strong> Guangdong University of Technology, Around Tech Company Ltd., Shenzhen Institute of Artificial Intelligence and Robotics for Society</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19972" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19972</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper provides a comprehensive review of knowledge distillation (KD) within collaborative machine learning, focusing on the roles and interactions of memory and knowledge. It analyzes how KD facilitates knowledge transfer across various collaborative patterns and heterogeneous tasks. The main conclusion is that understanding these dynamics is crucial for advancing KD techniques to address challenges like heterogeneity and privacy in distributed intelligent systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [over-the-air computation, unsourced random access, vector quantization, approximate message passing, federated edge learning, digital communication, learned decoder]</li>
<li class=""><strong>authors:</strong> Antonio Tarizzo, Mohammad Kazemi, Deniz Gündüz</li>
<li class=""><strong>institution:</strong> Imperial College London</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19777" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19777</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a learned digital over-the-air computation framework for federated edge learning, integrating unsourced random access codebooks with a vector quantizer and a trained AMP-style decoder. The method improves model update recovery accuracy and robustness in low SNR regimes while extending aggregation to symmetric functions beyond averaging. Experiments show the design extends reliable digital OTA operation by over 10 dB into low SNR conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [sparsity-aware communication, hierarchical communication, distributed SpMM, GPU-accelerated systems]</li>
<li class=""><strong>authors:</strong> Chen Zhuang, Lingqi Zhang, Benjamin Brock, Du Wu, Peng Chen, Toshio Endo, Satoshi Matsuoka, Mohamed Wahib</li>
<li class=""><strong>institution:</strong> Institute of Science Tokyo, RIKEN Center for Computational Science, Intel Corporation</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20178" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20178</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SHIRO, a distributed sparse matrix multiplication framework that uses a fine-grained, sparsity-aware communication strategy and a hierarchical strategy to reduce communication overhead in GPU clusters. It demonstrates strong scalability up to 128 GPUs, achieving significant speedups over state-of-the-art baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Reaching Agreement Among Reasoning LLM Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [multi-agent systems, consensus protocol, quorum detection, early termination, Aegean, Aegean-Serve]</li>
<li class=""><strong>authors:</strong> Chaoyi Ruan, Yiliang Wang, Ziji Shi, Jialin Li</li>
<li class=""><strong>institution:</strong> NUS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20184" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20184</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Aegean, a formal consensus protocol for multi-agent LLM reasoning systems, implemented in the Aegean-Serve serving engine. It introduces incremental quorum detection to enable early termination when agents converge, reducing latency by 1.2–20× compared to baselines while maintaining answer quality and providing safety and liveness guarantees.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [data parallelism, tensor parallelism, compression, overlapping, multi-level parallel framework]</li>
<li class=""><strong>authors:</strong> Yaojian Chen, Si-Qiu Gong, Lin Gan, Yanfei Liu, An Yang, Yinuo Wang, Chao-yang Lu, Guangwen Yang</li>
<li class=""><strong>institution:</strong> Tsinghua University, University of Science and Technology of China, National Supercomputing Center in Wuxi</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20064" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20064</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes FastMPS, a multi-level parallel framework for large-scale Matrix Product State sampling that combines data parallelism across samples with tensor parallelism along bond dimensions. It uses compression and overlapping techniques to overcome memory and I/O bottlenecks, reviving data parallelism for large-scale problems. The method achieves over 10x speedup compared to existing simulators and scales to thousands of processes, enabling simulations at unprecedented scales.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Population Protocols Revisited: Parity and Beyond</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [population protocols, parity problem, congruence modulo m, weight system, anomaly detection, switching mechanism, silent stabilisation]</li>
<li class=""><strong>authors:</strong> Leszek Gąsieniec, Tytus Grodzicki, Tomasz Jurdziński, Jakub Kowalski, Grzegorz Stachowiak</li>
<li class=""><strong>institution:</strong> University of Liverpool, University of Wrocław</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20163" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20163</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a new computing paradigm for population protocols that integrates population weights, a robust clocking mechanism, and anomaly detection with a switching mechanism. This approach yields the first efficient protocols for computing parity and congruences modulo m, using O(log³ n) states and achieving silent stabilisation in O(log³ n) time. The work demonstrates how this universal design facilitates efficient multistage stable protocols, extending beyond parity to other problems like population size computation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] UCCL-EP: Portable Expert-Parallel Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [expert parallelism, mixture-of-experts, GPUDirect RDMA, RDMA immediate data, GPU-CPU control channel, token-level communication]</li>
<li class=""><strong>authors:</strong> Ziming Mao, Yihan Zhang, Chihan Cui, Kaichao You, Zhongjie Chen, Zhiying Xu, Scott Shenker, Costin Raiciu, Yang Zhou, Ion Stoica</li>
<li class=""><strong>institution:</strong> UC Berkeley, UC Davis, UW–Madison, Tsinghua University, Amazon Web Services, Broadcom, University Politehnica of Bucharest</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.19849" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.19849</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UCCL-EP introduces a portable expert-parallel communication system that replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel and CPU proxies to issue RDMA operations. This design improves portability across heterogeneous GPU and NIC hardware while maintaining high performance. The system demonstrates significant throughput improvements over existing solutions on platforms like AWS EFA and AMD+Broadcom setups.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [population stability index, clustering, personalized federated learning, non-iid data, k-means++, silhouette analysis]</li>
<li class=""><strong>authors:</strong> Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti</li>
<li class=""><strong>institution:</strong> Sapienza University of Rome</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20363" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20363</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Clust-PSI-PFL, a personalized federated learning framework that uses the Population Stability Index (PSI) to quantify data distribution differences and cluster clients into homogeneous groups for more effective model training. The method demonstrates significant improvements in global accuracy and client fairness compared to state-of-the-art baselines under non-IID data conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] WOC: Dual-Path Weighted Object Consensus Made Efficient</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed consensus protocols], [dual-path consensus, weighted quorums, object-specific weighted quorums, node-weighted consensus, fast path, slow path]</li>
<li class=""><strong>authors:</strong> Tanisha Fonseca, Gengrui Zhang</li>
<li class=""><strong>institution:</strong> Concordia University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20485" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20485</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WOC is a dual-path consensus protocol that dynamically routes operations: independent operations use a fast path with object-specific weighted quorums for one-round completion, while conflicting operations use a leader-coordinated slow path with node-weighted consensus. It achieves up to 4x higher throughput than Cabinet for workloads with many independent objects while maintaining performance under high contention.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [reinforcement learning, proximal policy optimization, packet delivery ratio, greedy adaptive routing, gaussian interconnected networks]</li>
<li class=""><strong>authors:</strong> Mohammad Walid Charrwi, Zaid Hussain</li>
<li class=""><strong>institution:</strong> Kuwait University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20394" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20394</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a fault-aware routing scheme for Gaussian Interconnected Networks using a Proximal Policy Optimization (PPO) reinforcement learning agent. The RL agent is trained to avoid faulty regions by penalizing proximity to faults in its reward structure. The results show that the RL-based approach significantly outperforms a greedy adaptive routing algorithm, maintaining a much higher packet delivery ratio under high fault densities and low network loads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative decoding, diffusion LLMs, parallel token generation, dynamic adaptation, autoregressive verification]</li>
<li class=""><strong>authors:</strong> Rui Pan, Zhuofu Chen, Ravi Netravali</li>
<li class=""><strong>institution:</strong> Princeton University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20573" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20573</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces FailFast, a speculative decoding framework that uses a diffusion LLM as a fast, parallel drafter to generate long draft sequences for verification by an autoregressive LLM. The method dynamically adapts the speculation length to &quot;fail fast&quot; in difficult regions and &quot;win big&quot; by aggressively extending drafts in easy regions. Without fine-tuning, it achieves significant speedups over vanilla decoding and prior speculative decoding methods while maintaining lossless generation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251224] Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [Low-Rank Adaptation (LoRA), LSTM-based traffic predictor, page-based memory management, virtual memory, proactive prefetching]</li>
<li class=""><strong>authors:</strong> Yinan Ni, Xiao Yang, Yuqi Tang, Zhimin Qiu, Chen Wang, Tingzhou Yuan</li>
<li class=""><strong>institution:</strong> University of Illinois at Urbana–Champaign, Santa Clara University, New York University, University of Southern California, University of Missouri–Kansas City, Boston University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20210" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20210</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Predictive-LoRA (P-LoRA), a serverless inference system that uses an LSTM-based predictor to forecast adapter demand for proactive prefetching and a page-based memory manager to reduce GPU fragmentation for LoRA-based LLMs. It demonstrates significant improvements, reducing cold start latency by up to 68% and increasing throughput by 1.52x compared to S-LoRA.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 25</strong></p>
<ul>
<li class="">[arXiv251224] OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting <a href="https://arxiv.org/pdf/2512.19738" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches <a href="https://arxiv.org/pdf/2512.20082" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Offline Safe Policy Optimization From Heterogeneous Feedback <a href="https://arxiv.org/pdf/2512.20173" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Information-directed sampling for bandits: a primer <a href="https://arxiv.org/pdf/2512.20096" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Hard Negative Sample-Augmented DPO Post-Training for Small Language Models <a href="https://arxiv.org/pdf/2512.19728" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language <a href="https://arxiv.org/pdf/2512.20111" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering <a href="https://arxiv.org/pdf/2512.20115" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Scaling Reinforcement Learning for Content Moderation with Large Language Models <a href="https://arxiv.org/pdf/2512.20061" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] An Optimal Policy for Learning Controllable Dynamics by Exploration <a href="https://arxiv.org/pdf/2512.20053" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] FaithLens: Detecting and Explaining Faithfulness Hallucination <a href="https://arxiv.org/pdf/2512.20182" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Tiny, On-Device Decision Makers with the MiniConv Library <a href="https://arxiv.org/pdf/2512.19726" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization <a href="https://arxiv.org/pdf/2512.20135" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning <a href="https://arxiv.org/pdf/2512.19920" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference <a href="https://arxiv.org/pdf/2512.19717" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning <a href="https://arxiv.org/pdf/2512.19696" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Learning to Design City-scale Transit Routes <a href="https://arxiv.org/pdf/2512.19767" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning <a href="https://arxiv.org/pdf/2512.20220" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks <a href="https://arxiv.org/pdf/2512.20275" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning <a href="https://arxiv.org/pdf/2512.20312" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Identifying Appropriately-Sized Services with Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2512.20381" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Recurrent Off-Policy Deep Reinforcement Learning Doesn&#x27;t Have to be Slow <a href="https://arxiv.org/pdf/2512.20513" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Performative Policy Gradient: Optimality in Performative Reinforcement Learning <a href="https://arxiv.org/pdf/2512.20576" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information <a href="https://arxiv.org/pdf/2512.20589" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning <a href="https://arxiv.org/pdf/2512.20605" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] LongVideoAgent: Multi-Agent Reasoning with Long Videos <a href="https://arxiv.org/pdf/2512.20618" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 12</strong></p>
<ul>
<li class="">[arXiv251224] High-Performance Self-Supervised Learning by Joint Training of Flow Matching <a href="https://arxiv.org/pdf/2512.19729" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data <a href="https://arxiv.org/pdf/2512.19718" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] EdgeFlex-Transformer: Transformer Inference for Edge Devices <a href="https://arxiv.org/pdf/2512.19741" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research <a href="https://arxiv.org/pdf/2512.19799" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models <a href="https://arxiv.org/pdf/2512.20063" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] TongSIM: A General Platform for Simulating Intelligent Machines <a href="https://arxiv.org/pdf/2512.20206" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] DeepONet-accelerated Bayesian inversion for moving boundary problems <a href="https://arxiv.org/pdf/2512.20268" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning <a href="https://arxiv.org/pdf/2512.20329" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation <a href="https://arxiv.org/pdf/2512.20346" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale <a href="https://arxiv.org/pdf/2512.20469" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] QMBench: A Research Level Benchmark for Quantum Materials Research <a href="https://arxiv.org/pdf/2512.19753" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251224] KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis <a href="https://arxiv.org/pdf/2512.20305" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-25">2025-12-25<a href="#2025-12-25" class="hash-link" aria-label="Direct link to 2025-12-25" title="Direct link to 2025-12-25" translate="no">​</a></h2>
<p><strong>cs.DC total: 11</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251225] Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [3D parallelism, heterogeneous GPU training, auto-tuning, spot instance recovery, load balancing]</li>
<li class=""><strong>authors:</strong> Yuxiao Wang, Yuedong Xu, Qingyang Duan, Yuxuan Liu, Lei Jiao, Yinghao Yu, Jun Wu</li>
<li class=""><strong>institution:</strong> Fudan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20953" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20953</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces AutoHet, a system that automatically finds the optimal 3D parallelism plan for distributed training on heterogeneous GPUs by modeling device grouping and load balancing as an optimization problem. It also includes an efficient recovery strategy for spot instance preemption. Evaluations show AutoHet achieves significant speedups in training throughput and recovery speed compared to existing systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [multi-runtime middleware, uniform abstractions, Dragon, vLLM, AI-HPC coupling]</li>
<li class=""><strong>authors:</strong> Aymen Alsaadi, Mason Hooten, Mariya Goliyad, Andre Merzky, Andrew Shao, Mikhail Titov, Tianle Wang, Yian Chen, Maria Kalantzi, Kent Lee, Andrew Park, Indira Pimpalkhare, Nick Radcliffe, Colin Wahl, Pete Mendygral, Matteo Turilli, Shantenu Jha</li>
<li class=""><strong>institution:</strong> Rutgers University, Hewlett Packard Enterprise, Brookhaven National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20795" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20795</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents RHAPSODY, a multi-runtime middleware that coordinates existing runtimes to enable the concurrent execution of heterogeneous AI-HPC workflows within a single job allocation. The evaluation shows that RHAPSODY introduces minimal overhead, sustains heterogeneity at scale, and achieves efficient coupling between AI and HPC tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [Mesh-Attention, distributed attention, communication-computation ratio, greedy scheduling, Ring-Attention]</li>
<li class=""><strong>authors:</strong> Sirui Chen, Jingji Chen, Siqi Zhu, Ziheng Jiang, Yanghua Peng, Xuehai Qian</li>
<li class=""><strong>institution:</strong> Tsinghua University, Purdue University, University of Illinois Urbana-Champaign, ByteDance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20968" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20968</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Mesh-Attention, a new distributed attention algorithm that assigns two-dimensional computation tiles to GPUs to reduce communication overhead compared to prior methods like Ring-Attention. It achieves significant speedups and communication volume reductions, demonstrating superior scalability for large-context LLM training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] SoK: Speedy Secure Finality</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain consensus protocols], [Gasper, Speedy Secure Finality, reorg resilience, generalized sleepy model, Goldfish, RLMD-GHOST, Single Slot Finality, 3-Slot Finality]</li>
<li class=""><strong>authors:</strong> Yash Saraswat, Abhimanyu Nag</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology, Roorkee, University of Alberta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20715" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20715</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper surveys protocols for achieving fast finality in blockchain networks like Ethereum. It analyzes the evolution from foundational primitives to practical designs like 3-Slot Finality, concluding that this approach balances low latency with the engineering constraints of large validator sets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [GPU-centric parallel data structure, dynamic hypergraph representation, triad counting, hyperedge-based triads, incident-vertex-based triads, temporal triads]</li>
<li class=""><strong>authors:</strong> S. M. Shovan, Arindam Khanda, Sanjukta Bhowmick, Sajal K. Das</li>
<li class=""><strong>institution:</strong> Missouri University of Science and Technology, University of North Texas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21009" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21009</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ESCHER, a GPU-centric parallel data structure for efficiently representing and managing large, dynamic hypergraphs. It also introduces a triad-count update framework that minimizes redundant computation. Empirical results show ESCHER significantly outperforms existing methods, achieving speedups of over 100x for various triad counting types.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [online scheduling, integer programming, committed horizon control, spot instances, on-demand instances, policy selection, regret bound]</li>
<li class=""><strong>authors:</strong> Linggao Kong, Yuedong Xu, Lei Jiao, Chuan Xu</li>
<li class=""><strong>institution:</strong> Fudan University, University of Oregon, Inria</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20967" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20967</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an online scheduling framework for cost-efficient LLM fine-tuning by mixing volatile spot instances with reliable on-demand instances. It introduces a prediction-based algorithm and a complementary non-predictive algorithm, with an adaptive policy selector that learns the best approach. The framework consistently outperforms baselines, improving utility by up to 54.8% by adapting to market dynamics and prediction quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, zero-knowledge proofs, trusted execution environments, blockchain, medical AI, privacy, verifiability]</li>
<li class=""><strong>authors:</strong> Savvy Sharma, George Petrovic, Sarthak Kaushik</li>
<li class=""><strong>institution:</strong> George Brown Polytechnic</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21048" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21048</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes zkFL-Health, a framework that combines federated learning with zero-knowledge proofs and trusted execution environments to enable verifiable and privacy-preserving collaborative training for medical AI. It aims to mitigate privacy leakage from model updates and eliminate the need to trust a central aggregator by providing an immutable audit trail via blockchain. The conclusion is that this approach provides the strong confidentiality, integrity, and auditability required for clinical adoption and regulatory compliance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Stochastic well-structured transition systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems theory], [well-structured transition systems, stochastic scheduling, population protocols, chemical reaction networks, gossip models, phase clock, BPP, BPL]</li>
<li class=""><strong>authors:</strong> James Aspnes</li>
<li class=""><strong>institution:</strong> Yale University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20939" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20939</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper defines a new class of stochastic well-structured transition systems (SWSTSs) by extending classical well-structured transition systems with a probabilistic scheduling rule. It shows that in these systems, any phase clock implementation fails after polynomially many expected steps, and any terminating computation finishes in expected polynomial time. This leads to a characterization of computational power, showing that augmented systems compute exactly the languages in BPP, while unaugmented ones compute symmetric languages in BPL.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [hierarchical autoregressive modeling, KV-cache reduction, multi-resolution context access, bottom-up encoder, top-down decoder]</li>
<li class=""><strong>authors:</strong> Yuma Ichikawa, Naoya Takagi, Takumi Nakagawa, Yuzi Kanazawa, Akira Sakai</li>
<li class=""><strong>institution:</strong> Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20687" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20687</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PHOTON introduces a hierarchical autoregressive model that replaces the flat token-by-token scanning of Transformers with vertical, multi-resolution context access using a bottom-up encoder and lightweight top-down decoders. This approach significantly reduces KV-cache traffic during decoding, leading to up to 1000x higher throughput per unit memory and superior performance in long-context and multi-query tasks compared to Transformer-based models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [4D Gaussian Splatting, keyframe identification, integer linear programming, pruning level selection, temporal coherence, inflation loss, multi-channel 2D formats]</li>
<li class=""><strong>authors:</strong> Zhe Wang, Jinghang Li, Yifei Zhu</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.20943" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.20943</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces AirGS, a streaming-optimized framework for 4D Gaussian Splatting that enhances free-viewpoint video by converting Gaussian streams into 2D formats, using keyframes and temporal coherence to improve quality and reduce size. It models delivery as an integer linear programming problem with adaptive pruning to balance bandwidth and reconstruction quality. The method significantly reduces quality deviation, accelerates training, and cuts transmission size compared to state-of-the-art approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251225] Declarative distributed broadcast using three-valued modal logic and semitopologies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed algorithms], [modal logic, three-valued logic, semitopologies, axiomatic specification, declarative specification, broadcast protocol]</li>
<li class=""><strong>authors:</strong> Murdoch J. Gabbay</li>
<li class=""><strong>institution:</strong> Heriot-Watt University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21137" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21137</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method for formally specifying distributed algorithms as declarative axiomatic theories using three-valued modal logic and semitopologies. The approach abstracts away from low-level implementation details to capture the logical essence of protocols like voting, broadcast, and agreement. The authors demonstrate that this method scales well and has been used to find errors in an industrial protocol, providing a foundation for verification and design improvements.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 17</strong></p>
<ul>
<li class="">[arXiv251225] One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents <a href="https://arxiv.org/pdf/2512.20957" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design <a href="https://arxiv.org/pdf/2512.20958" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Policy-Conditioned Policies for Multi-Agent Task Solving <a href="https://arxiv.org/pdf/2512.21024" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent <a href="https://arxiv.org/pdf/2512.20745" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] NVIDIA Nemotron 3: Efficient and Open Intelligence <a href="https://arxiv.org/pdf/2512.20856" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems <a href="https://arxiv.org/pdf/2512.20688" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents <a href="https://arxiv.org/pdf/2512.20884" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Generalization of RLVR Using Causal Reasoning as a Testbed <a href="https://arxiv.org/pdf/2512.20760" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction <a href="https://arxiv.org/pdf/2512.20902" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization <a href="https://arxiv.org/pdf/2512.20623" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions <a href="https://arxiv.org/pdf/2512.20974" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment <a href="https://arxiv.org/pdf/2512.20624" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Safety Alignment of LMs via Non-cooperative Games <a href="https://arxiv.org/pdf/2512.20806" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication <a href="https://arxiv.org/pdf/2512.20739" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions <a href="https://arxiv.org/pdf/2512.20831" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Dyna-Style Reinforcement Learning Modeling and Control of Non-linear Dynamics <a href="https://arxiv.org/pdf/2512.21081" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models <a href="https://arxiv.org/pdf/2512.21231" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 8</strong></p>
<ul>
<li class="">[arXiv251225] ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design <a href="https://arxiv.org/pdf/2512.20958" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Improving Matrix Exponential for Generative AI Flows: A Taylor-Based Approach Beyond Paterson--Stockmeyer <a href="https://arxiv.org/pdf/2512.20777" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits <a href="https://arxiv.org/pdf/2512.20755" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate <a href="https://arxiv.org/pdf/2512.20941" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment <a href="https://arxiv.org/pdf/2512.20624" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention <a href="https://arxiv.org/pdf/2512.20724" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Efficient Asynchronous Federated Evaluation with Strategy Similarity Awareness for Intent-Based Networking in Industrial Internet of Things <a href="https://arxiv.org/pdf/2512.20627" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251225] Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks <a href="https://arxiv.org/pdf/2512.21241" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2026-02-19T03:30:40.000Z" itemprop="dateModified">Feb 19, 2026</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20251215-20251221"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251215-20251221</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20251229-20260104"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251229-20260104</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-22" class="table-of-contents__link toc-highlight">2025-12-22</a></li><li><a href="#2025-12-23" class="table-of-contents__link toc-highlight">2025-12-23</a></li><li><a href="#2025-12-24" class="table-of-contents__link toc-highlight">2025-12-24</a></li><li><a href="#2025-12-25" class="table-of-contents__link toc-highlight">2025-12-25</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>