<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20250922-20250928" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20250922-20250928 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20250922-20250928"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20250922-20250928 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-09-22"><meta data-rh="true" property="og:description" content="2025-09-22"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20250922-20250928"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250922-20250928" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250922-20250928" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20250922-20250928","item":"https://darkknight996.github.io/daily/20250922-20250928"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.db5bde67.js" defer="defer"></script>
<script src="/assets/js/main.809ccc97.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20250922-20250928</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20250922-20250928</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-22">2025-09-22<a href="#2025-09-22" class="hash-link" aria-label="Direct link to 2025-09-22" title="Direct link to 2025-09-22" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Enhancing Cluster Scheduling in HPC: A Continuous Transfer Learning for
Real-Time Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [cluster scheduling, transfer learning, real-time optimization, node-affinity constraints, Kubernetes]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko, Jolanta Mizera-Pietraszko</li>
<li class=""><strong>institution:</strong> University of Westminster, Opole University of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22701v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22701v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a continuous transfer learning approach for cluster scheduling optimization that dynamically adapts during operations without frequent retraining. The model focuses on node-affinity constraints and achieves over 99% accuracy on Google Cluster Data. It significantly reduces computational overhead and improves scheduling latency for constrained tasks in HPC environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Cluster Workload Allocation: A Predictive Approach Leveraging Machine
Learning Efficiency</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [machine learning, workload allocation, task constraints, ensemble classifier, google cluster data]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko</li>
<li class=""><strong>institution:</strong> University of Westminster</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17695v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17695v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This research uses machine learning classifiers to predict suitable node-task pairings for cluster workload allocation by analyzing task constraint operators from Google Cluster Data. Various ML models were fine-tuned and an ensemble voting classifier achieved 98% accuracy with low misclassification rates. The study demonstrates ML&#x27;s effectiveness in optimizing task scheduling in large-scale computing clusters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale
MoE Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Mixture-of-Experts, model serving, fault tolerance, resource scaling, peer-to-peer communication]</li>
<li class=""><strong>authors:</strong> Ziming Liu, Boyu Tian, Guoteng Wang, Zhen Jiang, Peng Sun, Zhenhua Han, Tian Tang, Xiaohe Hu, Yanmin Jia, Yan Zhang, He Liu, Mingjun Zhang, Yiqi Zhang, Qiaoling Chen, Shenggan Cheng, Mingyu Gao, Yang You, Siyuan Feng</li>
<li class=""><strong>institution:</strong> National University of Singapore, Shanghai Qiji Zhifeng Co., Ltd., Infrawaves, Nanyang Technological University, Tsinghua University, Shanghai Innovation Institute</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17863v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17863v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes EaaS, a novel serving system that disaggregates MoE modules into independent stateless services with a CPU-free peer-to-peer communication library. This architecture enables fine-grained resource scaling and inherent fault tolerance while maintaining performance comparable to monolithic systems. Experiments show EaaS achieves less than 2% throughput reduction under hardware failures and saves up to 37.5% computing resources through dynamic adaptation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [semantic caching, cross-region caching, LLM agents, tool access, performance optimization]</li>
<li class=""><strong>authors:</strong> Chaoyi Ruan, Chao Bi, Kaiwen Zheng, Ziji Shi, Xinyi Wan, Jialin Li</li>
<li class=""><strong>institution:</strong> NUS, USTC, University of Toronto, Sea AI Lab</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17360v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17360v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Asteria introduces a semantic-aware caching architecture with Semantic Elements and Semantic Retrieval Index for LLM agents, enabling efficient cross-region tool access. It uses two-stage retrieval combining vector similarity and lightweight LLM judgment for semantic validation. The system achieves up to 3.6× throughput improvement with over 85% cache hit rate while maintaining accuracy comparable to non-cached baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Odyssey: Adaptive Policy Selection for Resilient Distributed Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [fault-tolerant distributed training, adaptive policy selection, performance optimization, communication optimization]</li>
<li class=""><strong>authors:</strong> Yuhang Zhou, Zhibin Wang, Peng Jiang, Haoran Xia, Junhe Lu, Qianyu Jiang, Rong Gu, Hengxi Xu, Xinjing Huang, Guanghuan Fang, Zhiheng Hu, Jingyi Zhang, Yongjin Cai, Jian He, Chen Tian</li>
<li class=""><strong>institution:</strong> Nanjing University, Huawei</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.21613v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.21613v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Odyssey proposes an adaptive fault-tolerant system that intelligently selects optimal recovery strategies for distributed training through unified performance modeling and efficient communication optimizations. Experimental results show it maintains within 11.00% performance gap between post-recovery and failure-free training while achieving up to 1.355x higher throughput than state-of-the-art methods. The system preserves model convergence and memory efficiency while handling training interruptions effectively.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via
Partially Disaggregated Prefill</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [heterogeneous GPU clusters, workload balancing, partially disaggregated prefill, inference optimization]</li>
<li class=""><strong>authors:</strong> Yunzhao Liu, Qiang Xu, Y. Charlie Hu</li>
<li class=""><strong>institution:</strong> Purdue University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17357v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17357v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Cronus introduces a partially disaggregated prefill approach that partitions and schedules prefill stages across heterogeneous GPUs, overlapping computation between high-end and low-end devices. This method dynamically balances workloads to improve throughput and reduce latency compared to existing strategies. Evaluations show significant improvements in throughput over disaggregated prefill and better latency metrics over data and pipeline parallelism while maintaining or enhancing performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Intelligent Load Balancing in Cloud Computer Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [load balancing, cloud computing, task scheduling, resource management, virtual machine migration]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko</li>
<li class=""><strong>institution:</strong> University of Westminster</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22704v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22704v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This research developed intelligent load balancing strategies for cloud systems, including centralized metaheuristic and decentralized agent-based approaches. The work involved creating a cloud workload simulator based on Google traces and experimenting with virtual machine live migration. The proposed methods effectively maintained system stability while minimizing costs in cloud resource management.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-23">2025-09-23<a href="#2025-09-23" class="hash-link" aria-label="Direct link to 2025-09-23" title="Direct link to 2025-09-23" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Scheduler-Driven Job Atomization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU scheduling, job atomization, cluster management, resource utilization, MIG architecture]</li>
<li class=""><strong>authors:</strong> Michal Konopa, Jan Fesl, Ladislav Beránek</li>
<li class=""><strong>institution:</strong> University of South Bohemia</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.19086v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.19086v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Scheduler-Driven Job Atomization (SJA), a paradigm enabling bidirectional interaction between schedulers and jobs to create adaptable subjobs for available execution gaps. This approach avoids costly migrations and preemptions by proactively shaping workloads before execution. SJA aims to increase GPU utilization, reduce wait times, and minimize overhead by ensuring subjobs are correctly constructed for real-time opportunities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] TD3-Sched: Learning to Orchestrate Container-based Cloud-Edge Resources
via Distributed Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [distributed reinforcement learning, cloud-edge computing, resource orchestration, TD3, container scheduling]</li>
<li class=""><strong>authors:</strong> Shengye Song, Minxian Xu, Kan Hu, Wenxia Guo, Kejiang Ye</li>
<li class=""><strong>institution:</strong> Southern University of Science and Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Naval University of Engineering</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.18957v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.18957v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TD3-Sched proposes a distributed reinforcement learning scheduler using Twin Delayed Deep Deterministic Policy Gradient for continuous CPU and memory allocation in cloud-edge environments. The method achieves 17.9%-38.6% latency reduction and only 0.47% SLO violations compared to baselines. Results demonstrate superior performance in container-based cloud-edge resource orchestration under dynamic workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] In-Transit Data Transport Strategies for Coupled AI-Simulation Workflow
Patterns</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [AI-simulation workflows, data transport strategies, performance benchmarking, HPC workflows, in-situ processing]</li>
<li class=""><strong>authors:</strong> Harikrishna Tummalapalli, Riccardo Balin, Christine M. Simpson, Andrew Park, Aymen Alsaadi, Andrew E. Shao, Wesley Brewer, Shantenu Jha</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory, Rutgers University, Hewlett Packard Enterprise, Oak Ridge National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.19150v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.19150v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SimAI-Bench to benchmark data transport strategies for coupled AI-simulation workflows on HPC systems. It evaluates one-to-one and many-to-one workflow patterns on the Aurora supercomputer, finding node-local and DragonHPC strategies perform best for one-to-one workflows while file systems are optimal for many-to-one patterns as ensemble sizes grow. The study reveals data transport becomes a dominant bottleneck in many-to-one workflows, providing insights for optimizing coupled AI-simulation systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] LLM Agents for Interactive Workflow Provenance: Reference Architecture
and Evaluation Methodology</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [workflow provenance, natural language queries, retrieval-augmented generation, metadata-driven design, interactive analysis]</li>
<li class=""><strong>authors:</strong> Renan Souza, Timothy Poteet, Brian Etz, Daniel Rosendo, Amal Gueroudji, Woong Shin, Prasanna Balaprakash, Rafael Ferreira da Silva</li>
<li class=""><strong>institution:</strong> Oak Ridge National Laboratory, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.13978v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.13978v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a lightweight metadata-driven architecture that translates natural language queries into structured provenance queries using LLM agents. The approach leverages modular design, prompt tuning, and RAG techniques for runtime data analysis. Evaluations across multiple LLMs demonstrate accurate and insightful responses beyond recorded provenance data in scientific workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] On The Reproducibility Limitations of RAG Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [RAG, reproducibility, benchmarking, retrieval systems, embedding models, non-determinism]</li>
<li class=""><strong>authors:</strong> Baiqiang Wang, Dongfang Zhao, Nathan R Tallent, Luanzheng Guo</li>
<li class=""><strong>institution:</strong> University of Washington, Pacific Northwest National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.18869v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.18869v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces ReproRAG, a benchmarking framework that systematically measures reproducibility in RAG systems by analyzing embedding models, retrieval algorithms, and hardware configurations. The study reveals that embedding model choice significantly impacts reproducibility, while some ANN algorithms can achieve perfect run-to-run consistency. The framework helps practitioners make informed design decisions for more trustworthy AI systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Whack-a-Mole: Deterministic Packet Spraying Across Multiple Network
Paths</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [packet spraying, multipath transport, network protocols, distributed AI/ML, tail latency, transport imbalance]</li>
<li class=""><strong>authors:</strong> Michael Luby, John Byers</li>
<li class=""><strong>institution:</strong> BitRipple Inc, Boston University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.18519v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.18519v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Whack-a-Mole presents a deterministic packet spraying algorithm that distributes packets across multiple network paths using bit-reversal counters and discrete allocation units. The method provides provably tight discrepancy bounds of O(log m) between expected and actual packet counts per path. It enables quick congestion response while maintaining low overhead, making it effective for minimizing collective completion time and maximizing GPU utilization in distributed AI/ML workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Towards the Distributed Large-scale k-NN Graph Construction by Graph
Merge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [k-NN graph construction, graph merge, distributed computing, nearest neighbor search, indexing graph]</li>
<li class=""><strong>authors:</strong> Cheng Zhang, Wan-Lei Zhao, Shihai Xiao, Jiajie Yao, Xuecang Zhang</li>
<li class=""><strong>institution:</strong> Xiamen University, Huawei Technologies Ltd.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11697v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11697v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes distributed graph merge algorithms (Two-way Merge and Multi-way Merge) for large-scale k-NN and indexing graph construction. The methods enable efficient parallel graph building when data exceeds single-machine memory capacity. Experiments show billion-scale k-NN graphs can be constructed in 17 hours using three nodes while maintaining similar search performance to original graphs with reduced construction time.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] 6G Twin: Hybrid Gaussian Radio Fields for Channel Estimation and
Non-Linear Precoder Design for Radio Access Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [6G, channel state information, Gaussian Radio Fields, continual learning, nonlinear precoding, energy efficiency, massive MIMO]</li>
<li class=""><strong>authors:</strong> Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Muhammad Ali Jamshed, Dean F. Hougen, John M. Cioffi</li>
<li class=""><strong>institution:</strong> Stanford University, University of Oklahoma, University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.18735v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.18735v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces 6G Twin, an AI-native framework that uses Gaussian Radio Fields for compressed CSI acquisition with 100x pilot reduction and continual learning for robust channel tracking. It also proposes minPMAC, an energy-optimal nonlinear precoder achieving 4-10x lower energy consumption. The integrated system enables real-time channel estimation and efficient energy-rate tradeoffs in 6G networks.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-24">2025-09-24<a href="#2025-09-24" class="hash-link" aria-label="Direct link to 2025-09-24" title="Direct link to 2025-09-24" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Characterizing the Performance of Accelerated Jetson Edge Devices for
Training Deep Learning Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [edge computing, deep learning training, Jetson devices, performance characterization, resource utilization, energy efficiency]</li>
<li class=""><strong>authors:</strong> Prashanthi S. K., Sai Anuroop Kesanapalli, Yogesh Simmhan</li>
<li class=""><strong>institution:</strong> Indian Institute of Science</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20160v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20160v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts a systematic study of DNN training performance on NVIDIA Jetson edge devices by varying parameters like power modes, storage media, and mini-batch sizes. The analysis reveals resource interdependencies and counter-intuitive insights about training efficiency on constrained hardware. The authors develop a predictive model for training time and energy consumption across different power modes with minimal profiling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] BurstEngine: an Efficient Distributed Framework for Training
Transformers on Extremely Long Sequences of over 1M Tokens</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, long sequence training, attention optimization, memory optimization, workload balancing]</li>
<li class=""><strong>authors:</strong> Ao Sun, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong sun</li>
<li class=""><strong>institution:</strong> Tsinghua University, Beijing University of Posts and Telecommunications</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.19836v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.19836v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BurstEngine introduces BurstAttention with topology-aware ring communication and fine-grained computation-communication overlap for efficient distributed training of transformers on sequences over 1M tokens. It also employs sequence-level selective checkpointing and workload balancing optimizations. The framework achieves 1.2× speedup with lower memory overhead compared to state-of-the-art baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Adaptive Approach to Enhance Machine Learning Scheduling Algorithms
During Runtime Using Reinforcement Learning in Metascheduling Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [reinforcement learning, metascheduling, online learning, multi-schedule graph, time-triggered architectures, adaptive scheduling]</li>
<li class=""><strong>authors:</strong> Samer Alshaer, Ala Khalifeh, Roman Obermaisser</li>
<li class=""><strong>institution:</strong> University of Siegen</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20520v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20520v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an adaptive online learning unit using reinforcement learning to enhance machine learning scheduling algorithms during runtime in metascheduling applications. The RL approach continuously explores new scheduling solutions to expand the Multi-Schedule Graph and handle unexpected events. Experimental results demonstrate improved scheduling robustness and system efficiency in dynamic, safety-critical environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge
Accelerators</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [energy roofline model, time roofline model, edge accelerators, power modes, DNN workloads, Jetson Orin AGX]</li>
<li class=""><strong>authors:</strong> Prashanthi S. K., Kunal Kumar Sahoo, Amartya Ranjan Saikia, Pranav Gupta, Atharva Vinay Joshi, Priyanshu Pansari, Yogesh Simmhan</li>
<li class=""><strong>institution:</strong> Indian Institute of Science</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20189v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20189v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper develops time and energy roofline models for Nvidia Jetson edge accelerators across different power modes, combined with analytical models of DNN compute and memory access. It reveals that the default MAXN power mode is not the most energy efficient, and time efficiency implies energy efficiency for all modes. The models help optimize power modes to achieve up to 15% lower energy with minimal inference time degradation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Reconstruction-Based Adaptive Scheduling Using AI Inferences in
Safety-Critical Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [adaptive scheduling, schedule reconstruction, safety-critical systems, time-triggered systems, AI inferences]</li>
<li class=""><strong>authors:</strong> Samer Alshaer, Ala Khalifeh, Roman Obermaisser</li>
<li class=""><strong>institution:</strong> University of Siegen</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20513v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20513v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a reconstruction framework that transforms AI-generated scheduling priorities into executable schedules for safety-critical time-triggered systems. The method includes safety checks, allocation algorithms, and recovery mechanisms to handle dynamic operational conditions. Results show the framework enhances system adaptability, operational integrity, and runtime performance while maintaining computational efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge
Accelerators</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [edge computing, DNN training, DNN inference, GPU scheduling, power optimization, time-slicing]</li>
<li class=""><strong>authors:</strong> Prashanthi S. K., Saisamarth Taluri, Pranav Gupta, Amartya Ranjan Saikia, Kunal Kumar Sahoo, Atharva Vinay Joshi, Lakshya Karwa, Kedar Dhule, Yogesh Simmhan</li>
<li class=""><strong>institution:</strong> Indian Institute of Science</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20205v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20205v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Fulcrum, an intelligent scheduler for concurrent DNN training and inference on edge GPUs using two search strategies: GMD (multi-dimensional gradient descent) and ALS (Active Learning). These methods optimize power modes and batch sizes while meeting latency and power constraints with minimal profiling. Evaluation shows they achieve &gt;97% constraint satisfaction and are within 7% of optimal throughput across diverse DNN workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [energy efficiency, token throughput, test-time compute, large language models, GPU utilization, efficiency interventions]</li>
<li class=""><strong>authors:</strong> Felipe Oviedo, Fiodar Kazhamiaka, Esha Choukse, Allen Kim, Amy Luers, Melanie Nakagawa, Ricardo Bianchini, Juan M. Lavista Ferres</li>
<li class=""><strong>institution:</strong> Microsoft</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20241v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20241v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a bottom-up methodology to estimate per-query energy consumption of large-scale LLM systems based on token throughput. The study finds that frontier models consume 0.34 Wh per query under realistic conditions, and demonstrates that efficiency interventions at model, platform, and hardware levels can achieve 8-20x energy reductions. The results show that targeting efficiency in test-time scaling scenarios can deliver significant fleet-wide energy savings comparable to web search energy footprints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient
LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [parallelism transformation, KV cache optimization, dynamic scheduling, tensor parallelism, throughput optimization]</li>
<li class=""><strong>authors:</strong> Haoyu Chen, Xue Li, Kun Qian, Yu Guan, Jin Zhao, Xin Wang</li>
<li class=""><strong>institution:</strong> Fudan University, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.19729v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.19729v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Gyges proposes dynamic cross-instance parallelism transformation that adaptively adjusts parallelism strategies for running LLM instances to handle varying context lengths. It introduces page-friendly KV cache layouts, weight padding optimizations, and transformation-aware scheduling. Evaluations show 1.75x-6.57x throughput improvement over state-of-the-art solutions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale
Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [molecular dynamics, performance portability, GPU computing, exascale architectures, Kokkos library]</li>
<li class=""><strong>authors:</strong> Anders Johansson, Evan Weinberg, Christian R. Trott, Megan J. McCarthy, Stan G. Moore</li>
<li class=""><strong>institution:</strong> Sandia National Laboratories, NVIDIA Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.13523v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.13523v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper integrates the Kokkos performance portability library into LAMMPS molecular dynamics code to enable efficient execution across diverse GPU architectures. The approach demonstrates strong scaling on multiple exascale supercomputers while maintaining performance portability across different interatomic potentials including machine-learned force fields. Results show successful adaptation to modern heterogeneous computing environments with optimized performance metrics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] A Theory of Multi-Agent Generative Flow Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [multi-agent systems, generative flow networks, reinforcement learning, centralized training, decentralized execution]</li>
<li class=""><strong>authors:</strong> Leo Maxime Brunswic, Haozhi Wang, Shuang Luo, Jianye Hao, Amir Rasouli, Yinchuan Li</li>
<li class=""><strong>institution:</strong> Huawei Technologies Canada, Tianjin University, Zhejiang University, Huawei Noah&#x27;s Ark Lab</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20408v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20408v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a theoretical framework for multi-agent generative flow networks (MA-GFlowNets) with four algorithms enabling collaborative object generation. The joint flow network achieves centralized training with decentralized execution using a local-global principle. Experimental results show superiority over reinforcement learning and MCMC-based methods in generating samples proportionally to reward functions.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-25">2025-09-25<a href="#2025-09-25" class="hash-link" aria-label="Direct link to 2025-09-25" title="Direct link to 2025-09-25" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Communication Bias in Large Language Models: A Regulatory Perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [communication bias, regulatory frameworks, AI governance, fairness, transparency, societal impact]</li>
<li class=""><strong>authors:</strong> Adrian Kuenzler, Stefan Schmid</li>
<li class=""><strong>institution:</strong> University of Hong Kong, TU Berlin, Weizenbaum Institute</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21075v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21075v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes communication bias in LLMs from a regulatory perspective, examining frameworks like the EU AI Act and Digital Services Act. The authors argue that current regulations insufficiently address LLMs&#x27; influence on fundamental worldviews and democratic processes. They propose complementing value-chain regulation with competition and design governance approaches to ensure fair and trustworthy AI systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Redesigning GROMACS Halo Exchange: Improving Strong Scaling with
GPU-initiated NVSHMEM</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU communication, molecular dynamics, halo exchange, NVSHMEM, strong scaling]</li>
<li class=""><strong>authors:</strong> Mahesh Doijade, Andrey Alekseenko, Ania Brown, Alan Gray, Szilárd Páll</li>
<li class=""><strong>institution:</strong> NVIDIA, KTH Royal Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21527v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21527v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper redesigns GROMACS halo exchange using GPU-initiated NVSHMEM communication, fusing data packing and communication in highly tuned GPU kernels. The approach improves communication-computation overlap and achieves up to 2x performance improvement in strong scaling across different network configurations. The results demonstrate significant benefits of GPU-initiated communication for latency-sensitive applications like molecular dynamics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM
Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [pipeline parallelism, long-context training, distributed systems, scheduling optimization, gradient checkpointing]</li>
<li class=""><strong>authors:</strong> Shiju Wang, Yujie Wang, Ao Sun, Fangcheng Fu, Zijian Zhu, Bin Cui, Xu Han, Kaisheng Ma</li>
<li class=""><strong>institution:</strong> Tsinghua University, Peking University, Beijing University of Posts and Telecommunications, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21275v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21275v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Elastic Pipeline Parallelism (EPP) and implements InfiniPipe system to adaptively combine token-level and batch-level pipeline parallelism for efficient long-context LLM training. It introduces workload-balanced sequence processing and joint optimization of pipeline scheduling with gradient checkpointing. Experiments show InfiniPipe achieves 1.69× speedup over state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL
Post-Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, synchronous training, GPU utilization, long-tail distribution, tail batching, rollout scheduling]</li>
<li class=""><strong>authors:</strong> Wei Gao, Yuheng Zhao, Dakai An, Tianyuan Wu, Lunxi Cao, Shaopan Xiong, Ju Huang, Weixun Wang, Siran Yang, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang</li>
<li class=""><strong>institution:</strong> Hong Kong University of Science and Technology, Alibaba Group, Taobao &amp; Tmall Group of Alibaba</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21009v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21009v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RollPacker introduces tail batching, a novel scheduling strategy that separates long-tail responses into designated long rounds to reduce GPU idle time in synchronous RL training. The system employs holistic optimizations across rollout, reward, and training stages. Empirical results show 2.03x-2.56x faster training compared to existing systems while maintaining accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient
Method for Solving Large-Scale Linear Optimization Problem</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [in-memory computing, RRAM, linear optimization, primal-dual hybrid gradient, distributed computing, algorithm-hardware co-design]</li>
<li class=""><strong>authors:</strong> Huynh Q. N. Vo, Md Tawsif Rahman Chowdhury, Paritosh Ramanan, Gozde Tutuncuoglu, Junchi Yang, Feng Qiu, Murat Yildirim</li>
<li class=""><strong>institution:</strong> Oklahoma State University, Argonne National Laboratory, Wayne State University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21137v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21137v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a distributed in-memory primal-dual hybrid gradient method specifically designed for RRAM arrays, minimizing write cycles and addressing device non-idealities. The approach uses symmetric block-matrix formulation to unify operations across distributed crossbars and integrates physics-based simulation for realistic evaluation. Benchmarking shows the RRAM-based solver achieves comparable accuracy to GPU solvers while reducing energy consumption and latency by up to three orders of magnitude.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] SuperOffload: Unleashing the Power of Large-Scale LLM Training on
Superchips</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [Superchip, offloading, heterogeneous architecture, Grace CPU, Hopper GPU, NVLink-C2C, adaptive weight offloading, bucketization repartitioning, speculative execution]</li>
<li class=""><strong>authors:</strong> Xinyu Lian, Masahiro Tanaka, Olatunji Ruwase, Minjia Zhang</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, Anyscale, Snowflake</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21271v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21271v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SuperOffload introduces a novel offloading system optimized for Superchip architecture, combining adaptive weight offloading, bucketization repartitioning, and Superchip-aware casting. It achieves up to 2.5× throughput improvement over state-of-the-art systems, enabling training of 25B models on a single Superchip. The system also scales to 8 GH200 chips for training 13B models with 1 million token sequences while maintaining 55% MFU.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient
Multimodal Inference on Battery-Powered Small Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multimodal models, hardware-software co-design, edge computing, energy efficiency, on-device AI, dynamic offloading, unified-memory SoCs]</li>
<li class=""><strong>authors:</strong> Yilong Li, Shuai Zhang, Yijing Zeng, Hao Zhang, Xinmiao Xiong, Jingyu Liu, Pan Hu, Suman Banerjee</li>
<li class=""><strong>institution:</strong> University of Wisconsin–Madison, Amazon Web Services AI, Uber</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05109v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05109v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents NANOMIND, a hardware-software co-design framework that breaks multimodal models into modular components and dynamically schedules them across heterogeneous accelerators. The system achieves significant efficiency improvements, reducing energy consumption by 42.3% and GPU memory usage by 11.2%. This enables battery-powered devices to run large multimodal models entirely on-device for extended periods without network connectivity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Go With The Flow: Churn-Tolerant Decentralized Training of Large
Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [decentralized training, fault tolerance, flow optimization, volunteer computing, pipeline parallelism]</li>
<li class=""><strong>authors:</strong> Nikolay Blagoev, Bart Cox, Jérémie Decouchant, Lydia Y. Chen</li>
<li class=""><strong>institution:</strong> Université de Neuchâtel, Delft University of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21221v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21221v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GWTF introduces a decentralized flow algorithm for efficient LLM training on volunteer computing resources, addressing node churn and network instabilities. The method optimizes microbatch routing to minimize training delays and maximize throughput. Results show up to 45% training time reduction in heterogeneous, geographically distributed scenarios with high churn rates.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] RecIS: Sparse to Dense, A Unified Training Framework for Recommendation
Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [recommendation systems, sparse-dense training, PyTorch framework, system optimization, large models integration]</li>
<li class=""><strong>authors:</strong> Hua Zong, Qingtao Zeng, Zhengxiong Zhou, Zhihua Han, Zhensong Yan, Mingjie Liu, Hechen Sun, Jiawei Liu, Yiwen Hu, Qi Wang, YiHan Xian, Wenjie Guo, Houyuan Xiang, Zhiyuan Zeng, Xiangrong Sheng, Bencheng Yan, Nan Hu, Yuheng Huang, Jinqing Lian, Ziru Xu, Yan Zhang, Ju Huang, Siran Yang, Huimin Yi, Jiamang Wang, Pengjie Wang, Han Zhu, Jian Wu, Dan Ou, Jian Xu, Haihong Tang, Yuning Jiang, Bo Zheng, Lin Qu</li>
<li class=""><strong>institution:</strong> Alibaba</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20883v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20883v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RecIS proposes a unified sparse-dense training framework for recommendation models based on PyTorch, optimizing sparse components for better efficiency while leveraging existing dense optimizations. The framework supports industrial-grade recommendation models integrated with large models and is already deployed at Alibaba. It addresses the hybrid architecture challenges in modern recommendation systems by providing scalable and efficient training capabilities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [AI cluster scheduling, GPU resource management, training and inference co-scheduling, performance metrics]</li>
<li class=""><strong>authors:</strong> Lingling Zeng, Gen Zhang, Jialin Peng, Xiang Xu, Yuan Xu, Lijun Ma</li>
<li class=""><strong>institution:</strong> ZTE Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.01256v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.01256v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Kant introduces a unified scheduling system for large-scale AI clusters that employs Backfill and Enhanced Binpack strategies to optimize resource allocation. It supports both training and inference workloads, improving GPU utilization and reducing fragmentation. The system demonstrates high performance across clusters from hundreds to tens of thousands of GPUs and has been deployed in multiple data centers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Integrating and Characterizing HPC Task Runtime Systems for hybrid
AI-HPC workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [HPC, task runtime systems, AI-HPC workloads, resource management, high-throughput execution]</li>
<li class=""><strong>authors:</strong> Andre Merzky, Mikhail Titov, Matteo Turilli, Shantenu Jha</li>
<li class=""><strong>institution:</strong> RADICAL-Computing Inc., Brookhaven National Laboratory, Rutgers University, Princeton Plasma Physics Laboratory, Princeton University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20819v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20819v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper integrates RADICAL-Pilot with Flux and Dragon runtime systems to manage hybrid AI-HPC workloads, demonstrating superior performance over traditional launchers like Slurm&#x27;s srun. The combined system achieves over 1,500 tasks/s with high utilization and significantly reduces makespan for production workflows. Results show that hybrid runtime integration provides a scalable solution for dynamic, heterogeneous computational tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Guiding Application Users via Estimation of Computational Resources for
Massively Parallel Chemistry Computations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [computational chemistry, resource prediction, coupled-cluster methods, gradient boosting, active learning, supercomputing, performance optimization]</li>
<li class=""><strong>authors:</strong> Tanzila Tabassum, Omer Subasi, Ajay Panyala, Epiya Ebiapia, Gerald Baumgartner, Erdal Mutlu, P., Sadayappan, Karol Kowalski</li>
<li class=""><strong>institution:</strong> Louisiana State University, Pacific Northwest National Laboratory, University of Utah</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20667v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20667v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops machine learning models to predict computational resource requirements for massively parallel chemistry computations. Gradient Boosting models achieved low prediction errors for execution time on supercomputers, and active learning was shown to reduce data collection needs. The approach helps users optimize parameter configurations for either fastest execution or minimal resource consumption.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO
Serving and Fast Scaling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multi-SLO scheduling, dynamic scaling, P/D disaggregation, KV cache transfer, D2D weight transfer]</li>
<li class=""><strong>authors:</strong> Zahra Yousefijamarani, Xinglu Wang, Qian Wang, Morgan Lindsay Heisler, Taha Shabani, Niloofar Gholipour, Parham Yassini, Hong Chang, Kan Chen, Qiantao Zhang, Xiaolong Bai, Jiannan Wang, Ying Xiong, Yong Zhang, Zhenan Fan</li>
<li class=""><strong>institution:</strong> Huawei Technologies Canada Co., Ltd., Huawei Technologies Co., Ltd., Simon Fraser University, École de technologie supérieure</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.15919v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.15919v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HyperFlexis proposes a unified LLM serving system that jointly optimizes scheduling and scaling through multi-SLO-aware scheduling and device-to-device weight transfer mechanisms. It achieves proactive SLO compliance for diverse requests and enables rapid role transitions in disaggregated architectures. The system demonstrates up to 4.44× higher SLO attainment and 65.82% lower latency while maintaining cost parity with state-of-the-art baselines.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-26">2025-09-26<a href="#2025-09-26" class="hash-link" aria-label="Direct link to 2025-09-26" title="Direct link to 2025-09-26" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Agora: Bridging the GPU Cloud Resource-Price Disconnect</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [GPU pricing, cloud computing, memory bandwidth, feature-based pricing, resource allocation]</li>
<li class=""><strong>authors:</strong> Ian McDougall, Noah Scott, Joon Huh, Kirthevasan Kandasamy, Karthikeyan Sankaralingam</li>
<li class=""><strong>institution:</strong> University of Wisconsin-Madison</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05111v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05111v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a feature-based pricing framework that directly links GPU cloud costs to resource consumption, addressing inefficiencies in traditional time-based models. The authors introduce Agora, a practical system architecture that enables fine-grained resource measurement and pricing. Evaluation shows their approach creates a more transparent and efficient market for GPU resources, particularly benefiting bandwidth-bound workloads like LLM inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] The AI_INFN Platform: Artificial Intelligence Development in the Cloud</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Kubernetes, GPU orchestration, distributed computing, cloud-native, workflow management, INFN Cloud, Virtual Kubelet, InterLink API]</li>
<li class=""><strong>authors:</strong> Lucio Anderlini, Giulio Bianchini, Diego Ciangottini, Stefano Dal Pra, Diego Michelotto, Rosa Petrini, Daniele Spiga</li>
<li class=""><strong>institution:</strong> Istituto Nazionale di Fisica Nucleare (INFN)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22117v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22117v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a Kubernetes-based platform for managing GPU-powered AI workflows across heterogeneous distributed resources, including WLCG sites and supercomputers. It utilizes cloud-native solutions with Virtual Kubelet and InterLink API for resource offloading and orchestration. The platform enables scalable AI development while maintaining compatibility with diverse research infrastructures through functional tests and benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code
Generation Using LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multi-agent systems, code generation, HPC optimization, auto-tuning, prompt refinement]</li>
<li class=""><strong>authors:</strong> Shun-ichiro Hayashi, Koki Morita, Daichi Mukunoki, Tetsuya Hoshino, Takahiro Katagiri</li>
<li class=""><strong>institution:</strong> Nagoya University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.00031v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.00031v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VibeCodeHPC proposes a multi-agent LLM system with four specialized roles (Project Manager, System Engineer, Programmer, Continuous Delivery) that iteratively refines prompts for HPC code generation. The system demonstrated superior code quality per unit time compared to solo-agent approaches when converting CPU-based matrix multiplication to GPU CUDA code. Dynamic agent deployment and activity monitoring effectively identified requirement violations and optimization issues during the tuning process.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Efficient Fine-Grained GPU Performance Modeling for Distributed Deep
Learning of LLM</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [performance modeling, distributed training, GPU, parallelism strategies, computational primitives]</li>
<li class=""><strong>authors:</strong> Biyao Zhang, Mingkai Zheng, Debargha Ganguly, Xuecen Zhang, Vikash Singh, Vipin Chaudhary, Zhao Zhang</li>
<li class=""><strong>institution:</strong> Case Western Reserve University, Rutgers University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22832v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22832v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a fine-grained GPU performance modeling framework for distributed LLM training by decomposing models into computational primitives and using lightweight sampling. The method achieves low prediction errors (4.98% on A100, 9.38% on GH200) for models up to 20B parameters across 128 GPUs. The CPU-only framework enables rapid hardware configuration iteration without costly cluster experiments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Code once, Run Green: Automated Green Code Translation in Serverless
Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [serverless computing, energy efficiency, code translation, green computing, large language models]</li>
<li class=""><strong>authors:</strong> Sebastian Werner, Mathis Kähler, Alireza Hakamian</li>
<li class=""><strong>institution:</strong> University of Hamburg, Technische Universität Berlin</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22068v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22068v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes ReFaaS, a system that uses LLMs to automatically translate serverless functions into more energy-efficient programming languages while maintaining functional correctness. The approach achieves up to 70% energy reduction per invocation, with net savings realized after 3,000-5,000 invocations. However, the method faces challenges with function suitability and varying amortization thresholds, identifying key research directions for automated energy debt mitigation.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-27">2025-09-27<a href="#2025-09-27" class="hash-link" aria-label="Direct link to 2025-09-27" title="Direct link to 2025-09-27" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured
Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [Mixture-of-Experts, dynamic expert clustering, structured compression, hierarchical routing, parameter reduction, communication optimization]</li>
<li class=""><strong>authors:</strong> Peijun Zhu, Ning Yang, Jiayu Wei, Jinghang Wu, Haijun Zhang</li>
<li class=""><strong>institution:</strong> Guangzhou University, Institute of Automation Chinese Academy of Sciences, University of Science and Technology Beijing</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.02345v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.02345v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a unified framework combining dynamic expert clustering and structured compression to address MoE LLM challenges. The method uses online clustering with parameter-activation similarity metrics and decomposes expert weights into shared bases with low-rank residuals. Results show 80% parameter reduction, 10-20% throughput improvement, and 3x lower load variance while maintaining model quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Scaling LLM Test-Time Compute with Mobile NPU on Smartphones</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [mobile NPU, test-time scaling, quantization optimization, edge AI]</li>
<li class=""><strong>authors:</strong> Zixu Hao, Jianyu Wei, Tuowei Wang, Minxing Huang, Huiqiang Jiang, Shiqi Jiang, Ting Cao, Ju Ren</li>
<li class=""><strong>institution:</strong> Tsinghua University, University of Science and Technology of China, Microsoft Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23324v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23324v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes leveraging underutilized mobile NPU compute capacity for parallel test-time scaling of smaller LLMs, introducing hardware-aware tile quantization and LUT-based operation replacements. The implemented system achieves significant speedups (up to 19× for GEMM, 2.2× for Softmax) on Qualcomm Snapdragon platforms. Results show smaller models with test-time scaling can match or exceed larger model accuracy while establishing a new performance-cost Pareto frontier.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Towards Quantum-Ready Blockchain Fraud Detection via Ensemble Graph
Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [blockchain fraud detection, graph neural networks, ensemble learning, quantum machine learning]</li>
<li class=""><strong>authors:</strong> M. Z. Haider, Tayyaba Noreen, M. Salman</li>
<li class=""><strong>institution:</strong> Université du Québec, SZABIST University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23101v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23101v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an ensemble framework combining Graph Convolutional Networks, Graph Attention Networks and Graph Isomorphism Networks for blockchain fraud detection. The tuned soft voting ensemble achieves over 70% recall of illicit transactions with under 1% false positives on the Elliptic dataset. The architecture incorporates quantum-ready design for future integration of quantum machine learning components.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Memory Efficient and Staleness Free Pipeline Parallel DNN Training
Framework with Improved Convergence Speed</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [pipeline parallelism, memory efficiency, staleness-free training, deep neural networks, convergence speed]</li>
<li class=""><strong>authors:</strong> Ankita Dutta, Nabendu Chaki, Rajat K. De</li>
<li class=""><strong>institution:</strong> Indian Statistical Institute, University of Calcutta</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23241v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23241v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces two pipeline parallel DNN training frameworks - V-TiMePReSt provides completely staleness-free training using latest weights, while I-TiMePReSt uses mathematically formulated intermediate weights to balance staleness and convergence. Both frameworks demonstrate improved memory efficiency and convergence speed compared to existing methods, with V-TiMePReSt achieving better staleness reduction and I-TiMePReSt maintaining optimal trade-off between memory consumption and training epochs.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-28">2025-09-28<a href="#2025-09-28" class="hash-link" aria-label="Direct link to 2025-09-28" title="Direct link to 2025-09-28" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Disaggregated Prefill and Decoding Inference System for Large Language
Model Serving on Multi-Vendor GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [P-D disaggregated inference, heterogeneous GPUs, joint optimization algorithm]</li>
<li class=""><strong>authors:</strong> Xing Chen, Rong Shi, Lu Zhao, Lingbin Wang, Xiao Jin, Yueqiang Chen, Hongfeng Sun</li>
<li class=""><strong>institution:</strong> ZTE Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17542v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17542v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a P-D disaggregated inference system for LLM serving on multi-vendor GPUs, designing a heterogeneous compatible transmission module and joint optimization algorithm. Experimental results show the system effectively solves hybrid inference problems with heterogeneous GPUs and the optimization algorithm obtains optimal deployment solutions. This approach improves resource utilization while reducing costs and vendor dependency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on
Heterogeneous Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [pipeline parallelism, model partition, workload scheduling, performance optimization, heterogeneous models]</li>
<li class=""><strong>authors:</strong> Jihu Guo, Tenghui Ma, Wei Gao, Peng Sun, Jiaxing Li, Xun Chen, Yuyang Jin, Dahua Lin</li>
<li class=""><strong>institution:</strong> Fudan University, Shanghai AI Laboratory, Hong Kong University of Science and Technology, SenseTime, Tsinghua University, Chinese University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23722v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23722v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AdaPtis proposes an adaptive pipeline parallelism system that jointly optimizes model partition, model placement, and workload scheduling using a performance model. It introduces a unified pipeline executor to support diverse pipeline strategies. Experiments show AdaPtis achieves 1.42x average speedup over Megatron-LM I-1F1B across various LLM architectures and scales.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous
Retraining Alignment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM serving, edge computing, fine-tuning, iteration-level scheduling, memory management, GPU utilization, service-level objectives]</li>
<li class=""><strong>authors:</strong> Yufei Li, Yu Fu, Yue Dong, Cong Liu</li>
<li class=""><strong>institution:</strong> University of California, Riverside</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03283v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03283v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MACE proposes a hybrid LLM serving system that co-locates inference and fine-tuning with iteration-level scheduling and intelligent memory management. It balances inference latency and model accuracy by adapting retraining frequency to model drift while maintaining SLOs. Evaluation shows MACE reduces inference latency by up to 63% while sustaining high GPU utilization and matching continuous retraining performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating
Half a Trillion Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [distributed simulation, agent-based modeling, performance optimization, serialization, delta encoding]</li>
<li class=""><strong>authors:</strong> Lukas Breitwieser, Ahmad Hesam, Abdullah Giray Yağlıkçı, Mohammad Sadrosadati, Fons Rademakers, Onur Mutlu</li>
<li class=""><strong>institution:</strong> ETH Zurich, Delft University of Technology, CERN</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.24063v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.24063v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TeraAgent introduces a distributed agent-based simulation engine that overcomes scalability limitations through tailored serialization and delta encoding techniques. It enables simulations of half a trillion agents with 84x improvement over state-of-the-art systems. The solution provides better hardware flexibility and interoperability while reducing time-to-result through distributed computing.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-29T06:10:29.000Z" itemprop="dateModified">Oct 29, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20250915-20250921"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20250915-20250921</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20250929-20251005"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20250929-20251005</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-09-22" class="table-of-contents__link toc-highlight">2025-09-22</a></li><li><a href="#2025-09-23" class="table-of-contents__link toc-highlight">2025-09-23</a></li><li><a href="#2025-09-24" class="table-of-contents__link toc-highlight">2025-09-24</a></li><li><a href="#2025-09-25" class="table-of-contents__link toc-highlight">2025-09-25</a></li><li><a href="#2025-09-26" class="table-of-contents__link toc-highlight">2025-09-26</a></li><li><a href="#2025-09-27" class="table-of-contents__link toc-highlight">2025-09-27</a></li><li><a href="#2025-09-28" class="table-of-contents__link toc-highlight">2025-09-28</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>