<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20250922-20250928" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20250922-20250928 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20250922-20250928"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20250922-20250928 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-09-22"><meta data-rh="true" property="og:description" content="2025-09-22"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20250922-20250928"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250922-20250928" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250922-20250928" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20250922-20250928","item":"https://darkknight996.github.io/daily/20250922-20250928"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.78e1c784.js" defer="defer"></script>
<script src="/assets/js/main.be5f6e7c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20250922-20250928</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20250922-20250928</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-22">2025-09-22<a href="#2025-09-22" class="hash-link" aria-label="Direct link to 2025-09-22" title="Direct link to 2025-09-22" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via
Partially Disaggregated Prefill</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [heterogeneous GPU clusters, workload balancing, partially disaggregated prefill]</li>
<li class=""><strong>authors:</strong> Yunzhao Liu, Qiang Xu, Y. Charlie Hu</li>
<li class=""><strong>institution:</strong> Purdue University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17357v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17357v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Cronus introduces a partially disaggregated prefill approach that partitions and overlaps prefill and decode stages across heterogeneous GPUs to balance workloads. It significantly improves throughput over disaggregated prefill and reduces latency metrics compared to data and pipeline parallelism while maintaining or enhancing throughput in evaluations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Enhancing Cluster Scheduling in HPC: A Continuous Transfer Learning for
Real-Time Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [cluster scheduling, transfer learning, real-time optimization, node-affinity constraints, Kubernetes]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko, Jolanta Mizera-Pietraszko</li>
<li class=""><strong>institution:</strong> University of Westminster, Opole University of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22701v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22701v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a continuous transfer learning model for optimizing task scheduling in cluster systems with node-affinity constraints. The model dynamically evolves during operations, reducing retraining needs while achieving over 99% accuracy on Google Cluster Data. It significantly improves scheduling latency and computational overhead compared to traditional schedulers like Kubernetes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] High-Performance Statistical Computing (HPSC): Challenges,
Opportunities, and Future Directions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [statistical computing, high-performance computing, community integration, scalable applications, HPC technologies]</li>
<li class=""><strong>authors:</strong> Sameh Abdulah, Mary Lai O. Salvana, Ying Sun, David E. Keyes, Marc G. Genton</li>
<li class=""><strong>institution:</strong> King Abdullah University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.04013v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.04013v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes bridging the gap between statistical computing and high-performance computing communities through technical innovation and collaboration. It outlines challenges and opportunities for integrating statistical methods with modern HPC platforms to enable scalable statistical applications. The authors present a roadmap for developing a thriving high-performance statistical computing community that leverages HPC technologies for faster statistical insights.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Odyssey: Adaptive Policy Selection for Resilient Distributed Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [fault tolerance, distributed training, performance optimization, adaptive policy selection]</li>
<li class=""><strong>authors:</strong> Yuhang Zhou, Zhibin Wang, Peng Jiang, Haoran Xia, Junhe Lu, Qianyu Jiang, Rong Gu, Hengxi Xu, Xinjing Huang, Guanghuan Fang, Zhiheng Hu, Jingyi Zhang, Yongjin Cai, Jian He, Chen Tian</li>
<li class=""><strong>institution:</strong> Nanjing University, Huawei</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.21613v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.21613v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Odyssey proposes an adaptive fault-tolerant system that intelligently selects optimal recovery strategies through performance modeling and communication optimizations. It maintains within 11.00% performance gap between post-recovery and failure-free training while achieving up to 1.355x higher throughput than state-of-the-art methods. The system preserves model convergence and efficient memory usage during distributed LLM training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [semantic caching, cross-region caching, LLM agents, tool access, performance optimization]</li>
<li class=""><strong>authors:</strong> Chaoyi Ruan, Chao Bi, Kaiwen Zheng, Ziji Shi, Xinyi Wan, Jialin Li</li>
<li class=""><strong>institution:</strong> National University of Singapore (NUS)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17360v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17360v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Asteria introduces a semantic-aware cross-region caching architecture with Semantic Elements and Semantic Retrieval Index for LLM agent tool access. It uses two-stage retrieval combining vector similarity and lightweight LLM judgment. Evaluation shows 3.6× throughput improvement and 85%+ cache hit rates while maintaining accuracy comparable to non-cached baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Cluster Workload Allocation: A Predictive Approach Leveraging Machine
Learning Efficiency</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [workload allocation, task constraints, ensemble classifiers, Google Cluster Data, node affinity]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko</li>
<li class=""><strong>institution:</strong> University of Westminster</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17695v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17695v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This research uses machine learning classifiers to predict suitable node-task pairings in cluster workload allocation by analyzing task constraint operators from Google Cluster Data. Various ML models were evaluated, with an ensemble voting classifier achieving 98% accuracy in identifying tasks with single suitable nodes. The approach demonstrates ML&#x27;s effectiveness in optimizing workload allocation strategies for constrained tasks in large-scale computing clusters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale
MoE Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [Mixture-of-Experts, model serving, distributed systems, fault tolerance, resource scaling]</li>
<li class=""><strong>authors:</strong> Ziming Liu, Boyu Tian, Guoteng Wang, Zhen Jiang, Peng Sun, Zhenhua Han, Tian Tang, Xiaohe Hu, Yanmin Jia, Yan Zhang, He Liu, Mingjun Zhang, Yiqi Zhang, Qiaoling Chen, Shenggan Cheng, Mingyu Gao, Yang You, Siyuan Feng</li>
<li class=""><strong>institution:</strong> National University of Singapore, Shanghai Qiji Zhifeng Co., Ltd.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17863v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17863v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes EaaS, a serving system that disaggregates MoE modules into independent stateless services for efficient large-scale deployment. It achieves fine-grained resource scaling and inherent fault tolerance through decoupled compute units. Experiments show comparable performance to monolithic systems with 37.5% resource savings and robust fault tolerance under hardware failures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Intelligent Load Balancing in Cloud Computer Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [load balancing, cloud computing, task scheduling, resource management, virtual machine migration]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko</li>
<li class=""><strong>institution:</strong> University of Westminster</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22704v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22704v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This research developed intelligent load balancing strategies for cloud systems, including centralized metaheuristic and decentralized agent-based approaches. The study created a high-fidelity cloud workload simulator using Google&#x27;s workload traces and proposed models for resource utilization and VM migration traffic estimation. Experimental results demonstrated effective dynamic task allocation that maintains system stability while minimizing costs.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-23">2025-09-23<a href="#2025-09-23" class="hash-link" aria-label="Direct link to 2025-09-23" title="Direct link to 2025-09-23" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Accelerating Gravitational $N$-Body Simulations Using the RISC-V-Based
Tenstorrent Wormhole</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [RISC-V accelerator, gravitational N-body simulation, high-performance computing, energy efficiency, astrophysical application]</li>
<li class=""><strong>authors:</strong> Jenny Lynn Almerol, Elisabetta Boella, Mario Spera, Daniele Gregori</li>
<li class=""><strong>institution:</strong> E4 Computer Engineering, Scuola Internazionale Superiore di Studi Avanzati</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.19294v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.19294v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements gravitational N-body simulations on the RISC-V-based Tenstorrent Wormhole accelerator. The method achieves over 2× speedup and approximately 2× energy savings compared to optimized CPU implementations. Results demonstrate RISC-V accelerators&#x27; competitiveness for high-performance scientific computing beyond AI workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] In-Transit Data Transport Strategies for Coupled AI-Simulation Workflow
Patterns</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [AI-Simulation workflows, data transport strategies, performance benchmarking]</li>
<li class=""><strong>authors:</strong> Harikrishna Tummalapalli, Riccardo Balin, Christine M. Simpson, Andrew Park, Aymen Alsaadi, Andrew E. Shao, Wesley Brewer, Shantenu Jha</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.19150v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.19150v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SimAI-Bench to prototype and evaluate coupled AI-simulation workflows, benchmarking data transport strategies on the Aurora supercomputer. For one-to-one workflows, node-local and DragonHPC staging outperform Redis and Lustre, while for many-to-one workflows, file systems become optimal as ensemble sizes increase. The study highlights data transport bottlenecks and provides performance insights for different workflow patterns.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] TD3-Sched: Learning to Orchestrate Container-based Cloud-Edge Resources
via Distributed Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [distributed reinforcement learning, cloud-edge computing, resource orchestration, TD3 algorithm, container scheduling]</li>
<li class=""><strong>authors:</strong> Shengye Song, Minxian Xu, Kan Hu, Wenxia Guo, Kejiang Ye</li>
<li class=""><strong>institution:</strong> Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.18957v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.18957v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes TD3-Sched, a distributed reinforcement learning scheduler using Twin Delayed Deep Deterministic Policy Gradient for continuous CPU and memory allocation in cloud-edge environments. The method demonstrates significant latency reductions (17.9%-38.6%) and superior SLO compliance (only 0.47% violations) compared to baseline approaches, showing faster convergence and more stable performance while maintaining service quality in container-based cloud-edge systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Whack-a-Mole: Deterministic Packet Spraying Across Multiple Network
Paths</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [packet spraying, multipath transport, network load balancing, deterministic algorithm, distributed AI/ML training]</li>
<li class=""><strong>authors:</strong> Michael Luby, John Byers</li>
<li class=""><strong>institution:</strong> BitRipple, Inc. and Boston University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.18519v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.18519v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Whack-a-Mole is a deterministic packet spraying algorithm that distributes packets across multiple network paths using a bit-reversal counter and discrete path allocations. It provides provably tight discrepancy bounds of O(log m) between expected and actual packet counts per path. The algorithm responds quickly to congestion feedback and effectively minimizes collective completion time while maximizing GPU utilization for distributed AI/ML workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Supercomputing for High-speed Avoidance and Reactive Planning in Robots</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [robotic control, high-performance computing, trajectory planning, real-time systems, hybrid architecture]</li>
<li class=""><strong>authors:</strong> Kieran S. Lachmansingh, José R. González-Estrada, Ryan E. Grant, Matthew K. X. J. Pan</li>
<li class=""><strong>institution:</strong> Queen&#x27;s University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.19486v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.19486v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents SHARP, a system that offloads robotic trajectory planning to high-performance computing clusters using parallelized multi-goal A* search with MPI. The approach achieves millisecond-scale planning latencies (22.9-30.0 ms) for a 7-DOF manipulator dodging high-speed projectiles. Results demonstrate that HPC offloading enables reactive performance below human reaction times when network latency remains in tens of milliseconds, supporting hybrid control architectures for dynamic robotics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Towards the Distributed Large-scale k-NN Graph Construction by Graph
Merge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [k-NN graph construction, graph merge, distributed computing, indexing graph, parallel algorithms]</li>
<li class=""><strong>authors:</strong> Cheng Zhang, Wan-Lei Zhao, Shihai Xiao, Jiajie Yao, Xuecang Zhang</li>
<li class=""><strong>institution:</strong> Xiamen University, Huawei Technologies Ltd.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11697v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11697v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes efficient graph merge algorithms (Two-way Merge and Multi-way Merge) for constructing large-scale k-NN graphs and indexing graphs in distributed environments. The methods enable building billion-scale graphs in parallel with significantly reduced construction time. Experimental results show the merged graphs achieve similar nearest neighbor search performance as original graphs while being much faster to construct.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] LLM Agents for Interactive Workflow Provenance: Reference Architecture
and Evaluation Methodology</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [workflow provenance, natural language queries, metadata-driven design, Retrieval-Augmented Generation, interactive analysis]</li>
<li class=""><strong>authors:</strong> Renan Souza, Timothy Poteet, Brian Etz, Daniel Rosendo, Amal Gueroudji, Woong Shin, Prasanna Balaprakash, Rafael Ferreira da Silva</li>
<li class=""><strong>institution:</strong> Oak Ridge National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.13978v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.13978v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a reference architecture using interactive LLM agents for workflow provenance analysis through natural language queries. The approach employs a lightweight metadata-driven design with Retrieval-Augmented Generation to translate natural language into structured queries. Evaluations across multiple LLMs demonstrate that modular design, prompt tuning, and RAG enable accurate responses beyond recorded provenance data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Scheduler-Driven Job Atomization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU scheduling, job atomization, resource utilization, Multi-Instance GPU, cluster management]</li>
<li class=""><strong>authors:</strong> Michal Konopa, Jan Fesl, Ladislav Beránek</li>
<li class=""><strong>institution:</strong> University of South Bohemia</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.19086v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.19086v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Scheduler-Driven Job Atomization (SJA), a new paradigm that enables bidirectional interaction between schedulers and jobs to dynamically create subjobs fitting available execution gaps. Unlike traditional approaches, SJA proactively shapes workloads before execution to avoid costly state transfers and interruptions. The method aims to increase GPU utilization, reduce wait times, and minimize migration overhead by ensuring each subjob is correct by construction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] 6G Twin: Hybrid Gaussian Radio Fields for Channel Estimation and
Non-Linear Precoder Design for Radio Access Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [6G, channel estimation, Gaussian Radio Fields, precoder design, continual learning, radio access networks]</li>
<li class=""><strong>authors:</strong> Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Muhammad Ali Jamshed, Dean F. Hougen, John M. Cioffi</li>
<li class=""><strong>institution:</strong> Stanford University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.18735v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.18735v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes 6G Twin, an AI-native radio access network framework combining neural Gaussian Radio Fields for compressed channel acquisition, continual learning for mobility handling, and an energy-optimal nonlinear precoder. The method achieves 100x pilot reduction with 1.1ms inference time and maintains robust performance during handovers while significantly reducing transmit energy. The integrated system demonstrates superior throughput-energy tradeoffs in 3GPP-style environments with real-time operational capability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] On The Reproducibility Limitations of RAG Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [retrieval-augmented generation, reproducibility, benchmarking, embedding models, vector retrieval, uncertainty quantification]</li>
<li class=""><strong>authors:</strong> Baiqiang Wang, Dongfang Zhao, Nathan R Tallent, Luanzheng Guo</li>
<li class=""><strong>institution:</strong> University of Washington, Pacific Northwest National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.18869v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.18869v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces ReproRAG, a comprehensive benchmarking framework that systematically measures reproducibility in RAG systems by analyzing embedding models, retrieval algorithms, and hardware configurations. The study reveals that different embedding models significantly impact RAG reproducibility. The framework provides tools for validating deployments and making informed design decisions to enhance trustworthy AI for science.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-24">2025-09-24<a href="#2025-09-24" class="hash-link" aria-label="Direct link to 2025-09-24" title="Direct link to 2025-09-24" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale
Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [molecular dynamics, performance portability, exascale computing, GPU optimization, Kokkos library]</li>
<li class=""><strong>authors:</strong> Anders Johansson, Evan Weinberg, Christian R. Trott, Megan J. McCarthy, Stan G. Moore</li>
<li class=""><strong>institution:</strong> Sandia National Laboratories</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.13523v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.13523v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper integrates the Kokkos performance portability library into LAMMPS molecular dynamics code to achieve cross-platform compatibility across exascale architectures. It demonstrates strong scaling performance on multiple supercomputers using various interatomic potentials. The approach enables efficient execution on diverse GPU hardware while maintaining computational performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] FZModules: A Heterogeneous Computing Framework for Customizable
Scientific Data Compression Pipelines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [data compression, lossy compression, heterogeneous computing, GPU acceleration, scientific data]</li>
<li class=""><strong>authors:</strong> Skyler Ruiter, Jiannan Tian, Fengguang Song</li>
<li class=""><strong>institution:</strong> Indiana University, Oakland University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20563v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20563v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FZModules introduces a heterogeneous computing framework that enables customizable scientific data compression pipelines using high-performance modules and asynchronous task execution. The framework allows assembling error-bounded compression pipelines that achieve comparable speed to fused-kernel GPU compressors while maintaining similar rate-distortion performance to higher-fidelity CPU/hybrid compressors. This enables rapid development of domain-tailored compression solutions for large-scale scientific data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient
LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [parallelism transformation, KV cache optimization, scheduling optimization]</li>
<li class=""><strong>authors:</strong> Haoyu Chen, Xue Li, Kun Qian, Yu Guan, Jin Zhao, Xin Wang</li>
<li class=""><strong>institution:</strong> Fudan University and Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.19729v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.19729v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Gyges introduces dynamic cross-instance parallelism transformation that adaptively adjusts parallelism strategies to match incoming request dynamics. It employs page-friendly KV cache layouts, dedicated weight padding, and transformation-aware scheduling to optimize performance. Evaluations show throughput improvements of 1.75x-6.57x compared to state-of-the-art solutions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] BurstEngine: an Efficient Distributed Framework for Training
Transformers on Extremely Long Sequences of over 1M Tokens</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, long sequence, attention optimization, memory optimization, workload balance]</li>
<li class=""><strong>authors:</strong> Ao Sun, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong sun</li>
<li class=""><strong>institution:</strong> Tsinghua University, Beijing University of Posts and Telecommunications</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.19836v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.19836v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BurstEngine introduces BurstAttention with topology-aware ring communication and fine-grained computation-communication overlap for efficient distributed training of transformers on sequences over 1M tokens. It also employs sequence-level selective checkpointing and workload balancing optimizations to reduce memory overhead. The framework achieves 1.2× speedup with lower memory usage compared to state-of-the-art baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [energy efficiency, token throughput, test-time compute, GPU utilization, model serving]</li>
<li class=""><strong>authors:</strong> Felipe Oviedo, Fiodar Kazhamiaka, Esha Choukse, Allen Kim, Amy Luers, Melanie Nakagawa, Ricardo Bianchini, Juan M. Lavista Ferres</li>
<li class=""><strong>institution:</strong> Microsoft</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20241v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20241v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a bottom-up methodology to estimate AI inference energy consumption based on token throughput. It finds current energy estimates often overstate usage by 4-20x, with median energy of 0.34 Wh per query for large models. The study identifies efficiency interventions that could reduce energy consumption by 8-20x through combined model, platform, and hardware improvements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Adaptive Approach to Enhance Machine Learning Scheduling Algorithms
During Runtime Using Reinforcement Learning in Metascheduling Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [reinforcement learning, metascheduling, online learning, multi-schedule graph, adaptive scheduling, real-time optimization]</li>
<li class=""><strong>authors:</strong> Samer Alshaer, Ala Khalifeh, Roman Obermaisser</li>
<li class=""><strong>institution:</strong> University of Siegen</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20520v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20520v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an adaptive online learning unit using reinforcement learning to enhance machine learning scheduling algorithms during runtime in metascheduling applications. The RL models continuously explore new scheduling solutions and expand the Multi-Schedule Graph to handle unexpected events and complex scenarios. This approach improves system performance and flexibility in dynamic environments while optimizing existing schedulers for stricter deadlines and new criteria.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge
Accelerators</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [edge computing, DNN training, DNN inference, power optimization, time-slicing]</li>
<li class=""><strong>authors:</strong> Prashanthi S. K., Saisamarth Taluri, Pranav Gupta, Amartya Ranjan Saikia, Kunal Kumar Sahoo, Atharva Vinay Joshi, Lakshya Karwa, Kedar Dhule, Yogesh Simmhan</li>
<li class=""><strong>institution:</strong> Indian Institute of Science</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20205v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20205v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Fulcrum scheduler with GMD and ALS techniques for optimizing concurrent DNN training and inference on edge GPUs. These methods intelligently time-slice workloads and select power modes while meeting latency/power constraints. The solutions achieve &gt;97% constraint satisfaction and are within 7% of optimal throughput with minimal profiling overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Reconstruction-Based Adaptive Scheduling Using AI Inferences in
Safety-Critical Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [adaptive scheduling, safety-critical systems, time-triggered systems, schedule reconstruction, AI-generated priorities]</li>
<li class=""><strong>authors:</strong> Samer Alshaer, Ala Khalifeh, Roman Obermaisser</li>
<li class=""><strong>institution:</strong> University of Siegen</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20513v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20513v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a reconstruction framework that transforms AI-generated scheduling priorities into executable schedules while ensuring safety constraints like precedence rules and collision-free communication. The method incorporates safety checks, allocation algorithms, and recovery mechanisms for handling unexpected events. Results show the framework enhances system adaptability, operational integrity, and runtime performance while maintaining computational efficiency in safety-critical time-triggered systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] A Theory of Multi-Agent Generative Flow Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [multi-agent systems, generative flow networks, reinforcement learning, probabilistic modeling, decentralized execution]</li>
<li class=""><strong>authors:</strong> Leo Maxime Brunswic, Haozhi Wang, Shuang Luo, Jianye Hao, Amir Rasouli, Yinchuan Li</li>
<li class=""><strong>institution:</strong> Huawei Technologies</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20408v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20408v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a theoretical framework for multi-agent generative flow networks (MA-GFlowNets) that enables multiple agents to collaboratively generate objects through joint actions. It introduces four algorithms including centralized, independent, joint, and conditional flow networks with theoretical guarantees. Experimental results show the proposed framework outperforms reinforcement learning and MCMC-based methods in generating samples proportional to reward functions.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-25">2025-09-25<a href="#2025-09-25" class="hash-link" aria-label="Direct link to 2025-09-25" title="Direct link to 2025-09-25" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Guiding Application Users via Estimation of Computational Resources for
Massively Parallel Chemistry Computations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [machine learning, computational chemistry, resource prediction, coupled-cluster methods, supercomputing, active learning, execution time optimization]</li>
<li class=""><strong>authors:</strong> Tanzila Tabassum, Omer Subasi, Ajay Panyala, Epiya Ebiapia, Gerald Baumgartner, Erdal Mutlu, P., Sadayappan, Karol Kowalski</li>
<li class=""><strong>institution:</strong> Pacific Northwest National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20667v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20667v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops machine learning models to predict computational resources for chemistry simulations on supercomputers. It uses gradient boosting and active learning to optimize runtime parameters like node count and tile sizes. The approach achieves low prediction errors and helps users minimize execution time and resource costs for coupled-cluster computations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL
Post-Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, LLM post-training, GPU utilization, synchronous RL, tail batching, rollout scheduling]</li>
<li class=""><strong>authors:</strong> Wei Gao, Yuheng Zhao, Dakai An, Tianyuan Wu, Lunxi Cao, Shaopan Xiong, Ju Huang, Weixun Wang, Siran Yang, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang</li>
<li class=""><strong>institution:</strong> Hong Kong University of Science and Technology, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21009v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21009v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RollPacker introduces tail batching, a novel rollout scheduling strategy that groups long-tail responses into designated rounds to reduce GPU idle time in synchronous RL post-training. The system implements holistic optimizations across rollout, reward, and training stages. Empirical results show 2.03x-2.56x faster training compared to existing systems while maintaining accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Communication Bias in Large Language Models: A Regulatory Perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [AI regulation, bias mitigation, fairness assessment, EU AI Act, Digital Services Act, transparency requirements]</li>
<li class=""><strong>authors:</strong> Adrian Kuenzler, Stefan Schmid</li>
<li class=""><strong>institution:</strong> University of Hong Kong, TU Berlin, Weizenbaum Institute</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21075v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21075v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes communication bias in LLMs from a regulatory perspective, examining frameworks like the EU AI Act and Digital Services Act. It discusses methods for bias assessment and mitigation while highlighting limitations of current approaches. The authors conclude that beyond regulation, greater attention to competition and design governance is needed to ensure fair and trustworthy AI systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Redesigning GROMACS Halo Exchange: Improving Strong Scaling with
GPU-initiated NVSHMEM</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU-initiated communication, halo exchange, molecular dynamics, NVSHMEM, strong scaling]</li>
<li class=""><strong>authors:</strong> Mahesh Doijade, Andrey Alekseenko, Ania Brown, Alan Gray, Szilárd Páll</li>
<li class=""><strong>institution:</strong> NVIDIA, KTH Royal Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21527v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21527v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper redesigns GROMACS halo exchange using GPU-initiated NVSHMEM communication, fusing data packing and communication in tuned GPU kernels. The approach improves communication-computation overlap and achieves up to 2x performance gains in strong scaling. Results demonstrate significant benefits of GPU-initiated communication for latency-sensitive applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM
Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [pipeline parallelism, sequence scheduling, gradient checkpointing, distributed training, long-context training]</li>
<li class=""><strong>authors:</strong> Shiju Wang, Yujie Wang, Ao Sun, Fangcheng Fu, Zijian Zhu, Bin Cui, Xu Han, Kaisheng Ma</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21275v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21275v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Elastic Pipeline Parallelism (EPP) that adaptively combines token-level and batch-level pipeline parallelism to optimize long-context LLM training. The authors implement InfiniPipe system with workload-balanced sequence processing and co-optimized pipeline scheduling with adaptive checkpointing. Experiments show InfiniPipe achieves 1.69x speedup over state-of-the-art systems for long-context training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] RecIS: Sparse to Dense, A Unified Training Framework for Recommendation
Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [recommendation systems, sparse-dense training, PyTorch framework, system optimization]</li>
<li class=""><strong>authors:</strong> Hua Zong, Qingtao Zeng, Zhengxiong Zhou, Zhihua Han, Zhensong Yan, Mingjie Liu, Hechen Sun, Jiawei Liu, Yiwen Hu, Qi Wang, YiHan Xian, Wenjie Guo, Houyuan Xiang, Zhiyuan Zeng, Xiangrong Sheng, Bencheng Yan, Nan Hu, Yuheng Huang, Jinqing Lian, Ziru Xu, Yan Zhang, Ju Huang, Siran Yang, Huimin Yi, Jiamang Wang, Pengjie Wang, Han Zhu, Jian Wu, Dan Ou, Jian Xu, Haihong Tang, Yuning Jiang, Bo Zheng, Lin Qu</li>
<li class=""><strong>institution:</strong> Alibaba</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20883v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20883v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RecIS proposes a unified sparse-dense training framework for recommendation models based on PyTorch, optimizing sparse components for efficiency while leveraging existing dense optimizations. The framework supports industrial-scale recommendation training integrated with large models. It demonstrates superior performance over TensorFlow-based approaches and is deployed in Alibaba for various recommendation tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [AI cluster scheduling, GPU resource management, unified training and inference scheduling, performance metrics]</li>
<li class=""><strong>authors:</strong> Lingling Zeng, Gen Zhang, Jialin Peng, Xiang Xu, Yuan Xu, Lijun Ma</li>
<li class=""><strong>institution:</strong> ZTE Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.01256v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.01256v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Kant introduces a unified scheduling system for large-scale AI clusters using strategies like Backfill and Enhanced Binpack to optimize GPU allocation. The system significantly improves resource utilization and reduces fragmentation in distributed training. It has been successfully deployed in multiple data centers, supporting efficient LLM training and inference workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO
Serving and Fast Scaling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multi-SLO scheduling, dynamic scaling, P/D disaggregated architectures, KV cache transfer, device-to-device weight transfer]</li>
<li class=""><strong>authors:</strong> Zahra Yousefijamarani, Xinglu Wang, Qian Wang, Morgan Lindsay Heisler, Taha Shabani, Niloofar Gholipour, Parham Yassini, Hong Chang, Kan Chen, Qiantao Zhang, Xiaolong Bai, Jiannan Wang, Ying Xiong, Yong Zhang, Zhenan Fan</li>
<li class=""><strong>institution:</strong> Huawei Technologies</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.15919v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.15919v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HyperFlexis introduces a unified LLM serving system with multi-SLO-aware scheduling and fast scaling optimizations, including D2D weight transfer to reduce loading overhead. It achieves higher SLO attainment, lower latency, and cost efficiency compared to state-of-the-art baselines. The system supports both collocated and disaggregated prefill/decode architectures for improved performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient
Multimodal Inference on Battery-Powered Small Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multimodal models, hardware-software co-design, edge computing, energy efficiency, memory optimization, dynamic scheduling]</li>
<li class=""><strong>authors:</strong> Yilong Li, Shuai Zhang, Yijing Zeng, Hao Zhang, Xinmiao Xiong, Jingyu Liu, Pan Hu, Suman Banerjee</li>
<li class=""><strong>institution:</strong> University of Wisconsin – Madison</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05109v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05109v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NANOMIND presents a hardware-software co-design framework that breaks multimodal models into modular components and schedules them across heterogeneous accelerators. The system achieves significant energy and memory efficiency improvements through token-aware buffer management and optimized computation kernels. This enables battery-powered devices to run large multimodal models entirely on-device for extended periods without network connectivity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Integrating and Characterizing HPC Task Runtime Systems for hybrid
AI-HPC workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [HPC, task runtime systems, AI-HPC workloads, RADICAL-Pilot, Flux, Dragon, performance optimization, resource management]</li>
<li class=""><strong>authors:</strong> Andre Merzky, Mikhail Titov, Matteo Turilli, Shantenu Jha</li>
<li class=""><strong>institution:</strong> RADICAL-Computing Inc., Brookhaven National Laboratory, Rutgers University, Princeton Plasma Physics Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20819v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20819v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper integrates RADICAL-Pilot with Flux and Dragon runtime systems to manage hybrid AI-HPC workloads. Performance evaluation shows RP+Flux sustains 930 tasks/s and RP+Flux+Dragon exceeds 1,500 tasks/s, significantly outperforming traditional srun/Slurm. The approach reduces makespan by 30-60% and increases throughput over four times for drug discovery campaigns.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient
Method for Solving Large-Scale Linear Optimization Problem</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [in-memory computing, RRAM, linear optimization, primal-dual hybrid gradient, distributed computing]</li>
<li class=""><strong>authors:</strong> Huynh Q. N. Vo, Md Tawsif Rahman Chowdhury, Paritosh Ramanan, Gozde Tutuncuoglu, Junchi Yang, Feng Qiu, Murat Yildirim</li>
<li class=""><strong>institution:</strong> IBM Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21137v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21137v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a distributed in-memory primal-dual hybrid gradient method specifically designed for RRAM arrays, minimizing write cycles and incorporating robustness against device non-idealities. The approach uses a symmetric block-matrix formulation to unify operations across distributed crossbars and integrates physics-based simulation for realistic evaluation. Benchmarking shows the RRAM-based solver achieves comparable accuracy to GPU-accelerated solvers while reducing energy consumption and latency by up to three orders of magnitude.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Go With The Flow: Churn-Tolerant Decentralized Training of Large
Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [decentralized training, churn tolerance, flow optimization, pipeline parallelism, heterogeneous clients]</li>
<li class=""><strong>authors:</strong> Nikolay Blagoev, Bart Cox, Jérémie Decouchant, Lydia Y. Chen</li>
<li class=""><strong>institution:</strong> Université de Neuchâtel, Delft University of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21221v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21221v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GWTF introduces a decentralized flow algorithm for efficient LLM training across volunteer clients, addressing node churn and network instabilities. It optimizes routing to maximize microbatch processing with minimal delay. Experiments show up to 45% training time reduction in heterogeneous, high-churn environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] SuperOffload: Unleashing the Power of Large-Scale LLM Training on
Superchips</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [Superchips, offloading, heterogeneous architecture, Grace CPU, Hopper GPU, NVLink-C2C, adaptive weight offloading, bucketization repartitioning, speculative execution, Adam optimizer optimization]</li>
<li class=""><strong>authors:</strong> Xinyu Lian, Masahiro Tanaka, Olatunji Ruwase, Minjia Zhang</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign (SSAIL Lab), Anyscale, Snowflake</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21271v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21271v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SuperOffload introduces a novel offloading system optimized for Superchip architecture, combining techniques like adaptive weight offloading and Superchip-aware casting. It achieves up to 2.5x throughput improvement over state-of-the-art systems, enabling efficient training of large models up to 25B parameters on a single Superchip. The system also scales effectively with parallelism methods, supporting training of 13B models with 1M token sequences on multiple Superchips.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-26">2025-09-26<a href="#2025-09-26" class="hash-link" aria-label="Direct link to 2025-09-26" title="Direct link to 2025-09-26" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] The AI_INFN Platform: Artificial Intelligence Development in the Cloud</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Kubernetes platform, GPU acceleration, distributed computing, workflow management, cloud-native solutions, INFN Cloud]</li>
<li class=""><strong>authors:</strong> Lucio Anderlini, Giulio Bianchini, Diego Ciangottini, Stefano Dal Pra, Diego Michelotto, Rosa Petrini, Daniele Spiga</li>
<li class=""><strong>institution:</strong> Istituto Nazionale di Fisica Nucleare (INFN)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22117v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22117v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a Kubernetes-based platform for developing GPU-powered data analysis workflows that can scale across heterogeneous distributed computing resources. It leverages cloud-native solutions and offloading mechanisms to manage workflows across different resource providers including WLCG sites and supercomputers. The platform aims to effectively share hardware accelerators while supporting diverse research activities within INFN use cases.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Code once, Run Green: Automated Green Code Translation in Serverless
Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [serverless computing, energy efficiency, code translation, large language models, green computing]</li>
<li class=""><strong>authors:</strong> Sebastian Werner, Mathis Kähler, Alireza Hakamian</li>
<li class=""><strong>institution:</strong> University of Hamburg, Technische Universität Berlin</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22068v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22068v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes ReFaaS, a system that uses LLMs to automatically translate serverless functions into more energy-efficient programming languages. The translated functions can reduce invocation energy by up to 70%, achieving net energy savings after 3,000-5,000 invocations. However, the approach faces challenges with function suitability and varying amortization thresholds.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code
Generation Using LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multi-agent systems, code generation, HPC, auto-tuning, prompt refinement, GPU programming]</li>
<li class=""><strong>authors:</strong> Shun-ichiro Hayashi, Koki Morita, Daichi Mukunoki, Tetsuya Hoshino, Takahiro Katagiri</li>
<li class=""><strong>institution:</strong> Nagoya University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.00031v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.00031v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VibeCodeHPC uses multi-agent LLMs with four specialized roles (Project Manager, System Engineer, Programmer, Continuous Delivery) for iterative prompt refinement to automatically tune HPC code. In a case study converting CPU-based matrix multiplication to CUDA GPU code, the multi-agent approach achieved higher-quality code generation per unit time compared to solo-agent configuration and more effectively identified requirement violations. The system demonstrates improved efficiency in HPC code optimization through dynamic agent deployment and activity monitoring.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Agora: Bridging the GPU Cloud Resource-Price Disconnect</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [GPU cloud pricing, resource allocation, memory bandwidth, feature-based pricing, market efficiency]</li>
<li class=""><strong>authors:</strong> Ian McDougall, Noah Scott, Joon Huh, Kirthevasan Kandasamy, Karthikeyan Sankaralingam</li>
<li class=""><strong>institution:</strong> University of Wisconsin-Madison</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05111v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05111v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Agora, a feature-based pricing framework that links cloud GPU costs directly to resource consumption like memory bandwidth. It introduces a practical system architecture and shows that fine-grained sampling enables nearly ideal pricing with minimal revenue loss. The approach creates a more transparent and efficient market for GPU cloud resources by addressing the performance-price disconnect in bandwidth-bound workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Efficient Fine-Grained GPU Performance Modeling for Distributed Deep
Learning of LLM</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [performance modeling, distributed training, GPU, parallelism strategies, computational primitives]</li>
<li class=""><strong>authors:</strong> Biyao Zhang, Mingkai Zheng, Debargha Ganguly, Xuecen Zhang, Vikash Singh, Vipin Chaudhary, Zhao Zhang</li>
<li class=""><strong>institution:</strong> Case Western Reserve University, Rutgers University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22832v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22832v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a fine-grained GPU performance modeling framework that decomposes LLMs into computational primitives and uses operator-level analysis with lightweight sampling. The method achieves low prediction errors (4.98% on A100, 9.38% on GH200) for models up to 20B parameters across 128 GPUs. Crucially, the framework runs entirely on CPUs, enabling rapid hardware configuration exploration without expensive cluster experiments.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-27">2025-09-27<a href="#2025-09-27" class="hash-link" aria-label="Direct link to 2025-09-27" title="Direct link to 2025-09-27" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Scaling LLM Test-Time Compute with Mobile NPU on Smartphones</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [mobile NPU, test-time scaling, quantization optimization, edge AI, model acceleration]</li>
<li class=""><strong>authors:</strong> Zixu Hao, Jianyu Wei, Tuowei Wang, Minxing Huang, Huiqiang Jiang, Shiqi Jiang, Ting Cao, Ju Ren</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23324v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23324v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes leveraging underutilized mobile NPU compute for parallel test-time scaling of smaller LLMs through hardware-aware tile quantization and LUT-based operation replacements. The implemented system achieves up to 19x speedup for GEMM operations and enables smaller models to match larger model accuracy. This approach establishes a new performance-cost Pareto frontier for mobile LLM deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured
Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [Mixture-of-Experts, dynamic clustering, structured compression, hierarchical routing, parameter reduction, communication optimization]</li>
<li class=""><strong>authors:</strong> Peijun Zhu, Ning Yang, Jiayu Wei, Jinghang Wu, Haijun Zhang</li>
<li class=""><strong>institution:</strong> The primary research institution appears to be the organization(s) associated with authors Peijun Zhu, Jiayu Wei, Jinghang Wu (likely from institution 1), Ning Yang (likely from institution 2), and Haijun Zhang (from institution 3)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.02345v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.02345v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a unified framework combining dynamic expert clustering and structured compression to address the MoE LLM trilemma. The method uses online clustering with fused similarity metrics and decomposes expert weights into shared bases with low-rank residuals, enabling hierarchical routing. Results show 80% parameter reduction, 10-20% throughput improvement, and 3x lower load variance while maintaining model quality comparable to standard MoE models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Towards Quantum-Ready Blockchain Fraud Detection via Ensemble Graph
Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [blockchain fraud detection, graph neural networks, ensemble learning, quantum-ready systems, anti-money laundering]</li>
<li class=""><strong>authors:</strong> M. Z. Haider, Tayyaba Noreen, M. Salman</li>
<li class=""><strong>institution:</strong> Université du Québec, SZABIST University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23101v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23101v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an ensemble framework combining Graph Convolutional Networks, Graph Attention Networks, and Graph Isomorphism Networks for blockchain fraud detection. This approach achieves high recall of illicit transactions with low false positive rates on the Elliptic dataset. The architecture is designed to be quantum-ready for future integration with quantum computing technologies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Memory Efficient and Staleness Free Pipeline Parallel DNN Training
Framework with Improved Convergence Speed</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [pipeline parallelism, memory efficiency, convergence speed]</li>
<li class=""><strong>authors:</strong> Ankita Dutta, Nabendu Chaki, Rajat K. De</li>
<li class=""><strong>institution:</strong> Indian Statistical Institute, University of Calcutta</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23241v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23241v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces V-TiMePReSt and I-TiMePReSt, two pipeline parallel DNN training frameworks that address staleness and memory efficiency. V-TiMePReSt eliminates staleness by using latest weights, while I-TiMePReSt computes intermediate weights to balance memory usage and convergence. Experimental results show both frameworks improve training efficiency with better staleness management and convergence performance.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-28">2025-09-28<a href="#2025-09-28" class="hash-link" aria-label="Direct link to 2025-09-28" title="Direct link to 2025-09-28" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Disaggregated Prefill and Decoding Inference System for Large Language
Model Serving on Multi-Vendor GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [heterogeneous GPUs, prefill-decode disaggregation, multi-vendor systems, joint optimization algorithm, resource utilization]</li>
<li class=""><strong>authors:</strong> Xing Chen, Rong Shi, Lu Zhao, Lingbin Wang, Xiao Jin, Yueqiang Chen, Hongfeng Sun</li>
<li class=""><strong>institution:</strong> ZTE Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17542v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17542v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a P-D disaggregated inference system for LLMs using heterogeneous multi-vendor GPUs, designing a compatible transmission module and joint optimization algorithm. Experimental results show the system effectively handles hybrid inference across different GPU vendors while optimizing deployment solutions. This approach improves resource utilization and reduces costs compared to homogeneous GPU systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous
Retraining Alignment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM serving, continuous retraining, edge computing, iteration-level scheduling, GPU resource management, SLO-aware optimization]</li>
<li class=""><strong>authors:</strong> Yufei Li, Yu Fu, Yue Dong, Cong Liu</li>
<li class=""><strong>institution:</strong> University of Texas at Dallas</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03283v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03283v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MACE proposes a hybrid LLM serving system that co-locates inference and fine-tuning with iteration-level scheduling and intelligent memory management. It balances inference latency and model accuracy by dynamically allocating GPU cycles based on update importance. Evaluation shows MACE reduces inference latency by up to 63% while maintaining high GPU utilization and throughput under resource constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on
Heterogeneous Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [pipeline parallelism, model partition, workload scheduling, performance optimization, heterogeneous models]</li>
<li class=""><strong>authors:</strong> Jihu Guo, Tenghui Ma, Wei Gao, Peng Sun, Jiaxing Li, Xun Chen, Yuyang Jin, Dahua Lin</li>
<li class=""><strong>institution:</strong> Fudan University, Shanghai AI Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23722v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23722v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AdaPtis proposes an adaptive pipeline parallelism system that jointly optimizes model partition, placement, and scheduling using a performance model. It introduces a unified pipeline executor to support diverse pipeline strategies. Experiments show 1.42x average speedup over Megatron-LM across various LLM architectures and scales.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-24T13:11:58.000Z" itemprop="dateModified">Oct 24, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20250915-20250921"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20250915-20250921</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20250929-20251005"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20250929-20251005</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-09-22" class="table-of-contents__link toc-highlight">2025-09-22</a></li><li><a href="#2025-09-23" class="table-of-contents__link toc-highlight">2025-09-23</a></li><li><a href="#2025-09-24" class="table-of-contents__link toc-highlight">2025-09-24</a></li><li><a href="#2025-09-25" class="table-of-contents__link toc-highlight">2025-09-25</a></li><li><a href="#2025-09-26" class="table-of-contents__link toc-highlight">2025-09-26</a></li><li><a href="#2025-09-27" class="table-of-contents__link toc-highlight">2025-09-27</a></li><li><a href="#2025-09-28" class="table-of-contents__link toc-highlight">2025-09-28</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>