<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251201-20251207" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251201-20251207 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20251201-20251207"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251201-20251207 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-12-01"><meta data-rh="true" property="og:description" content="2025-12-01"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20251201-20251207"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251201-20251207" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251201-20251207" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20251201-20251207","item":"https://darkknight996.github.io/daily/20251201-20251207"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.5d9606b4.js" defer="defer"></script>
<script src="/assets/js/main.a9660ddb.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251201-20251207</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251201-20251207</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-01">2025-12-01<a href="#2025-12-01" class="hash-link" aria-label="Direct link to 2025-12-01" title="Direct link to 2025-12-01" translate="no">​</a></h2>
<p><strong>cs.DC total: 23</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251201] An Empirical Study of Cross-Language Interoperability in Replicated Data Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [replicated data libraries, foreign-function interface, common data format, cross-language interoperability, empirical study]</li>
<li class=""><strong>authors:</strong> Provakar Mondal, Eli Tilevich</li>
<li class=""><strong>institution:</strong> Virginia Tech</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22010" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22010</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper empirically compares two strategies for cross-language interoperability in replicated data systems: foreign-function interface (FFI) and common data format (CDF). The study found that CDF-based integration provides better software quality, lower latency, reduced memory consumption, and higher throughput. The authors validated their findings by implementing a CDF-based replicated data library that supports mixed language environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [prefix-aware attention, multi-tile kernel, KV cache optimization, pack-forward-merge, vLLM integration]</li>
<li class=""><strong>authors:</strong> Jinjun Yi, Zhixin Zhao, Yitao Hu, Ke Yan, Weiwei Sun, Hao Wang, Laiping Zhao, Yuhao Zhang, Wenxin Li, Keqiu Li</li>
<li class=""><strong>institution:</strong> Tianjin University, Stevens Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22333" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22333</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PAT introduces a prefix-aware attention kernel that organizes execution using a pack-forward-merge paradigm to reduce redundant KV cache loading. It employs multi-tile kernels and query packing to optimize resource utilization during LLM decoding. Evaluation shows PAT reduces attention latency by 67.4% on average and improves throughput compared to state-of-the-art attention kernels.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [latency-disaggregated architecture, bottleneck-based scheduler, Roofline-based performance model, fast preemption mechanism, Prefill/Decode disaggregation]</li>
<li class=""><strong>authors:</strong> Siyu Wu, Zihan Tang, Yuting Zeng, Hui Chen, Guiguang Ding, Tongxuan Liu, Ke Zhang, Hailong Yang</li>
<li class=""><strong>institution:</strong> Beihang University, Tsinghua University, University of Science and Technology of China, JD Company</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21862" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21862</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a latency-disaggregated architecture that separates cluster resources into latency-strict and latency-relaxed pools for co-locating online and offline LLM workloads. It introduces a bottleneck-based scheduler with Roofline modeling and fast preemption mechanism to maintain online SLOs while improving resource utilization. Experiments show the method achieves up to 3× higher offline throughput while preserving online performance compared to existing approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] A Fast and Flat Federated Learning Method via Weighted Momentum and Sharpness-Aware Minimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, weighted momentum, sharpness-aware minimization, non-IID convergence, cosine-similarity adaptive rule]</li>
<li class=""><strong>authors:</strong> Tianle Li, Yongzhi Huang, Linshan Jiang, Chang Liu, Qipeng Xie, Wenfeng Du, Lu Wang, Kaishun Wu</li>
<li class=""><strong>institution:</strong> Shenzhen University, The Hong Kong University of Science and Technology (Guangzhou), National University of Singapore, Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22080" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22080</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FedWMSAM, a federated learning method that combines weighted momentum and sharpness-aware minimization to address local-global curvature misalignment and momentum-echo oscillation. It introduces momentum-guided global perturbation and a two-phase training schedule to improve optimization. Experimental results demonstrate the method&#x27;s effectiveness in achieving fast convergence and robust generalization across non-IID data distributions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Optimality of Simultaneous Consensus with Limited Information Exchange (Extended Abstract)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed consensus], [epistemic logic, knowledge-based programs, simultaneous agreement, crash failures, limited information exchange]</li>
<li class=""><strong>authors:</strong> Kaya Alpturer, Ron van der Meyden, Sushmita Ruj, Godfrey Wong</li>
<li class=""><strong>institution:</strong> Princeton University, UNSW Sydney</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22380" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22380</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops optimal fault-tolerant simultaneous consensus protocols using epistemic logic and knowledge-based programming with limited information exchange. The authors introduce a new information exchange approach that achieves decisions at most one round later than the optimal Dwork-Moses protocol while reducing computation cost and space requirements. They derive protocols that are optimal for various limited information exchanges from the literature, including FloodSet variants and failure-counting approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [biophotonics simulation], [ray-tracing, GPU acceleration, Monte Carlo method, hardware RT-cores, OptiX platform]</li>
<li class=""><strong>authors:</strong> Shijie Yan, Douglas Dwyer, David R. Kaeli, Qianqian Fang</li>
<li class=""><strong>institution:</strong> Northeastern University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22779" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22779</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces RT-MMC, a mesh-based Monte Carlo method that leverages modern GPU ray-tracing hardware for accelerated light transport simulations. By using NVIDIA&#x27;s OptiX platform and RT-cores, the approach eliminates complex mesh generation while achieving 1.5x to 4.5x speed improvements over traditional methods. The hardware-based ray-tracing significantly simplifies simulation workflows and enhances practicality for biophotonics applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [Heard-Of model, asynchronous message-passing, model equivalence, colorless tasks, colored tasks, crash failures, message omissions, silencing]</li>
<li class=""><strong>authors:</strong> Hagit Attiya, Armando Castañeda, Dhrubajyoti Ghosh, Thomas Nowak</li>
<li class=""><strong>institution:</strong> Technion – Israel Institute of Technology, Instituto de Matemáticas, Universidad Nacional Autónoma de México, Université Paris-Saclay, CNRS, ENS Paris-Saclay, Laboratoire Méthodes Formelles, Institut Universitaire de France</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21859" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21859</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper analyzes the relationship between the asynchronous message-passing model (AMP_f) and the Heard-Of model (HO_f) through bidirectional simulations and an intermediate model. It concludes that the models are equivalent for solving colorless tasks when n &gt; 2f, but for colored tasks, equivalence holds only for f=1 due to the issue of silenced processes in HO_f.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] When AI Bends Metal: AI-Assisted Optimization of Design Parameters in Sheet Metal Forming</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Bayesian optimization, deep learning, active learning, design space exploration, numerical simulation]</li>
<li class=""><strong>authors:</strong> Ahmad Tarraf, Koutaiba Kassem-Manthey, Seyed Ali Mohammadi, Philipp Martin, Lukas Moj, Semih Burak, Enju Park, Christian Terboven, Felix Wolf</li>
<li class=""><strong>institution:</strong> Technical University of Darmstadt, GNS Gesellschaft für numerische Simulation mbH, RWTH Aachen University, GNS Systems GmbH</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22302" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22302</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an AI-assisted workflow that combines deep learning and Bayesian optimization to automate the tuning of design parameters in sheet metal forming simulations. The method reduces expert involvement and accelerates design space exploration by using a deep learning model for initial parameter estimation followed by iterative refinement. The approach demonstrates significant potential to shorten design iterations and lower computational costs in simulation-driven industrial processes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Areon: Latency-Friendly and Resilient Multi-Proposer Consensus</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain consensus], [directed acyclic graph, proof-of-stake, multi-proposer, fork choice rule, VRF-based eligibility, sliding window]</li>
<li class=""><strong>authors:</strong> Álvaro Castro-Castilla, Marcin Pawlowski, Hong-Sheng Zhou</li>
<li class=""><strong>institution:</strong> Nomos Institute of Free Technology, Jagiellonian University, Virginia Commonwealth University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.23025" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.23025</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Areon introduces a multi-proposer consensus protocol that organizes blocks into a directed acyclic graph (DAG) with a sliding window reference mechanism and weighted fork choice rule. The protocol achieves bounded-latency finality with lower reorganization frequency compared to chain-based baselines. Experimental results show consistent performance improvements across various adversarial conditions and network delays.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing], [retry policy, distributed framework, analytical model, microservices, auto-scaling]</li>
<li class=""><strong>authors:</strong> Jhonatan Tavori, Anat Bremler-Barr, Hanoch Levy, Ofek Lavi</li>
<li class=""><strong>institution:</strong> Tel Aviv University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.23278" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.23278</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces RetryGuard, a distributed framework that uses an analytical model to manage retry policies across microservices and prevent retry storms. Experimental results show RetryGuard significantly reduces resource usage and costs compared to AWS retry policies in both standard and complex Kubernetes deployments with Istio service mesh.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Silence Speaks Volumes: A New Paradigm for Covert Communication via History Timing Patterns</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [network security], [history covert channels, timing patterns, relative pointers, covert amplification factor, silent history protocol]</li>
<li class=""><strong>authors:</strong> Christoph Weissenborn, Steffen Wendzel</li>
<li class=""><strong>institution:</strong> Federal Office for Information Security, Ulm University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22259" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22259</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a novel covert communication method called History Covert Channels (HCC), which embeds hidden messages by using relative pointers to past network timing patterns instead of directly manipulating traffic. This approach reduces reliance on centralized timekeeping and aims to evade standard detection tools. The authors&#x27; experiments demonstrate that their method achieves a higher bitrate compared to prior work.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Closing the Generalization Gap in Parameter-efficient Federated Edge Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [model pruning, client selection, joint resource management, generalization analysis, alternating optimization]</li>
<li class=""><strong>authors:</strong> Xinnong Du, Zhonghao Lyu, Xiaowen Cao, Chunyang Wen, Shuguang Cui, Jie Xu</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong (Shenzhen), KTH Royal Institute of Technology, Shenzhen University, University of Science and Technology of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.23282" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.23282</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a parameter-efficient federated edge learning framework that jointly optimizes model pruning and client selection with communication-computation resources. The method formulates a generalization-aware optimization problem solved via alternating optimization. Experiments show the approach achieves superior learning performance compared to state-of-the-art baselines by coupling generalization analysis with system-level optimization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Beyond 2-Edge-Connectivity: Algorithms and Impossibility for Content-Oblivious Leader Election</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [content-oblivious communication, leader election, topology knowledge, graph symmetry, message complexity]</li>
<li class=""><strong>authors:</strong> Yi-Jun Chang, Lyuting Chen, Haoran Zhou</li>
<li class=""><strong>institution:</strong> National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.23297" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.23297</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper studies leader election in content-oblivious networks where nodes can only send asynchronous, content-less pulses. It shows that with topology knowledge, leader election is possible in many non-2-edge-connected graphs like asymmetric trees, but impossible in graphs symmetric about an edge or when topology knowledge is insufficient. The work provides both impossibility results and algorithms with specific message complexity bounds for different graph classes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] ZipperChain: Transmuting Trusted Third-Party Services Into Trustless Atomic Broadcast</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed ledger technology], [ZipperChain, atomic broadcast, trustless, third-party services, distributed consensus, immutability, agreement, availability, pipeline, fast data center network]</li>
<li class=""><strong>authors:</strong> Matteo Bjornsson, Taylor Hardin, Taylor Heinecke, Marcin Furtak, David L. Millman, Mike P. Wittie</li>
<li class=""><strong>institution:</strong> BLOCKY, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21969" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21969</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ZipperChain is a blockchain that replaces distributed consensus with a pipeline of specialized services on a small number of nodes, transferring trust from established third-party services to provide correctness guarantees. This approach enables high transaction throughput near network line speeds and block finality around 500 ms, without needing a native token for incentives.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [storage systems], [Clock2Q+, cache replacement algorithm, metadata cache, correlated references, S3-FIFO, three queues, correlation window]</li>
<li class=""><strong>authors:</strong> Yiyan Zhai, Bintang Dwi Marthen, Sarath Balivada, Vamsi Sudhakar Bojji, Eric Knauft, Jitender Rohilla, Jiaqi Zuo, Quanxing Liu, Maxime Austruy, Wenguang Wang, Juncheng Yang</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, Bandung Institute of Technology, Broadcom Inc., Harvard University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21958" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21958</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Clock2Q+, a cache replacement algorithm designed for metadata caches that uses three queues and a correlation window to mitigate the negative impact of correlated references. It demonstrates superior performance, achieving up to a 28.5% lower miss ratio than S3-FIFO on metadata traces, while maintaining low overhead and ease of implementation for large-scale storage systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [blockchain, proof-of-work, trust rating, Bayesian-time-decay, high-performance cluster computing, reward incentive]</li>
<li class=""><strong>authors:</strong> Murat Yaslioglu</li>
<li class=""><strong>institution:</strong> Istanbul University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.21844" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.21844</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a novel blockchain-based framework that integrates high-performance cluster computing with AI, using an evolved proof-of-work consensus linked to computational effort and a dynamic Bayesian-time-decay trust rating for node selection. This mechanism aims to optimize resource use, incentivize broad participation, and create a merit-based system for block generation. The main conclusion is that this approach fosters a more sustainable, equitable, and energy-efficient environment for AI development by balancing computational power with inclusivity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [Mixture-of-Experts scheduling, sparse attention acceleration, prefill-decode disaggregation, KV-cache reuse, continuous batching, load-aware scheduling]</li>
<li class=""><strong>authors:</strong> Jun Wang, Yunxiang Yao, Wenwei Kuang, Runze Mao, Zhenhao Sun, Zhuang Tao, Ziyang Zhang, Dengyu Li, Jiajun Chen, Zhili Wang, Kai Cui, Congzhi Cai, Longwen Lan, Ken Zhang</li>
<li class=""><strong>institution:</strong> Huawei Technologies Co., Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22481" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22481</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OmniInfer is a system-level acceleration framework built on vLLM that integrates three components—OmniPlacement, OmniAttn, and OmniProxy—to optimize LLM serving through expert placement, sparse attention, and disaggregation-aware scheduling. It achieves performance gains by adaptively disaggregating resources, exploiting sparsity, and coordinating prefill and decode phases. Evaluated on a 10-node cluster, it significantly reduces time-per-output-token and time-to-first-token while increasing query throughput.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] DisCEdge: Distributed Context Management for Large Language Models at the Edge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [distributed context management, tokenization, geo-distributed storage, edge computing, data replication]</li>
<li class=""><strong>authors:</strong> Mohammadreza Malekabbasi, Minghe Wang, David Bermbach</li>
<li class=""><strong>institution:</strong> TU Berlin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22599" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22599</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DisCEdge, a system for managing LLM user context by storing and replicating it in tokenized form across distributed edge nodes. This approach reduces redundant computation and enables efficient synchronization. The evaluation shows it improves response times, lowers synchronization overhead, and significantly reduces client request sizes compared to existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, aggregation methods, personalization, optimization, robustness, heterogeneity, privacy-preserving, IID, non-IID]</li>
<li class=""><strong>authors:</strong> Meriem Arbaoui, Mohamed-el-Amine Brahmia, Abdellatif Rahmoun, Mourad Zghal</li>
<li class=""><strong>institution:</strong> LabRi-SBA Laboratory, CESI LINEACT UR 7527</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22616" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22616</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This survey paper provides a multi-level taxonomy of Federated Learning (FL) aggregation techniques, combining bibliometric analysis and systematic review to classify research in personalization, optimization, and robustness. It concludes by comparing aggregation methods under IID and non-IID data distributions and outlines future research directions to advance the field.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [LoRA, dynamic adapter placement, GPU Direct RDMA, multi-tenant serving, rank heterogeneity]</li>
<li class=""><strong>authors:</strong> Shashwat Jaiswal, Shrikara Arun, Anjaly Parayil, Ankur Mallick, Spyros Mastorakis, Alind Khare, Chloi Alverti, Renee St Amant, Chetan Bansal, Victor Rühle, Josep Torrellas</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, Microsoft, National Technical University of Athens</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22880" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22880</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents LoRAServe, a framework that dynamically places and routes heterogeneous LoRA adapters across GPUs to address performance skew in multi-tenant LLM inference. It uses workload-aware rebalancing and GPU Direct RDMA for remote access to improve resource utilization. The evaluation shows LoRAServe achieves higher throughput and lower latency while using fewer GPUs compared to state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [split learning, pipeline parallelism, wireless edge networks, communication-computation overlap, alternating optimization, micro-batches]</li>
<li class=""><strong>authors:</strong> Chenyu Liu, Zhaoyang Zhang, Zirui Chen, Zhaohui Yang</li>
<li class=""><strong>institution:</strong> Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.23167" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.23167</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes C²P²SL, a method that applies pipeline parallelism to split learning in wireless edge networks to overlap communication and computation processes, thereby reducing training latency. It formulates a joint optimization problem for task split and resource allocation, solved via alternating optimization. Experiments show the method reduces system training time by over 38% while maintaining accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Probabilistic Forward Pass, Bayesian Neural Networks, Gaussian propagation, TVM compiler, code generation, operator tuning, uncertainty estimation]</li>
<li class=""><strong>authors:</strong> Bernhard Klein, Falk Selker, Hendrik Borras, Sophie Steger, Franz Pernkopf, Holger Fröning</li>
<li class=""><strong>institution:</strong> Heidelberg University, Graz University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.23440" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.23440</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an end-to-end pipeline for deploying Bayesian Neural Networks (BNNs) using a Probabilistic Forward Pass (PFP), which approximates inference by propagating Gaussian distributions through the network in a single deterministic pass. The method is implemented via custom operators in the TVM compiler and optimized for ARM CPUs, achieving speedups of up to 4200x compared to traditional sampling-based methods while maintaining comparable accuracy and uncertainty estimation. The results demonstrate that combining Bayesian approximations with code generation enables efficient BNN deployment on resource-constrained embedded systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251201] A lasso-alternative to Dijkstra&#x27;s algorithm for identifying short paths in networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [graph theory and optimization], [lasso, LARS algorithm, ADMM, bi-directional Dijkstra, ℓ1-regularized regression]</li>
<li class=""><strong>authors:</strong> Anqi Dong, Amirhossein Taghvaei, Tryphon T. Georgiou</li>
<li class=""><strong>institution:</strong> KTH Royal Institute of Technology, University of Washington, University of California, Irvine</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2511.22745" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2511.22745</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper formulates the shortest path problem in graphs as an ℓ1-regularized regression (lasso). It connects this formulation, solved via the LARS algorithm, to the bi-directional Dijkstra algorithm and highlights the applicability of ADMM for efficient updates to network topology changes.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 33</strong></p>
<ul>
<li class="">[arXiv251201] Heterogeneous Multi-Agent Reinforcement Learning with Attention for Cooperative and Scalable Feature Transformation <a href="https://arxiv.org/pdf/2511.21934" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba <a href="https://arxiv.org/pdf/2511.22101" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Factors That Support Grounded Responses in LLM Conversations: A Rapid Review <a href="https://arxiv.org/pdf/2511.21762" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information <a href="https://arxiv.org/pdf/2511.22176" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Representative Action Selection for Large Action Space: From Bandits to MDPs <a href="https://arxiv.org/pdf/2511.22104" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2511.22105" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning <a href="https://arxiv.org/pdf/2511.22210" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning <a href="https://arxiv.org/pdf/2511.22226" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis <a href="https://arxiv.org/pdf/2511.22018" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation <a href="https://arxiv.org/pdf/2511.22235" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks <a href="https://arxiv.org/pdf/2511.21726" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs <a href="https://arxiv.org/pdf/2511.21928" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] An energy-efficient spiking neural network with continuous learning for self-adaptive brain-machine interface <a href="https://arxiv.org/pdf/2511.22108" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Hybrid Stackelberg Game and Diffusion-based Auction for Two-tier Agentic AI Task Offloading in Internet of Agents <a href="https://arxiv.org/pdf/2511.22076" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices <a href="https://arxiv.org/pdf/2511.22138" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] GPS: General Per-Sample Prompter <a href="https://arxiv.org/pdf/2511.21714" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions <a href="https://arxiv.org/pdf/2511.22406" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning <a href="https://arxiv.org/pdf/2511.22570" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering <a href="https://arxiv.org/pdf/2511.22715" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] ORION: Teaching Language Models to Reason Efficiently in the Language of Thought <a href="https://arxiv.org/pdf/2511.22891" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Switching-time bioprocess control with pulse-width-modulated optogenetics <a href="https://arxiv.org/pdf/2511.22893" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Language-conditioned world model improves policy generalization by reading environmental descriptions <a href="https://arxiv.org/pdf/2511.22904" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary <a href="https://arxiv.org/pdf/2511.22963" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Evolutionary Discovery of Heuristic Policies for Traffic Signal Control <a href="https://arxiv.org/pdf/2511.23122" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Peer-to-Peer Energy Trading in Dairy Farms using Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2511.23148" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection <a href="https://arxiv.org/pdf/2511.23158" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging <a href="https://arxiv.org/pdf/2511.23193" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning <a href="https://arxiv.org/pdf/2511.23262" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Emergent Coordination and Phase Structure in Independent Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2511.23315" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts <a href="https://arxiv.org/pdf/2511.23442" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] ThetaEvolve: Test-time Learning on Open Problems <a href="https://arxiv.org/pdf/2511.23473" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] RELiQ: Scalable Entanglement Routing via Reinforcement Learning in Quantum Networks <a href="https://arxiv.org/pdf/2511.22321" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning <a href="https://arxiv.org/pdf/2511.23310" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 19</strong></p>
<ul>
<li class="">[arXiv251201] MRI-Based Brain Age Estimation with Supervised Contrastive Learning of Continuous Representation <a href="https://arxiv.org/pdf/2511.22102" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models <a href="https://arxiv.org/pdf/2511.21759" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] ResearchArcade: Graph Interface for Academic Tasks <a href="https://arxiv.org/pdf/2511.22036" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization <a href="https://arxiv.org/pdf/2511.21736" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Standardized Threat Taxonomy for AI Security, Governance, and Regulatory Compliance <a href="https://arxiv.org/pdf/2511.21901" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics <a href="https://arxiv.org/pdf/2511.21848" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Co-Evolving Agents: Learning from Failures as Hard Negatives <a href="https://arxiv.org/pdf/2511.22254" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code <a href="https://arxiv.org/pdf/2511.21920" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] GLA-Grad++: An Improved Griffin-Lim Guided Diffusion Model for Speech Synthesis <a href="https://arxiv.org/pdf/2511.22293" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Cacheback: Speculative Decoding With Nothing But Cache <a href="https://arxiv.org/pdf/2511.21699" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] FastFHE: Packing-Scalable and Depthwise-Separable CNN Inference Over FHE <a href="https://arxiv.org/pdf/2511.22434" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction <a href="https://arxiv.org/pdf/2511.22460" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization <a href="https://arxiv.org/pdf/2511.22586" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration <a href="https://arxiv.org/pdf/2511.23203" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Simultaneous Image Quality Improvement and Artefacts Correction in Accelerated MRI <a href="https://arxiv.org/pdf/2511.23274" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] QuantumChem-200K: A Large-Scale Open Organic Molecular Dataset for Quantum-Chemistry Property Screening and Language Model Benchmarking <a href="https://arxiv.org/pdf/2511.21747" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Automated Statistical and Machine Learning Platform for Biological Research <a href="https://arxiv.org/pdf/2511.21770" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Generative models for crystalline materials <a href="https://arxiv.org/pdf/2511.22652" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251201] Escaping Barren Plateaus in Variational Quantum Algorithms Using Negative Learning Rate in Quantum Internet of Things <a href="https://arxiv.org/pdf/2511.22861" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-12-01T04:41:17.000Z" itemprop="dateModified">Dec 1, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20251124-20251130"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251124-20251130</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/category/paper"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Paper</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-01" class="table-of-contents__link toc-highlight">2025-12-01</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>