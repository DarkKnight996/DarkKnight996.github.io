<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20260223-20260301" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20260223-20260301 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20260223-20260301"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20260223-20260301 | DarkKnight Note"><meta data-rh="true" name="description" content="2026-02-23"><meta data-rh="true" property="og:description" content="2026-02-23"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20260223-20260301"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20260223-20260301" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20260223-20260301" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20260223-20260301","item":"https://darkknight996.github.io/daily/20260223-20260301"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.5e1fce07.js" defer="defer"></script>
<script src="/assets/js/main.138dced8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260105-20260111"><span title="20260105-20260111" class="linkLabel_WmDU">20260105-20260111</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260112-20260118"><span title="20260112-20260118" class="linkLabel_WmDU">20260112-20260118</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260119-20260125"><span title="20260119-20260125" class="linkLabel_WmDU">20260119-20260125</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260126-20260201"><span title="20260126-20260201" class="linkLabel_WmDU">20260126-20260201</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260202-20260208"><span title="20260202-20260208" class="linkLabel_WmDU">20260202-20260208</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260209-20260215"><span title="20260209-20260215" class="linkLabel_WmDU">20260209-20260215</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260216-20260222"><span title="20260216-20260222" class="linkLabel_WmDU">20260216-20260222</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20260223-20260301"><span title="20260223-20260301" class="linkLabel_WmDU">20260223-20260301</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260302-20260308"><span title="20260302-20260308" class="linkLabel_WmDU">20260302-20260308</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20260223-20260301</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20260223-20260301</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2026-02-23">2026-02-23<a href="#2026-02-23" class="hash-link" aria-label="Direct link to 2026-02-23" title="Direct link to 2026-02-23" translate="no">​</a></h2>
<p><strong>cs.DC total: 13</strong></p>
<ul>
<li class="">
<p><strong>[arXiv260223] Message-Oriented Middleware Systems: Technology Overview</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [message-oriented middleware, publish/subscribe, brokers, multi-tenancy, flow control]</li>
<li class=""><strong>authors:</strong> Wael Al-Manasrah, Zuhair AlSader, Tim Brecht, Ahmed Alquraan, Samer Al-Kiswany</li>
<li class=""><strong>institution:</strong> University of Waterloo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.17774" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.17774</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a comprehensive characterization study of ten open-source message-oriented middleware (MOM) systems, analyzing 42 features across them. The main conclusion is that MOM systems have evolved into flexible frameworks for cloud applications, and the authors identify an opportunity for the community to consolidate efforts on fewer open-source projects.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260223] Distributed Triangle Enumeration in Hypergraphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed algorithms], [CONGEST model, hypergraph, triangle enumeration, computational models, sparse hypergraphs]</li>
<li class=""><strong>authors:</strong> Duncan Adamson, Will Rosenbaum, Paul G. Spirakis</li>
<li class=""><strong>institution:</strong> University of St Andrews, University of Liverpool</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.17834" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.17834</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces new computational models for distributed algorithms on hypergraphs, generalizing the CONGEST model. It presents algorithms for distributed triangle enumeration in these models, proves their optimality in some cases, and develops efficient methods for sparse hypergraph classes. The work establishes a foundational framework for distributed sub-hypergraph enumeration.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260223] Collaborative Processing for Multi-Tenant Inference on Memory-Constrained Edge TPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [collaborative processing, model partitioning, queueing model, memory swapping, adaptive scheduling, Edge TPU]</li>
<li class=""><strong>authors:</strong> Nathan Ng, Walid A. Hanafy, Prashanthi Kadambi, Balachandra Sunil, Ayush Gupta, David Irwin, Yogesh Simmhan, Prashant Shenoy</li>
<li class=""><strong>institution:</strong> University of Massachusetts Amherst, Indian Institute of Science</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.17808" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.17808</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents SwapLess, a system that uses an analytic queueing model to adaptively partition AI model inference between CPU and Edge TPU resources and allocate CPU cores online, minimizing end-to-end latency. It demonstrates that this approach significantly reduces mean latency for both single-tenant and multi-tenant workloads compared to default compiler strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260223] It&#x27;s Not Just Timestamps: A Study on Docker Reproducibility</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [container security], [Docker, reproducible builds, software supply chain, Dockerfile, OCI images, bitwise reproducibility, timestamps, metadata, caching]</li>
<li class=""><strong>authors:</strong> Oreofe Solarin</li>
<li class=""><strong>institution:</strong> Case Western Reserve University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.17678" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.17678</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper builds a measurement pipeline to analyze the bitwise reproducibility of Docker container images built from Dockerfiles in a sample of 2,000 GitHub repositories. It concludes that very few Docker builds are reproducible, and the primary causes are developer-controlled factors like uncleaned caches and floating software versions, not just timestamps.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260223] Distributed Security: From Isolated Properties to Synergistic Trust</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems security], [Byzantine fault tolerance, consensus protocols, secure multi-party computation, cryptographic primitives, agreement, consistency, privacy, verifiability, accountability]</li>
<li class=""><strong>authors:</strong> Minghui Xu</li>
<li class=""><strong>institution:</strong> Shandong University, Quan Cheng Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.18063" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.18063</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This vision paper argues for a paradigm shift in distributed security research, moving from the isolated study of foundational properties like agreement and privacy to understanding their synergistic combinations. The core method involves analyzing the convergence of these properties to create a unified fabric of trust. The main conclusion is that the future of the field lies in harnessing these synergies rather than optimizing individual properties in isolation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260223] Faster Parallel Batch-Dynamic Algorithms for Low Out-Degree Orientation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [graph algorithms], [parallel batch-dynamic algorithms, low out-degree orientation, arboricity, polylogarithmic span]</li>
<li class=""><strong>authors:</strong> Guy Blelloch, Andrew Brady, Laxman Dhulipala, Jeremy Fineman, Kishen Gowda, Chase Hutton</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, University of Maryland, Georgetown University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.17811" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.17811</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents new parallel batch-dynamic algorithms for maintaining a low out-degree orientation of an undirected graph. The algorithms achieve polylogarithmic span and improve upon prior work by reducing the work per edge update, offering results with asymptotically optimal expected work, expected worst-case work of O(√log n), and O(log² n) expected worst-case work for different orientation bounds.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260223] Joint Training on AMD and NVIDIA GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [heterogeneous training, CPU-Forwarding Communication, Device-Direct Communication, GPUDirect RDMA, CPU-offloading P2P, multi-NIC parallel data transfer]</li>
<li class=""><strong>authors:</strong> Jon Hu, Thomas Jia, Jing Zhu, Zhendong Yu</li>
<li class=""><strong>institution:</strong> Zettabyte AI, Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.18007" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.18007</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes two methods for joint training of large language models on heterogeneous clusters containing both AMD and NVIDIA GPUs. The core contribution is a high-performance Device-Direct Communication approach that enables direct data transfer between different vendor GPUs, eliminating host-memory staging. Experiments show this method achieves up to 98% of the throughput of a homogeneous NVIDIA system while maintaining training stability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260223] GPU Memory and Utilization Estimation for Training-Aware Resource Management: Opportunities and Limitations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [memory estimation, utilization estimation, analytical models, ML-based estimators, PyTorch FakeTensor, Horus, interference-aware scheduling]</li>
<li class=""><strong>authors:</strong> Ehsan Yousefzadeh-Asl-Miandoab, Reza Karimzadeh, Danyal Yorulmaz, Bulat Ibragimov, Pınar Tözün</li>
<li class=""><strong>institution:</strong> IT University of Copenhagen, University of Copenhagen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.17817" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.17817</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically analyzes three paradigms for estimating GPU memory and utilization—analytical models, CPU-side libraries, and ML-based estimators—to improve resource management for collocated deep learning training. The evaluation reveals key trade-offs: analytical models are hardware-dependent, libraries impose integration costs, and ML estimators struggle with generalization. The authors release datasets and tools to support further research in this area.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260223] Closing Africa&#x27;s Early Warning Gap: AI Weather Forecasting for Disaster Prevention</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [NVIDIA Earth-2, PostgreSQL, ProcessPoolExecutor, aiobotocore, async Python, database-backed serving, coordinate management, WhatsApp distribution]</li>
<li class=""><strong>authors:</strong> Qness Ndlovu</li>
<li class=""><strong>institution:</strong> Dimension Research Lab</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.17726" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.17726</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a low-cost, production-grade architecture using NVIDIA Earth-2 AI weather models and PostgreSQL caching to deliver national-scale weather forecasts in Africa. The system reduces deployment costs by over 2,000x compared to traditional radar, enabling effective early warning systems via widely accessible channels like WhatsApp. The main conclusion is that this AI-driven approach makes continent-scale disaster prevention economically viable, potentially reducing disaster death rates significantly.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260223] Mind the Boundary: Stabilizing Gemini Enterprise A2A via a Cloud Run Hub Across Projects and Accounts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [agent-to-agent (A2A) protocol, JSON-RPC, Cloud Run, retrieval-augmented generation (RAG), Vertex AI, Google Cloud Storage, IAM authentication]</li>
<li class=""><strong>authors:</strong> Takao Morita</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.17675" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.17675</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements an A2A Hub orchestrator on Cloud Run to route queries from the Gemini Enterprise UI to various backend agents and tools across different Google Cloud projects and accounts. It identifies and addresses a key interoperability gap where the UI&#x27;s text-only input constraints cause errors with structured JSON-RPC responses, solved by enforcing a text-only compatibility mode. The main conclusion is that practical, stable multi-agent orchestration requires managing not just protocol compliance but also UI constraints and cross-boundary authentication.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260223] A reliability- and latency-driven task allocation framework for workflow applications in the edge-hub-cloud continuum</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [edge computing], [binary integer linear programming, multi-objective optimization, task allocation, time redundancy]</li>
<li class=""><strong>authors:</strong> Andreas Kouloumpris, Georgios L. Stavrinides, Maria K. Michael, Theocharis Theocharides</li>
<li class=""><strong>institution:</strong> University of Cyprus, KIOS Research and Innovation Center of Excellence</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.18158" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.18158</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an exact multi-objective task allocation framework using binary integer linear programming to jointly optimize reliability and latency for workflow applications in an edge-hub-cloud architecture. The method incorporates time redundancy and key constraints, demonstrating significant improvements in reliability and latency over baselines in experiments with real-world and synthetic workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260223] It does not matter how you define locally checkable labelings</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed graph algorithms], [locally checkable labeling, LOCAL model, round elimination, symmetry-breaking oracle, node-edge checkable]</li>
<li class=""><strong>authors:</strong> Antonio Cruciani, Avinandan Das, Alesya Raevskaya, Jukka Suomela</li>
<li class=""><strong>institution:</strong> Aalto University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.18188" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.18188</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper shows that the family of Locally Checkable Labeling (LCL) problems is robust to definitional variations by proving local reductions between a standard LCL formalism and a more restricted &quot;node-edge checkable&quot; formalism. The main conclusion is that even with a stricter definition, LCL problems retain counterintuitive properties, indicating these properties are inherent to the problem family and not artifacts of the original definition.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260223] Green by Design: Constraint-Based Adaptive Deployment in the Cloud Continuum</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud-edge deployment], [green constraints, adaptive orchestration, energy-aware scheduling, cloud continuum, deployment plans]</li>
<li class=""><strong>authors:</strong> Andrea D&#x27;Iapico, Monica Vitali</li>
<li class=""><strong>institution:</strong> Politecnico di Milano</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.18287" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.18287</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a methodology for generating environmentally sustainable deployment plans for cloud-native applications across the cloud-edge continuum. The core method involves automatically learning and updating &quot;green constraints&quot; from monitoring data on energy consumption and infrastructure carbon intensity to guide an adaptive scheduler. The approach is validated to effectively reduce energy usage and associated emissions in realistic deployment scenarios.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 17</strong></p>
<ul>
<li class="">[arXiv260223] Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly <a href="https://arxiv.org/pdf/2602.17997" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards <a href="https://arxiv.org/pdf/2602.18037" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] MePoly: Max Entropy Polynomial Policy Optimization <a href="https://arxiv.org/pdf/2602.17832" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Epistemic Traps: Rational Misalignment Driven by Model Misspecification <a href="https://arxiv.org/pdf/2602.17676" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets <a href="https://arxiv.org/pdf/2602.18025" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling <a href="https://arxiv.org/pdf/2602.17685" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models <a href="https://arxiv.org/pdf/2602.17684" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Flow Actor-Critic for Offline Reinforcement Learning <a href="https://arxiv.org/pdf/2602.18015" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Mean-Field Reinforcement Learning without Synchrony <a href="https://arxiv.org/pdf/2602.18026" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance <a href="https://arxiv.org/pdf/2602.17930" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Learning Optimal and Sample-Efficient Decision Policies with Guarantees <a href="https://arxiv.org/pdf/2602.17978" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning <a href="https://arxiv.org/pdf/2602.17931" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Interacting safely with cyclists using Hamilton-Jacobi reachability and reinforcement learning <a href="https://arxiv.org/pdf/2602.18097" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] TempoNet: Slack-Quantized Transformer-Guided Reinforcement Scheduler for Adaptive Deadline-Centric Real-Time Dispatchs <a href="https://arxiv.org/pdf/2602.18109" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning <a href="https://arxiv.org/pdf/2602.18117" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] PRISM: Parallel Reward Integration with Symmetry for MORL <a href="https://arxiv.org/pdf/2602.18277" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies <a href="https://arxiv.org/pdf/2602.18291" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 9</strong></p>
<ul>
<li class="">[arXiv260223] Solving and learning advective multiscale Darcian dynamics with the Neural Basis Method <a href="https://arxiv.org/pdf/2602.17776" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Hardware-Friendly Input Expansion for Accelerating Function Approximation <a href="https://arxiv.org/pdf/2602.17952" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Multi-material Multi-physics Topology Optimization with Physics-informed Gaussian Process Priors <a href="https://arxiv.org/pdf/2602.17783" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] A Case Study of Selected PTQ Baselines for Reasoning LLMs on Ascend NPU <a href="https://arxiv.org/pdf/2602.17693" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Balancing Symmetry and Efficiency in Graph Flow Matching <a href="https://arxiv.org/pdf/2602.18084" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] A Probabilistic Framework for LLM-Based Model Discovery <a href="https://arxiv.org/pdf/2602.18266" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] PRISM: Parallel Reward Integration with Symmetry for MORL <a href="https://arxiv.org/pdf/2602.18277" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] Clever Materials: When Models Identify Good Materials for the Wrong Reasons <a href="https://arxiv.org/pdf/2602.17730" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260223] AgriVariant: Variant Effect Prediction using DeepChem-Variant for Precision Breeding in Rice <a href="https://arxiv.org/pdf/2602.17747" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2026-02-24">2026-02-24<a href="#2026-02-24" class="hash-link" aria-label="Direct link to 2026-02-24" title="Direct link to 2026-02-24" translate="no">​</a></h2>
<p><strong>cs.DC total: 19</strong></p>
<ul>
<li class="">
<p><strong>[arXiv260224] WANSpec: Leveraging Global Compute Capacity for LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative decoding, wide area network, load balancing, redundancy, auto-regressive decoding]</li>
<li class=""><strong>authors:</strong> Noah Martin, Fahad Dogar</li>
<li class=""><strong>institution:</strong> Tufts University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.18931" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.18931</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces WANSpec, a method that leverages speculative decoding to offload the draft model&#x27;s forward passes to under-utilized data centers across a wide area network. This approach reduces the computational load on high-demand data centers by over 50% while using judicious redundancy to avoid increasing request latency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] When Coordination Is Avoidable: A Monotonicity Analysis of Organizational Tasks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [multi-agent systems], [monotonicity, interdependence taxonomy, coordination tax, multi-agent simulation, O*NET]</li>
<li class=""><strong>authors:</strong> Harang Ju</li>
<li class=""><strong>institution:</strong> Johns Hopkins University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.18673" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.18673</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper applies a monotonicity criterion from distributed systems theory to organizational tasks, showing coordination is only necessary for non-monotonic tasks. It classifies enterprise workflows and occupational tasks, finding a significant portion are monotonic, implying much coordination spending is unnecessary for correctness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] A Formal Framework for Predicting Distributed System Performance under Faults</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems, fault tolerance], [Maude, formal methods, fault injection, performance prediction, statistical analysis]</li>
<li class=""><strong>authors:</strong> Ziwei Zhou, Si Liu, Zhou Zhou, Peixin Wang, MIn Zhang</li>
<li class=""><strong>institution:</strong> East China Normal University, Texas A&amp;M University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.19088" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.19088</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a formal framework that integrates a fault injector library with system models to predict the performance (e.g., throughput, latency) of distributed systems under various fault scenarios. The framework is formalized in Maude and implemented as an automated tool called PerF. The authors demonstrate that PerF accurately predicts performance, with results consistent with evaluations on real deployments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] Deep Reinforcement Learning for Optimizing Energy Consumption in Smart Grid Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep reinforcement learning, physics-informed neural networks, optimal power flow, surrogate models, smart grid]</li>
<li class=""><strong>authors:</strong> Abeer Alsheikhi, Amirfarhad Farhadi, Azadeh Zamanifar</li>
<li class=""><strong>institution:</strong> Iran University of Science and Technology, Islamic Azad University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.18531" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.18531</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using Physics-Informed Neural Networks (PINNs) as a surrogate model to accelerate Deep Reinforcement Learning training for smart grid energy management. The PINN-based approach, which incorporates knowledge of physical laws, significantly improves sample efficiency and reduces the need for costly simulator interactions. The results show that this method achieves 50% faster training while maintaining performance comparable to using the original simulator.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] What Distributed Computing Got Wrong: The Category Mistake That Turned Design Choices into Laws of Nature</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [FITO, bilateral transactions, category mistake, impossibility theorems, FLP, CAP, Two Generals]</li>
<li class=""><strong>authors:</strong> Paul Borrill</li>
<li class=""><strong>institution:</strong> DÆDÆLUS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.18723" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.18723</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper argues that foundational impossibility results in distributed computing, such as FLP and CAP, are not physical laws but consequences of the design choice of Forward-In-Time-Only (FITO) information flow. It proposes replacing unidirectional message passing with atomic bilateral transactions as an alternative model. The conclusion is that distributed computing has been optimizing within an unnecessarily constrained design space.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] Asymptotic Subspace Consensus in Dynamic Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [asymptotic subspace consensus, oblivious message adversaries, convex hull, dynamic networks, averaging algorithms]</li>
<li class=""><strong>authors:</strong> Matthias Függer, Thomas Nowak</li>
<li class=""><strong>institution:</strong> Université Paris-Saclay, CNRS, ENS Paris-Saclay, LMF, Institut Universitaire de France</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.19121" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.19121</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the asymptotic subspace consensus problem, a relaxation of classic asymptotic consensus where process outputs converge to a common subspace rather than a single point. The authors provide a complete characterization of its solvability under oblivious message adversaries and show that many existing consensus algorithms degrade gracefully to solve this problem in weaker network conditions. They also present bounds on the rate of dimension reduction during convergence.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] ucTrace: A Multi-Layer Profiling Tool for UCX-driven Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC communication profiling], [UCX, MPI, GPU-aware communication, transport-layer profiling, interactive visualization]</li>
<li class=""><strong>authors:</strong> Emir Gencer, Mohammad Kefah Taha Issa, Ilyas Turimbetov, James D. Trotter, Didem Unat</li>
<li class=""><strong>institution:</strong> Koc University, Simula Research Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.19084" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.19084</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces ucTrace, a multi-layer profiling tool that captures and visualizes fine-grained communication at the UCX transport layer in HPC systems, linking operations to their originating MPI functions. It demonstrates the tool&#x27;s utility in analyzing and optimizing communication patterns for large-scale, GPU-accelerated workloads. The main conclusion is that ucTrace effectively addresses the gap in existing tools by providing detailed, actionable insights into UCX-driven communication for performance tuning and debugging.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] BiScale: Energy-Efficient Disaggregated LLM Serving via Phase-Aware Placement and DVFS</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [prefill/decode disaggregation, DVFS, model predictive control, slack-aware adaptation, phase-aware placement]</li>
<li class=""><strong>authors:</strong> Omar Basit, Yunzhao Liu, Z. Jonny Kong, Y. Charlie Hu</li>
<li class=""><strong>institution:</strong> Purdue University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.18755" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.18755</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces BiScale, a two-tier energy optimization framework for disaggregated LLM serving that jointly optimizes GPU placement and frequency scaling (DVFS). It uses coarse-grained phase-aware placement and fine-grained, stage-specific frequency control (MPC for prefill, slack-aware for decode) to minimize energy while meeting latency SLOs. Evaluation shows it reduces energy by up to 39% in prefill and 48% in decode compared to DistServe while maintaining SLOs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] Carbon-aware decentralized dynamic task offloading in MIMO-MEC networks via multi-agent reinforcement learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-agent proximal policy optimization (PPO), decentralized execution with parameter sharing (DEPS), Decentralized Partially Observable Markov Decision Process (DEC-POMDP), carbon-aware computing, power control, task offloading]</li>
<li class=""><strong>authors:</strong> Mubshra Zulfiqar, Muhammad Ayzed Mirza, Basit Qureshi</li>
<li class=""><strong>institution:</strong> Wuhan University of Technology, Qilu Institute of Technology, Prince Sultan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.18797" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.18797</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CADDTO-PPO, a carbon-aware decentralized dynamic task offloading framework using multi-agent proximal policy optimization for MIMO-MEC networks. It models the system as a DEC-POMDP and employs decentralized execution with parameter sharing to enable autonomous agents to make local decisions on power control and offloading. The method achieves lower carbon intensity and near-zero packet overflow compared to baselines, with constant inference complexity suitable for sustainable IoT deployments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] The Category Mistake of Cislunar Time: Why NASA Cannot Synchronize What Doesn&#x27;t Exist</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [time synchronization, distributed systems], [Coordinated Lunar Time (LTC), atomic clocks, relativistic corrections, LunaNet, category mistake, ontic vs epistemic, Forward-In-Time-Only (FITO), Leibnizian operationalism, Wood-Spekkens fine-tuning argument]</li>
<li class=""><strong>authors:</strong> Paul Borrill</li>
<li class=""><strong>institution:</strong> DÆDÆLUS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.18641" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.18641</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper argues that NASA&#x27;s Coordinated Lunar Time (LTC) program is based on a philosophical category mistake, treating synchronized time as an independent ontic entity rather than an epistemic, model-dependent construct. It analyzes the program using concepts from quantum foundations, such as the ontic-epistemic distinction and Spekkens&#x27; operationalism, to conclude that the project is conceptually incoherent. The author proposes a transactional alternative based on bilateral atomic interactions instead of unidirectional time distribution.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] Semantic Conflict Model for Collaborative Data Structures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [CRDT, semantic conflicts, three-way merge, eventual consistency, optimistic concurrency control, replicated journal]</li>
<li class=""><strong>authors:</strong> Georgii Semenov, Vitaly Aksenov</li>
<li class=""><strong>institution:</strong> ITMO University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.19231" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.19231</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a semantic conflict model for collaborative data structures that identifies conflicts using semantic dependencies between operations and resolves them by rebasing operations via a three-way merge over a replicated journal. The model enables explicit, local-first conflict resolution without central coordination, as demonstrated on collaborative registers like a Last-Writer-Wins Register.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] Health+: Empowering Individuals via Unifying Health Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [multimodal data management, data fusion, user-centric privacy, cloud storage, information retrieval]</li>
<li class=""><strong>authors:</strong> Sujaya Maiyya, Shantanu Sharma, Avinash Kumar</li>
<li class=""><strong>institution:</strong> University of Waterloo, New Jersey Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.19319" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.19319</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This vision paper proposes Health+, a user-centric system designed to unify and manage an individual&#x27;s fragmented multimodal health data (e.g., images, reports, EHRs) through ingestion, storage, fusion, and querying. It emphasizes intuitive interfaces and privacy-aware sharing to empower patients. The main conclusion is that such a system can lay the foundation for a more connected and user-controlled health information ecosystem without requiring an institutional overhaul.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] Complex Event Processing in the Edge: A Combined Optimization Approach for Data and Code Placement</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [edge computing], [complex event processing, constrained programming optimization, critical path performance, virtual shared memory, task graph, code and data placement]</li>
<li class=""><strong>authors:</strong> Halit Uyanık, Tolga Ovatman</li>
<li class=""><strong>institution:</strong> Istanbul Technical University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.19338" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.19338</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a constrained programming optimization approach to balance execution costs and improve the critical path performance in Complex Event Processing (CEP) task graphs for IoT edge devices. It is implemented as a Python library that optimizes code and I/O assignments, virtualizing shared memory between devices. The results show that this optimization increases throughput and reduces delay during CEP operations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] A Context-Aware Knowledge Graph Platform for Stream Processing in Industrial IoT</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [industrial IoT systems], [knowledge graph, Apache Kafka, Apache Flink, SPARQL, SWRL, context-aware reasoning, stream processing]</li>
<li class=""><strong>authors:</strong> Monica Marconi Sciarroni, Emanuele Storti</li>
<li class=""><strong>institution:</strong> Polytechnic University of Marche</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.19990" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.19990</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a semantic platform that uses a Knowledge Graph to unify heterogeneous Industrial IoT data streams, enabling context-aware reasoning and dynamic access control. It relies on Apache Kafka and Flink for real-time processing and SPARQL/SWRL for stream discovery. The experimental evaluation demonstrates the effectiveness of this approach for creating interoperable data workflows in Industry 5.0 environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] Mitigating Artifacts in Pre-quantization Based Scientific Data Compressors with Quantization-aware Interpolation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing, data compression], [pre-quantization, error-bounded lossy compression, quantization-aware interpolation, artifact mitigation, parallel computing]</li>
<li class=""><strong>authors:</strong> Pu Jiao, Sheng Di, Jiannan Tian, Mingze Xia, Xuan Wu, Yang Zhang, Xin Liang, Franck Cappello</li>
<li class=""><strong>institution:</strong> University of Kentucky, Argonne National Laboratory, Oakland University, Oregon State University, Miami University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.20097" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.20097</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel quantization-aware interpolation algorithm to mitigate artifacts in pre-quantization based scientific data compressors. The method improves decompressed data quality while maintaining high compression throughput. Experiments on real-world datasets validate its effectiveness with leading compressors.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] GPU-Resident Gaussian Process Regression Leveraging Asynchronous Tasks with HPX</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [Gaussian Process Regression, Cholesky Decomposition, Asynchronous Many-task Runtimes, Tiled Algorithms, HPX, CUDA]</li>
<li class=""><strong>authors:</strong> Henrik Möllmann, Dirk Pflüger, Alexander Strack</li>
<li class=""><strong>institution:</strong> Institute of Parallel and Distributed Systems, University of Stuttgart</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.19683" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.19683</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper extends the GPRat library by implementing a fully GPU-resident Gaussian Process prediction pipeline using tiled algorithms and optimized CUDA libraries, managed asynchronously by the HPX runtime. The results show that the GPU implementation provides significant speedups for datasets larger than 128 samples, with performance gains of up to 4.6x for prediction and even surpassing cuSOLVER by up to 11% for large datasets when combining HPX with multiple CUDA streams.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] Why iCloud Fails: The Category Mistake of Cloud Synchronization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems, cloud storage], [cloud synchronization, POSIX semantics, network partitioning, causal graph, Open Atomic Ethernet (OAE), transactional semantics]</li>
<li class=""><strong>authors:</strong> Paul Borrill</li>
<li class=""><strong>institution:</strong> DAEDAELUS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.19433" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.19433</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper analyzes iCloud Drive&#x27;s failures, arguing they stem from a &quot;Category Mistake&quot; where its synchronization semantics (based on Forward-In-Time-Only assumptions) fundamentally conflict with POSIX filesystem expectations. It concludes that Open Atomic Ethernet&#x27;s bilateral, reversible transactional semantics provide a structural foundation to resolve these issues by aligning protocol behavior with physical reality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] Linear Reservoir: A Diagonalization-Based Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [diagonalization, eigenvalue decomposition, eigenbasis transformation, computational complexity reduction, linear echo state networks]</li>
<li class=""><strong>authors:</strong> Romain de Coudenhove, Yannis Bendi-Ouis, Anthony Strock, Xavier Hinaut</li>
<li class=""><strong>institution:</strong> ENS PSL, Inria, LaBRI, IMN, Stanford University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.19802" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.19802</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a diagonalization-based optimization for Linear Echo State Networks that reformulates reservoir dynamics in the eigenbasis of the recurrent matrix, reducing the per-step computational complexity from O(N²) to O(N). The proposed methods preserve predictive accuracy while offering significant computational speedups, suggesting a paradigm shift towards direct eigenvalue selection for linear ESNs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260224] A Risk-Aware UAV-Edge Service Framework for Wildfire Monitoring and Emergency Response</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [fire-history-weighted clustering, QoS-aware edge assignment, 2-opt route optimization, dynamic emergency rerouting, adaptive fleet sizing]</li>
<li class=""><strong>authors:</strong> Yulun Huang, Zhiyu Wang, Rajkumar Buyya</li>
<li class=""><strong>institution:</strong> The University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.19742" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.19742</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an integrated UAV-edge service framework that co-optimizes route planning, fleet sizing, and edge service provisioning using risk-aware clustering and dynamic rerouting for wildfire monitoring. Experiments show the framework significantly reduces response time, energy consumption, and fleet size compared to baseline methods, while its emergency mechanism meets strict deadlines with minimal operational disruption.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 42</strong></p>
<ul>
<li class="">[arXiv260224] Learning to Detect Language Model Training Data via Active Reconstruction <a href="https://arxiv.org/pdf/2602.19020" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Toward AI Autonomous Navigation for Mechanical Thrombectomy using Hierarchical Modular Multi-agent Reinforcement Learning (HM-MARL) <a href="https://arxiv.org/pdf/2602.18663" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] 1D-Bench: A Benchmark for Iterative UI Code Generation with Visual Feedback in Real-World <a href="https://arxiv.org/pdf/2602.18548" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Issues with Measuring Task Complexity via Random Policies in Robotic Tasks <a href="https://arxiv.org/pdf/2602.18856" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] TAG: Thinking with Action Unit Grounding for Facial Expression Recognition <a href="https://arxiv.org/pdf/2602.18763" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Learning to Remember: End-to-End Training of Memory Agents for Long-Context Reasoning <a href="https://arxiv.org/pdf/2602.18493" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] DeepInterestGR: Mining Deep Multi-Interest Using Multi-Modal LLMs for Generative Recommendation <a href="https://arxiv.org/pdf/2602.18907" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] VariBASed: Variational Bayes-Adaptive Sequential Monte-Carlo Planning for Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2602.18857" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] TPRU: Advancing Temporal and Procedural Understanding in Large Multimodal Models <a href="https://arxiv.org/pdf/2602.18884" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Hierarchical Reward Design from Language: Enhancing Alignment of Agent Behavior with Human Specifications <a href="https://arxiv.org/pdf/2602.18582" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] FineRef: Fine-Grained Error Reflection and Correction for Long-Form Generation with Citations <a href="https://arxiv.org/pdf/2602.18437" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] In-Context Planning with Latent Temporal Abstractions <a href="https://arxiv.org/pdf/2602.18694" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Task-Aware Exploration via a Predictive Bisimulation Metric <a href="https://arxiv.org/pdf/2602.18724" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Adaptive Time Series Reasoning via Segment Selection <a href="https://arxiv.org/pdf/2602.18645" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] MagicAgent: Towards Generalized Agent Planning <a href="https://arxiv.org/pdf/2602.19000" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] HONEST-CAV: Hierarchical Optimization of Network Signals and Trajectories for Connected and Automated Vehicles with Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2602.18740" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Adaptive Problem Generation via Symbolic Representations <a href="https://arxiv.org/pdf/2602.19187" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] How to Allocate, How to Learn? Dynamic Rollout Allocation and Advantage Modulation for Policy Optimization <a href="https://arxiv.org/pdf/2602.19208" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Characterizing MARL for Energy Control: A Multi-KPI Benchmark on the CityLearn Environment <a href="https://arxiv.org/pdf/2602.19223" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Robust Exploration in Directed Controller Synthesis via Reinforcement Learning with Soft Mixture-of-Experts <a href="https://arxiv.org/pdf/2602.19244" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] DGPO: RL-Steered Graph Diffusion for Neural Architecture Generation <a href="https://arxiv.org/pdf/2602.19261" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] ALPACA: A Reinforcement Learning Environment for Medication Repurposing and Treatment Optimization in Alzheimer&#x27;s Disease <a href="https://arxiv.org/pdf/2602.19298" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics <a href="https://arxiv.org/pdf/2602.19313" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Learning to Reason for Multi-Step Retrieval of Personal Context in Personalized Question Answering <a href="https://arxiv.org/pdf/2602.19317" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Soft Sequence Policy Optimization: Bridging GMPO and SAPO <a href="https://arxiv.org/pdf/2602.19327" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] LLMs Can Learn to Reason Via Off-Policy RL <a href="https://arxiv.org/pdf/2602.19362" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Stable Deep Reinforcement Learning via Isotropic Gaussian Representations <a href="https://arxiv.org/pdf/2602.19373" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] IR<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span>: Contrastive Inverse Reinforcement Learning for Interpretable Detection and Mitigation of Reward Hacking <a href="https://arxiv.org/pdf/2602.19416" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] RAmmStein: Regime Adaptation in Mean-reverting Markets with Stein Thresholds -- Optimal Impulse Control in Concentrated AMMs <a href="https://arxiv.org/pdf/2602.19419" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] SenTSR-Bench: Thinking with Injected Knowledge for Time-Series Reasoning <a href="https://arxiv.org/pdf/2602.19455" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Cost-Aware Diffusion Active Search <a href="https://arxiv.org/pdf/2602.19538" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Advantage-based Temporal Attack in Reinforcement Learning <a href="https://arxiv.org/pdf/2602.19582" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind&#x27;s Adaptive Agent <a href="https://arxiv.org/pdf/2602.19837" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning <a href="https://arxiv.org/pdf/2602.19895" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning <a href="https://arxiv.org/pdf/2602.19917" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling <a href="https://arxiv.org/pdf/2602.19919" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Sparse Masked Attention Policies for Reliable Generalization <a href="https://arxiv.org/pdf/2602.19956" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] A Secure and Private Distributed Bayesian Federated Learning Design <a href="https://arxiv.org/pdf/2602.20003" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning <a href="https://arxiv.org/pdf/2602.20078" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models <a href="https://arxiv.org/pdf/2602.20117" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] LAD: Learning Advantage Distribution for Reasoning <a href="https://arxiv.org/pdf/2602.20132" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] AAVGen: Precision Engineering of Adeno-associated Viral Capsids for Renal Selective Targeting <a href="https://arxiv.org/pdf/2602.18915" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 27</strong></p>
<ul>
<li class="">[arXiv260224] Can Multimodal LLMs See Science Instruction? Benchmarking Pedagogical Reasoning in K-12 Classroom Videos <a href="https://arxiv.org/pdf/2602.18466" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] DeepInnovator: Triggering the Innovative Capabilities of LLMs <a href="https://arxiv.org/pdf/2602.18920" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] IDLM: Inverse-distilled Diffusion Language Models <a href="https://arxiv.org/pdf/2602.19066" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Vectorized Bayesian Inference for Latent Dirichlet-Tree Allocation <a href="https://arxiv.org/pdf/2602.18795" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] VariBASed: Variational Bayes-Adaptive Sequential Monte-Carlo Planning for Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2602.18857" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Habilis-<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">β</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span>: A Fast-Motion and Long-Lasting On-Device Vision-Language-Action Model <a href="https://arxiv.org/pdf/2602.18813" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Learning from Complexity: Exploring Dynamic Sample Pruning of Spatio-Temporal Training <a href="https://arxiv.org/pdf/2602.19113" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] ConfSpec: Efficient Step-Level Speculative Reasoning via Confidence-Gated Verification <a href="https://arxiv.org/pdf/2602.18447" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Exponential Convergence of (Stochastic) Gradient Descent for Separable Logistic Regression <a href="https://arxiv.org/pdf/2602.18946" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] InfEngine: A Self-Verifying and Self-Optimizing Intelligent Engine for Infrared Radiation Computing <a href="https://arxiv.org/pdf/2602.18985" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Task-Aware Exploration via a Predictive Bisimulation Metric <a href="https://arxiv.org/pdf/2602.18724" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] LLM-Assisted Replication for Quantitative Social Science <a href="https://arxiv.org/pdf/2602.18453" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] CORVET: A CORDIC-Powered, Resource-Frugal Mixed-Precision Vector Processing Engine for High-Throughput AIoT applications <a href="https://arxiv.org/pdf/2602.19268" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Taming Preconditioner Drift: Unlocking the Potential of Second-Order Optimizers for Federated Learning on Non-IID Data <a href="https://arxiv.org/pdf/2602.19271" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] CTS-Bench: Benchmarking Graph Coarsening Trade-offs for GNNs in Clock Tree Synthesis <a href="https://arxiv.org/pdf/2602.19330" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Laplacian Multi-scale Flow Matching for Generative Modeling <a href="https://arxiv.org/pdf/2602.19461" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Relational Feature Caching for Accelerating Diffusion Transformers <a href="https://arxiv.org/pdf/2602.19506" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Leap+Verify: Regime-Adaptive Speculative Weight Prediction for Accelerating Neural Network Training <a href="https://arxiv.org/pdf/2602.19580" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Rules or Weights? Comparing User Understanding of Explainable AI Techniques with the Cognitive XAI-Adaptive Model <a href="https://arxiv.org/pdf/2602.19620" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Iconographic Classification and Content-Based Recommendation for Digitized Artworks <a href="https://arxiv.org/pdf/2602.19698" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Hexagon-MLIR: An AI Compilation Stack For Qualcomm&#x27;s Neural Processing Units (NPUs) <a href="https://arxiv.org/pdf/2602.19762" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Fully Convolutional Spatiotemporal Learning for Microstructure Evolution Prediction <a href="https://arxiv.org/pdf/2602.19915" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] DP-FedAdamW: An Efficient Optimizer for Differentially Private Federated Large Models <a href="https://arxiv.org/pdf/2602.19945" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] A Secure and Private Distributed Bayesian Federated Learning Design <a href="https://arxiv.org/pdf/2602.20003" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Adaptation to Intrinsic Dependence in Diffusion Language Models <a href="https://arxiv.org/pdf/2602.20126" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] AAVGen: Precision Engineering of Adeno-associated Viral Capsids for Renal Selective Targeting <a href="https://arxiv.org/pdf/2602.18915" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260224] Kaiwu-PyTorch-Plugin: Bridging Deep Learning and Photonic Quantum Computing for Energy-Based Models and Active Sample Selection <a href="https://arxiv.org/pdf/2602.19114" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2026-02-26">2026-02-26<a href="#2026-02-26" class="hash-link" aria-label="Direct link to 2026-02-26" title="Direct link to 2026-02-26" translate="no">​</a></h2>
<p><strong>cs.DC total: 16</strong></p>
<ul>
<li class="">
<p><strong>[arXiv260226] Lamport&#x27;s Arrow of Time: The Category Mistake in Logical Clocks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems theory], [logical clocks, happens-before relation, directed acyclic graph (DAG), mutual information conservation, indefinite causal order, CAP theorem, Fischer-Lynch-Paterson]</li>
<li class=""><strong>authors:</strong> Paul Borrill</li>
<li class=""><strong>institution:</strong> DÆDÆLUS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.21730" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.21730</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper critiques Lamport&#x27;s logical clocks, arguing they embed a &quot;forward-in-time-only&quot; assumption that conflates epistemic ordering with an ontic claim of global causal acyclicity. It traces this conflation through impossibility results and modern physics, which only permits local causal structure. The authors propose mutual information conservation as a more fundamental primitive for distributed consistency than temporal precedence.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [KV-Cache, disaggregated architecture, RDMA, dual-path loading, global scheduler]</li>
<li class=""><strong>authors:</strong> Yongtong Wu, Shaoyuan Chen, Yinmin Zhong, Rilin Huang, Yixuan Tan, Wentao Zhang, Liyue Zhang, Shangyan Zhou, Yuxuan Liu, Shunfeng Zhou, Mingxing Zhang, Xin Jin, Panpan Huang</li>
<li class=""><strong>institution:</strong> Peking University, Tsinghua University, DeepSeek-AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.21548" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.21548</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces DualPath, a system that breaks the KV-Cache storage bandwidth bottleneck in agentic LLM inference by enabling a novel storage-to-decode loading path alongside the traditional storage-to-prefill path, combined with a global scheduler. It demonstrates that this approach significantly improves both offline and online inference throughput without violating service-level objectives.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] Type-Based Enforcement of Non-Interference for Choreographic Programming</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [programming languages, security], [type system, non-interference, choreographic programming, constraint generation, program-counter discipline]</li>
<li class=""><strong>authors:</strong> Marco Bertoni, Saverio Giallorenzo, Marco Peressotti</li>
<li class=""><strong>institution:</strong> Inria Paris, Université di Bologna, Inria Sophia Antipolis, University of Southern Denmark</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.21630" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.21630</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a policy-parametric type system for choreographic programming to enforce information flow security. The method handles both explicit and implicit flows via a program-counter discipline and supports recursive procedures through constraint-based context reconstruction. The main conclusion is a proof of termination-insensitive non-interference, ensuring that secret data is not leaked to public observers in distributed protocols.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] Multi-Layer Scheduling for MoE-Based LLM Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [multi-layer scheduling, mixture-of-experts (MoE), Shortest-Job-First (SJF), priority-aware aging, load-aware dispatching, KV cache, expert hotspots]</li>
<li class=""><strong>authors:</strong> Yifan Sun, Gholamreza Haffar, Minxian Xu, Rajkumar Buyya, Adel N. Toosi</li>
<li class=""><strong>institution:</strong> The University of Melbourne, Monash University, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.21626" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.21626</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a multi-layer scheduling framework for efficient Mixture-of-Experts (MoE) based LLM serving, operating at the request, engine, and expert levels. It demonstrates that this approach outperforms the vLLM framework, achieving significant reductions in both Time To First Token and Time-Per-Output-Token latency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] JSAM: Privacy Straggler-Resilient Joint Client Selection and Incentive Mechanism Design in Differentially Private Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [differential privacy, federated learning, incentive mechanism design, client selection, Bayesian-optimal framework]</li>
<li class=""><strong>authors:</strong> Ruichen Xu, Ying-Jun Angela Zhang, Jianwei Huang</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Shenzhen Institute of Artificial Intelligence and Robotics for Society</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.21844" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.21844</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes JSAM, a framework that jointly optimizes client selection and privacy compensation in differentially private federated learning to maximize training effectiveness under a budget. It shows that servers should preferentially select privacy-tolerant clients and reveals that clients with minimal privacy sensitivity may incur the highest cumulative costs due to frequent participation. Evaluations demonstrate that JSAM achieves up to 15% higher test accuracy compared to unbiased selection mechanisms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] General Convex Agreement with Near-Optimal Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed consensus], [convex agreement, Byzantine agreement, extractor graphs, deterministic committees, communication complexity, Helly number]</li>
<li class=""><strong>authors:</strong> Marc Dufay, Diana Ghinea, Anton Paramonov</li>
<li class=""><strong>institution:</strong> ETH Zurich, Lucerne University of Applied Sciences and Arts</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.21411" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.21411</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents deterministic synchronous Convex Agreement protocols for abstract convexity spaces, using extractor graphs to assign parties to committees resilient against adaptive adversaries. It achieves near-optimal communication complexity and round complexity, with resilience depending on the Helly number of the convexity space and whether input length bounds are known.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] Make Every Draft Count: Hidden State based Speculative Decoding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative decoding, hidden state reuse, auto-regressive hidden states, token tree, draft model]</li>
<li class=""><strong>authors:</strong> Yuetao Chen, Xuliang Wang, Xinzhou Zheng, Ming Li, Peng Wang, Hong Xu</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, University of Waterloo, University of Science and Technology of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.21224" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.21224</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel speculative decoding system that reuses the hidden states from discarded draft tokens to improve computational efficiency. The core method involves performing auto-regressive prediction at the hidden state level and injecting token information later, enabling the construction of high-quality draft token trees from verification failures. The system achieves up to 3.3x speedup compared to standard speculative decoding by transforming wasted computation into reusable tokens.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] PiPNN: Ultra-Scalable Graph-Based Nearest Neighbor Indexing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [graph-based indexing, approximate nearest neighbor search, HashPrune, beam search, dense matrix multiplication, online pruning]</li>
<li class=""><strong>authors:</strong> Tobias Rubel, Richard Wen, Laxman Dhulipala, Lars Gottesbüren, Rajesh Jayaram, Jakub Łącki</li>
<li class=""><strong>institution:</strong> University of Maryland, Google Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.21247" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.21247</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces PiPNN, a new graph construction algorithm for approximate nearest neighbor search that uses a core component called HashPrune to dynamically prune edges and perform bulk distance comparisons. This method avoids the search bottleneck of traditional graph-based indexes like HNSW and Vamana, enabling much faster construction. The authors demonstrate that PiPNN builds high-quality indexes up to 12.9x faster than prior methods and can construct billion-scale indexes in under 20 minutes on a single machine.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] Epoch-based Optimistic Concurrency Control in Geo-replicated Databases</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed databases], [epoch-based replication, optimistic concurrency control, deterministic re-execution, conflict graph, maximum weight independent set, multi-leader replication]</li>
<li class=""><strong>authors:</strong> Yunhao Mao, Harunari Takata, Michail Bachras, Yuqiu Zhang, Shiquan Zhang, Gengrui Zhang, Hans-Arno Jacobsen</li>
<li class=""><strong>institution:</strong> University of Toronto, Keio University, Concordia University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.21566" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.21566</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Minerva, a geo-replicated database system that uses an epoch-based optimistic concurrency control protocol with deterministic re-execution and a conflict graph algorithm to resolve transaction conflicts. It decouples data propagation from commitment to enable high concurrency in multi-leader settings. The evaluation shows it significantly outperforms existing systems in throughput under high latency and scalability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal training], [dynamic hybrid parallelism, non-power-of-two parallelism, polynomial-time algorithm, data parallelism, tensor parallelism, pipeline parallelism, sequence parallelism]</li>
<li class=""><strong>authors:</strong> Yifan Niu, Han Xiao, Dongyi Liu, Wei Zhou, Jia Li</li>
<li class=""><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou), Huawei Technologies Co., Ltd., The Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.21788" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.21788</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Dynamic Hybrid Parallelism (DHP), a method that adaptively reconfigures communication groups and parallelism degrees during Multimodal Large Language Model (MLLM) training to handle heterogeneous data efficiently. It introduces a polynomial-time algorithm to generate near-optimal parallelism strategies with minimal overhead. The results show that DHP significantly outperforms existing frameworks like Megatron-LM and DeepSpeed, achieving up to 1.36x speedup in training throughput while maintaining near-linear scaling efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] Energy Efficient Federated Learning with Hyperdimensional Computing over Wireless Communication Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hyperdimensional computing, differential privacy, resource allocation, alternating optimization, energy efficiency]</li>
<li class=""><strong>authors:</strong> Yahao Ding, Yinchao Yang, Jiaxiang Wang, Zhaohui Yang, Dusit Niyato, Zhu Han, Mohammad Shikh-Bahaei</li>
<li class=""><strong>institution:</strong> King&#x27;s College London, Zhejiang University, Nanyang Technological University, University of Houston, Kyung Hee University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.21949" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.21949</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a federated learning framework that uses hyperdimensional computing for lightweight local training and applies differential privacy for security, jointly optimizing resource allocation to minimize total energy consumption. The method significantly reduces energy usage and communication rounds compared to neural network baselines while maintaining model accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] PASTA: A Modular Program Analysis Tool Framework for Accelerators</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [performance analysis, profiling, modular framework, GPU-accelerated backend, deep learning workloads]</li>
<li class=""><strong>authors:</strong> Mao Lin, Hyeran Jeon, Keren Zhou</li>
<li class=""><strong>institution:</strong> University of California, Merced, George Mason University, OpenAI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22103" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22103</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents PASTA, a modular and low-overhead program analysis tool framework for hardware accelerators. It abstracts low-level profiling APIs and deep learning frameworks to provide a unified interface for performance analysis. The evaluation shows PASTA offers detailed insights with significantly lower overhead than conventional tools, making it suitable for modern accelerator environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [checkpointing, layer-wise tailoring, selective checkpointing, checkpoint merging]</li>
<li class=""><strong>authors:</strong> Minqiu Sun, Xin Huang, Luanzheng Guo, Nathan R. Tallent, Kento Sato, Dong Dai</li>
<li class=""><strong>institution:</strong> University of Delaware, RIKEN Center for Computational Science, Pacific Northwest National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22158" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22158</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes LLMTailor, a checkpoint-merging framework that selectively saves only the layers of a large language model that have undergone significant updates, rather than the entire model state. This method significantly reduces checkpoint storage size and I/O time while maintaining model quality, as demonstrated in evaluations with models like Llama3.1-8B and Qwen2.5-7B.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] Hybrid Consensus with Quantum Sybil Resistance</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [quantum cryptography, consensus protocols], [quantum position verification, hybrid consensus, Sybil resistance, Proof-of-Work, Proof-of-Stake]</li>
<li class=""><strong>authors:</strong> Dar Gilboa, Siddhartha Jain, Or Sattath</li>
<li class=""><strong>institution:</strong> Google Quantum AI, University of Texas at Austin, Ben-Gurion University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22195" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22195</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a decentralized consensus protocol that uses quantum position verification as a Sybil resistance mechanism, combining it with classical hybrid consensus. The method improves energy efficiency compared to Proof-of-Work-based hybrid protocols and offers faster confirmation times while avoiding the compounding wealth issue of Proof-of-Stake. The protocol is secure in the standard model and includes a spam prevention mechanism in the Random Oracle model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] IOAgent: Democratizing Trustworthy HPC I/O Performance Diagnosis Capability via LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [RAG, tree-based merger, module-based pre-processor, Darshan, TraceBench]</li>
<li class=""><strong>authors:</strong> Chris Egersdoerfer, Arnav Sareen, Jean Luca Bez, Suren Byna, Dongkuan, Dong Dai</li>
<li class=""><strong>institution:</strong> University of Delaware, University of North Carolina at Charlotte, Lawrence Berkeley National Laboratory, The Ohio State University, North Carolina State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22017" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22017</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes IOAgent, a tool that uses LLMs to automate the diagnosis of I/O performance issues in HPC systems by analyzing Darshan trace files. It integrates a pre-processor, a RAG-based knowledge module, and a tree-based merger to provide accurate, referenced diagnoses. The evaluation shows it matches or outperforms existing tools and works with various LLMs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260226] A task-based data-flow methodology for programming heterogeneous systems with multiple accelerator APIs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [task-based data-flow, directed acyclic graph (DAG), OpenMP/OmpSs-2, task-aware APIs (TA-libs), TACUDA, TASYCL, nOS-V, runtime interoperability]</li>
<li class=""><strong>authors:</strong> Aleix Boné, Alejandro Aguirre, David Álvarez, Pedro J. Martinez-Ferrer, Vicenç Beltran</li>
<li class=""><strong>institution:</strong> Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya - BarcelonaTech (UPC)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.21897" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.21897</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a task-based data-flow methodology using task-aware libraries (e.g., TACUDA, TASYCL) and the nOS-V threading library to seamlessly integrate multiple accelerator programming models (like CUDA and SYCL) within a single application. This approach manages computations as a DAG via a runtime system to avoid manual orchestration and thread contention. The results show that this method enables efficient and transparent use of diverse accelerators, mitigating performance interference and is applicable to current and future heterogeneous systems.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 17</strong></p>
<ul>
<li class="">[arXiv260226] On the Structural Non-Preservation of Epistemic Behaviour under Policy Transformation <a href="https://arxiv.org/pdf/2602.21424" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] CCCaption: Dual-Reward Reinforcement Learning for Complete and Correct Image Captioning <a href="https://arxiv.org/pdf/2602.21655" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning <a href="https://arxiv.org/pdf/2602.21420" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Self-Correcting VLA: Online Action Refinement via Sparse World Imagination <a href="https://arxiv.org/pdf/2602.21633" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Generalisation of RLHF under Reward Shift and Clipped KL Regularisation <a href="https://arxiv.org/pdf/2602.21765" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Hierarchical Lead Critic based Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2602.21680" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Two-Stage Active Distribution Network Voltage Control via LLM-RL Collaboration: A Hybrid Knowledge-Data-Driven Approach <a href="https://arxiv.org/pdf/2602.21715" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Alignment-Weighted DPO: A principled reasoning approach to improve safety alignment <a href="https://arxiv.org/pdf/2602.21346" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Tool-R0: Self-Evolving LLM Agents for Tool-Learning from Zero Data <a href="https://arxiv.org/pdf/2602.21320" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Evaluating the relationship between regularity and learnability in recursive numeral systems using Reinforcement Learning <a href="https://arxiv.org/pdf/2602.21720" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning <a href="https://arxiv.org/pdf/2602.21534" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] GradAlign: Gradient-Aligned Data Selection for LLM Reinforcement Learning <a href="https://arxiv.org/pdf/2602.21492" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] ImpRIF: Stronger Implicit Reasoning Leads to Better Complex Instruction Following <a href="https://arxiv.org/pdf/2602.21228" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Training Generalizable Collaborative Agents via Strategic Risk Aversion <a href="https://arxiv.org/pdf/2602.21515" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Distill and Align Decomposition for Enhanced Claim Verification <a href="https://arxiv.org/pdf/2602.21857" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents <a href="https://arxiv.org/pdf/2602.22124" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual <a href="https://arxiv.org/pdf/2602.22146" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 8</strong></p>
<ul>
<li class="">[arXiv260226] SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks <a href="https://arxiv.org/pdf/2602.21307" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Error-awareness Accelerates Active Automata Learning <a href="https://arxiv.org/pdf/2602.21674" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Excitation: Momentum For Experts <a href="https://arxiv.org/pdf/2602.21798" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] AngelSlim: A more accessible, comprehensive, and efficient toolkit for large model compression <a href="https://arxiv.org/pdf/2602.21233" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Asymptotically Fast Clebsch-Gordan Tensor Products with Vector Spherical Harmonics <a href="https://arxiv.org/pdf/2602.21466" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] The Error of Deep Operator Networks Is the Sum of Its Parts: Branch-Trunk and Mode Error Decompositions <a href="https://arxiv.org/pdf/2602.21910" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Counterdiabatic Hamiltonian Monte Carlo <a href="https://arxiv.org/pdf/2602.21272" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260226] Towards single-shot coherent imaging via overlap-free ptychography <a href="https://arxiv.org/pdf/2602.21361" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2026-02-27">2026-02-27<a href="#2026-02-27" class="hash-link" aria-label="Direct link to 2026-02-27" title="Direct link to 2026-02-27" translate="no">​</a></h2>
<p><strong>cs.DC total: 24</strong></p>
<ul>
<li class="">
<p><strong>[arXiv260227] Fault-tolerant Reduce and Allreduce operations based on correction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [fault-tolerant collective communication], [reduce, allreduce, broadcast, gossip, tree-based communication, correction algorithm, fault-tolerance]</li>
<li class=""><strong>authors:</strong> Martin Kuettler, Hermann Haertig</li>
<li class=""><strong>institution:</strong> TU Dresden</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22445</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a fault-tolerant Reduce algorithm that uses a correction-like communication phase followed by a tree-based phase to tolerate process failures. The semantics of the algorithm are provided and proven. The approach is then extended to create a fault-tolerant Allreduce operation by combining the proposed Reduce with a Broadcast.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] AdapTBF: Decentralized Bandwidth Control via Adaptive Token Borrowing for HPC Storage</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC storage systems], [Token Bucket Filter (TBF), adaptive borrowing, decentralized bandwidth control, Lustre file system]</li>
<li class=""><strong>authors:</strong> Md Hasanur Rashid, Dong Dai</li>
<li class=""><strong>institution:</strong> University of Delaware</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22409" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22409</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AdapTBF, a decentralized bandwidth control method for HPC storage that extends the Token Bucket Filter with an adaptive borrowing and lending mechanism. It is implemented in the Lustre file system and evaluated with synthetic workloads. The results show that AdapTBF effectively manages I/O bandwidth to ensure fairness and high storage utilization, even under extreme conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] DIAL: Decentralized I/O AutoTuning via Learned Client-side Local Metrics for Parallel File System</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [decentralized autotuning, client-side local metrics, machine learning, parallel file system, I/O performance]</li>
<li class=""><strong>authors:</strong> Md Hasanur Rashid, Xinyi Li, Youbiao He, Forrest Sheng Bao, Dong Dai</li>
<li class=""><strong>institution:</strong> University of Delaware, Iowa State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22392" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22392</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DIAL, a decentralized I/O autotuning framework for parallel file systems that uses machine learning models on each client to make configuration decisions based solely on locally observable metrics. This approach avoids the overhead of global pattern detection and enables timely, collective tuning decisions. The method aims to achieve better global I/O performance for applications by reacting dynamically to system changes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] From Prompts to Performance: Evaluating LLMs for Task-based Parallel Code Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [OpenMP Tasking, C++ Standard Parallelism, HPX, task-based parallel programming, code generation, prompting strategies]</li>
<li class=""><strong>authors:</strong> Linus Bantel, Moritz Strack, Alexander Strack, Dirk Pflüger</li>
<li class=""><strong>institution:</strong> University of Stuttgart</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22240" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22240</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper evaluates the ability of Large Language Models to generate correct and scalable task-based parallel code from different prompt types (natural language, sequential code, pseudo code) for frameworks like OpenMP, C++ standard parallelism, and HPX. It finds that LLMs exhibit both strengths and weaknesses depending on the problem complexity and framework, highlighting implications for their use in high-performance and scientific computing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] CARAT: Client-Side Adaptive RPC and Cache Co-Tuning for Parallel File Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [autotuning, machine learning, client-side tuning, RPC, caching, parallel file systems, Lustre]</li>
<li class=""><strong>authors:</strong> Md Hasanur Rashid, Nathan R. Tallent, Forrest Sheng Bao, Dong Dai</li>
<li class=""><strong>institution:</strong> University of Delaware, Pacific Northwest National Laboratory, Iowa State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22423" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22423</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents CARAT, an ML-guided framework for online co-tuning of client-side RPC and caching parameters in parallel file systems using only locally observable metrics. It enables each client to make independent tuning decisions in response to dynamic I/O behaviors and system states. Evaluation shows CARAT can achieve up to 3x performance improvement over default configurations, demonstrating its effectiveness and scalability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] veScale-FSDP: Flexible and High-Performance FSDP at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [Fully Sharded Data Parallel (FSDP), ZeRO, RaggedShard, block-wise quantization, non-element-wise optimizers, Shampoo, Muon, structure-aware planning]</li>
<li class=""><strong>authors:</strong> Zezhou Wang, Youjie Li, Zhiqi Lin, Jiacheng Yang, Cong Xie, Guanyu Feng, Zheng Zhong, Ziyue Huang, Hongyu Zhu, Zhi Zhang, Yanghua Peng, Xin Liu</li>
<li class=""><strong>institution:</strong> ByteDance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22437" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22437</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces veScale-FSDP, a redesigned FSDP system that uses a flexible sharding format called RaggedShard and a structure-aware planning algorithm to support block-structured computations like block-wise quantization and non-element-wise optimizers. It achieves higher throughput and lower memory usage than existing FSDP systems while scaling efficiently to tens of thousands of GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] Energy Efficient Federated Learning with Hyperdimensional Computing (HDC)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, hyperdimensional computing, differential privacy, resource allocation, energy efficiency]</li>
<li class=""><strong>authors:</strong> Yahao Ding, Yinchao Yang, Jiaxiang Wang, Zhonghao Liu, Zhaohui Yang, Mingzhe Chen, Mohammad Shikh-Bahaei</li>
<li class=""><strong>institution:</strong> King&#x27;s College London, Zhejiang University, University of Miami</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22290" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22290</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a framework called FL-HDC-DP that combines Hyperdimensional Computing (HDC) for lightweight local training and Differential Privacy (DP) for secure model updates in federated learning. It minimizes total energy consumption by jointly optimizing the HDC dimension, transmit power, and CPU frequency. Simulation results show the framework achieves up to 83.3% energy reduction compared to baselines while maintaining high accuracy and faster convergence.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] GetBatch: Distributed Multi-Object Retrieval for ML Data Loading</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [GetBatch API, batched retrieval, object store, distributed execution, designated target coordination, streaming execution]</li>
<li class=""><strong>authors:</strong> Alex Aizman, Abhishek Gaikwad, Piotr Żelasko</li>
<li class=""><strong>institution:</strong> NVIDIA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22434" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22434</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces GetBatch, a new object store API that replaces thousands of individual GET requests for ML data loading with a single, deterministic, and fault-tolerant batch retrieval operation. This method combines the sampling flexibility of random access with the throughput of sequential I/O by internally fetching objects concurrently from across a storage cluster. The evaluation shows it achieves up to 15x throughput improvement and significantly reduces batch retrieval and per-object tail latency in production training workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] CCCL: Node-Spanning GPU Collectives with CXL Memory Pooling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [CXL, collective communication, memory pooling, GPU, NCCL, RDMA, InfiniBand]</li>
<li class=""><strong>authors:</strong> Dong Xu, Han Meng, Xinyu Chen, Dengcheng Zhu, Wei Tang, Fei Liu, Liguang Xie, Wu Xiang, Rui Shi, Yue Li, Henry Hu, Hui Zhang, Jianping Jiang, Dong Li</li>
<li class=""><strong>institution:</strong> University of California, Merced, Zhejiang University, ByteDance, Xconn-Tech</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22457" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22457</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CCCL, a collective communication library that leverages a CXL shared memory pool to enable cross-node GPU operations, eliminating the need for traditional RDMA-based networking. It addresses challenges in synchronization and data interleaving for this memory-centric approach. The evaluation shows CCCL achieves performance improvements over InfiniBand for operations like AllGather and Broadcast and can speed up LLM training while reducing hardware costs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] Engineered Simultaneity: The Physical Impossibility of Consolidated Price Discovery Across Spacelike-Separated Exchanges</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems, financial regulation], [engineered simultaneity, special relativity, frame-dependence, spacelike-separation, NBBO, latency arbitrage, causal precedence]</li>
<li class=""><strong>authors:</strong> Paul Borrill</li>
<li class=""><strong>institution:</strong> DAEDAELUS</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22350" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22350</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper applies concepts from special relativity and distributed systems theory to analyze the U.S. National Best Bid and Offer (NBBO) regulation. It demonstrates that the NBBO relies on an &quot;engineered simultaneity&quot; that is physically impossible across spacelike-separated exchanges, leading to frame-dependent price ordering. This inherent latency allows high-frequency traders to exploit information asymmetries, extracting billions annually from other market participants.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] FuxiShuffle: An Adaptive and Resilient Shuffle Service for Distributed Data Processing on Alibaba Cloud</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed data processing], [adaptive shuffle mode selection, progress-aware scheduling, multi-replica failover, incremental recovery]</li>
<li class=""><strong>authors:</strong> Yuhao Lin, Zhipeng Tang, Jiayan Tong, Junqing Xiao, Bin Lu, Yuhang Li, Chao Li, Zhiguo Zhang, Junhua Wang, Hao Luo, James Cheng, Chuang Hu, Jiawei Jiang, Xiao Yan</li>
<li class=""><strong>institution:</strong> Wuhan University, Alibaba Cloud, The Chinese University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22580" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22580</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents FuxiShuffle, an adaptive and resilient shuffle service designed for Alibaba Cloud&#x27;s MaxCompute platform. It dynamically selects shuffle modes and employs proactive fault tolerance mechanisms like multi-replica failover and incremental recovery to improve efficiency. Experiments show it reduces job completion time and resource consumption compared to baseline systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] FLYING SERVING: On-the-Fly Parallelism Switching for Large Language Model Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [data parallelism, tensor parallelism, vLLM, KV cache, online reconfiguration, model serving]</li>
<li class=""><strong>authors:</strong> Shouwei Gao, Junqi Yin, Feiyi Wang, Wenqian Dong</li>
<li class=""><strong>institution:</strong> Oregon State University, Oak Ridge National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22593" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22593</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents Flying Serving, a system that enables on-the-fly switching between data and tensor parallelism for LLM serving without restarting workers. It uses a zero-copy model weights manager and a KV cache adaptor to virtualize state during reconfiguration. The system significantly improves performance under varying loads by dynamically adapting to traffic and request requirements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] Workload Buoyancy: Keeping Apps Afloat by Identifying Shared Resource Bottlenecks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [resource management], [buoyancy, workload orchestration, bottleneck detection, shared resource contention, noisy-neighbor effects, observability]</li>
<li class=""><strong>authors:</strong> Oliver Larsson, Thijs Metsch, Cristian Klein, Erik Elmroth</li>
<li class=""><strong>institution:</strong> Umeå University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22852" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22852</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces &quot;buoyancy,&quot; a novel abstraction that integrates application-level and system-level metrics to holistically characterize workload performance and identify shared resource bottlenecks in multi-tenant systems. The method provides a 19.3% better indication of bottlenecks compared to traditional heuristics and enables more informed scheduling and optimization decisions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] Distributed LLM Pretraining During Renewable Curtailment Windows: A Feasibility Study</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [federated learning, curtailment-aware scheduling, geo-distributed training, elastic provisioning]</li>
<li class=""><strong>authors:</strong> Philipp Wiesner, Soeren Becker, Brett Cornick, Dominik Scheinert, Alexander Acker, Odej Kao</li>
<li class=""><strong>institution:</strong> Exalsius, Deep Science Ventures, TU Berlin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22760" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22760</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a system for distributed LLM pretraining that aligns with renewable energy curtailment windows, using federated learning to switch between local training and multi-site synchronization based on site availability. Preliminary results indicate that this approach maintains training quality while reducing operational carbon emissions to 5–12% of single-site baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] Tackling Privacy Heterogeneity in Differentially Private Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [differential privacy, federated learning, client selection, convex optimization, privacy heterogeneity, privacy budget]</li>
<li class=""><strong>authors:</strong> Ruichen Xu, Ying-Jun Angela Zhang, Jianwei Huang</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22633" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22633</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a privacy-aware client selection strategy for differentially private federated learning, formulated as a convex optimization problem, to address the challenge of heterogeneous privacy budgets among clients. It demonstrates that this approach can improve test accuracy by up to 10% on CIFAR-10 compared to conventional selection methods, highlighting the importance of adapting client selection to varying privacy requirements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] A Simple Distributed Deterministic Planar Separator</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed algorithms], [planar separator, deterministic algorithm, weight transfer, diameter, distributed computing]</li>
<li class=""><strong>authors:</strong> Yaseen Abd-Elhaleem, Michal Dory, Oren Weimann</li>
<li class=""><strong>institution:</strong> University of Haifa</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22916" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22916</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a simple deterministic distributed algorithm for finding a balanced separator of size O(D) in planar graphs. The key innovation is a straightforward method where each vertex transfers its weight to an arbitrary adjacent face, simplifying previous complex or randomized approaches. The algorithm runs in near-optimal Õ(D) rounds and enables the derandomization of several state-of-the-art distributed algorithms for planar graph problems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] Dynamic Hierarchical Birkhoff-von Neumann Decomposition for All-to-All GPU Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Birkhoff-von Neumann decomposition, dynamic frame sizing, load balancing, traffic matrix decomposition, all-to-all communication]</li>
<li class=""><strong>authors:</strong> Yen-Chieh Wu, Cheng-Shang Chang, Duan-Shin Lee, H. Jonathan Chao</li>
<li class=""><strong>institution:</strong> National Tsing Hua University, New York University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22756" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22756</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a dynamic hierarchical Birkhoff-von Neumann decomposition framework to schedule all-to-all communication in two-tier GPU clusters. It balances traffic within servers first and then applies a hierarchical decomposition to reduce complexity, integrating this with dynamic frame sizing for provable stability. Simulations show the method substantially reduces mean frame length, especially under skewed traffic.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] Accelerating Local LLMs on Resource-Constrained Edge Devices via Distributed Prompt Caching</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [distributed prompt caching, partial matching, bloom filter, KV cache, edge computing]</li>
<li class=""><strong>authors:</strong> Hiroki Matsutani, Naoki Matsuda, Naoto Sugiura</li>
<li class=""><strong>institution:</strong> Keio University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22812" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22812</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a distributed prompt caching method to accelerate local LLM inference on resource-constrained edge devices by sharing intermediate processing states across devices. It uses a Bloom-filter-based catalog to reduce communication overhead for state sharing. Experiments on a Raspberry Pi platform show significant reductions in Time to First Token (93.12%) and Time to Last Token (50.07%).</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] An Artificial Intelligence Framework for Joint Structural-Temporal Load Forecasting in Cloud Native Platforms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [microservice topology, attention computation, multi-objective regression, structural prior, multi-granularity fusion]</li>
<li class=""><strong>authors:</strong> Qingyuan Zhang</li>
<li class=""><strong>institution:</strong> Not specified in provided text</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22780" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22780</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a joint structural-temporal load forecasting framework for cloud native platforms, integrating a time-evolving service invocation graph with multivariate load sequences and using a unified sequence encoder with a structural prior in attention computation. It employs a multi-objective regression strategy for joint optimization across service and cluster levels. The framework&#x27;s effectiveness is validated through sensitivity analyses, supporting the necessity of multi-granularity fusion and structural injection for improved load prediction in cloud environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] RLHFless: Serverless Computing for Efficient RLHF</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [serverless computing, synchronous RLHF, cost-aware actor scaling, workload assignment, prefix pre-computation]</li>
<li class=""><strong>authors:</strong> Rui Wei, Hanfei Yu, Shubham Jain, Yogarajan Sivakumar, Devesh Tiwari, Jian Li, Seung-Jong Park, Hao Wang</li>
<li class=""><strong>institution:</strong> Stevens Institute of Technology, Northeastern University, Stony Brook University, Missouri University of Science &amp; Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.22718" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.22718</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces RLHFless, a scalable training framework for synchronous Reinforcement Learning from Human Feedback (RLHF) built on serverless computing. It adapts to dynamic resource demands, pre-computes shared prefixes, and uses a cost-aware scaling strategy to improve efficiency. Experiments show RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to state-of-the-art baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] LLMServingSim 2.0: A Unified Simulator for Heterogeneous and Disaggregated LLM Serving Infrastructure</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [system-level simulator, hardware-software co-design, heterogeneous accelerators, disaggregated serving, profile-based modeling, runtime loop, batching, offloading, power modeling]</li>
<li class=""><strong>authors:</strong> Jaehong Cho, Hyunmin Choi, Guseul Heo, Jongse Park</li>
<li class=""><strong>institution:</strong> KAIST</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.23036" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.23036</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents LLMServingSim 2.0, a unified system-level simulator that models runtime interactions between heterogeneous hardware and disaggregated software in LLM serving infrastructures. It embeds serving decisions and hardware behavior into a single runtime loop for interaction-aware analysis. The simulator is validated to reproduce key metrics with high accuracy, providing a practical tool for co-designing next-generation LLM serving systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] STELLAR: Storage Tuning Engine Leveraging LLM Autonomous Reasoning for High Performance Parallel File Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [large language models (LLMs), retrieval-augmented generation (RAG), multiagent design, autonomous agentic tuning, I/O performance tuning]</li>
<li class=""><strong>authors:</strong> Chris Egersdoerfer, Philip Carns, Shane Snyder, Robert Ross, Dong Dai</li>
<li class=""><strong>institution:</strong> University of Delaware, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.23220" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.23220</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes STELLAR, an autonomous storage tuner that uses large language models (LLMs) in a multiagent system to perform end-to-end tuning of parallel file systems. It integrates retrieval-augmented generation (RAG) and tool execution to analyze logs, select parameters, and iteratively optimize performance. The evaluation shows STELLAR can find near-optimal configurations within five attempts, making complex I/O tuning accessible with minimal manual effort.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] Exploiting network topology in brain-scale simulations of spiking neural networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [spiking neural networks, collective communication, synchronization, local-global hybrid communication, network topology mapping, performance profiling]</li>
<li class=""><strong>authors:</strong> Melissa Lober, Markus Diesmann, Susanne Kunkel</li>
<li class=""><strong>institution:</strong> Jülich Research Centre, RWTH Aachen University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.23274" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.23274</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a structure-aware mapping strategy for brain-scale spiking neural network simulations, exploiting the brain&#x27;s area-based organization to implement a local-global hybrid communication architecture that reduces synchronization overhead. This approach significantly improves performance by minimizing waiting times for the slowest compute node, challenging the traditional view that interconnect speed is the primary bottleneck. The work provides guidelines for energy-efficient simulations on conventional supercomputers and sets a higher benchmark for neuromorphic systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260227] A High-Throughput AES-GCM Implementation on GPUs for Secure, Policy-Based Access to Massive Astronomical Catalogs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [High-Performance Computing / Cryptography], [GPU acceleration, AES-GCM, parallel tree-reduction, GHASH, authenticated encryption, policy engine]</li>
<li class=""><strong>authors:</strong> Samuel Lemes-Perera, Miguel R. Alarcon, Pino Caballero-Gil, Miquel Serra-Ricart</li>
<li class=""><strong>institution:</strong> Light Bridges S.L., Universidad de La Laguna (ULL), Instituto Tecnológico y de Energías Renovables (ITER), Instituto de Astrofísica de Canarias (IAC)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2602.23067" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2602.23067</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a framework that combines a policy engine for access control with a GPU-accelerated AES-GCM implementation. The core method uses a parallel tree-reduction strategy to overcome the sequential bottleneck of the GHASH authentication, enabling high-throughput encryption. The main conclusion is that this approach provides a secure, efficient, and policy-based solution for managing access to massive astronomical catalogs during pre-publication phases.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 27</strong></p>
<ul>
<li class="">[arXiv260227] UpSkill: Mutual Information Skill Learning for Structured Response Diversity in LLMs <a href="https://arxiv.org/pdf/2602.22296" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation <a href="https://arxiv.org/pdf/2602.22215" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] SmartChunk Retrieval: Query-Aware Chunk Compression with Planning for Efficient Document RAG <a href="https://arxiv.org/pdf/2602.22225" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Learning Rewards, Not Labels: Adversarial Inverse Reinforcement Learning for Machinery Fault Detection <a href="https://arxiv.org/pdf/2602.22297" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Reinforcement-aware Knowledge Distillation for LLM Reasoning <a href="https://arxiv.org/pdf/2602.22495" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Space Syntax-guided Post-training for Residential Floor Plan Generation <a href="https://arxiv.org/pdf/2602.22507" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] A Mathematical Theory of Agency and Intelligence <a href="https://arxiv.org/pdf/2602.22519" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Agentic AI for Intent-driven Optimization in Cell-free O-RAN <a href="https://arxiv.org/pdf/2602.22539" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Multilingual Safety Alignment Via Sparse Weight Editing <a href="https://arxiv.org/pdf/2602.22554" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation <a href="https://arxiv.org/pdf/2602.22556" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] EvolveGen: Algorithmic Level Hardware Model Checking Benchmark Generation through Reinforcement Learning <a href="https://arxiv.org/pdf/2602.22609" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning <a href="https://arxiv.org/pdf/2602.22642" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising <a href="https://arxiv.org/pdf/2602.22650" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue <a href="https://arxiv.org/pdf/2602.22697" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning <a href="https://arxiv.org/pdf/2602.22703" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Same Words, Different Judgments: Modality Effects on Preference Alignment <a href="https://arxiv.org/pdf/2602.22710" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Generative Recommendation for Large-Scale Advertising <a href="https://arxiv.org/pdf/2602.22732" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Know What You Know: Metacognitive Entropy Calibration for Verifiable RL Reasoning <a href="https://arxiv.org/pdf/2602.22751" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] QSIM: Mitigating Overestimation in Multi-Agent Reinforcement Learning via Action Similarity Weighted Q-Learning <a href="https://arxiv.org/pdf/2602.22786" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Unleashing the Potential of Diffusion Models for End-to-End Autonomous Driving <a href="https://arxiv.org/pdf/2602.22801" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks <a href="https://arxiv.org/pdf/2602.22817" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning <a href="https://arxiv.org/pdf/2602.22963" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization <a href="https://arxiv.org/pdf/2602.23008" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Learning-based Multi-agent Race Strategies in Formula 1 <a href="https://arxiv.org/pdf/2602.23056" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive <a href="https://arxiv.org/pdf/2602.23239" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] A Model-Free Universal AI <a href="https://arxiv.org/pdf/2602.23242" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Physics Informed Viscous Value Representations <a href="https://arxiv.org/pdf/2602.23280" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 21</strong></p>
<ul>
<li class="">[arXiv260227] SQaLe: A Large Text-to-SQL Corpus Grounded in Real Schemas <a href="https://arxiv.org/pdf/2602.22223" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] GRAU: Generic Reconfigurable Activation Unit Design for Neural Network Hardware Accelerators <a href="https://arxiv.org/pdf/2602.22352" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models <a href="https://arxiv.org/pdf/2602.22500" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] LUMOS: Democratizing SciML Workflows with L0-Regularized Learning for Unified Feature and Parameter Adaptation <a href="https://arxiv.org/pdf/2602.22537" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Vectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators <a href="https://arxiv.org/pdf/2602.22647" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] dLLM: Simple Diffusion Language Modeling <a href="https://arxiv.org/pdf/2602.22661" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions <a href="https://arxiv.org/pdf/2602.22680" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement <a href="https://arxiv.org/pdf/2602.22681" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue <a href="https://arxiv.org/pdf/2602.22697" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] The AI Research Assistant: Promise, Peril, and a Proof of Concept <a href="https://arxiv.org/pdf/2602.22842" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design <a href="https://arxiv.org/pdf/2602.23092" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models <a href="https://arxiv.org/pdf/2602.23200" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction <a href="https://arxiv.org/pdf/2602.23214" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] LLM Novice Uplift on Dual-Use, In Silico Biology Tasks <a href="https://arxiv.org/pdf/2602.23329" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Toward Expert Investment Teams<!-- -->:A<!-- --> Multi-Agent LLM System with Fine-Grained Trading Tasks <a href="https://arxiv.org/pdf/2602.23330" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Utilizing LLMs for Industrial Process Automation <a href="https://arxiv.org/pdf/2602.23331" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Bitwise Systolic Array Architecture for Runtime-Reconfigurable Multi-precision Quantized Multiplication on Hardware Accelerators <a href="https://arxiv.org/pdf/2602.23334" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] FlashOptim: Optimizers for Memory Efficient Training <a href="https://arxiv.org/pdf/2602.23349" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] CryoNet.Refine: A One-step Diffusion Model for Rapid Refinement of Structural Models with Cryo-EM Density Map Restraints <a href="https://arxiv.org/pdf/2602.22263" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Advancing accelerator virtual beam diagnostics through latent evolution modeling: an integrated solution to forward, inverse, tuning, and UQ problems <a href="https://arxiv.org/pdf/2602.22618" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260227] Accelerated Online Risk-Averse Policy Evaluation in POMDPs with Theoretical Guarantees and Novel CVaR Bounds <a href="https://arxiv.org/pdf/2602.23073" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2026-03-02T03:24:49.000Z" itemprop="dateModified">Mar 2, 2026</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20260216-20260222"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20260216-20260222</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20260302-20260308"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20260302-20260308</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2026-02-23" class="table-of-contents__link toc-highlight">2026-02-23</a></li><li><a href="#2026-02-24" class="table-of-contents__link toc-highlight">2026-02-24</a></li><li><a href="#2026-02-26" class="table-of-contents__link toc-highlight">2026-02-26</a></li><li><a href="#2026-02-27" class="table-of-contents__link toc-highlight">2026-02-27</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>