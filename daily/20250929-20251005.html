<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20250929-20251005" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20250929-20251005 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20250929-20251005"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20250929-20251005 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-09-29"><meta data-rh="true" property="og:description" content="2025-09-29"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20250929-20251005"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250929-20251005" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250929-20251005" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20250929-20251005","item":"https://darkknight996.github.io/daily/20250929-20251005"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.cd79e544.js" defer="defer"></script>
<script src="/assets/js/main.394a41bc.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20250929-20251005</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20250929-20251005</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-29">2025-09-29<a href="#2025-09-29" class="hash-link" aria-label="Direct link to 2025-09-29" title="Direct link to 2025-09-29" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [FPGA acceleration, Vision GNNs, Dynamic Image Graph Construction, hardware optimization, parallel sorting]</li>
<li class=""><strong>authors:</strong> Anvitha Ramachandran, Dhruv Parikh, Viktor Prasanna</li>
<li class=""><strong>institution:</strong> University of Southern California</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25121v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25121v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a streaming FPGA accelerator for Dynamic Image Graph Construction in Vision GNNs, using on-chip buffers and parallel sorting to process features efficiently. This design minimizes memory traffic and achieves significant speedups over CPU and GPU baselines. The modular architecture supports various image resolutions and ViG model variants while maintaining high performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Context-Driven Performance Modeling for Causal Inference Operators on
Neural Processing Units</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [NPU performance analysis, causal inference operators, attention mechanisms, edge computing, hardware-aware optimization]</li>
<li class=""><strong>authors:</strong> Neelesh Gupta, Rakshith Jayanth, Dhruv Parikh, Viktor Prasanna</li>
<li class=""><strong>institution:</strong> University of Southern California</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25155v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25155v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts comprehensive performance analysis of causal inference operators including quadratic attention and sub-quadratic alternatives on Neural Processing Units. The study benchmarks various attention mechanisms and identifies that quadratic attention becomes memory-bound with cache inefficiency, while sub-quadratic models can become compute-bound on vector cores. These findings provide insights for co-designing hardware-aware models and optimization strategies for on-device long-context inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in
Long-Context LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [dynamic sparse attention, KV cache management, hierarchical storage, HBM-DRAM optimization, long-context LLM serving]</li>
<li class=""><strong>authors:</strong> Qihui Zhou, Peiqi Yin, Pengfei Zuo, James Cheng</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Huawei Cloud</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.24626v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.24626v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SparseServe introduces an efficient hierarchical HBM-DRAM management system for dynamic sparse attention in long-context LLM serving. It addresses KV cache fragmentation, HBM contention, and prefill demands through fragmentation-aware transfers, working-set-aware batching, and layer-segmented prefill. The system achieves significantly lower latency and higher throughput compared to state-of-the-art serving systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Intent-Driven Storage Systems: From Low-Level Tuning to High-Level
Understanding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [storage systems, intent-driven optimization, parameter configuration, workload adaptation]</li>
<li class=""><strong>authors:</strong> Shai Bergman, Won Wook Song, Lukas Cavigelli, Konstantin Berestizshevsky, Ke Zhou, Ji Zhang</li>
<li class=""><strong>institution:</strong> Huawei Zurich Research Center, Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15917v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15917v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Intent-Driven Storage Systems (IDSS) that use large language models to infer workload intent from unstructured signals and generate adaptive storage configurations. Experimental results show IDSS can improve IOPS by up to 2.45x through optimized caching and prefetching configurations. This demonstrates LLMs can effectively bridge application semantics with low-level system control when properly constrained within policy guardrails.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] HAPT: Heterogeneity-Aware Automated Parallel Training on Heterogeneous
Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [heterogeneous clusters, automated parallel training, inter-operator parallelism, 1F1B scheduler, computation-communication overlap]</li>
<li class=""><strong>authors:</strong> Antian Liang, Zhigang Zhao, Kai Zhang, Xuri Shi, Chuantao Li, Chunxiao Wang, Zhenying He, Yinan Jing, X. Sean Wang</li>
<li class=""><strong>institution:</strong> Fudan University, Shandong Computer Science Center (National Supercomputer Center in Jinan)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.24859v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.24859v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hapt introduces a fine-grained planner for inter-operator parallel strategy search and a heterogeneity-aware 1F1B scheduler to optimize distributed training on heterogeneous clusters. The framework achieves balanced workload distribution and maximizes computation-communication overlap with minimal memory overhead. Evaluation shows Hapt delivers 1.3x-1.6x higher performance than state-of-the-art training frameworks on heterogeneous clusters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts
Routing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [mixture-of-experts, load balancing, routing algorithm, inference optimization, plug-and-play]</li>
<li class=""><strong>authors:</strong> Rana Shahout, Colin Cai, Yilun Du, Minlan Yu, Michael Mitzenmacher</li>
<li class=""><strong>institution:</strong> Harvard University, University of California, Berkeley</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03293v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03293v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LASER is a plug-and-play inference-time routing algorithm that balances expert load in Mixture-of-Experts models by adapting to gate score distributions. It routes tokens to strongest experts when scores show clear preference, and expands to less-loaded experts when scores are uniform. The method improves load balancing, reduces latency, and increases throughput while maintaining accuracy, without requiring model retraining or fine-tuning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Asynchronous Policy Gradient Aggregation for Efficient Distributed
Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed reinforcement learning, policy gradient methods, asynchronous computation, communication efficiency, heterogeneous environments]</li>
<li class=""><strong>authors:</strong> Alexander Tyurin, Andrei Spiridonov, Varvara Rudenko</li>
<li class=""><strong>institution:</strong> Opta</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.24305v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.24305v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces two new algorithms, Rennala NIGT and Malenia NIGT, for asynchronous policy gradient aggregation in distributed reinforcement learning. Rennala NIGT improves computational and communication complexity in homogeneous settings, while Malenia NIGT handles heterogeneous environments with better theoretical guarantees. Experimental results show both methods significantly outperform prior approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] RServe: Overlapping Encoding and Prefill for Efficient LMM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multimodal models, inference serving, scheduling, parallelism, latency optimization]</li>
<li class=""><strong>authors:</strong> Tianyu Guo, Tianming Xu, Xianjie Chen, Junru Chen, Nong Xiao, Xianwei Zhang</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University, Rednote</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.24381v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.24381v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> REDServe proposes a novel scheduling strategy that overlaps multimodal encoding with language model computation within requests and balances computational loads across requests using schedulable tokens and token budgets. The system achieves up to 66% latency reduction and 109% throughput improvement compared to existing approaches. This demonstrates significant performance gains in large multimodal model inference serving through improved intra- and inter-request parallelism.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Experience Deploying Containerized GenAI Services at an HPC Center</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [containerization, HPC, Kubernetes, vLLM, converged computing, reproducibility]</li>
<li class=""><strong>authors:</strong> Angel M. Beltre, Jeff Ogden, Kevin Pedretti</li>
<li class=""><strong>institution:</strong> Sandia National Laboratories</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20603v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20603v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a converged computing architecture integrating HPC and Kubernetes platforms to deploy containerized GenAI services, specifically demonstrating Llama LLM deployment using vLLM inference server. This approach enables reproducible GenAI workload deployment across different computing environments using multiple container runtimes. The experience provides practical insights for HPC container community and guides future tool development for containerized AI service deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Zeppelin: Balancing Variable-length Workloads in Data Parallel Large
Model Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, attention optimization, communication efficiency, load balancing, sequence partitioning]</li>
<li class=""><strong>authors:</strong> Chang Chen, Tiancheng Chen, Jiangfei Duan, Qianchao Zhu, Zerui Wang, Qinghao Hu, Peng Sun, Xiuhong Li, Chao Yang, Torsten Hoefler</li>
<li class=""><strong>institution:</strong> Peking University, ETH Zurich, The Chinese University of Hong Kong, Shanghai AI Laboratory, Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21841v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21841v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Zeppelin introduces a hierarchical sequence partitioning method for attention modules, a routing layer for inter-node transfers, and a remapping layer for layout transformations between modules. The system addresses load imbalance in data-parallel LLM training with variable sequence lengths. Evaluations show Zeppelin achieves an average 2.80× speedup over state-of-the-art methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] A Scalable Distributed Framework for Multimodal GigaVoxel Image
Registration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [image registration, distributed computing, GPU acceleration, medical imaging, memory optimization]</li>
<li class=""><strong>authors:</strong> Rohit Jena, Vedant Zope, Pratik Chaudhari, James C. Gee</li>
<li class=""><strong>institution:</strong> University of Pennsylvania</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25044v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25044v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FFDP, a distributed framework with optimized non-GEMM kernels for large-scale multimodal image registration. It enables convolution-aware tensor sharding and achieves 6-7x speedup over existing methods while reducing memory consumption by 20-59%. The system successfully registered a 100-micron human brain MRI volume 570x larger than standard clinical data in about a minute using 8 GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in
Cloud Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [reinforcement learning, ant colony optimization, IoT data scheduling, cloud computing, resource optimization]</li>
<li class=""><strong>authors:</strong> Noor Islam S. Mohammad</li>
<li class=""><strong>institution:</strong> New York University Tandon School of Engineering</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.04334v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.04334v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid scheduling algorithm combining deep reinforcement learning and ant colony optimization for IoT data scheduling in cloud environments. The method achieves significant improvements in response time (18.4% reduction), resource utilization (12.7% improvement), and energy consumption (9.3% decrease) compared to existing approaches. The integration of model-free RL with swarm intelligence proves effective for scalable and energy-efficient IoT-cloud data scheduling.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-30">2025-09-30<a href="#2025-09-30" class="hash-link" aria-label="Direct link to 2025-09-30" title="Direct link to 2025-09-30" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] I Like To Move It -- Computation Instead of Data in the Brain</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [brain simulation, structural plasticity, connectome, spike exchange, computational efficiency, Barnes-Hut approximation]</li>
<li class=""><strong>authors:</strong> Fabian Czappa, Marvin Kaster, Felix Wolf</li>
<li class=""><strong>institution:</strong> RWTH Aachen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26193v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26193v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a new algorithm that reduces communication overhead in brain simulations by moving computation instead of data. This approach decreases connectivity update time by a factor of six and spike exchange time by over two orders of magnitude. The method enhances scalability for simulating large-scale neural networks with structural plasticity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Parallax: Efficient LLM Inference Service over Decentralized Environment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [decentralized inference, LLM serving, heterogeneous GPUs, two-phase scheduler, model allocation, pipeline selection]</li>
<li class=""><strong>authors:</strong> Chris Tong, Youhe Jiang, Gufeng Chen, Tianyi Zhao, Sibian Lu, Wenjie Qu, Eric Yang, Lynn Ai, Binhang Yuan</li>
<li class=""><strong>institution:</strong> Gradient, HKUST, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26182v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26182v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Parallax introduces a decentralized LLM serving system with a two-phase scheduler that allocates model layers across heterogeneous GPUs and dynamically selects execution pipelines. It optimizes latency and throughput under memory and network constraints. Evaluation shows consistent improvements over baselines, making volunteer computing practical for LLM inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Artificial Intelligence for Cost-Aware Resource Prediction in Big Data
Pipelines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [resource prediction, random forest, cloud computing, cost-aware autoscaling, big data pipelines]</li>
<li class=""><strong>authors:</strong> Harshit Goyal</li>
<li class=""><strong>institution:</strong> BITS Pilani</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05127v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05127v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses Random Forest regression on Google Borg cluster traces to predict resource utilization in big data pipelines. The model achieves high accuracy (R²≈0.99) in capturing non-linear workload-resource relationships. Results demonstrate AI-driven prediction enables cost-aware autoscaling while maintaining service quality in cloud environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Tuning the Tuner: Introducing Hyperparameter Optimization for
Auto-Tuning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hyperparameter optimization, auto-tuning, performance optimization, search space exploration]</li>
<li class=""><strong>authors:</strong> Floris-Jan Willemsen, Rob V. van Nieuwpoort, Ben van Werkhoven</li>
<li class=""><strong>institution:</strong> Leiden University, Netherlands eScience Center</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26300v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26300v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a method for hyperparameter tuning of optimization algorithms in auto-tuning systems, featuring a statistical evaluation approach and simulation mode that reduces tuning costs by two orders of magnitude. The research demonstrates that hyperparameter tuning improves auto-tuner performance by 94.8% on average and can be further optimized with meta-strategies for 204.7% improvement. The work establishes hyperparameter tuning as a powerful technique for advancing auto-tuning research and practice.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Efficient Construction of Large Search Spaces for Auto-Tuning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [auto-tuning, constraint satisfaction problem, search space construction, performance optimization, HPC]</li>
<li class=""><strong>authors:</strong> Floris-Jan Willemsen, Rob V. van Nieuwpoort, Ben van Werkhoven</li>
<li class=""><strong>institution:</strong> Leiden University, Netherlands eScience Center</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26253v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26253v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper reformulates search space construction for auto-tuning as a Constraint Satisfaction Problem (CSP) and develops an optimized CSP solver with runtime constraint translation. The approach achieves 4 orders of magnitude speedup over brute-force methods and 1-2 orders over chain-of-trees frameworks. This eliminates scalability barriers in auto-tuning, enabling exploration of previously unattainable problem scales.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] LoRAFusion: Efficient LoRA Fine-Tuning for LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [finetuning], [LoRA, kernel fusion, adaptive batching, distributed training, parameter-efficient fine-tuning]</li>
<li class=""><strong>authors:</strong> Zhanda Zhu, Qidong Su, Yaoyao Ding, Kevin Song, Shang Wang, Gennady Pekhimenko</li>
<li class=""><strong>institution:</strong> University of Toronto, Vector Institute, NVIDIA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.00206v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.00206v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LoRAFusion introduces kernel-level graph-splitting to fuse memory-bound operations and scheduling-level adaptive batching for multi-job fine-tuning. It achieves up to 1.96× speedup over Megatron-LM and 1.46× improvement over mLoRA while maintaining model quality. The system reduces redundant memory accesses and enables concurrent fine-tuning of multiple LoRA adapters on shared GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [datacenter lifecycle, TCO optimization, hardware refresh, power provisioning, cooling systems, networking, operational software]</li>
<li class=""><strong>authors:</strong> Jovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Ricardo Bianchini</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, Microsoft Azure Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26534v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26534v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a holistic framework for AI datacenter lifecycle management across building, hardware refresh, and operation stages. The framework co-optimizes decisions across power, cooling, networking, and refresh strategies while accounting for workload dynamics and hardware evolution. The proposed system reduces total cost of ownership by up to 40% compared to traditional approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Accelerating LLM Inference with Precomputed Query Storage</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [query caching, storage optimization, semantic matching, vector database, latency reduction]</li>
<li class=""><strong>authors:</strong> Jay H. Park, Youngju Cho, Choungsol Lee, Moonwook Oh, Euiseong Seo</li>
<li class=""><strong>institution:</strong> Samsung Electronics, Sungkyunkwan University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25919v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25919v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> StorInfer precomputes and stores query-response pairs offline, then uses semantic matching to retrieve stored responses during inference, bypassing GPU computation when possible. The system employs adaptive query generation techniques to maximize coverage while avoiding duplicates. Evaluation shows up to 17.3% latency reduction with no quality loss, demonstrating effective storage-assisted inference for predictable query distributions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] SysMoBench: Evaluating AI on Formally Modeling Complex Real-World
Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [formal modeling, system verification, TLA+, benchmark evaluation, distributed systems, concurrent systems]</li>
<li class=""><strong>authors:</strong> Qian Cheng, Ruize Tang, Emilie Ma, Finn Hackett, Peiyang He, Yiming Su, Ivan Beschastnikh, Yu Huang, Xiaoxing Ma, Tianyin Xu</li>
<li class=""><strong>institution:</strong> Nanjing University, University of British Columbia, University of Illinois Urbana-Champaign</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23130v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23130v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SysMoBench, a benchmark for evaluating AI&#x27;s ability to generate formal models of complex real-world systems using TLA+. The research demonstrates current limitations in LLMs&#x27; capacity to accurately model complete systems while providing automated evaluation metrics. The benchmark enables systematic assessment of AI capabilities in formal system modeling and opens new research directions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Efficient Distributed Training via Dual Batch Sizes and Cyclic
Progressive Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed training, batch size optimization, progressive learning, parameter server, image resolution]</li>
<li class=""><strong>authors:</strong> Kuan-Wei Lu, Ding-Yong Hong, Pangfeng Liu, Jan-Jan Wu</li>
<li class=""><strong>institution:</strong> Academia Sinica, National Taiwan University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26092v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26092v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid distributed training method combining dual batch sizes (using large batches for efficiency and small batches for generalization) with cyclic progressive learning (gradually increasing image resolution). Experimental results show the approach improves both accuracy and training efficiency on CIFAR-100 and ImageNet datasets compared to conventional methods.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-05">2025-10-05<a href="#2025-10-05" class="hash-link" aria-label="Direct link to 2025-10-05" title="Direct link to 2025-10-05" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Speculative Actions: A Lossless Framework for Faster Agentic Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [speculative execution, agent systems, parallel processing, latency optimization]</li>
<li class=""><strong>authors:</strong> Naimeng Ye, Arnav Ahuja, Georgios Liargkovas, Yunan Lu, Kostis Kaffes, Tianyi Peng</li>
<li class=""><strong>institution:</strong> Columbia University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.04371v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.04371v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes speculative actions, a lossless framework inspired by speculative execution in microprocessors and speculative decoding in LLM inference. The method predicts likely agent actions using faster models to enable parallel execution of multiple steps. Evaluation across gaming, e-commerce, and web search environments shows up to 55% next-action prediction accuracy and significant latency reductions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] From Patchwork to Network: A Comprehensive Framework for Demand Analysis
and Fleet Optimization of Urban Air Mobility</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [urban air mobility, fleet optimization, parallel simulation, demand forecasting, ground transportation integration]</li>
<li class=""><strong>authors:</strong> Xuan Jiang, Xuanyu Zhou, Yibo Zhao, Shangqing Cao, Jinhua Zhao, Mark Hansen, Raja Sengupta</li>
<li class=""><strong>institution:</strong> University of California, Berkeley</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.04186v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.04186v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces LPSim, a large-scale parallel simulation framework using multi-GPU computing to co-optimize Urban Air Mobility demand, fleet operations, and ground transportation interactions. The extended equilibrium search algorithm forecasts demand and determines optimal fleet composition. Results from the San Francisco Bay Area case study show over 20 minutes&#x27; travel time savings for 230,000 trips, but highlight critical dependence on ground access integration and dynamic scheduling.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-24T14:22:49.000Z" itemprop="dateModified">Oct 24, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20250922-20250928"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20250922-20250928</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20251006-20251012"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251006-20251012</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-09-29" class="table-of-contents__link toc-highlight">2025-09-29</a></li><li><a href="#2025-09-30" class="table-of-contents__link toc-highlight">2025-09-30</a></li><li><a href="#2025-10-05" class="table-of-contents__link toc-highlight">2025-10-05</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>