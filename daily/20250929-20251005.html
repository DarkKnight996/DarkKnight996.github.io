<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20250929-20251005" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20250929-20251005 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20250929-20251005"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20250929-20251005 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-09-29"><meta data-rh="true" property="og:description" content="2025-09-29"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20250929-20251005"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250929-20251005" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250929-20251005" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20250929-20251005","item":"https://darkknight996.github.io/daily/20250929-20251005"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.78e1c784.js" defer="defer"></script>
<script src="/assets/js/main.be5f6e7c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20250929-20251005</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20250929-20251005</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-29">2025-09-29<a href="#2025-09-29" class="hash-link" aria-label="Direct link to 2025-09-29" title="Direct link to 2025-09-29" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Context-Driven Performance Modeling for Causal Inference Operators on
Neural Processing Units</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [neural processing units, performance analysis, causal attention, sub-quadratic attention, edge computing, hardware-aware models]</li>
<li class=""><strong>authors:</strong> Neelesh Gupta, Rakshith Jayanth, Dhruv Parikh, Viktor Prasanna</li>
<li class=""><strong>institution:</strong> University of Southern California</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25155v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25155v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts performance analysis of causal inference operators including standard quadratic attention and sub-quadratic alternatives on NPUs. The study reveals quadratic attention becomes memory-bound with cache inefficiency and pipeline stalls exceeding 95% at long contexts, while sub-quadratic models become compute-bound on vector cores. These findings provide insights for co-designing hardware-aware models and optimization strategies for on-device AI inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Intent-Driven Storage Systems: From Low-Level Tuning to High-Level
Understanding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [storage systems, workload intent, parameter reconfiguration, caching, prefetching]</li>
<li class=""><strong>authors:</strong> Shai Bergman, Won Wook Song, Lukas Cavigelli, Konstantin Berestizshevsky, Ke Zhou, Ji Zhang</li>
<li class=""><strong>institution:</strong> Huawei Zurich Research Center</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15917v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15917v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Intent-Driven Storage Systems (IDSS) that use LLMs to interpret workload intent from unstructured signals and generate adaptive storage configurations. The system improves IOPS by up to 2.45× on FileBench workloads through optimized caching and prefetching decisions. Results show LLMs can effectively bridge application semantics with low-level storage control when properly constrained within policy guardrails.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Asynchronous Policy Gradient Aggregation for Efficient Distributed
Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed reinforcement learning, asynchronous policy gradient, heterogeneous computing, communication efficiency]</li>
<li class=""><strong>authors:</strong> Alexander Tyurin, Andrei Spiridonov, Varvara Rudenko</li>
<li class=""><strong>institution:</strong> Opta</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.24305v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.24305v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces two new algorithms, Rennala NIGT and Malenia NIGT, for asynchronous policy gradient aggregation in distributed reinforcement learning. The methods achieve state-of-the-art efficiency by handling both homogeneous and heterogeneous computing environments with improved computational and communication complexity. Experimental results demonstrate significant performance improvements over prior approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in
Long-Context LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [dynamic sparse attention, KV cache management, hierarchical storage, HBM-DRAM optimization, long-context LLM serving]</li>
<li class=""><strong>authors:</strong> Qihui Zhou, Peiqi Yin, Pengfei Zuo, James Cheng</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Huawei Cloud</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.24626v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.24626v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SparseServe introduces hierarchical HBM-DRAM management with fragmentation-aware KV cache transfer, working-set-aware batch control, and layer-segmented prefill to optimize dynamic sparse attention. It achieves up to 9.26x lower TTFT latency and 3.14x higher throughput compared to state-of-the-art systems. This enables efficient serving of long-context LLMs by addressing HBM capacity bottlenecks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Experience Deploying Containerized GenAI Services at an HPC Center</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [containerization, HPC-Kubernetes integration, GenAI deployment]</li>
<li class=""><strong>authors:</strong> Angel M. Beltre, Jeff Ogden, Kevin Pedretti</li>
<li class=""><strong>institution:</strong> Sandia National Laboratories</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20603v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20603v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a converged computing architecture integrating HPC and Kubernetes platforms to deploy containerized GenAI workloads, using Llama LLM with vLLM inference server as a case study. It demonstrates successful deployment across multiple container runtimes and highlights practical considerations for reproducibility. The experience provides guidance for future HPC container tool development and research directions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [FPGA acceleration, graph construction, Vision GNNs, dynamic image processing, hardware optimization]</li>
<li class=""><strong>authors:</strong> Anvitha Ramachandran, Dhruv Parikh, Viktor Prasanna</li>
<li class=""><strong>institution:</strong> University of Southern California</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25121v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25121v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a streaming FPGA accelerator for Dynamic Image Graph Construction (DIGC) in Vision GNNs, using on-chip buffers and parallel sorting to process features efficiently. The design minimizes memory traffic and achieves significant speedups over CPU and GPU baselines. It provides a scalable solution that maintains flexibility across different image resolutions and model variants.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] RServe: Overlapping Encoding and Prefill for Efficient LMM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multimodal models, scheduling, parallel processing, latency optimization, throughput improvement]</li>
<li class=""><strong>authors:</strong> Tianyu Guo, Tianming Xu, Xianjie Chen, Junru Chen, Nong Xiao, Xianwei Zhang</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.24381v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.24381v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> REDServe proposes an efficient LMM inference system that overlaps multimodal encoding with language model computation using fine-grained scheduling. It employs chunked prefill and token budgeting to balance computational loads across requests. Experimental results show up to 66% latency reduction and 109% throughput improvement compared to existing approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts
Routing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [mixture-of-experts, load balancing, routing algorithm, inference optimization, plug-and-play]</li>
<li class=""><strong>authors:</strong> Rana Shahout, Colin Cai, Yilun Du, Minlan Yu, Michael Mitzenmacher</li>
<li class=""><strong>institution:</strong> Harvard University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03293v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03293v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LASER is a plug-and-play routing algorithm that balances expert load in Mixture-of-Experts models by adapting to gate score distributions. It routes tokens to strongest experts when scores show clear preference, and to least-loaded experts when scores are uniform. The method improves inference latency and throughput while maintaining accuracy, requiring no model retraining or fine-tuning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] HAPT: Heterogeneity-Aware Automated Parallel Training on Heterogeneous
Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed training, heterogeneous clusters, parallel training, load balancing, communication optimization]</li>
<li class=""><strong>authors:</strong> Antian Liang, Zhigang Zhao, Kai Zhang, Xuri Shi, Chuantao Li, Chunxiao Wang, Zhenying He, Yinan Jing, X. Sean Wang</li>
<li class=""><strong>institution:</strong> Fudan University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.24859v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.24859v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hapt introduces a fine-grained planner for inter-operator parallel strategy search and a heterogeneity-aware 1F1B scheduler to optimize distributed training on heterogeneous clusters. It achieves better load balancing and computation-communication overlap while minimizing memory overhead. Evaluation shows Hapt delivers 1.3x-1.6x higher performance compared to state-of-the-art frameworks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Zeppelin: Balancing Variable-length Workloads in Data Parallel Large
Model Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [load balancing, variable-length sequences, communication optimization, hierarchical partitioning, attention mechanisms, data-parallel training]</li>
<li class=""><strong>authors:</strong> Chang Chen, Tiancheng Chen, Jiangfei Duan, Qianchao Zhu, Zerui Wang, Qinghao Hu, Peng Sun, Xiuhong Li, Chao Yang, Torsten Hoefler</li>
<li class=""><strong>institution:</strong> Peking University, ETH Zurich, The Chinese University of Hong Kong, Shanghai AI Laboratory, Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21841v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21841v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Zeppelin introduces a hierarchical sequence partitioning method for attention modules, a routing layer for inter-node transfers, and a remapping layer for layout transformations. These techniques address load imbalance in large-scale data-parallel training with variable sequence lengths. The system achieves an average 2.80x speedup over state-of-the-art methods in comprehensive evaluations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in
Cloud Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [IoT data scheduling, reinforcement learning, ant colony optimization, cloud computing, resource optimization]</li>
<li class=""><strong>authors:</strong> Noor Islam S. Mohammad</li>
<li class=""><strong>institution:</strong> IEEE (based on publication venue)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.04334v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.04334v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid scheduling algorithm combining deep reinforcement learning and ant colony optimization for IoT data processing in cloud environments. The method achieves significant improvements in response time (18.4% reduction), resource utilization (12.7% improvement), and energy consumption (9.3% decrease) compared to existing approaches. The integration of model-free RL with swarm intelligence proves effective for scalable and energy-efficient IoT-cloud data scheduling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] A Scalable Distributed Framework for Multimodal GigaVoxel Image
Registration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [image registration, distributed computing, GPU optimization, medical imaging, tensor sharding, memory efficiency]</li>
<li class=""><strong>authors:</strong> Rohit Jena, Vedant Zope, Pratik Chaudhari, James C. Gee</li>
<li class=""><strong>institution:</strong> University of Pennsylvania</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25044v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25044v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FFDP, a distributed framework with optimized non-GEMM kernels for large-scale multimodal image registration. It enables convolution-aware tensor sharding and significantly accelerates existing pipelines while reducing memory usage. The method demonstrates unprecedented scalability by registering gigavoxel medical images efficiently on limited GPU resources.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-30">2025-09-30<a href="#2025-09-30" class="hash-link" aria-label="Direct link to 2025-09-30" title="Direct link to 2025-09-30" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Artificial Intelligence for Cost-Aware Resource Prediction in Big Data
Pipelines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [resource prediction, random forest, cloud computing, cost-aware autoscaling, big data pipelines]</li>
<li class=""><strong>authors:</strong> Harshit Goyal</li>
<li class=""><strong>institution:</strong> BITS Pilani</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05127v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05127v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses Random Forest regression on Google Borg cluster traces to predict resource utilization in big data pipelines. The model achieves high accuracy (R²≈0.99) in capturing non-linear workload-resource relationships. Results demonstrate AI-driven prediction enables cost-aware autoscaling, reducing unnecessary provisioning while maintaining service quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Accelerating LLM Inference with Precomputed Query Storage</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [query caching, storage optimization, latency reduction, edge computing, vector database]</li>
<li class=""><strong>authors:</strong> Jay H. Park, Youngju Cho, Choungsol Lee, Moonwook Oh, Euiseong Seo</li>
<li class=""><strong>institution:</strong> Samsung Electronics, Sungkyunkwan University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25919v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25919v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> StorInfer accelerates LLM inference by precomputing and storing query-response pairs offline, using adaptive generation techniques to ensure diversity. When incoming queries match precomputed ones, it bypasses GPU computation for instant responses. This approach reduces latency by up to 17.3% without quality loss, demonstrating efficient storage-assisted inference for predictable query scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] SysMoBench: Evaluating AI on Formally Modeling Complex Real-World
Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [formal modeling, system specification, TLA+, concurrent systems, distributed systems, benchmark evaluation]</li>
<li class=""><strong>authors:</strong> Qian Cheng, Ruize Tang, Emilie Ma, Finn Hackett, Peiyang He, Yiming Su, Ivan Beschastnikh, Yu Huang, Xiaoxing Ma, Tianyin Xu</li>
<li class=""><strong>institution:</strong> Nanjing University, University of British Columbia, University of Illinois Urbana-Champaign</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23130v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23130v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SysMoBench, a benchmark for evaluating AI&#x27;s ability to generate formal models of complex concurrent and distributed systems using TLA+. It automates evaluation metrics including syntactic correctness and conformance to system code. The benchmark helps understand current LLM capabilities and limitations in formal system modeling, opening new research directions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Parallax: Efficient LLM Inference Service over Decentralized Environment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [decentralized inference, GPU heterogeneity, two-phase scheduler, model allocation, pipeline selection]</li>
<li class=""><strong>authors:</strong> Chris Tong, Youhe Jiang, Gufeng Chen, Tianyi Zhao, Sibian Lu, Wenjie Qu, Eric Yang, Lynn Ai, Binhang Yuan</li>
<li class=""><strong>institution:</strong> Gradient, HKUST, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26182v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26182v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Parallax introduces a decentralized LLM serving system with a two-phase scheduler that optimizes model allocation across heterogeneous GPUs and performs request-time pipeline selection. It effectively handles GPU diversity and network constraints by distributing model layers and creating dynamic execution chains. The system demonstrates improved latency and throughput compared to baseline approaches, making volunteer computing viable for LLM inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Efficient Construction of Large Search Spaces for Auto-Tuning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [auto-tuning, constraint satisfaction problem, search space construction, performance optimization]</li>
<li class=""><strong>authors:</strong> Floris-Jan Willemsen, Rob V. van Nieuwpoort, Ben van Werkhoven</li>
<li class=""><strong>institution:</strong> Leiden University, Netherlands eScience Center</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26253v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26253v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper reformulates search space construction for auto-tuning as a Constraint Satisfaction Problem (CSP) and develops an optimized CSP solver with runtime parsing capabilities. The approach achieves 4 orders of magnitude speedup over brute-force methods and 1-2 orders of magnitude improvement over chain-of-trees frameworks. This breakthrough enables exploration of previously unattainable problem scales in auto-tuning domains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] AGOCS -- Accurate Google Cloud Simulator Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [cloud workload simulation, google cluster traces, resource usage statistics, parallel execution, scala implementation]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko, Vladimir Getov</li>
<li class=""><strong>institution:</strong> University of Westminster</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26120v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26120v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AGOCS is a high-fidelity cloud workload simulator that parses real Google cluster traces from 12,500 nodes over one month. The framework provides detailed parameters of jobs, tasks, and nodes with actual resource usage statistics. Implemented in Scala with parallel execution capabilities, it enables accurate cloud workload simulation on desktop machines for research purposes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] LoRAFusion: Efficient LoRA Fine-Tuning for LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [finetuning], [LoRA, parameter-efficient fine-tuning, kernel optimization, scheduling algorithm, multi-job fine-tuning]</li>
<li class=""><strong>authors:</strong> Zhanda Zhu, Qidong Su, Yaoyao Ding, Kevin Song, Shang Wang, Gennady Pekhimenko</li>
<li class=""><strong>institution:</strong> University of Toronto, Vector Institute, NVIDIA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.00206v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.00206v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LoRAFusion introduces kernel-level graph-splitting to fuse memory-bound operations and a scheduling-level adaptive batching algorithm for multi-job fine-tuning. This system eliminates redundant memory accesses and enables concurrent fine-tuning of multiple LoRA adapters. The approach achieves up to 1.96× speedup over existing systems while maintaining model quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] I Like To Move It -- Computation Instead of Data in the Brain</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [brain simulation, structural plasticity, spike exchange, computational neuroscience, Barnes-Hut algorithm, communication optimization]</li>
<li class=""><strong>authors:</strong> Fabian Czappa, Marvin Kaster, Felix Wolf</li>
<li class=""><strong>institution:</strong> Technical University of Darmstadt (based on authors&#x27; affiliations)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26193v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26193v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a novel algorithm that reduces communication overhead in brain simulations by moving computation instead of data. This approach decreases connectivity update time by 6x and spike exchange time by over 100x. The method enables more scalable simulation of neural networks with structural plasticity for studying brain functions like memory and learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Tuning the Tuner: Introducing Hyperparameter Optimization for
Auto-Tuning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hyperparameter optimization, auto-tuning, performance optimization, search space, statistical evaluation]</li>
<li class=""><strong>authors:</strong> Floris-Jan Willemsen, Rob V. van Nieuwpoort, Ben van Werkhoven</li>
<li class=""><strong>institution:</strong> Leiden University, Netherlands eScience Center</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26300v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26300v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a novel method for hyperparameter tuning of optimization algorithms in auto-tuning systems. The authors propose a robust statistical evaluation approach, provide a FAIR dataset and simulation mode that reduces tuning costs by two orders of magnitude. Results show hyperparameter tuning improves auto-tuner performance by 94.8% on average, demonstrating its significant potential for advancing auto-tuning research and practice.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Efficient Distributed Training via Dual Batch Sizes and Cyclic
Progressive Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed training, batch size optimization, progressive learning, parameter server, ResNet-18]</li>
<li class=""><strong>authors:</strong> Kuan-Wei Lu, Ding-Yong Hong, Pangfeng Liu, Jan-Jan Wu</li>
<li class=""><strong>institution:</strong> Academia Sinica</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26092v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26092v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a dual batch size learning scheme that uses both large and small batch sizes simultaneously to improve generalization while maintaining efficiency, combined with cyclic progressive learning that adjusts image resolution during training. The hybrid approach achieves better accuracy and faster training times compared to conventional methods. Experimental results show 3.3% accuracy improvement with 10.6% faster training on CIFAR-100 and 0.1% accuracy improvement with 35.7% faster training on ImageNet using ResNet-18.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [datacenter lifecycle, TCO optimization, GPU infrastructure, hardware refresh, operational software]</li>
<li class=""><strong>authors:</strong> Jovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Ricardo Bianchini</li>
<li class=""><strong>institution:</strong> Microsoft Azure Research, University of Illinois Urbana-Champaign</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26534v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26534v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a holistic framework for managing AI datacenter lifecycle across building, hardware refresh, and operation stages. The framework co-optimizes decisions across these stages considering workload dynamics and hardware evolution. It achieves up to 40% TCO reduction compared to traditional approaches.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-05">2025-10-05<a href="#2025-10-05" class="hash-link" aria-label="Direct link to 2025-10-05" title="Direct link to 2025-10-05" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Speculative Actions: A Lossless Framework for Faster Agentic Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [speculative execution, agent systems, parallel processing, latency optimization]</li>
<li class=""><strong>authors:</strong> Naimeng Ye, Arnav Ahuja, Georgios Liargkovas, Yunan Lu, Kostis Kaffes, Tianyi Peng</li>
<li class=""><strong>institution:</strong> Columbia University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.04371v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.04371v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes speculative actions, a lossless framework inspired by speculative execution in microprocessors and speculative decoding in LLM inference. The method predicts likely agent actions using faster models to enable parallel execution of multiple steps. Evaluation across gaming, e-commerce, and web search environments shows up to 55% next-action prediction accuracy and significant latency reductions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] From Patchwork to Network: A Comprehensive Framework for Demand Analysis
and Fleet Optimization of Urban Air Mobility</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [urban air mobility, fleet optimization, parallel simulation, demand forecasting, ground transportation integration]</li>
<li class=""><strong>authors:</strong> Xuan Jiang, Xuanyu Zhou, Yibo Zhao, Shangqing Cao, Jinhua Zhao, Mark Hansen, Raja Sengupta</li>
<li class=""><strong>institution:</strong> University of California, Berkeley</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.04186v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.04186v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces LPSim, a large-scale parallel simulation framework using multi-GPU computing to co-optimize Urban Air Mobility demand, fleet operations, and ground transportation interactions. The extended equilibrium search algorithm forecasts demand and determines optimal fleet composition. Results from the San Francisco Bay Area case study show over 20 minutes&#x27; travel time savings for 230,000 trips, but highlight critical dependence on ground access integration and dynamic scheduling.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-24T13:11:58.000Z" itemprop="dateModified">Oct 24, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20250922-20250928"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20250922-20250928</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20251006-20251012"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251006-20251012</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-09-29" class="table-of-contents__link toc-highlight">2025-09-29</a></li><li><a href="#2025-09-30" class="table-of-contents__link toc-highlight">2025-09-30</a></li><li><a href="#2025-10-05" class="table-of-contents__link toc-highlight">2025-10-05</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>