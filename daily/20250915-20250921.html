<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20250915-20250921" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20250915-20250921 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20250915-20250921"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20250915-20250921 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-09-15"><meta data-rh="true" property="og:description" content="2025-09-15"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20250915-20250921"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250915-20250921" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250915-20250921" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20250915-20250921","item":"https://darkknight996.github.io/daily/20250915-20250921"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.df888d8c.js" defer="defer"></script>
<script src="/assets/js/main.928bd3f8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20250915-20250921</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20250915-20250921</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-15">2025-09-15<a href="#2025-09-15" class="hash-link" aria-label="Direct link to 2025-09-15" title="Direct link to 2025-09-15" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] FineServe: Precision-Aware KV Slab and Two-Level Scheduling for
Heterogeneous Precision LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [KV cache management, memory fragmentation, quantization, GPU sharing, scheduling, mixed-precision models]</li>
<li class=""><strong>authors:</strong> Kyungmin Bin, Seungbeom Choi, Jimyoung Son, Jieun Choi, Daseul Bae, Daehyeon Baek, Kihyo Moon, Minsung Jang, Hyojung Lee</li>
<li class=""><strong>institution:</strong> Samsung SDS</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.06261v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.06261v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FineServe proposes a precision-aware KV slab memory management technique and two-level scheduling framework for serving mixed-precision LLMs. The KV slab dynamically allocates KV cache based on quantization characteristics to reduce memory fragmentation, while the scheduler optimizes model placement and batch sizing. Experiments show FineServe achieves up to 2.2× higher SLO attainment and 1.8× higher throughput compared to state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code
Translation in HPC</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [finetuning], [parallel code translation, HPC, LLM evaluation, compiler-guided repair]</li>
<li class=""><strong>authors:</strong> Tomer Bitan, Tal Kadosh, Erel Kaplan, Shira Meiri, Le Chen, Peter Morales, Niranjan Hasabnis, Gal Oren</li>
<li class=""><strong>institution:</strong> Technion, Ben-Gurion University, IAEC, Argonne National Laboratory, Code Metal, Stanford University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12136v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12136v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UniPar introduces a systematic framework for evaluating LLM-based parallel code translation between serial code, CUDA, and OpenMP. The methodology combines fine-tuning, hyperparameter optimization, and compiler-guided repair to improve translation performance. Results show that while off-the-shelf models struggle, UniPar&#x27;s approach doubles compilation and functional correctness rates, achieving up to 69% compilation and 33% correctness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Machine Learning-Driven Predictive Resource Management in Complex
Science Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [machine learning, resource management, workflow management, predictive modeling, distributed computing]</li>
<li class=""><strong>authors:</strong> Tasnuva Chowdhury, Tadashi Maeno, Fatih Furkan Akman, Joseph Boudreau, Sankha Dutta, Shengyu Feng, Adolfy Hoisie, Kuan-Chieh Hsu, Raees Khan, Jaehyung Kim, Ozgur O. Kilic, Scott Klasky, Alexei Klimentov, Tatiana Korchuganova, Verena Ingrid Martinez Outschoorn, Paul Nilsson, David K. Park, Norbert Podhorszki, Yihui Ren, John Rembrandt Steele, Frédéric Suter, Sairam Sri Vatsavai, Torre Wenaus, Wei Yang, Yiming Yang, Shinjae Yoo</li>
<li class=""><strong>institution:</strong> Brookhaven National Laboratory, Oak Ridge National Laboratory, University of Pittsburgh, Carnegie Mellon University, University of Massachusetts Amherst, SLAC National Accelerator Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11512v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11512v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a machine learning pipeline within the PanDA workflow management system to predict resource requirements for complex science workflows. The models overcome challenges of limited upfront knowledge by forecasting key resource needs using advanced ML techniques. This enables proactive decision-making and enhances workflow efficiency across heterogeneous computing resources.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous
GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [diffusion models, parallel inference, heterogeneous GPUs, load balancing, scheduling]</li>
<li class=""><strong>authors:</strong> Han Liang, Jiahui Zhou, Zicheng Zhou, Xiaoxi Zhang, Xu Chen</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04719v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04719v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> STADI introduces a hybrid scheduler with fine-grained parallelism across temporal and spatial dimensions for diffusion model inference. It uses computation-aware step allocation and elastic patch parallelism to balance workloads across heterogeneous GPUs. Experiments show up to 45% latency reduction and improved resource utilization compared to existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Distributed 3D Gaussian Splatting for High-Resolution Isosurface
Visualization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [3D Gaussian Splatting, distributed computing, scientific visualization, high-performance computing, parallel training]</li>
<li class=""><strong>authors:</strong> Mengjiao Han, Andres Sewell, Joseph Insley, Janet Knowles, Victor A. Mateevitsi, Michael E. Papka, Steve Petruzza, Silvio Rizzi</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory, Utah State University, University of Illinois Chicago</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12138v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12138v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a distributed 3D Gaussian Splatting pipeline for high-performance computing systems that partitions data across nodes, trains Gaussian splats in parallel using multi-node/multi-GPU setups, and merges results for global rendering. The method incorporates ghost cells and background masking to eliminate rendering artifacts while maintaining image quality. Results demonstrate up to 3x speedup across 8 nodes while enabling scalable visualization of large-scale scientific datasets.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-16">2025-09-16<a href="#2025-09-16" class="hash-link" aria-label="Direct link to 2025-09-16" title="Direct link to 2025-09-16" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Scaling Up Throughput-oriented LLM Inference Applications on
Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [throughput-oriented inference, opportunistic resource allocation, context management, GPU clusters, dynamic resource pooling]</li>
<li class=""><strong>authors:</strong> Thanh Son Phung, Douglas Thain</li>
<li class=""><strong>institution:</strong> University of Notre Dame</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.13201v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.13201v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes pervasive context management to enable efficient LLM inference on heterogeneous opportunistic GPU clusters by reusing computational contexts across dynamically allocated resources. This approach allows throughput-oriented applications to utilize idle resources without requiring static allocations. Evaluation shows 98.1% execution time reduction compared to traditional approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] AI Factories: It&#x27;s time to rethink the Cloud-HPC divide</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [AI Factories, HPC-cloud convergence, dual-stack approach, EuroHPC, Sovereign AI]</li>
<li class=""><strong>authors:</strong> Pedro Garcia Lopez, Daniel Barcelona Pons, Marcin Copik, Torsten Hoefler, Eduardo Quiñones, Maciej Malawski, Peter Pietzutch, Alberto Marti, Thomas Ohlson Timoudas, Aleksander Slominski</li>
<li class=""><strong>institution:</strong> URV, ETH Zurich, BSC, Sano &amp; AGH, ICL, OpenNebula, RISE, IBM Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12849v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12849v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a dual-stack approach integrating HPC and cloud-native technologies in supercomputers to bridge the performance-usability divide for AI Factories. The method combines high-performance computing with cloud-native tools like Kubernetes to create service-oriented AI platforms. The conclusion advocates for this convergence to support Sovereign AI initiatives while maintaining both computational power and accessibility.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [diffusion transformer, weather forecasting, high-performance computing, ensemble prediction, climate modeling]</li>
<li class=""><strong>authors:</strong> Väinö Hatanpää, Eugene Ku, Jason Stock, Murali Emani, Sam Foreman, Chunyong Jung, Sandeep Madireddy, Tung Nguyen, Varuni Sastry, Ray A. O. Sinurat, Sam Wheeler, Huihuo Zheng, Troy Arcomano, Venkatram Vishwanath, Rao Kotamarthi</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory, University of Chicago, Allen Institute for AI, University of California, Los Angeles</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.13523v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.13523v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces AERIS, a billion-parameter Swin diffusion transformer for Earth system modeling, and SWiPe, a parallelization technique for efficient scaling. It achieves record computational performance on the Aurora supercomputer and demonstrates superior forecasting skill compared to traditional methods, maintaining stability for seasonal-scale predictions up to 90 days. This work highlights the potential of large-scale diffusion models for advancing weather and climate prediction capabilities.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-17">2025-09-17<a href="#2025-09-17" class="hash-link" aria-label="Direct link to 2025-09-17" title="Direct link to 2025-09-17" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Julia GraphBLAS with Nonblocking Execution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GraphBLAS, nonblocking execution, Julia programming, multi-stage programming, DAG optimization]</li>
<li class=""><strong>authors:</strong> Pascal Costanza, Timothy G. Mattson, Raye Kimmerer, Benjamin Brock</li>
<li class=""><strong>institution:</strong> University of Bristol, National Energy Research Scientific Computing Center, Lawrence Berkeley National Laboratory, Intel</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.14211v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.14211v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements nonblocking GraphBLAS execution in Julia using multi-stage programming to generate and compile symbolic representations of computation DAGs at runtime. The approach enables aggressive optimization strategies like operation fusion and parallelization while maintaining DAG semantics. Julia&#x27;s language features significantly simplify implementing this nonblocking execution model, demonstrating promising results for GraphBLAS operations like PageRank.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] FLAME: A Serving System Optimized for Large-Scale Generative
Recommendation with Efficiency</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [generative recommendation, serving system optimization, CPU-GPU heterogeneous computing, memory optimization, kernel fusion, request orchestration]</li>
<li class=""><strong>authors:</strong> Xianwen Guo, Bin Huang, Xiaomeng Wu, Guanlin Wu, Fangjian Li, Shijia Wang, Qiang Xiao, Chuanjiang Luo, Yong Li</li>
<li class=""><strong>institution:</strong> Netease Cloud Music</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22681v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22681v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FLAME introduces an optimized serving system for large-scale generative recommendation models using CPU-GPU heterogeneous hardware, memory optimization through PDA, kernel fusion via FKE, and dynamic request coordination with DSO. The system achieves significant performance improvements including 1.9x throughput gain, 1.7x latency reduction, and 4.6x-6.1x computation speedup. Comprehensive evaluations confirm FLAME effectively supports large-scale online deployment of GR models with remarkable system performance enhancements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] GPU Programming for AI Workflow Development on AWS SageMaker: An
Instructional Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [GPU programming, AI workflow development, AWS SageMaker, parallel computing, HPC profiling, RAG]</li>
<li class=""><strong>authors:</strong> Sriram Srinivasan, Hamdan Alabsi, Rand Obeidat, Nithisha Ponnala, Azene Zenebe</li>
<li class=""><strong>institution:</strong> Bowie State University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.13703v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.13703v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a specialized course design that teaches GPU programming and parallel computing concepts using AWS SageMaker for developing AI workflows. Students gained hands-on experience with cloud GPU instances, parallel algorithms, and performance optimization tools. Evaluation showed AWS provided an effective platform for practical learning, experiential methods enhanced technical skills, and the course improved problem-solving abilities through exposure to performance analysis tools.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU performance, Zero-Knowledge Proofs, Number-Theoretic Transform, Multi-Scalar Multiplication, cryptographic protocols]</li>
<li class=""><strong>authors:</strong> Tarunesh Verma, Yichao Yuan, Nishil Talati, Todd Austin</li>
<li class=""><strong>institution:</strong> University of Michigan</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22684v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22684v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ZKProphet presents a comprehensive performance analysis of Zero-Knowledge Proofs on GPUs, identifying NTT as the primary bottleneck after MSM optimizations. The study shows that current implementations underutilize GPU resources and lack architectural optimizations like asynchronous operations. It provides optimization guidelines including parameter tuning and alternative data representations to improve ZKP performance on GPUs.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-18">2025-09-18<a href="#2025-09-18" class="hash-link" aria-label="Direct link to 2025-09-18" title="Direct link to 2025-09-18" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Conditional Prior-based Non-stationary Channel Estimation Using
Accelerated Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [diffusion models, channel estimation, non-stationary channels, MIMO systems, wireless communication]</li>
<li class=""><strong>authors:</strong> Muhammad Ahmed Mohsin, Ahsan Bilal, Muhammad Umer, Asad Aali, Muhammad Ali Jamshed, Dean F. Hougen, John M. Cioffi</li>
<li class=""><strong>institution:</strong> Stanford University, University of Oklahoma, University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15182v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15182v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a conditional diffusion model for non-stationary channel estimation that uses historical channel observations to guide the denoising process. The method employs cross-time attention and SNR-matched initialization to accelerate inference while maintaining accuracy. Experimental results show superior performance over traditional baselines across all SNR ranges with strong high-SNR fidelity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Channel Prediction under Network Distribution Shift Using Continual
Learning-based Loss Regularization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [channel prediction, continual learning, loss regularization, wireless networks, catastrophic forgetting, EWC, Synaptic Intelligence]</li>
<li class=""><strong>authors:</strong> Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Muhammad Ibtsaam Qadir, Muhammad Ali Jamshed, Dean F. Hougen, John M. Cioffi</li>
<li class=""><strong>institution:</strong> Stanford University, University of Oklahoma, Purdue University, University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15192v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15192v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a continual learning framework using loss regularization to address catastrophic forgetting in wireless channel prediction under network distribution shifts. The method employs Elastic Weight Consolidation and Synaptic Intelligence to preserve important parameters from previous configurations while adapting to new environments. Results show SI reduces NMSE by up to 1.8 dB with better memory efficiency than EWC, making it suitable for resource-constrained wireless infrastructure.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] PCCL: Photonic circuit-switched collective communication for distributed
ML</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [collective communication, photonic networks, distributed machine learning, network topology, GPU clusters]</li>
<li class=""><strong>authors:</strong> Abhishek Vijaya Kumar, Arjun Devraj, Rachee Singh</li>
<li class=""><strong>institution:</strong> Cornell University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15450v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15450v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PCCL introduces a photonic circuit-switched collective communication library that dynamically reconfigures network topology to match communication patterns, eliminating congestion and dilation. It achieves up to 3× speedup over state-of-the-art algorithms on 128 GPUs by creating direct contention-free circuits between communicating GPUs. The hardware-agnostic optimization framework makes it practical across different optical hardware with varying switching speeds.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless
and GPU-Based Training Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [serverless computing, distributed machine learning, cost-performance analysis, training architectures, communication overhead]</li>
<li class=""><strong>authors:</strong> Amine Barrak, Fabio Petrillo, Fehmi Jaafar</li>
<li class=""><strong>institution:</strong> Oakland University, École de technologie supérieure (ETS), University of Quebec at Chicoutimi</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.14920v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.14920v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares serverless distributed ML architectures (SPIRT, MLLess, AllReduce, ScatterReduce) with GPU-based training, evaluating training time, cost, and communication overhead. The proposed SPIRT architecture demonstrates improved performance through parallel batch processing and RedisAI integration. While GPU training achieves fastest convergence, serverless frameworks offer cost advantages for lightweight models with optimized configurations.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-19">2025-09-19<a href="#2025-09-19" class="hash-link" aria-label="Direct link to 2025-09-19" title="Direct link to 2025-09-19" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Characterizing the Efficiency of Distributed Training: A Power,
Performance, and Thermal Perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, GPU clusters, parallelism strategies, power consumption, thermal behavior, scale-up vs scale-out]</li>
<li class=""><strong>authors:</strong> Seokjin Go, Joongun Park, Spandan More, Hanjiang Wu, Irene Wang, Aaron Jezghani, Tushar Krishna, Divya Mahajan</li>
<li class=""><strong>institution:</strong> Georgia Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.10371v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.10371v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper comprehensively characterizes LLM training efficiency across multiple hardware platforms and parallelism strategies, analyzing power, performance and thermal behavior. The study reveals that performance depends on complex interactions between hardware, topology and model execution rather than just hardware scaling. Key findings show scale-up systems can outperform scale-out in communication-bound regimes, while certain parallelism combinations cause bandwidth underutilization and large microbatch sizes worsen thermal throttling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Efficient Pre-Training of LLMs via Topology-Aware Communication
Alignment on More Than 9600 GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [scheduling, communication patterns, network topology, GPU clusters, resource allocation]</li>
<li class=""><strong>authors:</strong> Guoliang He, Youhe Jiang, Wencong Xiao, Kaihua Jiang, Shuguang Wang, Jun Wang, Zixian Du, Zhuo Jiang, Xinlei Zhang, Binhang Yuan, Eiko Yoneki</li>
<li class=""><strong>institution:</strong> University of Cambridge, ByteDance Seed, HKUST</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15940v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15940v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents Arnold, a topology-aware scheduling system that aligns LLM communication patterns with data center network topology. The system reduces communication group spread by up to 1.67x and improves end-to-end training performance by 10.6% when scaling to over 9600 GPUs, demonstrating significant efficiency gains for large-scale LLM pre-training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] RLinf: Flexible and Efficient Large-scale Reinforcement Learning via
Macro-to-Micro Flow Transformation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [reinforcement learning, system optimization, workflow transformation, elastic pipelining, context switching]</li>
<li class=""><strong>authors:</strong> Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, Zixiao Huang, Mingjie Wei, Yuqing Xie, Ke Yang, Bo Dai, Zhexuan Xu, Xiangyuan Wang, Xu Fu, Zhihao Liu, Kang Chen, Weilin Liu, Gang Liu, Boxun Li, Jianlei Yang, Zhi Yang, Guohao Dai, Yu Wang</li>
<li class=""><strong>institution:</strong> Tsinghua University, Zhongguancun Academy, Infinigence AI, Peking University, UC Berkeley, Beihang University, Shanghai Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15965v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15965v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RLinf introduces a macro-to-micro flow transformation (M2Flow) paradigm that automatically decomposes and recomposes RL workflows with context switching and elastic pipelining. The system achieves 1.1x-2.13x speedup in training throughput through profiling-guided scheduling and adaptive communication. Evaluations demonstrate consistent performance improvements over state-of-the-art systems on both reasoning and embodied RL tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] LongCat-Flash Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [Mixture-of-Experts, computational efficiency, agentic capabilities, scaling framework, inference throughput]</li>
<li class=""><strong>authors:</strong> Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han, Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen, Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong, Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu, Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan, Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang, Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang, Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu, Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, Liang Shi, Lianhui Ma, Lin Qiu, Lingbin Kong, Lingtong Si, Linkun Lyu, Linsen Guo, Liqi Yang, Lizhi Yan, Mai Xia, Man Gao, Manyuan Zhang, Meng Zhou, Mengxia Shen, Mingxiang Tuo, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pingwei Sun, Qi Gu, Qianyun Li, Qingyuan Li, Qiong Huang, Qiyuan Duan, Ran Meng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shizhe Wu, Shuai Liang, Shuo Wang, Suogui Dang, Tao Fang, Tao Li, Tefeng Chen, Tianhao Bai, Tianhao Zhou, Tingwen Xie, Wei He, Wei Huang, Wei Liu, Wei Shi, Wei Wang, Wei Wu, Weikang Zhao, Wen Zan, Wenjie Shi, Xi Nan, Xi Su, Xiang Li, Xiang Mei, Xiangyang Ji, Xiangyu Xi, Xiangzhou Huang, Xianpeng Li, Xiao Fu, Xiao Liu, Xiao Wei, Xiaodong Cai, Xiaolong Chen, Xiaoqing Liu, Xiaotong Li, Xiaowei Shi, Xiaoyu Li, Xili Wang, Xin Chen, Xing Hu, Xingyu Miao, Xinyan He, Xuemiao Zhang, Xueyuan Hao, Xuezhi Cao, Xunliang Cai, Xurui Yang, Yan Feng, Yang Bai, Yang Chen, Yang Yang, Yaqi Huo, Yerui Sun, Yifan Lu, Yifan Zhang, Yipeng Zang, Yitao Zhai, Yiyang Li, Yongjing Yin, Yongkang Lv, Yongwei Zhou, Yu Yang, Yuchen Xie, Yueqing Sun, Yuewen Zheng, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunfang Tai, Yunke Zhao, Zeyang Yu, Zhao Zhang, Zhaohua Yang, Zhenchao Zhang, Zhikang Xia, Zhiye Zou, Zhizhao Zeng, Zhongda Su, Zhuofan Chen, Zijian Zhang, Ziwen Wang, Zixu Jiang, Zizhe Zhao, Zongyu Wang, Zunhai Su</li>
<li class=""><strong>institution:</strong> Meituan</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01322v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01322v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LongCat-Flash introduces a 560B parameter Mixture-of-Experts model with Zero-computation Experts and Shortcut-connected MoE for dynamic computation allocation and improved inference efficiency. It achieves training on 20+ trillion tokens in 30 days with over 100 TPS inference at $0.70 per million tokens. The model demonstrates competitive performance, especially in agentic tasks, and is open-sourced for community research.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-20">2025-09-20<a href="#2025-09-20" class="hash-link" aria-label="Direct link to 2025-09-20" title="Direct link to 2025-09-20" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Trace Replay Simulation of MIT SuperCloud for Studying Optimal
Sustainability Policies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [digital twin, power simulation, reinforcement learning, job scheduling, energy efficiency, trace replay]</li>
<li class=""><strong>authors:</strong> Wesley Brewer, Matthias Maiterth, Damien Fay</li>
<li class=""><strong>institution:</strong> Oak Ridge National Laboratory, Hewlett Packard Enterprise</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16513v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16513v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper extends the ExaDigiT digital twin framework to simulate MIT SuperCloud workloads using trace replay and reinforcement learning. The method enables experimentation with energy-aware scheduling policies through Proximal Policy Optimization. Preliminary results demonstrate feasibility for learning optimal sustainability policies that improve datacenter throughput and efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Shift Parallelism: Low-Latency, High-Throughput LLM Inference for
Dynamic Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [parallelism, dynamic workloads, latency-throughput tradeoff, tensor parallelism, sequence parallelism, KV cache]</li>
<li class=""><strong>authors:</strong> Mert Hidayetoglu, Aurick Qiao, Michael Wyatt, Jeff Rasley, Yuxiong He, Samyam Rajbhandari</li>
<li class=""><strong>institution:</strong> Snowflake AI Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16495v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16495v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Shift Parallelism, a method that dynamically switches between tensor parallelism and sequence parallelism to optimize LLM inference. It achieves up to 1.51x faster response in interactive workloads and 50% higher throughput in batch workloads compared to TP-only solutions. The approach provides better latency-throughput tradeoffs for dynamic workloads without sacrificing performance in either low or high traffic scenarios.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-21">2025-09-21<a href="#2025-09-21" class="hash-link" aria-label="Direct link to 2025-09-21" title="Direct link to 2025-09-21" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] MoA-Off: Adaptive Heterogeneous Modality-Aware Offloading with
Edge-Cloud Collaboration for Efficient Multimodal LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multimodal LLM, edge-cloud collaboration, adaptive offloading, inference optimization]</li>
<li class=""><strong>authors:</strong> Zheming Yang, Qi Guo, Yunqing Hu, Chang Zhao, Chang Zhang, Jian Zhao, Wen Ji</li>
<li class=""><strong>institution:</strong> Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Peng Cheng Laboratory, Institute of AI for Industries</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16995v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16995v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MoA-Off proposes an adaptive edge-cloud collaborative framework with modality-aware complexity estimation for efficient multimodal LLM inference. It dynamically schedules workloads between edge and cloud based on input complexity and system states. The method achieves over 30% latency reduction and 30%-65% resource savings while maintaining competitive accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix
Caching</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [distributed prefix caching, KV cache fetching, SmartNIC acceleration, interference-free, chunked pipeline, minimal-copy memory management]</li>
<li class=""><strong>authors:</strong> Xingyu Xiang, Raj Joshi, Yuhan Liu, Jiayi Yao, Chenxingyu Zhao, Junchen Jiang, Yang Zhou, Eddie Kohler, Minlan Yu</li>
<li class=""><strong>institution:</strong> Harvard University, University of Chicago, University of Washington, UC Davis</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16857v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16857v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ShadowServe introduces a SmartNIC-accelerated system that separates control and data planes to eliminate interference during KV cache fetching for distributed prefix caching. It employs a chunked pipeline and minimal-copy memory management to overcome SmartNIC resource limitations. The system achieves up to 2.2x lower TPOT and 1.38x lower TTFT in low-bandwidth scenarios, improving throughput by up to 1.35x.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-29T04:16:08.000Z" itemprop="dateModified">Oct 29, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20250908-20250914"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20250908-20250914</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20250922-20250928"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20250922-20250928</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-09-15" class="table-of-contents__link toc-highlight">2025-09-15</a></li><li><a href="#2025-09-16" class="table-of-contents__link toc-highlight">2025-09-16</a></li><li><a href="#2025-09-17" class="table-of-contents__link toc-highlight">2025-09-17</a></li><li><a href="#2025-09-18" class="table-of-contents__link toc-highlight">2025-09-18</a></li><li><a href="#2025-09-19" class="table-of-contents__link toc-highlight">2025-09-19</a></li><li><a href="#2025-09-20" class="table-of-contents__link toc-highlight">2025-09-20</a></li><li><a href="#2025-09-21" class="table-of-contents__link toc-highlight">2025-09-21</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>