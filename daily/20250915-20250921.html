<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20250915-20250921" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20250915-20250921 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20250915-20250921"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20250915-20250921 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-09-15"><meta data-rh="true" property="og:description" content="2025-09-15"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20250915-20250921"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250915-20250921" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250915-20250921" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20250915-20250921","item":"https://darkknight996.github.io/daily/20250915-20250921"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.78e1c784.js" defer="defer"></script>
<script src="/assets/js/main.be5f6e7c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20250915-20250921</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20250915-20250921</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-15">2025-09-15<a href="#2025-09-15" class="hash-link" aria-label="Direct link to 2025-09-15" title="Direct link to 2025-09-15" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Machine Learning-Driven Predictive Resource Management in Complex
Science Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [machine learning, resource management, workflow management, predictive modeling, distributed computing, science workflows]</li>
<li class=""><strong>authors:</strong> Tasnuva Chowdhury, Tadashi Maeno, Fatih Furkan Akman, Joseph Boudreau, Sankha Dutta, Shengyu Feng, Adolfy Hoisie, Kuan-Chieh Hsu, Raees Khan, Jaehyung Kim, Ozgur O. Kilic, Scott Klasky, Alexei Klimentov, Tatiana Korchuganova, Verena Ingrid Martinez Outschoorn, Paul Nilsson, David K. Park, Norbert Podhorszki, Yihui Ren, John Rembrandt Steele, Frédéric Suter, Sairam Sri Vatsavai, Torre Wenaus, Wei Yang, Yiming Yang, Shinjae Yoo</li>
<li class=""><strong>institution:</strong> Brookhaven National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11512v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11512v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a machine learning pipeline within the PanDA workflow management system to predict resource requirements for complex science workflows. The models overcome limited upfront knowledge challenges by forecasting key resource needs. This enables proactive decision-making and enhances workflow efficiency across heterogeneous computing resources.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] FineServe: Precision-Aware KV Slab and Two-Level Scheduling for
Heterogeneous Precision LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [KV cache management, memory fragmentation, GPU sharing, mixed-precision models, scheduling optimization]</li>
<li class=""><strong>authors:</strong> Kyungmin Bin, Seungbeom Choi, Jimyoung Son, Jieun Choi, Daseul Bae, Daehyeon Baek, Kihyo Moon, Minsung Jang, Hyojung Lee</li>
<li class=""><strong>institution:</strong> Samsung SDS</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.06261v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.06261v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FineServe introduces a precision-aware KV Slab memory management technique and two-level scheduling framework for serving mixed-precision LLMs. It dynamically allocates KV cache based on quantization characteristics and adapts batch sizes to request fluctuations. The system achieves up to 2.2× higher SLO attainment and 1.8× higher throughput compared to state-of-the-art GPU sharing systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code
Translation in HPC</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [finetuning], [parallel code translation, HPC, LLM evaluation, compiler-guided repair]</li>
<li class=""><strong>authors:</strong> Tomer Bitan, Tal Kadosh, Erel Kaplan, Shira Meiri, Le Chen, Peter Morales, Niranjan Hasabnis, Gal Oren</li>
<li class=""><strong>institution:</strong> Technion</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12136v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12136v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> UniPar introduces a systematic framework for evaluating LLM-based parallel code translation between serial code, CUDA, and OpenMP using strategies like fine-tuning and compiler-guided repair. The approach significantly improves performance over off-the-shelf models, achieving up to 2x gains in compilation and functional correctness. This demonstrates the potential of enhanced LLM methodologies for high-performance computing code translation tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous
GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [diffusion models, parallel inference, heterogeneous GPUs, workload balancing, scheduling]</li>
<li class=""><strong>authors:</strong> Han Liang, Jiahui Zhou, Zicheng Zhou, Xiaoxi Zhang, Xu Chen</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04719v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04719v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> STADI introduces a hybrid scheduler with fine-grained parallelism across temporal and spatial dimensions, using computation-aware step allocation and elastic patch parallelism for heterogeneous GPUs. The method achieves up to 45% latency reduction and improved resource utilization compared to existing approaches. Experiments demonstrate effective load balancing and bottleneck mitigation in heterogeneous multi-GPU environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Distributed 3D Gaussian Splatting for High-Resolution Isosurface
Visualization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed computing, 3D Gaussian Splatting, high-performance computing, scientific visualization, parallel training]</li>
<li class=""><strong>authors:</strong> Mengjiao Han, Andres Sewell, Joseph Insley, Janet Knowles, Victor A. Mateevitsi, Michael E. Papka, Steve Petruzza, Silvio Rizzi</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12138v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12138v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a distributed 3D Gaussian Splatting pipeline for high-performance computing systems that partitions data across nodes, trains Gaussian splats in parallel using multi-nodes and multi-GPUs, and merges splats for global rendering. The method employs ghost cells at partition boundaries and background masks to eliminate artifacts during distributed processing. Results demonstrate up to 3X speedup across 8 nodes while preserving image quality, enabling scalable visualization of large-scale scientific data for future in situ applications.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-16">2025-09-16<a href="#2025-09-16" class="hash-link" aria-label="Direct link to 2025-09-16" title="Direct link to 2025-09-16" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Scaling Up Throughput-oriented LLM Inference Applications on
Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [throughput-oriented inference, opportunistic GPU clusters, pervasive context management, dynamic resource allocation, context reuse]</li>
<li class=""><strong>authors:</strong> Thanh Son Phung, Douglas Thain</li>
<li class=""><strong>institution:</strong> University of Notre Dame</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.13201v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.13201v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces pervasive context management, a technique that exploits common computational contexts in LLM applications to enable efficient context reuse on heterogeneous opportunistic GPU clusters. The approach allows dynamic resource allocation for throughput-oriented LLM inference, avoiding static allocations and long queues. Evaluation shows this method reduces execution time by 98.1% compared to traditional approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] AI Factories: It&#x27;s time to rethink the Cloud-HPC divide</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [AI Factories, HPC-cloud integration, dual-stack approach, EuroHPC, cloud-native technologies]</li>
<li class=""><strong>authors:</strong> Pedro Garcia Lopez, Daniel Barcelona Pons, Marcin Copik, Torsten Hoefler, Eduardo Quiñones, Maciej Malawski, Peter Pietzutch, Alberto Marti, Thomas Ohlson Timoudas, Aleksander Slominski</li>
<li class=""><strong>institution:</strong> URV, ETH Zurich, BSC, AGH, ICL, OpenNebula, RISE, IBM Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12849v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12849v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a dual-stack approach integrating HPC and cloud-native technologies in supercomputers to bridge the performance-usability gap for AI Factories. It advocates combining HPC&#x27;s raw performance with cloud&#x27;s accessibility through service-oriented interfaces. The convergence aims to support Sovereign AI initiatives by enabling both high-performance computing and user-friendly AI services on shared infrastructure.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [diffusion models, weather forecasting, climate modeling, high-performance computing, transformer architecture]</li>
<li class=""><strong>authors:</strong> Väinö Hatanpää, Eugene Ku, Jason Stock, Murali Emani, Sam Foreman, Chunyong Jung, Sandeep Madireddy, Tung Nguyen, Varuni Sastry, Ray A. O. Sinurat, Sam Wheeler, Huihuo Zheng, Troy Arcomano, Venkatram Vishwanath, Rao Kotamarthi</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.13523v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.13523v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces AERIS, a billion-parameter Swin diffusion transformer for Earth system modeling, and SWiPe, a parallelization technique for efficient scaling. It demonstrates state-of-the-art performance on the Aurora supercomputer, outperforming traditional weather forecasting systems and maintaining stability for seasonal predictions up to 90 days. This highlights the potential of large-scale diffusion models for climate and weather prediction tasks.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-17">2025-09-17<a href="#2025-09-17" class="hash-link" aria-label="Direct link to 2025-09-17" title="Direct link to 2025-09-17" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Julia GraphBLAS with Nonblocking Execution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GraphBLAS, nonblocking execution, Julia programming language, DAG optimization, PageRank]</li>
<li class=""><strong>authors:</strong> Pascal Costanza, Timothy G. Mattson, Raye Kimmerer, Benjamin Brock</li>
<li class=""><strong>institution:</strong> Lawrence Berkeley National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.14211v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.14211v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements GraphBLAS with aggressive nonblocking execution using Julia&#x27;s language features, which simplifies building operation DAGs for optimization. The approach enables function fusion, object elision, and parallelism exploitation while preserving DAG semantics. Current work demonstrates potential through PageRank implementation, showing Julia&#x27;s advantages for nonblocking GraphBLAS execution.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU performance, Zero-Knowledge Proofs, Number-Theoretic Transform, Multi-Scalar Multiplication, cryptographic proofs]</li>
<li class=""><strong>authors:</strong> Tarunesh Verma, Yichao Yuan, Nishil Talati, Todd Austin</li>
<li class=""><strong>institution:</strong> University of Michigan</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22684v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22684v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents ZKProphet, a comprehensive performance analysis of Zero-Knowledge Proofs on GPUs, identifying Number-Theoretic Transform as the primary bottleneck after MSM optimizations. The study reveals that current NTT implementations underutilize GPU resources and lack architectural optimizations like asynchronous operations. The authors propose runtime parameter tuning and alternative data representations as methods to improve ZKP performance on GPU architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] GPU Programming for AI Workflow Development on AWS SageMaker: An
Instructional Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [GPU programming, AWS SageMaker, parallel computing, AI workflow development, HPC profiling]</li>
<li class=""><strong>authors:</strong> Sriram Srinivasan, Hamdan Alabsi, Rand Obeidat, Nithisha Ponnala, Azene Zenebe</li>
<li class=""><strong>institution:</strong> Bowie State University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.13703v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.13703v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a specialized course design that teaches GPU programming and parallel computing concepts using AWS SageMaker for developing AI agents. The course evaluation showed AWS served as an effective platform for practical GPU programming, experiential learning enhanced technical proficiency, and the approach strengthened students&#x27; problem-solving skills. The findings advocate for integrating parallel computing into STEM education to prepare students for compute-intensive fields.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] FLAME: A Serving System Optimized for Large-Scale Generative
Recommendation with Efficiency</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [generative recommendation, serving system, CPU-GPU heterogeneous computing, memory optimization, kernel fusion, request orchestration]</li>
<li class=""><strong>authors:</strong> Xianwen Guo, Bin Huang, Xiaomeng Wu, Guanlin Wu, Fangjian Li, Shijia Wang, Qiang Xiao, Chuanjiang Luo, Yong Li</li>
<li class=""><strong>institution:</strong> Netease Cloud Music</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22681v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22681v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FLAME introduces a CPU-GPU heterogeneous serving system with three key modules: Proximal Data Accelerator for memory optimization, Fused Kernel Engine for computation acceleration using TensorRT, and Dynamic Stream Orchestrator for request coordination. The system achieves significant performance improvements including 1.9x throughput gain, 1.7x latency reduction, and 4.6x-6.1x computation speedup. This enables efficient large-scale deployment of generative recommendation models that require high computational resources.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-18">2025-09-18<a href="#2025-09-18" class="hash-link" aria-label="Direct link to 2025-09-18" title="Direct link to 2025-09-18" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Conditional Prior-based Non-stationary Channel Estimation Using
Accelerated Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [diffusion models, channel estimation, non-stationary channels, MIMO systems, wireless communication]</li>
<li class=""><strong>authors:</strong> Muhammad Ahmed Mohsin, Ahsan Bilal, Muhammad Umer, Asad Aali, Muhammad Ali Jamshed, Dean F. Hougen, John M. Cioffi</li>
<li class=""><strong>institution:</strong> Stanford University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15182v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15182v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a conditional prior diffusion method for non-stationary channel estimation that uses a temporal encoder with cross-time attention and accelerated diffusion with SNR-matched initialization. The approach achieves lower NMSE than baseline methods across all SNRs on 3GPP benchmarks, demonstrating stable performance and strong high-SNR fidelity in wireless channel estimation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Channel Prediction under Network Distribution Shift Using Continual
Learning-based Loss Regularization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [continual learning, channel prediction, wireless networks, loss regularization, catastrophic forgetting, Elastic Weight Consolidation, Synaptic Intelligence]</li>
<li class=""><strong>authors:</strong> Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Muhammad Ibtsaam Qadir, Muhammad Ali Jamshed, Dean F. Hougen, John M. Cioffi</li>
<li class=""><strong>institution:</strong> Stanford University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15192v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15192v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a continual learning framework using loss regularization to address catastrophic forgetting in wireless channel prediction under network distribution shifts. The method employs Elastic Weight Consolidation and Synaptic Intelligence to preserve important parameters from previous configurations while adapting to new environments. Results show SI reduces NMSE by up to 1.8 dB with better memory efficiency than EWC, making it suitable for resource-constrained wireless systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine
Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [zero-knowledge proofs, distributed machine learning, cryptographic verification, modular framework, trust minimization]</li>
<li class=""><strong>authors:</strong> Dan Ivanov, Tristan Freiberg, Shirin Shahabi, Jonathan Gold, Haruna Isah</li>
<li class=""><strong>institution:</strong> Inference Labs Inc.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.06972v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.06972v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DSperse introduces a modular framework for distributed ML inference that uses targeted cryptographic verification of strategic subcomputations called &quot;slices&quot; to avoid full-model circuitization costs. It enables flexible proof boundaries aligned with model structure and enforces consistency through audit, replication, or economic incentives. Empirical evaluation shows the framework supports scalable verification strategies with practical trust minimization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] PCCL: Photonic circuit-switched collective communication for distributed
ML</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [photonic communication, collective communication, distributed ML, network optimization]</li>
<li class=""><strong>authors:</strong> Abhishek Vijaya Kumar, Arjun Devraj, Rachee Singh</li>
<li class=""><strong>institution:</strong> Cornell University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15450v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15450v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PCCL introduces a photonic circuit-switched collective communication library that dynamically reconfigures network topology to match communication patterns, eliminating congestion and dilation. It uses a hardware-agnostic optimization framework to balance reconfiguration delays with performance gains. The approach achieves up to 3× speedup over state-of-the-art algorithms and 1.3× improvement in end-to-end training throughput on 128 GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless
and GPU-Based Training Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [serverless computing, distributed machine learning, cost-performance analysis, training architectures, communication overhead]</li>
<li class=""><strong>authors:</strong> Amine Barrak, Fabio Petrillo, Fehmi Jaafar</li>
<li class=""><strong>institution:</strong> Oakland University, École de technologie supérieure (ETS), University of Quebec at Chicoutimi</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.14920v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.14920v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares serverless distributed ML architectures (SPIRT, MLLess, AllReduce, ScatterReduce) with GPU-based training using CNN models on CIFAR-10. The study evaluates training time, cost, and communication efficiency, finding that while GPU training achieves fastest convergence, serverless frameworks offer cost advantages for lightweight models. Optimizations like gradient accumulation and in-database computation improve serverless performance, revealing key trade-offs between different architectures.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-19">2025-09-19<a href="#2025-09-19" class="hash-link" aria-label="Direct link to 2025-09-19" title="Direct link to 2025-09-19" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Characterizing the Efficiency of Distributed Training: A Power,
Performance, and Thermal Perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, parallelism strategies, power-performance-thermal analysis, hardware utilization, optimization techniques]</li>
<li class=""><strong>authors:</strong> Seokjin Go, Joongun Park, Spandan More, Hanjiang Wu, Irene Wang, Aaron Jezghani, Tushar Krishna, Divya Mahajan</li>
<li class=""><strong>institution:</strong> Georgia Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.10371v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.10371v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper comprehensively characterizes LLM training efficiency across different hardware platforms and parallelism strategies. It analyzes how hardware utilization, power consumption, and thermal behavior are affected by various configurations. The study reveals that performance depends on complex interactions between hardware, system topology, and model execution, with scale-up systems sometimes outperforming scale-out systems in communication-bound scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] RLinf: Flexible and Efficient Large-scale Reinforcement Learning via
Macro-to-Micro Flow Transformation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [reinforcement learning, system optimization, workflow transformation, scheduling policy, elastic pipelining, context switching]</li>
<li class=""><strong>authors:</strong> Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, Zixiao Huang, Mingjie Wei, Yuqing Xie, Ke Yang, Bo Dai, Zhexuan Xu, Xiangyuan Wang, Xu Fu, Zhihao Liu, Kang Chen, Weilin Liu, Gang Liu, Boxun Li, Jianlei Yang, Zhi Yang, Guohao Dai, Yu Wang</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15965v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15965v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RLinf introduces a macro-to-micro flow transformation (M2Flow) paradigm that decomposes and recomposes RL workflows for optimized execution. It employs context switching, elastic pipelining, and profiling-guided scheduling to enhance system flexibility and efficiency. Evaluations show RLinf achieves 1.1x-2.13x speedup in training throughput compared to state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Efficient Pre-Training of LLMs via Topology-Aware Communication
Alignment on More Than 9600 GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM pre-training, communication patterns, network topology, GPU clusters, resource scheduling]</li>
<li class=""><strong>authors:</strong> Guoliang He, Youhe Jiang, Wencong Xiao, Kaihua Jiang, Shuguang Wang, Jun Wang, Zixian Du, Zhuo Jiang, Xinlei Zhang, Binhang Yuan, Eiko Yoneki</li>
<li class=""><strong>institution:</strong> University of Cambridge, ByteDance Seed, HKUST</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15940v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15940v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents Arnold, a topology-aware scheduling system that aligns LLM communication patterns with data center network topology. The system reduces communication group spread by up to 1.67x and improves end-to-end training performance by 10.6% when scaling to over 9600 GPUs. The approach effectively addresses bandwidth contention issues in large-scale LLM pre-training through intelligent resource scheduling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] LongCat-Flash Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training, LLM inference], [Mixture-of-Experts, computational efficiency, agentic capabilities, scaling framework, open-source]</li>
<li class=""><strong>authors:</strong> Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han, Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen, Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong, Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu, Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan, Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang, Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang, Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu, Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, Liang Shi, Lianhui Ma, Lin Qiu, Lingbin Kong, Lingtong Si, Linkun Lyu, Linsen Guo, Liqi Yang, Lizhi Yan, Mai Xia, Man Gao, Manyuan Zhang, Meng Zhou, Mengxia Shen, Mingxiang Tuo, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pingwei Sun, Qi Gu, Qianyun Li, Qingyuan Li, Qiong Huang, Qiyuan Duan, Ran Meng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shizhe Wu, Shuai Liang, Shuo Wang, Suogui Dang, Tao Fang, Tao Li, Tefeng Chen, Tianhao Bai, Tianhao Zhou, Tingwen Xie, Wei He, Wei Huang, Wei Liu, Wei Shi, Wei Wang, Wei Wu, Weikang Zhao, Wen Zan, Wenjie Shi, Xi Nan, Xi Su, Xiang Li, Xiang Mei, Xiangyang Ji, Xiangyu Xi, Xiangzhou Huang, Xianpeng Li, Xiao Fu, Xiao Liu, Xiao Wei, Xiaodong Cai, Xiaolong Chen, Xiaoqing Liu, Xiaotong Li, Xiaowei Shi, Xiaoyu Li, Xili Wang, Xin Chen, Xing Hu, Xingyu Miao, Xinyan He, Xuemiao Zhang, Xueyuan Hao, Xuezhi Cao, Xunliang Cai, Xurui Yang, Yan Feng, Yang Bai, Yang Chen, Yang Yang, Yaqi Huo, Yerui Sun, Yifan Lu, Yifan Zhang, Yipeng Zang, Yitao Zhai, Yiyang Li, Yongjing Yin, Yongkang Lv, Yongwei Zhou, Yu Yang, Yuchen Xie, Yueqing Sun, Yuewen Zheng, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunfang Tai, Yunke Zhao, Zeyang Yu, Zhao Zhang, Zhaohua Yang, Zhenchao Zhang, Zhikang Xia, Zhiye Zou, Zhizhao Zeng, Zhongda Su, Zhuofan Chen, Zijian Zhang, Ziwen Wang, Zixu Jiang, Zizhe Zhao, Zongyu Wang, Zunhai Su</li>
<li class=""><strong>institution:</strong> Meituan</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01322v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01322v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LongCat-Flash introduces a 560B parameter MoE model with Zero-computation Experts and Shortcut-connected MoE for dynamic computation allocation and improved inference efficiency. It was trained on over 20T tokens in 30 days using a comprehensive scaling framework, achieving competitive performance with strengths in agentic tasks. The model is open-sourced to support community research.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-20">2025-09-20<a href="#2025-09-20" class="hash-link" aria-label="Direct link to 2025-09-20" title="Direct link to 2025-09-20" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Trace Replay Simulation of MIT SuperCloud for Studying Optimal
Sustainability Policies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [digital twin, power simulation, reinforcement learning, job scheduling, sustainability policies, trace replay]</li>
<li class=""><strong>authors:</strong> Wesley Brewer, Matthias Maiterth, Damien Fay</li>
<li class=""><strong>institution:</strong> Oak Ridge National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16513v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16513v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper extends the ExaDigiT digital twin framework to simulate MIT SuperCloud workloads using trace replay and reinforcement learning. The RAPS module enables experimentation with energy-aware scheduling policies through Proximal Policy Optimization. Preliminary results demonstrate the feasibility of learning sustainability-focused scheduling decisions that improve datacenter efficiency and throughput.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Shift Parallelism: Low-Latency, High-Throughput LLM Inference for
Dynamic Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [parallelism, dynamic workloads, latency-throughput tradeoff, KV cache, tensor parallelism, sequence parallelism]</li>
<li class=""><strong>authors:</strong> Mert Hidayetoglu, Aurick Qiao, Michael Wyatt, Jeff Rasley, Yuxiong He, Samyam Rajbhandari</li>
<li class=""><strong>institution:</strong> Snowflake AI Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16495v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16495v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Shift Parallelism combines tensor parallelism and sequence parallelism to dynamically adapt to varying traffic loads, achieving both low latency and high throughput. It enables up to 1.51x faster response times in interactive workloads and 50% higher throughput in batch workloads compared to TP-only solutions. The method demonstrates superior latency-throughput tradeoffs across diverse models and traffic patterns.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-21">2025-09-21<a href="#2025-09-21" class="hash-link" aria-label="Direct link to 2025-09-21" title="Direct link to 2025-09-21" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] MoA-Off: Adaptive Heterogeneous Modality-Aware Offloading with
Edge-Cloud Collaboration for Efficient Multimodal LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multimodal LLM, edge-cloud collaboration, adaptive offloading, inference optimization]</li>
<li class=""><strong>authors:</strong> Zheming Yang, Qi Guo, Yunqing Hu, Chang Zhao, Chang Zhang, Jian Zhao, Wen Ji</li>
<li class=""><strong>institution:</strong> Institute of Computing Technology, Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16995v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16995v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MoA-Off introduces a modality-aware complexity estimation module and adaptive edge-cloud offloading strategy for multimodal LLM inference. It dynamically schedules workloads between edge and cloud based on input complexity and system states. The framework achieves over 30% latency reduction and 30%-65% resource savings while maintaining competitive accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix
Caching</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [KV cache, prefix caching, SmartNIC, distributed systems, performance optimization]</li>
<li class=""><strong>authors:</strong> Xingyu Xiang, Raj Joshi, Yuhan Liu, Jiayi Yao, Chenxingyu Zhao, Junchen Jiang, Yang Zhou, Eddie Kohler, Minlan Yu</li>
<li class=""><strong>institution:</strong> Harvard University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16857v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16857v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ShadowServe introduces a SmartNIC-accelerated system that separates control and data planes to eliminate interference during KV cache fetching for distributed prefix caching. It employs a chunked pipeline and minimal-copy memory management to overcome SmartNIC resource limitations. The system achieves up to 2.2× lower TPOT and 1.38× lower TTFT in low-bandwidth scenarios, improving throughput by up to 1.35× compared to state-of-the-art solutions.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-24T13:11:58.000Z" itemprop="dateModified">Oct 24, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20250908-20250914"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20250908-20250914</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20250922-20250928"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20250922-20250928</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-09-15" class="table-of-contents__link toc-highlight">2025-09-15</a></li><li><a href="#2025-09-16" class="table-of-contents__link toc-highlight">2025-09-16</a></li><li><a href="#2025-09-17" class="table-of-contents__link toc-highlight">2025-09-17</a></li><li><a href="#2025-09-18" class="table-of-contents__link toc-highlight">2025-09-18</a></li><li><a href="#2025-09-19" class="table-of-contents__link toc-highlight">2025-09-19</a></li><li><a href="#2025-09-20" class="table-of-contents__link toc-highlight">2025-09-20</a></li><li><a href="#2025-09-21" class="table-of-contents__link toc-highlight">2025-09-21</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>