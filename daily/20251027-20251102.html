<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251027-20251102" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251027-20251102 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20251027-20251102"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251027-20251102 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-10-27"><meta data-rh="true" property="og:description" content="2025-10-27"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20251027-20251102"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251027-20251102" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251027-20251102" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20251027-20251102","item":"https://darkknight996.github.io/daily/20251027-20251102"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.a505882f.js" defer="defer"></script>
<script src="/assets/js/main.e847bade.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251027-20251102</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251027-20251102</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-27">2025-10-27<a href="#2025-10-27" class="hash-link" aria-label="Direct link to 2025-10-27" title="Direct link to 2025-10-27" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Learning to Schedule: A Supervised Learning Framework for Network-Aware
Scheduling of Data-Intensive Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [network-aware scheduling, supervised learning, data-intensive workloads, Kubernetes, job completion time prediction]</li>
<li class=""><strong>authors:</strong> Sankalpa Timilsina, Susmit Shannigrahi</li>
<li class=""><strong>institution:</strong> Tennessee Technological University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21419v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21419v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a network-aware job scheduler using supervised learning to predict job completion times based on real-time cluster telemetry. The system employs a prediction-and-ranking mechanism that evaluates nodes and selects optimal placements for data-intensive workloads. Evaluation on a geo-distributed Kubernetes cluster showed 34-54% higher accuracy in node selection compared to the default Kubernetes scheduler.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] From SLA to vendor-neutral metrics: An intelligent knowledge-based
approach for multi-cloud SLA-based broker</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [cloud computing, multi-cloud, SLA management, vendor-neutral metrics, intelligent knowledge-based system, auto-scaling]</li>
<li class=""><strong>authors:</strong> Víctor Rampérez, Javier Soriano, David Lizcano, Shadi Aljawarneh, Juan A. Lara</li>
<li class=""><strong>institution:</strong> Universidad Politécnica de Madrid (UPM), Madrid Open University (UDIMA)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21173v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21173v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an intelligent knowledge-based system that automatically translates high-level SLAs into vendor-neutral metrics for multi-cloud environments. The approach enables cross-provider metric measurement and provides consumer feedback through an intelligent tutoring system. Validation with IaaS and PaaS use cases demonstrates the system allows transparent multi-cloud exploitation across various application domains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep
Learning Training Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [GPU memory estimation, dynamic analysis, resource management, scheduling]</li>
<li class=""><strong>authors:</strong> Jiabo Shi, Dimitrios Pezaros, Yehia Elkhatib</li>
<li class=""><strong>institution:</strong> University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21048v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21048v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> xMem proposes a CPU-based dynamic analysis framework to accurately estimate peak GPU memory requirements for deep learning training workloads without consuming GPU resources. The method achieves 91% reduction in median relative error and 75% reduction in OOM probability compared to existing solutions. This enables better GPU sharing and scheduling in cluster environments while significantly improving memory conservation potential.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Lincoln AI Computing Survey (LAICS) and Trends</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training, Other models inference], [AI accelerators, performance analysis, power consumption, market segmentation, computing architectures]</li>
<li class=""><strong>authors:</strong> Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally, Jeremy Kepner</li>
<li class=""><strong>institution:</strong> MIT Lincoln Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20931v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20931v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper updates the Lincoln AI Computing Survey by collecting performance and power consumption data of commercial AI accelerators, plotting them on scatter graphs, and analyzing market trends. It introduces a new categorization of computing architectures and examines how GenAI models have shifted computational demands toward matrix-vector operations and high memory bandwidth. The survey highlights ongoing innovations in AI hardware across various deployment scales from embedded systems to data centers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] ParaRNN: Unlocking Parallel Training of Nonlinear RNNs for Large
Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [parallel training, nonlinear RNNs, sequence modeling, Newton&#x27;s iterations, parallel reductions]</li>
<li class=""><strong>authors:</strong> Federico Danieli, Pau Rodriguez, Miguel Sarabia, Xavier Suau, Luca Zappella</li>
<li class=""><strong>institution:</strong> Apple</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21450v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21450v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ParaRNN enables parallel training of nonlinear RNNs by formulating recurrence relationships as a system of equations and solving them using Newton&#x27;s iterations with parallel reductions. This approach achieves up to 665x speedup over sequential methods and allows training 7B parameter RNNs with performance comparable to Transformers and Mamba2. The framework is released as open-source to facilitate scalable nonlinear RNN research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] REVE: A Foundation Model for EEG -- Adapting to Any Setup with
Large-Scale Pretraining on 25,000 Subjects</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [EEG foundation model, 4D positional encoding, masked autoencoding, brain-computer interfaces, clinical neuroscience]</li>
<li class=""><strong>authors:</strong> Yassine El Ouahidi, Jonathan Lys, Philipp Thölke, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon, Karim Jerbi, Giulia Lioi</li>
<li class=""><strong>institution:</strong> IMT Atlantique, Université de Montréal, Mila, UNIQUE</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21585v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21585v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> REVE introduces a novel 4D positional encoding scheme and uses masked autoencoding pretraining on 60,000 hours of EEG data from 25,000 subjects. The model achieves state-of-the-art performance across 10 EEG tasks including motor imagery and seizure detection. It demonstrates strong generalization with minimal fine-tuning and enables standardized EEG research through released code and weights.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-28">2025-10-28<a href="#2025-10-28" class="hash-link" aria-label="Direct link to 2025-10-28" title="Direct link to 2025-10-28" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary
Weights and Distilled Knowledge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [model compression, ternary quantization, knowledge distillation, vision-language models, CLIP]</li>
<li class=""><strong>authors:</strong> Shu-Hao Zhang, Wei-Cheng Tang, Chen Wu, Peng Hu, Nan Li, Liang-Jie Zhang, Qi Zhang, Shao-Qun Zhang</li>
<li class=""><strong>institution:</strong> Nanjing University, Microsoft</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21879v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21879v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TernaryCLIP converts CLIP model weights to ternary format using quantization-aware training and knowledge distillation. The method achieves 99% ternarization with 1.58-bit representation while maintaining performance on zero-shot tasks. This enables efficient deployment on resource-constrained devices with significant compression and acceleration benefits.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Encoder-Decoder Diffusion Language Models for Efficient Training and
Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [diffusion models, encoder-decoder architecture, efficient inference, parallel token sampling, block diffusion]</li>
<li class=""><strong>authors:</strong> Marianne Arriola, Yair Schiff, Hao Phung, Aaron Gokaslan, Volodymyr Kuleshov</li>
<li class=""><strong>institution:</strong> Cornell University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.22852v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.22852v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an encoder-decoder architecture for discrete diffusion language models that separates clean token representation from denoising computation. The method enables faster training and inference by using a lightweight decoder for iterative refinement while periodically updating encoder representations. Experimental results show superior trade-offs between generation quality and inference throughput on summarization, translation, and mathematical reasoning tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Dopamine-driven synaptic credit assignment in neural networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [neural networks, credit assignment problem, dopamine optimizer, weight perturbation, reinforcement learning, adaptive learning rate, neuroAI]</li>
<li class=""><strong>authors:</strong> Saranraj Nambusubramaniyan, Shervin Safavi, Raja Guru, Andreas Knoblauch</li>
<li class=""><strong>institution:</strong> University of Ulm, University of Tübingen</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.22178v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.22178v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a dopamine-inspired derivative-free optimizer for neural networks that uses weight perturbation and reward prediction error to adapt learning rates. This method achieves comparable performance to gradient-based algorithms while being more computationally efficient and biologically plausible. The optimizer demonstrates accelerated convergence in XOR tasks and chaotic time series forecasting with reduced memory and computation requirements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Impact and Implications of Generative AI for Enterprise Architects in
Agile Environments: A Systematic Literature Review</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [generative AI, enterprise architecture, agile environments, systematic literature review, architectural decision support, prompt engineering]</li>
<li class=""><strong>authors:</strong> Stefan Julian Kooy, Jean Paul Sebastian Piest, Rob Henk Bemthuis</li>
<li class=""><strong>institution:</strong> University of Twente</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.22003v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.22003v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts a systematic literature review of 1,697 records to analyze the impact of Generative AI on enterprise architects in agile environments. The study identifies key use cases including design ideation, artifact creation, and architectural decision support, while highlighting risks like opacity, bias, and compliance concerns. The findings provide implications for capability building, governance, and establish a research agenda for human-AI collaboration in enterprise architecture.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] To Use or to Refuse? Re-Centering Student Agency with Generative AI in
Engineering Design Education</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [generative AI, engineering education, design thinking, reflective practice, student agency]</li>
<li class=""><strong>authors:</strong> Thijs Willems, Sumbul Khan, Qian Huang, Bradley Camburn, Nachamma Sockalingam, King Wang Poon</li>
<li class=""><strong>institution:</strong> Singapore University of Technology and Design</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19342v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19342v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study implemented a three-way framework (tool/teammate/non-use) in an AI-enhanced engineering design course to analyze student interactions with generative AI. Students developed practices like accelerated prototyping, prompt refinement, and hallucination recognition while learning to strategically reject AI outputs. The approach transformed AI efficiency into innovation by cultivating selective usage and deeper user research, making AI uptake an assessable design habit.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] PACR: Progressively Ascending Confidence Reward for LLM Reasoning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [reinforcement learning, reasoning, confidence reward, exploration efficiency, model-intrinsic supervision]</li>
<li class=""><strong>authors:</strong> Eunseop Yoon, Hee Suk Yoon, Jaehyun Jang, SooHwan Eom, Qi Dai, Chong Luo, Mark A. Hasegawa-Johnson, Chang D. Yoo</li>
<li class=""><strong>institution:</strong> Korea Advanced Institute of Science and Technology (KAIST), Microsoft Research Asia (MSRA), University of Illinois at Urbana-Champaign (UIUC)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.22255v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.22255v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes PACR, a dense reward signal based on the model&#x27;s progressively ascending confidence in the correct answer during reasoning. This method accelerates exploration and improves training efficiency in reinforcement learning with verifiable rewards. Results show PACR achieves better performance with fewer trajectories across multiple benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Step2Motion: Locomotion Reconstruction from Pressure Sensing Insoles</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [motion reconstruction, pressure sensing insoles, wearable sensors, locomotion analysis, IMU data]</li>
<li class=""><strong>authors:</strong> Jose Luis Ponton, Eduardo Alvarado, Lin Geng Foo, Nuria Pelechano, Carlos Andujar, Marc Habermann</li>
<li class=""><strong>institution:</strong> Universitat Politècnica de Catalunya, Max Planck Institute for Informatics</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.22712v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.22712v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Step2Motion reconstructs human locomotion using multi-modal data from sensor insoles combining pressure measurements and inertial data (accelerations and angular rates). The method demonstrates robust performance across diverse locomotion styles including walking, jogging, dancing and crouching movements. This approach enables unconstrained motion capture in real-world environments without line-of-sight limitations or movement restrictions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] QuArch: A Benchmark for Evaluating LLM Reasoning in Computer
Architecture</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [computer architecture, benchmark, reasoning evaluation, question-answering]</li>
<li class=""><strong>authors:</strong> Shvetank Prakash, Andrew Cheng, Arya Tschand, Mark Mazumder, Varun Gohil, Jeffrey Ma, Jason Yik, Zishen Wan, Jessica Quaye, Elisavet Lydia Alvanaki, Avinash Kumar, Chandrashis Mazumdar, Tuhin Khare, Alexander Ingare, Ikechukwu Uchendu, Radhika Ghosal, Abhishek Tyagi, Chenyu Wang, Andrea Mattia Garavagno, Sarah Gu, Alice Guo, Grace Hur, Luca Carloni, Tushar Krishna, Ankita Nayak, Amir Yazdanbakhsh, Vijay Janapa Reddi</li>
<li class=""><strong>institution:</strong> Harvard University, Massachusetts Institute of Technology, Georgia Institute of Technology, Columbia University, University of Texas at Austin, UC Santa Cruz, University of Rochester, University of Genoa, Scuola Superiore Sant&#x27;Anna, Qualcomm AI Research, Google DeepMind</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.22087v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.22087v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces QuArch, the first benchmark with 2,671 expert-validated QA pairs for evaluating LLM reasoning in computer architecture. The evaluation shows frontier models possess domain knowledge but struggle with higher-order architectural reasoning, achieving accuracies between 34%-72%. This community-developed benchmark establishes foundational standards for measuring LLM capabilities in computer architecture.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] AndroidControl-Curated: Revealing the True Potential of GUI Agents
through Benchmark Purification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [GUI agents, benchmark purification, on-device models, AndroidControl-Curated, Magma-R1 model]</li>
<li class=""><strong>authors:</strong> Ho Fai Leung, Xiaoyan Xi, Fei Zuo</li>
<li class=""><strong>institution:</strong> BMW ArcherMind Information Technology Co. Ltd. (BA TechWorks)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18488v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18488v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper identifies flaws in existing GUI agent benchmarks and creates AndroidControl-Curated through a purification pipeline. It develops Magma-R1-3B model via post-training on curated samples, achieving 75.3% success rate comparable to much larger models. The work demonstrates that GUI agents are more capable than previously measured and closer to practical deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] CosmoCore Affective Dream-Replay Reinforcement Learning for Code
Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, code generation, affective computing, hallucination mitigation, experience replay]</li>
<li class=""><strong>authors:</strong> Santhosh Kumar Ravindran</li>
<li class=""><strong>institution:</strong> Microsoft Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18895v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18895v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CosmoCore introduces an affective reinforcement learning framework that tags code generation trajectories with valence and surprise signals to prioritize replay of error episodes. This neuroscience-inspired approach reduces hallucinated code by 48% and accelerates self-correction by 45% in benchmarks. The system enhances traditional RLHF by incorporating emotional cues for more robust code generation in LLMs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] DynaQuery: A Self-Adapting Framework for Querying Structured and
Multimodal Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [natural language querying, structured databases, multimodal data, schema linking, query planning, retrieval-augmented generation]</li>
<li class=""><strong>authors:</strong> Aymane Hassini</li>
<li class=""><strong>institution:</strong> Al Akhawayn University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18029v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18029v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DynaQuery introduces a self-adapting framework with a Schema Introspection and Linking Engine (SILE) that enhances schema linking in query planning for structured and multimodal databases. It demonstrates superior robustness compared to unstructured RAG approaches by nearly eliminating catastrophic failures like schema hallucination. The framework provides a validated basis for developing reliable natural language database interfaces.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Approximate Gradient Coding for Distributed Learning with Heterogeneous
Stragglers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [gradient coding, distributed learning, straggler mitigation, optimization, convergence analysis]</li>
<li class=""><strong>authors:</strong> Heekang Song, Wan Choi</li>
<li class=""><strong>institution:</strong> Korea Advanced Institute of Science and Technology, Seoul National University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.22539v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.22539v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an optimized gradient coding scheme that minimizes residual error while ensuring unbiased gradient estimation by explicitly considering heterogeneous straggler probabilities. The authors derive closed-form solutions for encoding/decoding coefficients and develop data allocation strategies to reduce redundancy and computation load. Numerical results demonstrate that their approach significantly reduces straggler impact and accelerates convergence compared to existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FineVision: Open Data Is All You Need</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [vision-language models, dataset curation, data decontamination, human-in-the-loop pipeline, multi-modal datasets]</li>
<li class=""><strong>authors:</strong> Luis Wiedmann, Orr Zohar, Amir Mahla, Xiaohan Wang, Rui Li, Thibaud Frere, Leandro von Werra, Aritra Roy Gosthipaty, Andrés Marafioti</li>
<li class=""><strong>institution:</strong> Hugging Face, Technical University Munich, Stanford University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.17269v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.17269v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FineVision introduces a semi-automated human-in-the-loop pipeline to create a unified vision-language dataset of 24 million samples from over 200 sources, featuring rigorous de-duplication and decontamination. The method combines automated processing with human oversight for quality control and schema validation. Models trained on this curated dataset consistently outperform those using existing open mixtures, demonstrating the value of scale and data hygiene in vision-language model development.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-28T07:10:18.000Z" itemprop="dateModified">Oct 28, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20251020-20251026"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251020-20251026</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/category/paper"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Paper</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-10-27" class="table-of-contents__link toc-highlight">2025-10-27</a></li><li><a href="#2025-10-28" class="table-of-contents__link toc-highlight">2025-10-28</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>