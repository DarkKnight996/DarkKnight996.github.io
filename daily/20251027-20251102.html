<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251027-20251102" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251027-20251102 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20251027-20251102"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251027-20251102 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-10-27"><meta data-rh="true" property="og:description" content="2025-10-27"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20251027-20251102"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251027-20251102" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251027-20251102" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20251027-20251102","item":"https://darkknight996.github.io/daily/20251027-20251102"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.8e854334.js" defer="defer"></script>
<script src="/assets/js/main.bc7992e9.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251027-20251102</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251027-20251102</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-27">2025-10-27<a href="#2025-10-27" class="hash-link" aria-label="Direct link to 2025-10-27" title="Direct link to 2025-10-27" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Learning to Schedule: A Supervised Learning Framework for Network-Aware
Scheduling of Data-Intensive Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [network-aware scheduling, supervised learning, data-intensive workloads, Kubernetes, job completion time prediction]</li>
<li class=""><strong>authors:</strong> Sankalpa Timilsina, Susmit Shannigrahi</li>
<li class=""><strong>institution:</strong> Tennessee Technological University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21419v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21419v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a network-aware job scheduler using supervised learning to predict job completion times based on real-time cluster telemetry. The system employs a prediction-and-ranking mechanism that evaluates nodes and selects optimal placements for data-intensive workloads. Evaluation on a geo-distributed Kubernetes cluster showed 34-54% higher accuracy in node selection compared to the default Kubernetes scheduler.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] From SLA to vendor-neutral metrics: An intelligent knowledge-based
approach for multi-cloud SLA-based broker</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [cloud computing, multi-cloud, SLA management, vendor-neutral metrics, intelligent knowledge-based system, auto-scaling]</li>
<li class=""><strong>authors:</strong> Víctor Rampérez, Javier Soriano, David Lizcano, Shadi Aljawarneh, Juan A. Lara</li>
<li class=""><strong>institution:</strong> Universidad Politécnica de Madrid (UPM), Madrid Open University (UDIMA)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21173v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21173v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an intelligent knowledge-based system that automatically translates high-level SLAs into vendor-neutral metrics for multi-cloud environments. The approach enables cross-provider metric measurement and provides consumer feedback through an intelligent tutoring system. Validation with IaaS and PaaS use cases demonstrates the system allows transparent multi-cloud exploitation across various application domains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep
Learning Training Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [GPU memory estimation, dynamic analysis, resource management, scheduling]</li>
<li class=""><strong>authors:</strong> Jiabo Shi, Dimitrios Pezaros, Yehia Elkhatib</li>
<li class=""><strong>institution:</strong> University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21048v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21048v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> xMem proposes a CPU-based dynamic analysis framework to accurately estimate peak GPU memory requirements for deep learning training workloads without consuming GPU resources. The method achieves 91% reduction in median relative error and 75% reduction in OOM probability compared to existing solutions. This enables better GPU sharing and scheduling in cluster environments while significantly improving memory conservation potential.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Lincoln AI Computing Survey (LAICS) and Trends</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training, Other models inference], [AI accelerators, performance analysis, power consumption, market segmentation, computing architectures]</li>
<li class=""><strong>authors:</strong> Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally, Jeremy Kepner</li>
<li class=""><strong>institution:</strong> MIT Lincoln Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20931v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20931v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper updates the Lincoln AI Computing Survey by collecting performance and power consumption data of commercial AI accelerators, plotting them on scatter graphs, and analyzing market trends. It introduces a new categorization of computing architectures and examines how GenAI models have shifted computational demands toward matrix-vector operations and high memory bandwidth. The survey highlights ongoing innovations in AI hardware across various deployment scales from embedded systems to data centers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] ParaRNN: Unlocking Parallel Training of Nonlinear RNNs for Large
Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [parallel training, nonlinear RNNs, sequence modeling, Newton&#x27;s iterations, parallel reductions]</li>
<li class=""><strong>authors:</strong> Federico Danieli, Pau Rodriguez, Miguel Sarabia, Xavier Suau, Luca Zappella</li>
<li class=""><strong>institution:</strong> Apple</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21450v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21450v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ParaRNN enables parallel training of nonlinear RNNs by formulating recurrence relationships as a system of equations and solving them using Newton&#x27;s iterations with parallel reductions. This approach achieves up to 665x speedup over sequential methods and allows training 7B parameter RNNs with performance comparable to Transformers and Mamba2. The framework is released as open-source to facilitate scalable nonlinear RNN research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] REVE: A Foundation Model for EEG -- Adapting to Any Setup with
Large-Scale Pretraining on 25,000 Subjects</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [EEG foundation model, 4D positional encoding, masked autoencoding, brain-computer interfaces, clinical neuroscience]</li>
<li class=""><strong>authors:</strong> Yassine El Ouahidi, Jonathan Lys, Philipp Thölke, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon, Karim Jerbi, Giulia Lioi</li>
<li class=""><strong>institution:</strong> IMT Atlantique, Université de Montréal, Mila, UNIQUE</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21585v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21585v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> REVE introduces a novel 4D positional encoding scheme and uses masked autoencoding pretraining on 60,000 hours of EEG data from 25,000 subjects. The model achieves state-of-the-art performance across 10 EEG tasks including motor imagery and seizure detection. It demonstrates strong generalization with minimal fine-tuning and enables standardized EEG research through released code and weights.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-28">2025-10-28<a href="#2025-10-28" class="hash-link" aria-label="Direct link to 2025-10-28" title="Direct link to 2025-10-28" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Anonymized Network Sensing using C++26 std::execution on GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [GPU computing, network sensing, C++26 std::execution, asynchronous programming, multi-GPU systems]</li>
<li class=""><strong>authors:</strong> Michael Mandulak, Sayan Ghosh, S M Ferdous, Mahantesh Halappanavar, George Slota</li>
<li class=""><strong>institution:</strong> Rensselaer Polytechnic Institute, Pacific Northwest National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.14050v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.14050v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a C++26 std::execution Senders model implementation for anonymized network sensing on dense-GPU systems. The method uses standardized asynchronous semantics to efficiently deploy analytics tasks across multiple GPUs. The approach achieves up to 55x performance improvement compared to serial GraphBLAS baseline while maintaining programming productivity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficiently Executing High-throughput Lightweight LLM Inference
Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive
Context Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [pervasive context management, high-throughput applications, heterogeneous GPU clusters, opportunistic resource allocation, context decoupling, fact verification]</li>
<li class=""><strong>authors:</strong> Thanh Son Phung, Douglas Thain</li>
<li class=""><strong>institution:</strong> University of Notre Dame</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.14024v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.14024v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Pervasive Context Management, which decouples LLM initialization context from actual inferences and retains contexts in GPUs to avoid repeated startup costs. This approach enables efficient execution on heterogeneous opportunistic GPU clusters, reducing execution time by 72.1% and allowing scaling across 32.8% of cluster GPUs to further cut time from 3 hours to 13 minutes in a fact verification application.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI
Model Access</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [federated inference, resource scheduling, HPC clusters, OpenAI-compatible API, vLLM, Globus]</li>
<li class=""><strong>authors:</strong> Aditya Tanikanti, Benoit Côté, Yanfei Guo, Le Chen, Nickolaus Saint, Ryan Chard, Ken Raffenetti, Rajeev Thakur, Thomas Uram, Ian Foster, Michael E. Papka, Venkatram Vishwanath</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory, The University of Chicago, University of Illinois Chicago</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.13724v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.13724v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FIRST provides a federated inference framework that enables Inference-as-a-Service across distributed HPC clusters using an OpenAI-compatible API and Globus services. It supports multiple inference backends, auto-scales resources, and maintains hot nodes for low-latency execution. The system allows researchers to perform scalable AI inference on-premises without relying on commercial cloud infrastructure, generating billions of tokens daily for scientific workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [adaptive scheduling, length prediction, workload balancing, KV cache management, prefill-decode disaggregation]</li>
<li class=""><strong>authors:</strong> Zhibin Wang, Zetao Hong, Xue Li, Zibo Wang, Shipeng Li, Qingkai Meng, Qing Wang, Chengying Huan, Rong Gu, Sheng Zhong, Chen Tian</li>
<li class=""><strong>institution:</strong> Nanjing University, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.13668v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.13668v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ARES proposes an adaptive rescheduling system using lightweight LLM-native length prediction to forecast future decode workloads. It reduces prediction MAE by 49.42% and parameters by 93.28% while dynamically balancing workloads. The system achieves 74.77% P99 TPOT reduction and up to 2.24× higher goodput by preventing OOM failures and SLO violations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [quantization, edge devices, FPGA accelerator, block floating point, matrix multiplication]</li>
<li class=""><strong>authors:</strong> Jude Haris, José Cano</li>
<li class=""><strong>institution:</strong> University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.13401v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.13401v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes F-BFQ, a flexible block floating-point quantization accelerator that dynamically switches between BFP variants for efficient LLM inference on edge devices. Implemented on AMD Kria board, it achieves 1.4x faster inference over CPU execution while maintaining 5.2 tokens per second throughput. This demonstrates effective hardware acceleration for quantized LLMs without accuracy loss.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Laminar: A Scalable Asynchronous RL Post-Training Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, large language models, asynchronous training, GPU utilization, distributed systems]</li>
<li class=""><strong>authors:</strong> Guangming Sheng, Yuxuan Tong, Borui Wan, Wang Zhang, Chaobo Jia, Xibin Wu, Yuqi Wu, Xiang Li, Chi Zhang, Yanghua Peng, Haibin Lin, Xin Liu, Chuan Wu</li>
<li class=""><strong>institution:</strong> The University of Hong Kong, ByteDance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.12633v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.12633v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Laminar introduces a fully decoupled architecture for RL post-training that replaces global synchronization with distributed relay workers and implements dynamic trajectory repacking. This approach enables asynchronous weight updates and handles long-tail latency in trajectory generation. The system achieves up to 5.48× throughput improvement and faster convergence on large GPU clusters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Distributed Reductions for the Maximum Weight Independent Set Problem</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [distributed algorithms, graph algorithms, maximum weight independent set, parallel computing, data reduction]</li>
<li class=""><strong>authors:</strong> Jannick Borowitz, Ernestine Großmann, Mattthias Schimek</li>
<li class=""><strong>institution:</strong> Karlsruhe Institute of Technology, University of Heidelberg</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.13306v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.13306v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the first distributed-memory parallel reduction algorithms for the maximum weight independent set problem, enabling processing of billion-scale graphs. The proposed methods include distributed reduce-and-greedy and reduce-and-peel heuristics that achieve significant speedups (33-50×) over sequential approaches while maintaining comparable solution quality. The distributed approach successfully handles graphs with over one billion vertices and 17 billion edges while demonstrating good scalability across up to 1024 processors.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization
of Banded Matrices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU acceleration, banded matrix bidiagonalization, SVD algorithm, Julia programming, performance optimization]</li>
<li class=""><strong>authors:</strong> Evelyne Ringoot, Rabab Alomairy, Alan Edelman</li>
<li class=""><strong>institution:</strong> Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.12705v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.12705v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents the first GPU algorithm for banded matrix bidiagonalization using cache-efficient bulge chasing adapted for GPU throughput. The implementation leverages Julia&#x27;s abstractions to create hardware-agnostic functions across multiple GPU platforms. Results show the GPU algorithm outperforms CPU libraries by over 100x for large matrices and breaks memory bandwidth barriers for bandwidth-bound workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing
Disaggregated LLM Serving in AI Infrastructure</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [KV cache migration, dynamic resource allocation, load balancing, disaggregated serving, performance optimization]</li>
<li class=""><strong>authors:</strong> Yiyuan He, Minxian Xu, Jingfeng Wu, Jianmin Hu, Chong Ma, Min Shen, Le Chen, Chengzhong Xu, Lin Qu, Kejiang Ye</li>
<li class=""><strong>institution:</strong> Southern University of Science and Technology, Shenzhen Institutes of Advanced Technology Chinese Academy of Sciences, University of Chinese Academy of Sciences, University of Macau, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.13223v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.13223v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BanaServe introduces dynamic module migration and unified KV cache management to balance computational and memory resources in disaggregated LLM serving. It enables both coarse-grained layer migration and fine-grained KV cache migration with minimal latency overhead. The system achieves 1.2x-3.9x higher throughput than vLLM and 1.1x-2.8x higher than DistServe while significantly reducing latency across diverse workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching
for Heterogeneous Tasks and Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [decentralized scheduling, load caching, balls-into-bins, heterogeneous clusters, task scheduling]</li>
<li class=""><strong>authors:</strong> Wei Da, Evangelia Kalyvianaki</li>
<li class=""><strong>institution:</strong> The University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.12889v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.12889v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dodoor introduces a decentralized scheduler using cached server information and a novel load score to reduce communication overhead in heterogeneous clusters. It achieves significant reductions in scheduling messages (55-66%) while improving throughput and latency across different workloads. The system demonstrates practical efficiency gains through batch updates and anti-affinity-aware load balancing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] GPU-Accelerated Algorithms for Process Mapping</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [GPU acceleration, graph partitioning, process mapping, high-performance computing]</li>
<li class=""><strong>authors:</strong> Petr Samoldekin, Christian Schulz, Henning Woydt</li>
<li class=""><strong>institution:</strong> Heidelberg University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.12196v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.12196v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes two GPU-accelerated algorithms for process mapping using hierarchical multisection and multilevel graph partitioning approaches. Both methods achieve significant speedups exceeding 300x compared to CPU-based algorithms, with the first maintaining competitive solution quality and the second achieving even higher speeds at reduced quality. These represent the first GPU-based algorithms developed for the process mapping problem.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] nuGPR: GPU-Accelerated Gaussian Process Regression with Iterative
Algorithms and Low-Rank Approximations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [Gaussian Process Regression, GPU acceleration, low-rank approximations, iterative algorithms, numerical gradients]</li>
<li class=""><strong>authors:</strong> Ziqi Zhao, Vivek Sarin</li>
<li class=""><strong>institution:</strong> Texas A&amp;M University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.12128v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.12128v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes nuGPR, a GPU-accelerated Gaussian Process Regression framework that combines preconditioned conjugate gradient methods, clustering-based block-diagonal covariance structures, low-rank approximations, and numerical gradients to optimize hyperparameters. These techniques significantly reduce computational costs and memory requirements. Experimental results show nuGPR achieves up to 2x faster training and 12x lower memory consumption compared to existing GPU-based GPR implementations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Metronome: Efficient Scheduling for Periodic Traffic Jobs with Network
and Priority Awareness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [cloud native networks, periodic traffic patterns, network-aware scheduling, priority-aware scheduling, distributed training, time-division multiplexing, multi-objective optimization]</li>
<li class=""><strong>authors:</strong> Hao Jiang, Meng Qin, Ruijie Kuai, Dandan Liang</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Pengcheng Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.12274v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.12274v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Metronome proposes a network and priority aware scheduling mechanism using time-division multiplexing and multi-objective optimization for periodic traffic jobs in cloud native networks. It dynamically allocates bandwidth resources while considering job priorities and latency requirements. Experiments show Metronome reduces job completion time by up to 19.50% and improves bandwidth utilization by 23.20% compared to existing Kubernetes schedulers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Decentralized Microservice Scheduling Approach Using Service Mesh in
Cloud-Edge Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [scheduling], [decentralized scheduling, microservices, service mesh, cloud-edge systems, sidecar proxy, scalability, latency]</li>
<li class=""><strong>authors:</strong> Yangyang Wen, Paul Townend, Per-Olov Östberg, Abel Souza, Clément Courageux-Sudan</li>
<li class=""><strong>institution:</strong> Umeå University, UC Santa Cruz</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.11189v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.11189v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a decentralized microservice scheduling architecture using service mesh sidecar proxies to enable local scheduling decisions without centralized control. The approach embeds lightweight autonomous scheduling logic into sidecar proxies for distributed traffic management. Initial results demonstrate improved scalability potential with reduced response time and latency under varying request rates compared to traditional centralized scheduling mechanisms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] An Explorative Study on Distributed Computing Techniques in Training and
Inference of Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [distributed computing, model parallelism, metaheuristics, NSGA-II, attention mechanisms]</li>
<li class=""><strong>authors:</strong> Sheikh Azizul Hakim, Saem Hasan</li>
<li class=""><strong>institution:</strong> Bangladesh University of Engineering and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.11211v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.11211v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study explores distributed computing techniques for large language models from two perspectives: democratizing LLMs through consumer-grade hardware with metaheuristic modifications, and comparing state-of-the-art serving systems. The research implements NSGA-II-based optimizations and evaluates model parallelism approaches. Findings demonstrate effective methods for running large models on limited hardware while maintaining performance through distributed computing strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline
Refactoring in Fragmented Serverless Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [pipeline parallelism, dynamic scaling, serverless computing, resource efficiency]</li>
<li class=""><strong>authors:</strong> Yanying Lin, Shijie Peng, Chengzhi Lu, Chengzhong Xu, Kejiang Ye</li>
<li class=""><strong>institution:</strong> Shenzhen Institutes of Advanced Technology CAS, University of Chinese Academy of Sciences, UC San Diego, Nanyang Technological University, University of Macau</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.11938v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.11938v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlexPipe dynamically reconfigures LLM serving pipelines during runtime by decomposing models into fine-grained stages and adjusting pipeline granularity based on real-time request patterns. It implements fine-grained model partitioning, inflight pipeline refactoring with cache consistency, and topology-aware resource allocation. The system achieves up to 8.5× better resource efficiency and 38.3% lower latency while reducing GPU reservation requirements from 75% to 30% of peak capacity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Improving AI Efficiency in Data Centres by Power Dynamic Response</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [power management, dynamic power response, energy efficiency, data centers, sustainability, computational gain, cost reduction]</li>
<li class=""><strong>authors:</strong> Andrea Marinoni, Sai Shivareddy, Pietro Lio&#x27;, Weisi Lin, Erik Cambria, Clare Grey</li>
<li class=""><strong>institution:</strong> University of Cambridge, Nyobolt Limited, Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.11119v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.11119v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a dynamic power management strategy for AI data centers that makes input power as flexible as computing power. It evaluates passive and active devices by analyzing global power trends, showing improvements in computational gain, energy efficiency, and cost reduction. The approach represents a paradigm shift that enhances sustainability across environmental, financial, and societal dimensions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] DCP: Addressing Input Dynamism In Long-Context Training via Dynamic
Context Parallelism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [context parallelism, dynamic partitioning, distributed training, attention mechanism, long-context training]</li>
<li class=""><strong>authors:</strong> Chenyu Jiang, Zhenkun Cai, Ye Tian, Zhen Jia, Yida Wang, Chuan Wu</li>
<li class=""><strong>institution:</strong> The University of Hong Kong, Amazon Web Services</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.10620v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.10620v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DCP introduces a dynamic context parallel training framework with fine-grained blockwise partitioning of data and computation to address input variability in long-context training. It enables flexible mapping of blocks to devices, reducing communication overhead and improving computation balance. Benchmark results show 1.19x-3.77x attention acceleration and up to 1.46x end-to-end training speed-up under different attention patterns.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based
Model Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [speculative decoding, mixture-of-experts, expert prefetching, model offloading, inference acceleration]</li>
<li class=""><strong>authors:</strong> Liangkun Chen, Zijian Wen, Tian Wu, Xiaoxi Zhang, Chuan Wu</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University, The University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.10302v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.10302v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SP-MoE introduces a speculative decoding-aware framework that combines expert prefetching with compute-communication pipelining to accelerate MoE-based model inference. The method uses structural correspondence between draft and target models to prefetch experts ahead of verification and implements a cutoff-layer policy to optimize prefetch depth. Experiments show SP-MoE achieves 1.07-3.5× speedup over state-of-the-art methods across various datasets and MoE models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude
Economy Networks via LLM-Enhanced Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [UAV networks, vision-language models, resource optimization, trajectory optimization, reinforcement learning]</li>
<li class=""><strong>authors:</strong> Yang Li, Ruichen Zhang, Yinqiu Liu, Guangyuan Liu, Dusit Niyato, Abbas Jamalipour, Xianbin Wang, Dong In Kim</li>
<li class=""><strong>institution:</strong> Nanyang Technological University, The University of Sydney, Western University, Sungkyunkwan University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.10028v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.10028v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hierarchical optimization framework combining alternating resource optimization and LLM-enhanced reinforcement learning for UAV trajectory planning. The approach minimizes latency and power consumption while meeting accuracy constraints in vision-language inference tasks. Numerical results demonstrate improved inference performance and communication efficiency under dynamic network conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Slicing Is All You Need: Towards A Universal One-Sided Algorithm for
Distributed Matrix Multiplication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [distributed matrix multiplication, one-sided communication, slicing algorithm, partitioning strategies, GPU-to-GPU communication]</li>
<li class=""><strong>authors:</strong> Benjamin Brock, Renato Golin</li>
<li class=""><strong>institution:</strong> Intel Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08874v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08874v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a universal one-sided algorithm for distributed matrix multiplication that uses slicing operations to support all combinations of partitionings and replication factors. The method computes overlapping tile sets through index arithmetic and can execute operations directly or optimize via reordering. The implementation demonstrates competitive performance with PyTorch DTensor across various partitioning configurations while eliminating the need for operand redistribution.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Hierarchical Scheduling for Multi-Vector Image Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [image retrieval, hierarchical scheduling, multi-vector retrieval, efficiency optimization, cross-hierarchy similarity, hierarchy sparsity]</li>
<li class=""><strong>authors:</strong> Maoliang Li, Ke Li, Yaoyang Liu, Jiayu Chen, Zihao Zheng, Yinjun Wu, Xiang Chen</li>
<li class=""><strong>institution:</strong> Peking University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08976v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08976v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes HiMIR, a hierarchical scheduling framework for multi-vector image retrieval that uses multiple granularities for better query-image alignment and reduces redundancy through cross-hierarchy similarity consistency and hierarchy sparsity. It automatically configures parameters for different datasets to improve practicality. Empirical results show HiMIR achieves significant accuracy improvements while reducing computation by up to 3.5× compared to existing MVR systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Maple: A Multi-agent System for Portable Deep Learning across Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [deep learning, multi-agent system, natural language processing, GPU clusters, distributed training, HPC]</li>
<li class=""><strong>authors:</strong> Molang Wu, Zhao Zhang</li>
<li class=""><strong>institution:</strong> Rutgers University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08842v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08842v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Maple introduces a multi-agent system that generates correct deep learning command lines using natural language input, addressing the challenge of portable DL deployment across heterogeneous GPU clusters. The system achieves 92.0% accuracy across 567 test cases spanning nine clusters and various DL frameworks. It demonstrates comparable performance to state-of-the-art models like GPT-5 while enabling scalable distributed DL in HPC environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Man-Made Heuristics Are Dead. Long Live Code Generators!</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [automated policy design, code generation, heuristic synthesis, web caching, congestion control, Linux kernel integration]</li>
<li class=""><strong>authors:</strong> Rohit Dwivedula, Divyanshu Saxena, Aditya Akella, Swarat Chaudhuri, Daehyeok Kim</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08803v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08803v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PolicySmith uses LLM-driven code generation to automatically synthesize instance-optimal heuristics for systems policies. The framework generates policies that outperform established baselines in web caching and creates safe congestion control policies that integrate directly into the Linux kernel. This demonstrates how LLMs can automate and improve traditional manual policy design processes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] DYNAMIX: RL-based Adaptive Batch Size Optimization in Distributed
Machine Learning Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [distributed machine learning, batch size optimization, reinforcement learning, adaptive systems, Proximal Policy Optimization]</li>
<li class=""><strong>authors:</strong> Yuanjun Dai, Keqiang He, An Wang</li>
<li class=""><strong>institution:</strong> Case Western Reserve University, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08522v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08522v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DYNAMIX uses reinforcement learning with Proximal Policy Optimization to dynamically optimize batch sizes in distributed machine learning systems. The framework employs multi-dimensional state representations including network metrics and resource utilization without requiring explicit system modeling. Evaluations show improvements of up to 6.3% in model accuracy and 46% reduction in training time while maintaining performance across cluster scales.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Towards Energy-Efficient Serverless Computing with Hardware Isolation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [serverless], [hardware isolation, energy efficiency, serverless computing, function-as-a-service]</li>
<li class=""><strong>authors:</strong> Natalie Carl, Tobias Pfandzelter, David Bermbach</li>
<li class=""><strong>institution:</strong> Technische Universität Berlin</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08180v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08180v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using hardware isolation with individual processors per function instead of software isolation in serverless computing. The approach aligns serverless hardware architecture with software requirements to reduce energy consumption. Preliminary evaluation shows this could reduce energy consumption overheads by 90.63% compared to traditional approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM
Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [hardware specialization, prefill-decode disaggregation, cost optimization, energy efficiency, systolic arrays, GDDR memory]</li>
<li class=""><strong>authors:</strong> Hengrui Zhang, Pratyush Patel, August Ning, David Wentzlaff</li>
<li class=""><strong>institution:</strong> Princeton University, University of Washington</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08544v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08544v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SPAD proposes specialized hardware designs with Prefill Chips featuring larger systolic arrays and GDDR memory, and Decode Chips maintaining high bandwidth with reduced compute. This approach achieves 8% higher prefill performance at 52% lower cost and 97% decode performance with 28% lower TDP compared to H100s. End-to-end simulations show 19%-41% hardware cost reduction and 2%-17% TDP reduction while maintaining performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] pyGinkgo: A Sparse Linear Algebra Operator Framework for Python</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [sparse linear algebra, high-performance computing, Python libraries, GPU acceleration, scientific computing]</li>
<li class=""><strong>authors:</strong> Keshvi Tuteja, Gregor Olenik, Roman Mishchuk, Yu-Hsiang Tsai, Markus Götz, Achim Streit, Hartwig Anzt, Charlotte Debus</li>
<li class=""><strong>institution:</strong> Karlsruhe Institute of Technology, Technical University of Munich</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08230v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08230v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> pyGinkgo provides a Python interface to the high-performance Ginkgo sparse linear algebra library using Pybind11, with compatibility for NumPy and PyTorch. It demonstrates superior performance over existing Python libraries in sparse matrix operations and iterative solvers across multiple hardware platforms. The framework maintains performance parity with native C++ implementations while offering Python usability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving
with Layered Prefill</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM inference, Mixture-of-Experts, stall-free scheduling, layered prefill, energy efficiency, memory bandwidth]</li>
<li class=""><strong>authors:</strong> Gunjun Lee, Jiwon Kim, Jaiyoung Park, Younjoo Lee, Jung Ho Ahn</li>
<li class=""><strong>institution:</strong> Seoul National University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08055v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08055v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes layered prefill, a new scheduling paradigm that vertically partitions transformer models into layer groups and interleaves prefill and decode operations across these groups. This approach eliminates redundant weight reloads in Mixture-of-Experts models, reducing memory traffic and energy consumption. Evaluations show layered prefill improves TTFT by up to 70%, end-to-end latency by 41%, and per-token energy by 22% while maintaining stall-free decoding performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Adaptive Execution Scheduler for DataDios SmartDiff</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [adaptive scheduler, differencing engine, latency optimization, memory management, parallel execution]</li>
<li class=""><strong>authors:</strong> Aryan Poduri</li>
<li class=""><strong>institution:</strong> DataDios</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.07811v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.07811v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents an adaptive execution scheduler for DataDios SmartDiff that dynamically tunes batch sizes and worker counts while selecting between in-memory threads and Dask-based parallelism. It uses a lightweight profiler, online cost/memory models, and guarded hill-climbing to minimize p95 latency under fixed CPU/memory constraints. Experimental results show 23-28% latency reduction and 16-22% memory savings compared to tuned baselines while eliminating out-of-memory errors.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Vectorized FlashAttention with Low-cost Exponential Computation in
RISC-V Vector Processors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [RISC-V, vector processors, FlashAttention, exponential approximation, tiling strategies]</li>
<li class=""><strong>authors:</strong> Vasileios Titopoulos, Kosmas Alexandridis, Giorgos Dimitrakopoulos</li>
<li class=""><strong>institution:</strong> Democritus University of Thrace</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.06834v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.06834v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a vectorized implementation of FlashAttention for RISC-V processors using low-cost exponential approximations to simplify softmax computations. The approach eliminates the need for custom ISA extensions while employing tiling strategies to improve memory locality. Experimental results demonstrate significant performance improvements in attention layer processing across practical applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] REACH: Reinforcement Learning for Adaptive Microservice Rescheduling in
the Cloud-Edge Continuum</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [reinforcement learning, microservice rescheduling, cloud-edge continuum, latency optimization, adaptive placement]</li>
<li class=""><strong>authors:</strong> Xu Bai, Muhammed Tawfiqul Islam, Rajkumar Buyya, Adel N. Toosi</li>
<li class=""><strong>institution:</strong> University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.06675v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.06675v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> REACH proposes a reinforcement learning-based algorithm for dynamic microservice rescheduling in cloud-edge environments. The method adapts to fluctuating resource availability and performance variations in real-time. Experimental results show REACH reduces end-to-end latency by 7.9-10% across benchmark applications while mitigating latency fluctuations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] On-Package Memory with Universal Chiplet Interconnect Express (UCIe): A
Low Power, High Bandwidth, Low Latency and Low Cost Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [on-package memory, UCIe, bandwidth optimization, power efficiency, chiplet interconnect]</li>
<li class=""><strong>authors:</strong> Debendra Das Sharma, Swadesh Choudhary, Peter Onufryk, Rob Pelt</li>
<li class=""><strong>institution:</strong> Intel Corporation, AMD Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.06513v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.06513v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes enhancing UCIe with memory semantics to create power-efficient on-package memory solutions by reusing existing LPDDR6 and HBM through a logic die or enabling DRAM dies to natively support UCIe. The approaches achieve significantly higher bandwidth density (up to 10x), lower latency (up to 3x), reduced power consumption (up to 3x), and lower cost compared to existing HBM4 and LPDDR solutions, addressing memory bottlenecks in AI and high-performance computing applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] EARL: Efficient Agentic Reinforcement Learning Systems for Large
Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [agentic reinforcement learning, distributed training, dynamic parallelism, memory optimization, context length management]</li>
<li class=""><strong>authors:</strong> Zheyue Tan, Mustapha Abdullahi, Tuo Shi, Huining Yuan, Zelai Xu, Chao Yu, Boxun Li, Bo Zhao</li>
<li class=""><strong>institution:</strong> Aalto University, Tsinghua University, Infinigence-AI</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05943v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05943v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EARL introduces a parallelism selector that dynamically adapts model and training parallelism based on sequence length and system load, along with a data dispatcher for layout-aware decentralized data exchange. These components collectively enhance throughput, reduce long-context failures, and enable stable large-scale training of agentic LLMs without imposing hard limits on context length.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Toward Systems Foundations for Agentic Exploration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [checkpointing], [agentic exploration, snapshot/restore mechanisms, fork semantics, external side-effects, native forking]</li>
<li class=""><strong>authors:</strong> Jiakai Xu, Tianle Zhou, Eugene Wu, Kostis Kaffes</li>
<li class=""><strong>institution:</strong> Columbia University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05556v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05556v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper benchmarks six snapshot/restore mechanisms for LLM-powered agent exploration and finds current tools like CRIU insufficient for real deployments. It identifies three fundamental challenges: fork semantics, external side-effects, and native forking requirements. The research demonstrates that enabling exploration from intermediate states significantly improves agent performance compared to traditional pass@k methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Review of Ontology-Driven Big Data Analytics in Healthcare:
Challenges, Tools, and Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [ontology-driven analytics, healthcare data management, semantic interoperability, big data frameworks, knowledge graphs]</li>
<li class=""><strong>authors:</strong> Ritesh Chandra, Sonali Agarwal, Navjot Singh, Sadhana Tiwari</li>
<li class=""><strong>institution:</strong> Indian Institute of Information Technology Allahabad</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05738v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05738v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This review systematically examines ontology-driven approaches for managing and analyzing healthcare big data, classifying methods into six semantic analytics categories. It explores the integration of ontology technologies with big data frameworks like Hadoop and Spark to enhance semantic interoperability and data discoverability. The study concludes that ontology-driven semantic management can transform data lakes into organized, intelligent healthcare ecosystems supporting scalable analytics and decision-making.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data
Movement Forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [MoE, data movement, expert selection, performance optimization, profiling, trace analysis, wafer-scale GPUs]</li>
<li class=""><strong>authors:</strong> Zhongkai Yu, Yue Guan, Zihao Yu, Chenyang Zhou, Shuyi Pei, Yangwook Kang, Yufei Ding, Po-An Tsai</li>
<li class=""><strong>institution:</strong> UCSD, Indiana University Bloomington, Columbia University, Samsung Semiconductor, Inc, NVIDIA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05497v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05497v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts comprehensive data-movement profiling of large-scale MoE LLMs and identifies key patterns through temporal and spatial analysis. By leveraging these insights for architectural modifications, the authors achieve significant performance improvements with 6.3× and 4.0× speedups on DeepSeek V3 and Qwen3 respectively. The work provides the first data-centric analysis of MoE models at scale and releases profiling traces to facilitate future research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM
Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [pipeline parallelism, memory optimization, scheduling optimization, activation offloading, constrained optimization]</li>
<li class=""><strong>authors:</strong> Hongpei Li, Han Zhang, Huikang Liu, Dongdong Ge, Yinyu Ye</li>
<li class=""><strong>institution:</strong> Shanghai University of Finance and Economics, National University of Singapore, Shanghai Jiao Tong University, Stanford University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05186v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05186v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OptPipe formulates pipeline scheduling as a constrained optimization problem that jointly considers memory capacity, activation reuse, and pipeline bubble minimization. The approach dynamically optimizes memory-time tradeoffs based on model structure and hardware configuration. Experimental results show up to 50% reduction in pipeline idle time and improved throughput under memory constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Next-Generation Event-Driven Architectures: Performance, Scalability,
and Intelligent Orchestration Across Messaging Frameworks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [event-driven architecture, messaging frameworks, performance benchmarking, intelligent orchestration, distributed systems]</li>
<li class=""><strong>authors:</strong> Jahidul Arafat, Fariha Tasmin, Sanjaya Poudel</li>
<li class=""><strong>institution:</strong> Auburn University, Bangladesh University of Professionals</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.04404v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.04404v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces AIEO (AI-Enhanced Event Orchestration) using machine learning for predictive scaling and optimization in event-driven systems. It comprehensively benchmarks 12 messaging frameworks across different workloads, revealing performance trade-offs between systems like Kafka and Pulsar. The proposed AIEO solution demonstrates significant improvements in latency reduction, resource utilization, and cost optimization across all platforms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Speculative Actions: A Lossless Framework for Faster Agentic Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [speculative execution, agentic systems, parallel processing, latency reduction, action prediction]</li>
<li class=""><strong>authors:</strong> Naimeng Ye, Arnav Ahuja, Georgios Liargkovas, Yunan Lu, Kostis Kaffes, Tianyi Peng</li>
<li class=""><strong>institution:</strong> Columbia University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.04371v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.04371v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes speculative actions, a lossless framework inspired by speculative execution in microprocessors and speculative decoding in LLM inference, which predicts likely agent actions using faster models to enable parallel execution of multiple steps. The method achieves up to 55% accuracy in next-action prediction across gaming, e-commerce, and web search environments, significantly reducing end-to-end latency. Performance can be further improved through stronger guessing models, top-K prediction, multi-step speculation, and uncertainty-aware optimization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] SATER: A Self-Aware and Token-Efficient Approach to Routing and
Cascading</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [routing strategies, cascade routing, pre-generation routing, model efficiency, computational cost reduction]</li>
<li class=""><strong>authors:</strong> Yuanzhe Shen, Yide Liu, Zisu Huang, Ruicheng Yin, Xiaoqing Zheng, Xuanjing Huang</li>
<li class=""><strong>institution:</strong> Fudan University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05164v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05164v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SATER introduces a dual-mode compatible approach that combines shortest-response preference optimization and confidence-aware rejection mechanisms to optimize both pre-generation and cascade routing strategies. The method significantly reduces computational costs by over 50% and cascade latency by over 80% while maintaining comparable performance across various tasks and datasets. Experimental results demonstrate improved efficiency in model routing without sacrificing accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Datacenter Energy Optimized Power Profiles</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [GPU power management, energy efficiency, HPC optimization, Blackwell architecture]</li>
<li class=""><strong>authors:</strong> Sreedhar Narayanaswamy, Pratikkumar Dilipkumar Patel, Ian Karlin, Apoorv Gupta, Sudhir Saripalli, Janey Guo</li>
<li class=""><strong>institution:</strong> NVIDIA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03872v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03872v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NVIDIA introduces datacenter power profiles, a software feature for Blackwell GPUs that provides workload-aware power optimization recipes. The method leverages hardware/software innovations and domain knowledge to adjust power settings for HPC and AI workloads. This achieves up to 15% energy savings while maintaining &gt;97% performance, enabling 13% throughput increase in power-constrained facilities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Toward Co-adapting Machine Learning Job Shape and Cluster Topology</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [cluster scheduling, job placement, torus topology, resource utilization, optical circuit switching]</li>
<li class=""><strong>authors:</strong> Shawn Shuoshuo Chen, Daiyaan Arfeen, Minlan Yu, Peter Steenkiste, Srinivasan Seshan</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, Harvard University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03891v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03891v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RFold co-adapts job shapes and cluster topology at runtime by identifying homomorphic job shapes and reconfiguring optical circuit switch-enabled topology. Evaluation on a 4096-node torus cluster simulator shows RFold improves cluster utilization by 57% and reduces job completion time by up to 11× compared to existing methods. This demonstrates simultaneous optimization of network contention minimization and cluster utilization maximization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model
Adaptation for Long-Term Daily Runoff Forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [runoff forecasting, time series analysis, semi-supervised learning, multi-network fusion, hydrologic modeling, non-stationarity, large model adaptation]</li>
<li class=""><strong>authors:</strong> Qianfei Fan, Jiayu Wei, Peijun Zhu, Wensheng Ye, Meie Fang</li>
<li class=""><strong>institution:</strong> Guangzhou University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03744v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03744v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HydroFusion-LMF proposes a unified framework combining learnable trend-seasonal-residual decomposition with heterogeneous expert networks and hydrologic context-aware gating. It employs semi-supervised multi-task learning with optional foundation model adaptation via adapters/LoRA. The method achieves significant improvements over baselines in long-term daily runoff forecasting while balancing interpretability and performance under non-stationary conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Short-circuiting Rings for Low-Latency AllReduce</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [AllReduce, collective communication, photonic interconnects, circuit-switching, network topology]</li>
<li class=""><strong>authors:</strong> Sarah-Michelle Hammer, Stefan Schmid, Rachee Singh, Vamsi Addanki</li>
<li class=""><strong>institution:</strong> TU Berlin, Cornell University, Purdue University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03491v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03491v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper challenges the conventional wisdom that Recursive Doubling outperforms Ring AllReduce for small messages by showing Ring remains optimal in ring-based GPU topologies when accounting for realistic network constraints. The authors propose a circuit-switching heuristic that enables Recursive Doubling to leverage dynamically reconfigurable photonic paths, balancing reconfiguration delays and congestion. Preliminary evaluations demonstrate their approach achieves faster completion times than static Ring AllReduce, highlighting the potential of adaptive topologies for collective communication.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Paris: A Decentralized Trained Open-Weight Diffusion Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [decentralized training, diffusion models, expert models, distributed computation, text-to-image generation]</li>
<li class=""><strong>authors:</strong> Zhiying Jiang, Raihan Seraj, Marcos Villagra, Bidhan Roy</li>
<li class=""><strong>institution:</strong> Bagel Labs</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03434v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03434v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Paris introduces a decentralized training approach for diffusion models using 8 isolated expert models trained without synchronization. The method employs data partitioning and a lightweight router for expert selection during inference. Results show comparable quality to centralized training while using significantly less data and compute resources.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] iDDS: Intelligent Distributed Dispatch and Scheduling for Workflow
Orchestration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [workflow orchestration, distributed systems, scientific computing, data management, adaptive decision-making]</li>
<li class=""><strong>authors:</strong> Wen Guan, Tadashi Maeno, Aleksandr Alekseev, Fernando Harald Barreiro Megino, Kaushik De, Edward Karavakis, Alexei Klimentov, Tatiana Korchuganova, FaHui Lin, Paul Nilsson, Torre Wenaus, Zhaoyu Yang, Xin Zhao</li>
<li class=""><strong>institution:</strong> Brookhaven National Laboratory, University of Texas at Arlington, University of Pittsburgh</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.02930v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.02930v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> iDDS introduces an intelligent distributed dispatch and scheduling system that extends traditional workflow management with data-aware execution and programmable workflows. It integrates with systems like PanDA and Rucio to automate complex processing pipelines across heterogeneous infrastructures. The platform demonstrates versatility in scientific applications while reducing operational overhead and enabling reproducible, high-throughput workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical
images within PyRadiomics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [GPU acceleration, medical imaging, radiomics, 3D feature extraction, PyRadiomics, computational efficiency]</li>
<li class=""><strong>authors:</strong> Jakub Lisowski, Piotr Tyrakowski, Szymon Zyguła, Krzysztof Kaczmarski</li>
<li class=""><strong>institution:</strong> Warsaw University of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.02894v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.02894v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PyRadiomics-cuda extends the PyRadiomics library by offloading key geometric computations to GPU hardware, significantly accelerating 3D shape feature extraction from medical images. The system maintains full API compatibility with the original library while achieving dramatic processing time reductions for large volumetric datasets. Testing across various computational environments demonstrates its effectiveness for high-throughput AI pipelines in medical imaging analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Distributed Low-Communication Training with Decoupled Momentum
Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed training, communication reduction, momentum optimization, discrete cosine transform, model synchronization]</li>
<li class=""><strong>authors:</strong> Sasho Nedelkoski, Alexander Acker, Odej Kao, Soeren Becker, Dominik Scheinert</li>
<li class=""><strong>institution:</strong> TU Berlin, LogSight AI</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03371v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03371v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a distributed training method that reduces communication by combining infrequent model synchronization with momentum compression using discrete cosine transform. The approach decomposes Nesterov momentum into frequency components and only synchronizes high-frequency parts periodically. Empirical results show up to 16× communication reduction while maintaining performance across transformer and CNN architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] UPMEM Unleashed: Software Secrets for Speed</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [processing-in-memory, performance optimization, compiler optimization, bit-serial processing, NUMA-aware allocation]</li>
<li class=""><strong>authors:</strong> Krystian Chmielewski, Jarosław Ławnicki, Uladzislau Lukyanau, Tadeusz Kobus, Maciej Maciejewski</li>
<li class=""><strong>institution:</strong> Huawei Technologies</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15927v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15927v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper reveals inefficiencies in UPMEM&#x27;s PIM software stack and demonstrates optimization techniques including assembly-level modifications and bit-serial processing. The authors achieve significant speedups (1.4-5.9×) for integer operations and improved data transfer consistency through NUMA-aware API extensions. Optimized kernels outperform CPU servers by 3-10× for matrix-vector multiplication operations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] GRNND: A GPU-Parallel Relative NN-Descent Algorithm for Efficient
Approximate Nearest Neighbor Graph Construction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU acceleration, approximate nearest neighbor search, graph construction]</li>
<li class=""><strong>authors:</strong> Xiang Li, Qiong Chang, Yun Li, Jun Miyazaki</li>
<li class=""><strong>institution:</strong> Nanjing University, Institute of Science Tokyo</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.02774v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.02774v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GRNND introduces a GPU-parallel version of RNN-Descent algorithm with disordered neighbor propagation strategy and warp-level cooperative operations. It achieves significant speedups of 2.4-51.7× over GPU methods and 17.8-49.8× over CPU methods for approximate nearest neighbor graph construction. The method effectively addresses computational bottlenecks in high-dimensional data processing through optimized parallel execution.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] TridentServe: A Stage-level Serving System for Diffusion Pipelines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [diffusion pipelines, serving system, resource allocation, stage-level optimization, dynamic scheduling]</li>
<li class=""><strong>authors:</strong> Yifei Xia, Fangcheng Fu, Hao Yuan, Hanke Zhang, Xupeng Miao, Yijun Liu, Suhan Ling, Jie Jiang, Bin Cui</li>
<li class=""><strong>institution:</strong> Peking University, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.02838v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.02838v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TridentServe introduces a dynamic stage-level serving system that automatically optimizes resource allocation for diffusion pipelines by co-optimizing placement and dispatch plans. It addresses inefficiencies in static pipeline-level serving through dynamic resource allocation across different stages and requests. Experimental results show significant improvements in SLO attainment and latency reduction compared to existing systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts
Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [elastic scaling, mixture-of-experts, cloud serving, zero-downtime, memory management]</li>
<li class=""><strong>authors:</strong> Gursimran Singh, Timothy Yu, Haley Li, Cheng Chen, Hanieh Sadri, Qintao Zhang, Yu Zhang, Ying Xiong, Yong Zhang, Zhenan Fan</li>
<li class=""><strong>institution:</strong> Huawei Technologies</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.02613v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.02613v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ElasticMoE introduces an elastic scaling framework for MoE models that decouples inference execution from memory operations, enabling concurrent scaling and serving. It uses zero-copy remapping and peer-to-peer transfers to add accelerators without service interruption. Evaluation shows up to 9x lower scale-up latency and 2x better throughput during scaling compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FlashResearch: Real-time Agent Orchestration for Efficient Deep Research</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [agent orchestration, parallel processing, dynamic planning, resource allocation, query decomposition]</li>
<li class=""><strong>authors:</strong> Lunyiu Nie, Nedim Lipka, Ryan A. Rossi, Swarat Chaudhuri</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin, Adobe Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05145v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05145v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlashResearch introduces a framework that transforms sequential research processes into parallel execution by dynamically decomposing queries into tree-structured sub-tasks. It employs adaptive planning, real-time orchestration, and multi-dimensional parallelization to optimize resource allocation. Experiments show it achieves up to 5x speedup while maintaining quality under fixed time budgets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Semantic-Aware Scheduling for GPU Clusters with Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU cluster scheduling, semantic-aware scheduling, large language models, job completion time optimization]</li>
<li class=""><strong>authors:</strong> Zerui Wang, Qinghao Hu, Ana Klimovic, Tianwei Zhang, Yonggang Wen, Peng Sun, Dahua Lin</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Shanghai AI Laboratory, Nanyang Technological University, ETH Zurich, The Chinese University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03334v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03334v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SchedMate introduces semantic-aware scheduling for GPU clusters by leveraging LLMs to extract insights from source code, runtime logs, and historical job data. The framework enhances existing DL schedulers non-intrusively through three LLM-based components. Evaluations show SchedMate reduces average job completion times by up to 1.91×, demonstrating significant performance improvements in DL cluster scheduling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] TetriServe: Efficient DiT Serving for Heterogeneous Image Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [diffusion transformer, serving system, parallel computing, GPU optimization, SLO attainment]</li>
<li class=""><strong>authors:</strong> Runyu Lu, Shiqi He, Wenxuan Tan, Shenggui Li, Ruofan Wu, Jeff J. Ma, Ang Chen, Mosharaf Chowdhury</li>
<li class=""><strong>institution:</strong> University of Michigan, University of Wisconsin-Madison, Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.01565v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.01565v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TetriServe introduces step-level sequence parallelism and round-based scheduling to dynamically adjust parallelism for DiT serving requests based on their deadlines. The system discretizes time into fixed rounds, adapts parallelism at step level, and jointly packs requests to minimize GPU consumption and late completions. Evaluation shows TetriServe achieves up to 32% higher SLO attainment compared to existing solutions without degrading image quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] An Efficient, Reliable and Observable Collective Communication Library
in Large-scale GPU Training Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [collective communication, GPU clusters, network optimization, fault tolerance, observability]</li>
<li class=""><strong>authors:</strong> Ziteng Chen, Xiaohe Hu, Menghao Zhang, Yanmin Jia, Yan Zhang, Mingjun Zhang, Da Liu, Fangzheng Jiao, Jun Chen, He Liu, Aohan Zeng, Shuaixing Duan, Ruya Gu, Yang Jing, Bowen Han, Jiahao Cao, Wei Chen, Wenqi Xie, Jinlong Hou, Yuan Cheng, Bohua Xu, Mingwei Xu, Chunming Hu</li>
<li class=""><strong>institution:</strong> Infrawaves, Southeast University, Shanghai Innovation Institute, Beihang University, Tsinghua University, Zhipu AI, China Unicom Research Institute</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.00991v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.00991v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ICCL proposes a collective communication library that offloads P2P communication from GPU kernels to CPU threads and removes redundant memory copies. It introduces primary-backup QP mechanism for NIC port failure tolerance and window-based monitoring for network anomalies. Results show 23.4%/28.5% improvement in P2P throughput/latency and 6.02% training throughput increase compared to NCCL.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] PolyLink: A Blockchain Based Decentralized Edge AI Platform for LLM
Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [blockchain, decentralized AI, edge computing, TIQE protocol, incentive mechanism]</li>
<li class=""><strong>authors:</strong> Hongbo Liu, Jiannong Cao, Bo Yang, Dongbin Bai, Yinfeng Cao, Xiaoming Shen, Yinan Zhang, Jinwen Liang, Shan Jiang, Mingjin Zhang</li>
<li class=""><strong>institution:</strong> The Hong Kong Polytechnic University, China Mobile (Hong Kong) Innovation Research Institute, Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.02395v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.02395v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PolyLink proposes a blockchain-based decentralized platform for LLM inference using edge devices, featuring a crowdsourcing architecture and TIQE verification protocol. The system integrates token incentives and demonstrates practical latency in real-world deployments. Security analysis shows resilience against model degradation attacks and validator corruptions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Towards Efficient VM Placement: A Two-Stage ACO-PSO Approach for Green
Cloud Infrastructure</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [VM placement, energy efficiency, hybrid algorithm, cloud computing, resource management]</li>
<li class=""><strong>authors:</strong> Ali M. Baydoun, Ahmed S. Zekri</li>
<li class=""><strong>institution:</strong> Beirut Arab University, Alexandria University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.00541v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.00541v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a two-stage hybrid algorithm (HAPSO) combining Ant Colony Optimization and Particle Swarm Optimization for energy-efficient virtual machine placement in cloud datacenters. The method achieves significant improvements with up to 25% lower energy consumption and 18% fewer SLA violations compared to existing approaches. Results demonstrate the effectiveness of bio-inspired hybridization for sustainable cloud resource management.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Net-Zero 6G from Earth to Orbit: Sustainable Design of Integrated
Terrestrial and Non-Terrestrial Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [6G networks, energy efficiency, AI optimization, integrated terrestrial-non-terrestrial networks, net-zero energy]</li>
<li class=""><strong>authors:</strong> Muhammad Ali Jamshed, Malik Muhammad Saad, Muhammad Ahmed Mohsin, Dongkyun Kim, Octavia A. Dobre, Halim Yanikomeroglu, Lina Mohjazi</li>
<li class=""><strong>institution:</strong> University of Glasgow, Kyungpook National University, Stanford University, Memorial University, Carleton University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.00678v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.00678v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes AI-driven solutions to enhance energy efficiency in integrated terrestrial and non-terrestrial 6G networks. It identifies key technologies and use cases for achieving net-zero energy targets while maintaining ubiquitous connectivity. The research outlines sustainable design principles and future directions for evolving integrated network systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Artificial Intelligence for Cost-Aware Resource Prediction in Big Data
Pipelines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [resource prediction, random forest, cloud computing, cost-aware autoscaling, big data pipelines]</li>
<li class=""><strong>authors:</strong> Harshit Goyal</li>
<li class=""><strong>institution:</strong> BITS Pilani</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05127v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05127v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Random Forest regression approach using Google Borg cluster traces to predict resource utilization in big data pipelines. The model achieves high accuracy (R²≈0.99) in capturing non-linear workload-resource relationships. Results demonstrate AI-driven prediction enables cost-aware autoscaling, reducing unnecessary provisioning while maintaining service quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] ElasWave: An Elastic-Native System for Scalable Hybrid-Parallel Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [elastic training, fault tolerance, hybrid parallelism, parameter consistency, computation consistency]</li>
<li class=""><strong>authors:</strong> Xueze Kang, Guangyu Xiang, Yuxin Wang, Hao Zhang, Yuchu Fang, Yuhang Zhou, Zhenheng Tang, Youhui Lv, Eliran Maman, Mark Wasserman, Alon Zameret, Zhipeng Bian, Shushu Chen, Zhiyou Yu, Jin Wang, Xiaoyu Wu, Yang Zheng, Chen Tian, Xiaowen Chu</li>
<li class=""><strong>institution:</strong> HKUST(GZ), HKUST, NJU, Huawei</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.00606v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.00606v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ElasWave introduces an elastic-native system for large-scale LLM training that achieves per-step fault tolerance through multi-dimensional scheduling across graph, dataflow, DVFS and RNG. The system maintains parameter and computation consistency while enabling fast recovery and high throughput during scale changes. Evaluation shows 1.35-1.60× throughput improvements over baselines, 51% MTTR reduction, and 78% convergence deviation reduction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed
Mixture-of-Experts Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [Mixture-of-Experts, pipeline scheduling, distributed training, communication optimization]</li>
<li class=""><strong>authors:</strong> Yunqi Gao, Bing Hu, Mahdi Boloursaz Mashhadi, A-Long Jin, Yanfeng Zhang, Pei Xiao, Rahim Tafazolli, Merouane Debbah</li>
<li class=""><strong>institution:</strong> Zhejiang University, University of Surrey, Xi&#x27;an Jiaotong-Liverpool University, Northeastern University, Khalifa University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.00207v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.00207v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlowMoE proposes a scalable pipeline scheduling framework that unifies multiple task types including MHA computing, gating, expert computing, and A2A communication. It introduces a tensor chunk-based priority scheduling mechanism to overlap all-reduce communication with computing tasks. Experiments show FlowMoE reduces training time by 13%-57%, energy consumption by 10%-39%, and memory usage by 7%-32% compared to state-of-the-art frameworks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] LoRAFusion: Efficient LoRA Fine-Tuning for LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [finetuning], [LoRA, kernel fusion, adaptive batching, parameter-efficient fine-tuning, distributed training]</li>
<li class=""><strong>authors:</strong> Zhanda Zhu, Qidong Su, Yaoyao Ding, Kevin Song, Shang Wang, Gennady Pekhimenko</li>
<li class=""><strong>institution:</strong> University of Toronto, Vector Institute, NVIDIA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.00206v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.00206v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LoRAFusion introduces kernel-level graph-splitting to fuse memory-bound operations and scheduling-level adaptive batching for multi-job fine-tuning. It eliminates redundant memory accesses and enables concurrent fine-tuning of multiple LoRA adapters. The system achieves up to 1.96× speedup over Megatron-LM and 1.46× improvement over mLoRA while providing plug-and-play kernel replacements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Lattica: A Decentralized Cross-NAT Communication Framework for Scalable
AI Inference and Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [decentralized communication, NAT traversal, peer-to-peer mesh, CRDTs, distributed hash tables, edge intelligence, collaborative reinforcement learning]</li>
<li class=""><strong>authors:</strong> Ween Yang, Jason Liu, Suli Wang, Xinyuan Song, Lynn Ai, Eric Yang, Bill Shi</li>
<li class=""><strong>institution:</strong> Gradient, Technische Universität Darmstadt, Emory University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.00183v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.00183v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Lattica presents a decentralized cross-NAT communication framework that integrates NAT traversal mechanisms, CRDT-based decentralized data storage, and DHT-based content discovery to establish globally addressable peer-to-peer networks. The framework enables sovereign and resilient AI systems operating independently of centralized intermediaries. It supports scalable AI inference and training in heterogeneous environments like edge intelligence and collaborative reinforcement learning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [datacenter lifecycle management, TCO optimization, AI inference infrastructure, GPU clusters, hardware refresh strategies]</li>
<li class=""><strong>authors:</strong> Jovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Ricardo Bianchini</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, Microsoft Azure Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26534v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26534v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a holistic framework for AI datacenter lifecycle management across building, hardware refresh, and operation stages. It demonstrates how coordinated optimization across these stages can reduce total cost of ownership by up to 40% compared to traditional approaches. The framework addresses AI-specific challenges including evolving models, high-power hardware, and performance requirements through integrated design decisions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] TASP: Topology-aware Sequence Parallelism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [sequence parallelism, communication optimization, topology-aware, Ring AllGather, AlltoAll topology, Hamiltonian decomposition]</li>
<li class=""><strong>authors:</strong> Yida Wang, Ke Hong, Xiuhong Li, Yuanchao Xu, Wenxun Wang, Guohao Dai, Yu Wang</li>
<li class=""><strong>institution:</strong> Capital Normal University, Tsinghua University, Infinigence-AI, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26541v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26541v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TASP proposes a topology-aware sequence parallelism method that decomposes both modern accelerator topologies and Ring AllGather primitives into multiple orthogonal ring datapaths to enable concurrent data transfers. This approach fully utilizes the communication capacity of AlltoAll topologies in modern accelerators. Experimental results show TASP achieves up to 3.58× speedup over Ring Attention and its variants on NVIDIA H100 and AMD MI300X systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficient Construction of Large Search Spaces for Auto-Tuning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [auto-tuning, constraint satisfaction problem, search space construction, high-performance computing]</li>
<li class=""><strong>authors:</strong> Floris-Jan Willemsen, Rob V. van Nieuwpoort, Ben van Werkhoven</li>
<li class=""><strong>institution:</strong> Leiden University, Netherlands eScience Center</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26253v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26253v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper reformulates search space construction for auto-tuning as a Constraint Satisfaction Problem (CSP) and develops an optimized CSP solver with runtime parsing capabilities. The approach achieves significant performance improvements, reducing construction time by 3-4 orders of magnitude compared to existing methods. This enables efficient exploration of previously unattainable problem scales in auto-tuning applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Parallax: Efficient LLM Inference Service over Decentralized Environment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [decentralized inference, LLM serving, GPU scheduling, model allocation, pipeline selection]</li>
<li class=""><strong>authors:</strong> Chris Tong, Youhe Jiang, Gufeng Chen, Tianyi Zhao, Sibian Lu, Wenjie Qu, Eric Yang, Lynn Ai, Binhang Yuan</li>
<li class=""><strong>institution:</strong> Gradient, HKUST, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26182v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26182v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Parallax introduces a two-phase scheduler for decentralized LLM inference that first allocates model layers across heterogeneous GPUs and then dynamically selects execution pipelines at request time. The system demonstrates reduced latency and increased throughput compared to baseline approaches. This shows principled scheduling can make volunteer computing practical for LLM inference services.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficient Distributed Training via Dual Batch Sizes and Cyclic
Progressive Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed training, batch size optimization, progressive learning, parameter server, ResNet-18]</li>
<li class=""><strong>authors:</strong> Kuan-Wei Lu, Ding-Yong Hong, Pangfeng Liu, Jan-Jan Wu</li>
<li class=""><strong>institution:</strong> Academia Sinica, National Taiwan University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.26092v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.26092v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid distributed training method combining dual batch size learning and cyclic progressive learning. The dual batch size approach uses both large and small batches to improve efficiency and generalization, while progressive learning adjusts image resolution cyclically to accelerate training. Experiments show improved accuracy (up to 3.3% on CIFAR-100) and reduced training time (up to 35.7% on ImageNet) compared to conventional methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Accelerating LLM Inference with Precomputed Query Storage</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [storage-assisted inference, precomputed queries, latency reduction, edge computing, vector database]</li>
<li class=""><strong>authors:</strong> Jay H. Park, Youngju Cho, Choungsol Lee, Moonwook Oh, Euiseong Seo</li>
<li class=""><strong>institution:</strong> Samsung Electronics, Sungkyunkwan University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25919v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25919v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> StorInfer accelerates LLM inference by precomputing and storing query-response pairs offline, using semantic matching to bypass GPU computation when possible. The system employs adaptive query generation techniques to ensure diversity and uses vector indexing for fast retrieval. Evaluation shows up to 17.3% latency reduction without quality loss, demonstrating practical benefits for predictable query distributions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [FPGA acceleration, Vision GNNs, Dynamic Image Graph Construction, hardware optimization, parallel sorting]</li>
<li class=""><strong>authors:</strong> Anvitha Ramachandran, Dhruv Parikh, Viktor Prasanna</li>
<li class=""><strong>institution:</strong> University of Southern California</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25121v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25121v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a streaming FPGA accelerator for Dynamic Image Graph Construction in Vision GNNs, using on-chip buffers and parallel sorting with local merge-sort and global k-way merging. This design minimizes memory traffic and achieves significant speedups of up to 16.6x over CPU and 6.8x over GPU baselines, effectively addressing the computational bottleneck in ViG inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Context-Driven Performance Modeling for Causal Inference Operators on
Neural Processing Units</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [NPU performance analysis, causal inference operators, attention mechanisms, edge computing, hardware-aware optimization]</li>
<li class=""><strong>authors:</strong> Neelesh Gupta, Rakshith Jayanth, Dhruv Parikh, Viktor Prasanna</li>
<li class=""><strong>institution:</strong> University of Southern California</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25155v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25155v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts comprehensive performance benchmarking of quadratic and sub-quadratic causal inference operators on modern NPUs. The analysis reveals quadratic attention becomes severely memory-bound with cache inefficiency and pipeline stalls exceeding 95% at long contexts, while sub-quadratic models can become compute-bound on vector cores. These findings provide critical insights for hardware-aware model co-design and optimization strategies for on-device long-context inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts
Routing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Mixture-of-Experts, load balancing, routing algorithm, inference optimization, plug-and-play]</li>
<li class=""><strong>authors:</strong> Rana Shahout, Colin Cai, Yilun Du, Minlan Yu, Michael Mitzenmacher</li>
<li class=""><strong>institution:</strong> Harvard University, University of California, Berkeley</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03293v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03293v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LASER is a plug-and-play inference-time routing algorithm that improves load balancing in Mixture-of-Experts models by adapting to gate score distributions. It routes tokens to strongest experts when scores show clear preference, and to least-loaded experts when scores are uniform. The method reduces latency and increases throughput while maintaining accuracy, requiring no retraining or fine-tuning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] GRACE-MoE: Grouping and Replication with Locality-Aware Routing for
Efficient Distributed MoE Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [distributed inference, sparse mixture of experts, communication optimization, load balancing]</li>
<li class=""><strong>authors:</strong> Yu Han, Lehan Pan, Jie Peng, Ziyang Tao, Wuyang Zhang, Yanyong Zhang</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25041v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25041v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GRACE-MoE proposes a co-optimization framework with grouping, replication, and locality-aware routing to reduce communication overhead and balance computational load in distributed MoE inference. The method achieves up to 3.79× speedup over state-of-the-art systems by minimizing cross-device communication while maintaining load balance across GPUs. Experimental results demonstrate significant latency reduction in multi-node, multi-GPU environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Intent-Driven Storage Systems: From Low-Level Tuning to High-Level
Understanding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [storage systems, workload intent, parameter optimization, caching, prefetching]</li>
<li class=""><strong>authors:</strong> Shai Bergman, Won Wook Song, Lukas Cavigelli, Konstantin Berestizshevsky, Ke Zhou, Ji Zhang</li>
<li class=""><strong>institution:</strong> Huawei Zurich Research Center, Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15917v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15917v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Intent-Driven Storage Systems (IDSS) that use LLMs to infer workload intent from unstructured signals and generate adaptive storage configurations. Experimental results show IDSS can improve IOPS by up to 2.45× on FileBench workloads. This demonstrates LLMs can effectively bridge application semantics with low-level system control when properly constrained by policy guardrails.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] HAPT: Heterogeneity-Aware Automated Parallel Training on Heterogeneous
Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [heterogeneous clusters, automated parallel training, computation-communication overlap, inter-operator parallel, 1F1B scheduler]</li>
<li class=""><strong>authors:</strong> Antian Liang, Zhigang Zhao, Kai Zhang, Xuri Shi, Chuantao Li, Chunxiao Wang, Zhenying He, Yinan Jing, X. Sean Wang</li>
<li class=""><strong>institution:</strong> Fudan University, Shandong Computer Science Center (National Supercomputer Center in Jinan)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.24859v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.24859v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hapt introduces a fine-grained planner for inter-operator parallel strategy search and a heterogeneity-aware 1F1B scheduler to optimize computation-communication overlap in heterogeneous clusters. The framework achieves 1.3x-1.6x higher performance compared to state-of-the-art training frameworks by effectively utilizing heterogeneous accelerators while maintaining balanced workloads and minimal memory overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in
Long-Context LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [dynamic sparse attention, KV cache management, hierarchical storage, HBM optimization]</li>
<li class=""><strong>authors:</strong> Qihui Zhou, Peiqi Yin, Pengfei Zuo, James Cheng</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Huawei Cloud</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.24626v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.24626v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SparseServe introduces a hierarchical HBM-DRAM management system for dynamic sparse attention in LLM serving, featuring fragmentation-aware KV cache transfer, working-set-aware batch control, and layer-segmented prefill. It achieves significant improvements with up to 9.26x lower TTFT latency and 3.14x higher throughput compared to state-of-the-art systems by efficiently handling long-context LLM inference bottlenecks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Scalable Distributed Framework for Multimodal GigaVoxel Image
Registration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [image registration, distributed computing, GPU optimization, memory efficiency, multimodal imaging]</li>
<li class=""><strong>authors:</strong> Rohit Jena, Vedant Zope, Pratik Chaudhari, James C. Gee</li>
<li class=""><strong>institution:</strong> University of Pennsylvania</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25044v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25044v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FFDP, a distributed framework with IO-aware fused kernels for large-scale multimodal image registration. It optimizes non-GEMM bottlenecks and enables convolution-aware tensor sharding to handle gigavoxel-scale problems. FFDP achieves 6-7x speedup and reduces memory consumption by 20-59% while supporting problems 64x larger than existing methods on single GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] RServe: Overlapping Encoding and Prefill for Efficient LMM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multimodal models, inference optimization, scheduling, parallel processing]</li>
<li class=""><strong>authors:</strong> Tianyu Guo, Tianming Xu, Xianjie Chen, Junru Chen, Nong Xiao, Xianwei Zhang</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University, Rednote</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.24381v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.24381v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> REDServe proposes an LMM inference system that overlaps multimodal encoding with language model computation using fine-grained scheduling and chunked prefill. It achieves significant latency reduction (up to 66%) and throughput improvement (up to 109%) by optimizing intra-request and inter-request parallelism. The system outperforms existing approaches through efficient pipeline coordination and load balancing across micro-batches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] LogAction: Consistent Cross-system Anomaly Detection through Logs via
Active Domain Adaptation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [log-based anomaly detection, active domain adaptation, transfer learning, active learning, free energy-based sampling, uncertainty-based sampling]</li>
<li class=""><strong>authors:</strong> Chiming Duan, Minghua He, Pei Xiao, Tong Jia, Xin Zhang, Zhewei Zhong, Xiang Luo, Yan Niu, Lingzhe Zhang, Yifan Wu, Siyu Yu, Weijie Hong, Ying Li, Gang Huang</li>
<li class=""><strong>institution:</strong> Peking University, Bytedance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03288v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03288v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LogAction proposes an active domain adaptation approach that combines transfer learning from mature systems with active learning using free energy-based and uncertainty-based sampling strategies. This method addresses both cold-start problems and data distribution gaps between source and target systems. Experimental results show it achieves 93.01% F1 score with only 2% manual labels, outperforming state-of-the-art methods by 26.28%.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Asynchronous Policy Gradient Aggregation for Efficient Distributed
Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed reinforcement learning, policy gradient methods, asynchronous computation, communication efficiency, heterogeneous environments]</li>
<li class=""><strong>authors:</strong> Alexander Tyurin, Andrei Spiridonov, Varvara Rudenko</li>
<li class=""><strong>institution:</strong> Opta</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.24305v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.24305v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces two novel algorithms, Rennala NIGT and Malenia NIGT, for asynchronous policy gradient aggregation in distributed reinforcement learning. These methods address computational and communication challenges in both homogeneous and heterogeneous settings, achieving improved efficiency and better theoretical guarantees. Experimental results demonstrate that the proposed approaches significantly outperform prior methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] RL in the Wild: Characterizing RLVR Training in LLM Deployment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [RLVR training, workload characterization, system challenges, GPU utilization, sequence length distribution, parallel strategies, data management, load imbalance]</li>
<li class=""><strong>authors:</strong> Jiecheng Zhou, Qinghao Hu, Yuyang Jin, Zerui Wang, Peng Sun, Yuzhe Gu, Wenwei Zhang, Mingshu Zhai, Xingcheng Zhang, Weiming Zhang</li>
<li class=""><strong>institution:</strong> USTC, Shanghai AI Laboratory, NTU, Tsinghua University, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.25279v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.25279v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a characterization study of RLVR training workloads in LLM deployment, identifying system challenges like GPU idling and inefficient parallel strategies. The authors propose the PolyTrace benchmark suite for realistic workload evaluation, which achieves 94.7% accuracy in practical use cases. The study calls for further investigation into optimizing RLVR training systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous
Retraining Alignment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference, scheduling, edge computing, finetuning], [hybrid serving system, iteration-level scheduling, SLO-aware retraining, GPU resource management, concurrent inference and fine-tuning]</li>
<li class=""><strong>authors:</strong> Yufei Li, Yu Fu, Yue Dong, Cong Liu</li>
<li class=""><strong>institution:</strong> University of California, Riverside</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03283v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03283v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MACE proposes a hybrid LLM serving system that co-locates inference and fine-tuning with iteration-level scheduling to balance latency and model accuracy. It uses intelligent memory management to allocate GPU cycles based on update importance. Evaluation shows 63% latency reduction while maintaining throughput and high GPU utilization on edge platforms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on
Heterogeneous Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [pipeline parallelism, model partition, workload scheduling, performance optimization]</li>
<li class=""><strong>authors:</strong> Jihu Guo, Tenghui Ma, Wei Gao, Peng Sun, Jiaxing Li, Xun Chen, Yuyang Jin, Dahua Lin</li>
<li class=""><strong>institution:</strong> Fudan University, Shanghai AI Laboratory, Hong Kong University of Science and Technology, SenseTime, Tsinghua University, Chinese University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23722v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23722v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AdaPtis proposes an adaptive pipeline parallelism system for LLM training that jointly optimizes model partition, placement, and scheduling using a performance model. It achieves significant speedup over existing methods by reducing pipeline bubbles in heterogeneous model architectures. The system demonstrates 1.42x average speedup (up to 2.14x) compared to Megatron-LM across various LLM scales.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] CrediBench: Building Web-Scale Network Datasets for Information
Integrity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [misinformation detection, web graph analysis, credibility scoring, temporal network analysis, content-structure integration]</li>
<li class=""><strong>authors:</strong> Emma Kondrup, Sebastian Sabry, Hussein Abdallah, Zachary Yang, James Zhou, Kellin Pelrine, Jean-François Godbout, Michael M. Bronstein, Reihaneh Rabbany, Shenyang Huang</li>
<li class=""><strong>institution:</strong> Mila - Quebec AI Institute, McGill University, Concordia University, UC Berkeley, Université de Montréal, University of Oxford, AITHYRA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23340v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23340v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CrediBench introduces a large-scale pipeline for constructing temporal web graphs that jointly model textual content and hyperlink structure to detect misinformation. The system processes web-scale data from Common Crawl, creating graphs with 45M nodes and 1B edges. Experiments demonstrate that combining structural and content signals effectively learns credibility scores measuring source reliability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Flexible Programmable Pipeline Parallelism Framework for Efficient DNN
Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [pipeline parallelism, distributed training, schedule exploration, domain-specific language, automated scheduler]</li>
<li class=""><strong>authors:</strong> Lijuan Jiang, Xingjian Qian, Zhenxiang Ma, Zan Zong, Hengjie Li, Chao Yang, Jidong Zhai</li>
<li class=""><strong>institution:</strong> Shanghai AI Lab, Zhejiang University, Shanghai Jiao Tong University, Tsinghua University, Peking University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05112v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05112v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlexPipe introduces a programmable pipeline parallelism framework with a domain-specific language and automated scheduler for efficient DNN training. It enables automated schedule exploration and customization, achieving significant performance speedups over existing frameworks like Megatron-LM and state-of-the-art automated pipeline systems. The framework enhances productivity and flexibility in handling diverse model architectures and parallel scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Predictive and Synergistic Two-Layer Scheduling Framework for LLM
Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM inference, predictive scheduling, cross-layer optimization, SLO-aware, performance modeling]</li>
<li class=""><strong>authors:</strong> Yue Zhang, Yuansheng Chen, Xuan Mo, Alex Xi, Jialun Li, WeiGang Wu</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University, Kaon AI, Guangdong Polytechnic Normal University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23384v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23384v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes NexusSched, a predictive two-layer scheduling framework for LLM serving that bridges information gaps between cluster and engine layers through an online performance model. The framework includes LENS for engine-layer adaptive scheduling and PRISM for cluster-layer predictive routing. Evaluations show 43% higher SLO attainment and up to 3× throughput improvement in heterogeneous scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Towards Quantum-Ready Blockchain Fraud Detection via Ensemble Graph
Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [blockchain fraud detection, ensemble graph neural networks, quantum-ready systems, anti-money laundering, cryptocurrency monitoring]</li>
<li class=""><strong>authors:</strong> M. Z. Haider, Tayyaba Noreen, M. Salman</li>
<li class=""><strong>institution:</strong> Université du Québec, SZABIST University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23101v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23101v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an ensemble framework combining Graph Convolutional Networks, Graph Attention Networks, and Graph Isomorphism Networks for blockchain fraud detection. The tuned soft voting ensemble achieves over 70% recall of illicit transactions with under 1% false positives on the Elliptic dataset. The modular design incorporates quantum-ready features for future integration with quantum computing technologies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured
Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [Mixture-of-Experts, dynamic expert clustering, structured compression, hierarchical routing, parameter reduction]</li>
<li class=""><strong>authors:</strong> Peijun Zhu, Ning Yang, Jiayu Wei, Jinghang Wu, Haijun Zhang</li>
<li class=""><strong>institution:</strong> Guangzhou University, Institute of Automation Chinese Academy of Sciences, University of Science and Technology Beijing</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.02345v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.02345v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a unified framework combining dynamic expert clustering and structured compression to address the MoE LLM trilemma. The method uses online clustering with parameter-activation similarity metrics and decomposes expert weights into shared bases with low-rank adapters. Results show 80% parameter reduction, 10-20% throughput improvement, and 3x lower load variance while maintaining model quality comparable to standard MoE models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Memory Efficient and Staleness Free Pipeline Parallel DNN Training
Framework with Improved Convergence Speed</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [pipeline parallelism, deep neural network training, memory efficiency, staleness-free, convergence speed]</li>
<li class=""><strong>authors:</strong> Ankita Dutta, Nabendu Chaki, Rajat K. De</li>
<li class=""><strong>institution:</strong> Indian Statistical Institute, University of Calcutta</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.23241v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.23241v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces two pipeline parallel DNN training frameworks - V-TiMePReSt, a completely staleness-free system using latest weights, and I-TiMePReSt, a staleness-aware system that computes intermediate weights. Experimental results show V-TiMePReSt achieves better GPU memory efficiency while I-TiMePReSt maintains optimal trade-off between memory consumption and convergence speed. Both frameworks demonstrate improved training performance compared to existing approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficient Fine-Grained GPU Performance Modeling for Distributed Deep
Learning of LLM</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [performance modeling, distributed training, GPU computing, parallelism strategies, computational primitives]</li>
<li class=""><strong>authors:</strong> Biyao Zhang, Mingkai Zheng, Debargha Ganguly, Xuecen Zhang, Vikash Singh, Vipin Chaudhary, Zhao Zhang</li>
<li class=""><strong>institution:</strong> Case Western Reserve University, Rutgers University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22832v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22832v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a fine-grained GPU performance modeling framework that decomposes LLMs into computational primitives and uses lightweight sampling for hardware-aware predictions. It integrates operator-level analysis with end-to-end system modeling across complex parallelization strategies. The method achieves low prediction errors (4.98%-9.38%) on large-scale HPC systems and runs entirely on CPUs for rapid configuration testing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Agora: Bridging the GPU Cloud Resource-Price Disconnect</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [GPU pricing, cloud computing, memory bandwidth, resource allocation, feature-based pricing]</li>
<li class=""><strong>authors:</strong> Ian McDougall, Noah Scott, Joon Huh, Kirthevasan Kandasamy, Karthikeyan Sankaralingam</li>
<li class=""><strong>institution:</strong> University of Wisconsin-Madison</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05111v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05111v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a feature-based pricing framework for cloud GPUs that directly links costs to resource consumption rather than time. It introduces Agora, a practical system architecture that enables this pricing model through fine-grained resource sampling. Evaluation shows this approach creates more efficient GPU markets, with 10us sampling achieving near-ideal pricing with only 2.4% revenue loss.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Code once, Run Green: Automated Green Code Translation in Serverless
Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [serverless computing, energy efficiency, code translation, green computing, energy debt, large language models]</li>
<li class=""><strong>authors:</strong> Sebastian Werner, Mathis Kähler, Alireza Hakamian</li>
<li class=""><strong>institution:</strong> University of Hamburg, Technische Universität Berlin</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22068v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22068v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ReFaaS, a system that uses large language models to automatically translate serverless functions into more energy-efficient programming languages while maintaining functional correctness. Results show translated functions can reduce invocation energy by up to 70%, achieving net energy savings after 3,000-5,000 invocations. However, the approach faces challenges with function suitability and varying amortization thresholds.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code
Generation Using LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multi-agent LLM, code generation, HPC, auto-tuning, CUDA, iterative prompting]</li>
<li class=""><strong>authors:</strong> Shun-ichiro Hayashi, Koki Morita, Daichi Mukunoki, Tetsuya Hoshino, Takahiro Katagiri</li>
<li class=""><strong>institution:</strong> Nagoya University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.00031v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.00031v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VibeCodeHPC introduces a multi-agent LLM system with four specialized roles (Project Manager, System Engineer, Programmer, Continuous Delivery) that collaboratively generate and optimize HPC code through iterative prompt refinement. The system demonstrated superior performance in converting CPU-based matrix multiplication code to optimized GPU CUDA code compared to single-agent approaches. Dynamic agent deployment and activity monitoring effectively identified requirement violations and improved code generation quality per unit time.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Go With The Flow: Churn-Tolerant Decentralized Training of Large
Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [decentralized training, fault tolerance, flow optimization, volunteer computing, pipeline parallelism]</li>
<li class=""><strong>authors:</strong> Nikolay Blagoev, Bart Cox, Jérémie Decouchant, Lydia Y. Chen</li>
<li class=""><strong>institution:</strong> Université de Neuchâtel, Delft University of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21221v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21221v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GWTF proposes a decentralized flow algorithm for efficient LLM training on volunteer computing resources, addressing node churn and network instability. The framework optimizes microbatch routing to minimize training delays while maximizing throughput. Evaluation shows GWTF reduces training time by up to 45% in heterogeneous environments with high churn rates compared to prior approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Zeppelin: Balancing Variable-length Workloads in Data Parallel Large
Model Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, sequence partitioning, attention optimization, communication efficiency, load balancing]</li>
<li class=""><strong>authors:</strong> Chang Chen, Tiancheng Chen, Jiangfei Duan, Qianchao Zhu, Zerui Wang, Qinghao Hu, Peng Sun, Xiuhong Li, Chao Yang, Torsten Hoefler</li>
<li class=""><strong>institution:</strong> Peking University, ETH Zurich, The Chinese University of Hong Kong, Shanghai AI Laboratory, Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21841v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21841v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Zeppelin introduces a hierarchical sequence partitioning method for attention modules, a routing layer for inter-node transfers, and a remapping layer between attention and linear modules to address load imbalance in large model training. The system optimizes both computational efficiency and communication overhead for variable-length sequences. Comprehensive evaluations show Zeppelin achieves an average 2.80× speedup over state-of-the-art methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM
Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [pipeline parallelism, long-context training, distributed systems, scheduling, gradient checkpointing]</li>
<li class=""><strong>authors:</strong> Shiju Wang, Yujie Wang, Ao Sun, Fangcheng Fu, Zijian Zhu, Bin Cui, Xu Han, Kaisheng Ma</li>
<li class=""><strong>institution:</strong> Tsinghua University, Peking University, Beijing University of Posts and Telecommunications, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21275v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21275v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Elastic Pipeline Parallelism (EPP) and implements InfiniPipe system to adaptively combine token-level and batch-level pipeline parallelism for efficient long-context LLM training. It introduces workload balancing through sequence splitting/packing and co-optimizes pipeline scheduling with gradient checkpointing. Experiments show InfiniPipe achieves 1.69× speedup over state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] SuperOffload: Unleashing the Power of Large-Scale LLM Training on
Superchips</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [Superchips, offloading, heterogeneous architecture, Grace CPU, Hopper GPU, NVLink-C2C, adaptive weight offloading, bucketization repartitioning, speculative execution]</li>
<li class=""><strong>authors:</strong> Xinyu Lian, Masahiro Tanaka, Olatunji Ruwase, Minjia Zhang</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, Anyscale, Snowflake</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21271v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21271v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SuperOffload introduces a novel offloading system specifically designed for Superchip architectures, combining techniques like adaptive weight offloading and Superchip-aware casting. The system achieves up to 2.5x throughput improvement over state-of-the-art offloading systems, enabling training of 25B models on a single Superchip. It also scales to 8 GH200 systems for training 13B models with 1M token sequences while maintaining 55% MFU.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Communication Bias in Large Language Models: A Regulatory Perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [AI regulation, communication bias, fairness, EU AI Act, Digital Services Act, bias mitigation, competition governance, design governance]</li>
<li class=""><strong>authors:</strong> Adrian Kuenzler, Stefan Schmid</li>
<li class=""><strong>institution:</strong> University of Hong Kong, TU Berlin, Weizenbaum Institute</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21075v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21075v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes communication bias in LLMs through a regulatory lens, examining frameworks like the EU AI Act and Digital Services Act. It argues that current regulations insufficiently address LLMs&#x27; influence on fundamental worldviews and democratic processes. The authors advocate for complementary approaches combining competition governance and technology design oversight with existing regulatory measures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient
Method for Solving Large-Scale Linear Optimization Problem</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [in-memory computing, RRAM, optimization algorithms, linear programming, hardware-software co-design]</li>
<li class=""><strong>authors:</strong> Huynh Q. N. Vo, Md Tawsif Rahman Chowdhury, Paritosh Ramanan, Gozde Tutuncuoglu, Junchi Yang, Feng Qiu, Murat Yildirim</li>
<li class=""><strong>institution:</strong> Oklahoma State University, Argonne National Laboratory, Wayne State University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21137v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21137v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a distributed in-memory primal-dual hybrid gradient method specifically co-designed for RRAM arrays, minimizing costly write cycles while maintaining robustness against device non-idealities. The approach uses a symmetric block-matrix formulation to unify operations across distributed crossbars and integrates physics-based simulation for realistic evaluation. Benchmarking shows the RRAM-based solver achieves comparable accuracy to GPU-accelerated solvers with three orders-of-magnitude reductions in energy consumption and latency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL
Post-Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, large language models, GPU utilization, synchronous training, rollout scheduling, tail batching]</li>
<li class=""><strong>authors:</strong> Wei Gao, Yuheng Zhao, Dakai An, Tianyuan Wu, Lunxi Cao, Shaopan Xiong, Ju Huang, Weixun Wang, Siran Yang, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang</li>
<li class=""><strong>institution:</strong> Hong Kong University of Science and Technology, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.21009v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.21009v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RollPacker introduces tail batching, a novel scheduling strategy that groups long-tail responses into designated rounds to reduce GPU idle time in synchronous RL post-training. The system implements holistic optimizations across rollout, reward, and training stages. Empirical results show 2.03x-2.56x faster training compared to existing methods while maintaining accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Prompt-Aware Scheduling for Low-Latency LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM inference, task scheduling, latency optimization, prompt-aware scheduling, vLLM integration]</li>
<li class=""><strong>authors:</strong> Yiheng Tao, Yihe Zhang, Matthew T. Dearing, Xin Wang, Yuping Fan, Zhiling Lan</li>
<li class=""><strong>institution:</strong> University of Illinois Chicago, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.03243v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.03243v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PARS introduces a prompt-aware scheduler that approximates shortest-job-first scheduling using pairwise ranking with margin ranking loss to predict task execution times. It integrates seamlessly with vLLM and reduces latency by minimizing head-of-line blocking. Experiments show significant performance improvements across multiple LLMs and reasoning workloads while maintaining generalization across different models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] RecIS: Sparse to Dense, A Unified Training Framework for Recommendation
Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [recommendation systems, sparse-dense training, PyTorch ecosystem, system optimization]</li>
<li class=""><strong>authors:</strong> Hua Zong, Qingtao Zeng, Zhengxiong Zhou, Zhihua Han, Zhensong Yan, Mingjie Liu, Hechen Sun, Jiawei Liu, Yiwen Hu, Qi Wang, YiHan Xian, Wenjie Guo, Houyuan Xiang, Zhiyuan Zeng, Xiangrong Sheng, Bencheng Yan, Nan Hu, Yuheng Huang, Jinqing Lian, Ziru Xu, Yan Zhang, Ju Huang, Siran Yang, Huimin Yi, Jiamang Wang, Pengjie Wang, Han Zhu, Jian Wu, Dan Ou, Jian Xu, Haihong Tang, Yuning Jiang, Bo Zheng, Lin Qu</li>
<li class=""><strong>institution:</strong> Alibaba</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20883v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20883v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RecIS proposes a unified sparse-dense training framework for recommendation models that integrates with large models and optimizes sparse components for better efficiency than TensorFlow-based approaches. The framework leverages PyTorch ecosystem technologies for dense components and is currently deployed at Alibaba for large-model enhanced recommendation training tasks. It addresses the hybrid architecture challenges in modern recommendation systems by providing scalable and efficient training capabilities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Integrating and Characterizing HPC Task Runtime Systems for hybrid
AI-HPC workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [HPC task runtime systems, RADICAL-Pilot, Flux, Dragon, hybrid AI-HPC workloads, performance characterization, workflow scheduling]</li>
<li class=""><strong>authors:</strong> Andre Merzky, Mikhail Titov, Matteo Turilli, Shantenu Jha</li>
<li class=""><strong>institution:</strong> RADICAL-Computing Inc., Brookhaven National Laboratory, Rutgers University, IE University, Princeton Plasma Physics Laboratory, Princeton University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20819v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20819v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper integrates RADICAL-Pilot with Flux and Dragon runtime systems to manage hybrid AI-HPC workloads, demonstrating superior performance over traditional Slurm/srun. The integrated system achieves over 1,500 tasks/second with high utilization and reduces workflow makespan by 30-60% in drug discovery campaigns. The results show hybrid runtime integration provides scalable execution for complex scientific workflows combining simulation and machine learning tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [AI cluster scheduling, GPU resource management, training and inference co-scheduling, backfill, enhanced binpack, performance metrics]</li>
<li class=""><strong>authors:</strong> Lingling Zeng, Gen Zhang, Jialin Peng, Xiang Xu, Yuan Xu, Lijun Ma</li>
<li class=""><strong>institution:</strong> ZTE Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.01256v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.01256v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Kant introduces a unified scheduling system for large-scale AI clusters that employs strategies like Backfill and Enhanced Binpack to co-schedule training and inference jobs. It defines key metrics such as GPU Allocation Ratio and Scheduling Occupancy Rate for performance evaluation. Experimental results show Kant significantly improves resource utilization, reduces fragmentation, and supports stable operation in clusters with thousands to tens of thousands of GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Experience Deploying Containerized GenAI Services at an HPC Center</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [containerized deployment, HPC-Kubernetes integration, GenAI services, vLLM inference server, converged computing architecture]</li>
<li class=""><strong>authors:</strong> Angel M. Beltre, Jeff Ogden, Kevin Pedretti</li>
<li class=""><strong>institution:</strong> Sandia National Laboratories</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20603v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20603v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a converged computing architecture integrating HPC and Kubernetes platforms for deploying containerized GenAI workloads, using Llama LLM with vLLM inference server as a case study. The approach enables reproducible deployment across multiple container runtimes and demonstrates practical integration of cloud-native technologies in HPC environments. The experience provides guidance for future tool development and highlights opportunities for the HPC container community.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Reconstruction-Based Adaptive Scheduling Using AI Inferences in
Safety-Critical Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [adaptive scheduling, schedule reconstruction, safety-critical systems, time-triggered systems, AI inferences]</li>
<li class=""><strong>authors:</strong> Samer Alshaer, Ala Khalifeh, Roman Obermaisser</li>
<li class=""><strong>institution:</strong> University of Siegen</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20513v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20513v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a reconstruction framework that transforms AI-generated scheduling priorities into executable schedules for safety-critical time-triggered systems. The method includes safety checks, allocation algorithms, and recovery mechanisms to handle dynamic operational conditions. Results show the framework enhances system adaptability, operational integrity, and runtime performance while maintaining computational efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Adaptive Approach to Enhance Machine Learning Scheduling Algorithms
During Runtime Using Reinforcement Learning in Metascheduling Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [reinforcement learning, metascheduling, online learning, multi-schedule graph, time-triggered architectures]</li>
<li class=""><strong>authors:</strong> Samer Alshaer, Ala Khalifeh, Roman Obermaisser</li>
<li class=""><strong>institution:</strong> University of Siegen</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20520v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20520v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an adaptive online learning unit using reinforcement learning to enhance machine learning scheduling algorithms during runtime in metascheduling applications. The RL approach continuously explores new scheduling solutions and expands the Multi-Schedule Graph to handle dynamic environments and unexpected events. Experimental results show the method significantly improves scheduling robustness and system efficiency in time-triggered architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [energy efficiency, token throughput, test-time compute, efficiency interventions, production-scale configurations]</li>
<li class=""><strong>authors:</strong> Felipe Oviedo, Fiodar Kazhamiaka, Esha Choukse, Allen Kim, Amy Luers, Melanie Nakagawa, Ricardo Bianchini, Juan M. Lavista Ferres</li>
<li class=""><strong>institution:</strong> Microsoft</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20241v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20241v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a bottom-up methodology to estimate per-query energy consumption in large-scale LLM systems based on token throughput. The study finds that realistic production estimates show significantly lower energy use (0.34 Wh median) compared to non-production benchmarks, and identifies that test-time scaling scenarios with longer queries increase energy demand substantially. The research demonstrates that combined efficiency improvements across model, platform, and hardware levels can achieve 8-20x energy reductions, bringing AI inference energy consumption comparable to web search at scale.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] BurstEngine: an Efficient Distributed Framework for Training
Transformers on Extremely Long Sequences of over 1M Tokens</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, long sequence training, attention optimization, memory optimization, workload balancing]</li>
<li class=""><strong>authors:</strong> Ao Sun, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong sun</li>
<li class=""><strong>institution:</strong> Tsinghua University, Beijing University of Posts and Telecommunications</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.19836v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.19836v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BurstEngine introduces an optimized distributed attention mechanism called BurstAttention with topology-aware ring communication and fine-grained communication-computation overlap. It also employs sequence-level selective checkpointing and workload balance optimization for attention masking. The framework achieves 1.2× speedup with lower memory overhead when training LLMs on sequences exceeding 1M tokens compared to state-of-the-art baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient
LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [parallelism transformation, KV cache optimization, dynamic scheduling, throughput optimization]</li>
<li class=""><strong>authors:</strong> Haoyu Chen, Xue Li, Kun Qian, Yu Guan, Jin Zhao, Xin Wang</li>
<li class=""><strong>institution:</strong> Fudan University, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.19729v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.19729v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Gyges proposes dynamic cross-instance parallelism transformation that adaptively adjusts parallelism strategies to handle varying context lengths in LLM serving. It introduces page-friendly KV cache layouts, weight padding optimizations, and transformation-aware scheduling. Evaluations show 1.75x-6.57x throughput improvement over state-of-the-art solutions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Theory of Multi-Agent Generative Flow Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [multi-agent systems, generative flow networks, flow-matching loss, centralized training, decentralized execution, joint actions]</li>
<li class=""><strong>authors:</strong> Leo Maxime Brunswic, Haozhi Wang, Shuang Luo, Jianye Hao, Amir Rasouli, Yinchuan Li</li>
<li class=""><strong>institution:</strong> Huawei Technologies Canada, Tianjin University, Zhejiang University, Huawei Noah&#x27;s Ark Lab</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.20408v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.20408v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a theoretical framework for multi-agent generative flow networks (MA-GFlowNets) with four algorithms enabling collaborative object generation through joint actions. The joint flow training uses a local-global principle to provide theoretical guarantees that independent policies generate samples proportionally to reward functions. Experimental results show the framework outperforms reinforcement learning and MCMC-based methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Scheduler-Driven Job Atomization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU scheduling, job atomization, resource fragmentation, MIG architecture, cluster efficiency]</li>
<li class=""><strong>authors:</strong> Michal Konopa, Jan Fesl, Ladislav Beránek</li>
<li class=""><strong>institution:</strong> University of South Bohemia</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.19086v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.19086v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Scheduler-Driven Job Atomization (SJA), a paradigm enabling bidirectional communication between schedulers and jobs to dynamically create subjobs fitting available execution gaps. Unlike traditional approaches, SJA proactively shapes workloads before execution to avoid costly state transfers and interruptions. The method aims to increase GPU utilization, reduce wait times, and minimize migration overhead by ensuring subjobs are correct by construction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] TD3-Sched: Learning to Orchestrate Container-based Cloud-Edge Resources
via Distributed Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [distributed reinforcement learning, cloud-edge computing, resource orchestration, TD3, container scheduling]</li>
<li class=""><strong>authors:</strong> Shengye Song, Minxian Xu, Kan Hu, Wenxia Guo, Kejiang Ye</li>
<li class=""><strong>institution:</strong> Southern University of Science and Technology, Shenzhen Institutes of Advanced Technology Chinese Academy of Sciences, Naval University of Engineering</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.18957v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.18957v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes TD3-Sched, a distributed reinforcement learning scheduler using Twin Delayed Deep Deterministic Policy Gradient for continuous CPU and memory allocation in cloud-edge environments. The method demonstrates significant latency reductions (17.9%-38.6%) and superior SLO compliance (only 0.47% violations) compared to baseline approaches. Results show faster convergence and more stable performance while maintaining service quality in container-based cloud-edge systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Whack-a-Mole: Deterministic Packet Spraying Across Multiple Network
Paths</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [packet spraying, multipath transport, network protocols, load balancing, distributed AI/ML]</li>
<li class=""><strong>authors:</strong> Michael Luby, John Byers</li>
<li class=""><strong>institution:</strong> BitRipple, Inc., Boston University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.18519v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.18519v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Whack-a-Mole is a deterministic packet spraying algorithm that distributes packets across multiple network paths using bit-reversal counters with provably tight discrepancy bounds. It dynamically responds to congestion by reallocating traffic from degraded paths to healthier ones. The algorithm effectively minimizes collective completion time and maximizes GPU utilization for distributed AI/ML workloads through deterministic load distribution and compatibility with erasure-coded transport.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] 6G Twin: Hybrid Gaussian Radio Fields for Channel Estimation and
Non-Linear Precoder Design for Radio Access Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [6G networks, channel estimation, Gaussian Radio Fields, nonlinear precoding, energy efficiency, massive MIMO, continual learning]</li>
<li class=""><strong>authors:</strong> Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Muhammad Ali Jamshed, Dean F. Hougen, John M. Cioffi</li>
<li class=""><strong>institution:</strong> Stanford University, University of Oklahoma, University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.18735v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.18735v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces 6G Twin, an AI-native framework combining neural Gaussian Radio Fields for compressed channel state acquisition, continual learning for mobility handling, and an energy-optimal nonlinear precoder. The system reduces pilot overhead by 100x while maintaining millisecond-scale operation and achieves 4-10x lower energy consumption with improved data rates. The integrated approach enables real-time channel tracking and efficient handovers in dynamic 6G network environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Cluster Workload Allocation: A Predictive Approach Leveraging Machine
Learning Efficiency</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [workload allocation, machine learning, cluster management, task constraints, ensemble classifier]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko</li>
<li class=""><strong>institution:</strong> University of Westminster</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17695v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17695v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This research uses machine learning classifiers to optimize cluster workload allocation by analyzing task constraints and node attributes from Google Cluster Data. Various ML algorithms were fine-tuned and evaluated, with an ensemble voting classifier achieving 98% accuracy. The approach effectively identifies suitable node-task pairings, particularly for tasks constrained to single nodes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale
MoE Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Mixture-of-Experts, model serving, distributed systems, fault tolerance, resource efficiency]</li>
<li class=""><strong>authors:</strong> Ziming Liu, Boyu Tian, Guoteng Wang, Zhen Jiang, Peng Sun, Zhenhua Han, Tian Tang, Xiaohe Hu, Yanmin Jia, Yan Zhang, He Liu, Mingjun Zhang, Yiqi Zhang, Qiaoling Chen, Shenggan Cheng, Mingyu Gao, Yang You, Siyuan Feng</li>
<li class=""><strong>institution:</strong> National University of Singapore, Shanghai Qiji Zhifeng Co., Ltd., Infrawaves, Nanyang Technological University, Tsinghua University, Shanghai Innovation Institute</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17863v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17863v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes EaaS, a novel serving system that disaggregates MoE modules into independent stateless services with a peer-to-peer communication library. This architecture enables fine-grained resource scaling and inherent fault tolerance while maintaining high performance. Experiments show EaaS achieves comparable performance to monolithic systems with less than 2% throughput reduction under failures and saves up to 37.5% computing resources through dynamic adaptation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Enhancing Cluster Scheduling in HPC: A Continuous Transfer Learning for
Real-Time Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [cluster scheduling, transfer learning, real-time optimization, node-affinity constraints, Kubernetes]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko, Jolanta Mizera-Pietraszko</li>
<li class=""><strong>institution:</strong> University of Westminster, Opole University of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22701v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22701v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a continuous transfer learning approach for real-time optimization of task scheduling in cluster systems, focusing on node-affinity constraints. The model dynamically evolves during operations to minimize retraining needs while achieving over 99% accuracy on Google Cluster Data. This method reduces computational overhead and improves scheduling latency for constrained tasks, advancing machine learning integration in cluster management.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Disaggregated Prefill and Decoding Inference System for Large Language
Model Serving on Multi-Vendor GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [P-D disaggregated inference, heterogeneous GPUs, joint optimization algorithm, multi-vendor GPUs, prefill and decoding]</li>
<li class=""><strong>authors:</strong> Xing Chen, Rong Shi, Lu Zhao, Lingbin Wang, Xiao Jin, Yueqiang Chen, Hongfeng Sun</li>
<li class=""><strong>institution:</strong> ZTE Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17542v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17542v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a P-D disaggregated inference system for LLM serving that separates prefill and decoding stages across heterogeneous GPUs from different vendors. It designs a heterogeneous compatible transmission module and a joint optimization algorithm for parallel strategy and instance allocation. Experimental results demonstrate the system effectively solves hybrid inference problems on heterogeneous GPUs and the optimization algorithm achieves optimal deployment solutions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via
Partially Disaggregated Prefill</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [heterogeneous GPU clusters, workload balancing, partially disaggregated prefill]</li>
<li class=""><strong>authors:</strong> Yunzhao Liu, Qiang Xu, Y. Charlie Hu</li>
<li class=""><strong>institution:</strong> Purdue University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17357v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17357v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Cronus introduces a partially disaggregated prefill approach that partitions the prefill stage between low-end and high-end GPUs, overlapping remaining prefill with decode stages. This dynamic workload balancing significantly improves throughput over disaggregated prefill while reducing P99 TTFT and TBT latency compared to data and pipeline parallelism methods. The system demonstrates superior performance across various GPU combinations in heterogeneous clusters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] MoA-Off: Adaptive Heterogeneous Modality-Aware Offloading with
Edge-Cloud Collaboration for Efficient Multimodal LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multimodal LLM, edge-cloud collaboration, adaptive offloading, inference optimization]</li>
<li class=""><strong>authors:</strong> Zheming Yang, Qi Guo, Yunqing Hu, Chang Zhao, Chang Zhang, Jian Zhao, Wen Ji</li>
<li class=""><strong>institution:</strong> Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Peng Cheng Laboratory, Institute of AI for Industries</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16995v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16995v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MoA-Off proposes an adaptive modality-aware offloading framework that uses a lightweight module to estimate input complexity and dynamically schedules workloads between edge and cloud. The system achieves over 30% latency reduction and 30%-65% resource overhead decrease while maintaining competitive accuracy compared to traditional approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [semantic caching, cross-region caching, LLM agents, tool access, performance optimization]</li>
<li class=""><strong>authors:</strong> Chaoyi Ruan, Chao Bi, Kaiwen Zheng, Ziji Shi, Xinyi Wan, Jialin Li</li>
<li class=""><strong>institution:</strong> National University of Singapore, University of Science and Technology of China, University of Toronto, Sea AI Lab</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.17360v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.17360v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Asteria introduces a semantic-aware cross-region caching architecture for LLM agents with two core components: Semantic Elements capturing query embeddings and metadata, and Semantic Retrieval Index providing two-stage retrieval. The system achieves up to 3.6× throughput improvement on search workloads while maintaining over 85% cache hit rate and preserving accuracy comparable to non-cached baselines. It also demonstrates 20% throughput improvement for complex coding tasks, showing effectiveness across diverse agentic workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Trace Replay Simulation of MIT SuperCloud for Studying Optimal
Sustainability Policies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [digital twin, power management, reinforcement learning, job scheduling, sustainability, supercomputing]</li>
<li class=""><strong>authors:</strong> Wesley Brewer, Matthias Maiterth, Damien Fay</li>
<li class=""><strong>institution:</strong> Oak Ridge National Laboratory, Hewlett Packard Enterprise</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16513v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16513v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper extends the ExaDigiT digital twin framework to simulate MIT SuperCloud workloads for studying sustainability policies. It uses trace replay and reinforcement learning to optimize job scheduling decisions. Preliminary experiments demonstrate the feasibility of learning energy-aware scheduling policies that improve datacenter efficiency and sustainability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix
Caching</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [distributed prefix caching, KV cache fetching, SmartNIC acceleration, interference-free decompression, chunked pipeline, minimal-copy memory management]</li>
<li class=""><strong>authors:</strong> Xingyu Xiang, Raj Joshi, Yuhan Liu, Jiayi Yao, Chenxingyu Zhao, Junchen Jiang, Yang Zhou, Eddie Kohler, Minlan Yu</li>
<li class=""><strong>institution:</strong> Harvard University, University of Chicago, University of Washington, UC Davis</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16857v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16857v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ShadowServe presents a SmartNIC-accelerated distributed prefix caching system that offloads KV cache decompression to eliminate interference with GPU computation. It employs a chunked pipeline and minimal-copy memory scheme to overcome SmartNIC resource limitations. The system achieves up to 2.2x lower TPOT and 1.38x lower TTFT in low-bandwidth scenarios, improving throughput by 1.35x compared to state-of-the-art solutions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Intelligent Load Balancing in Cloud Computer Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [load balancing, cloud computing, resource management, virtual machine migration, workload simulation]</li>
<li class=""><strong>authors:</strong> Leszek Sliwko</li>
<li class=""><strong>institution:</strong> University of Westminster</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22704v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22704v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This research developed intelligent load balancing strategies for cloud systems including centralized metaheuristic and decentralized agent-based approaches. The study created a high-fidelity cloud workload simulator using Google&#x27;s workload traces and experimented with virtual machine live migration. The proposed methods demonstrated effective dynamic task allocation while maintaining system stability at minimal cost through extensive experiments on an HPC cluster.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU hash tables, concurrent data structures, performance optimization, benchmarking framework]</li>
<li class=""><strong>authors:</strong> Hunter McCoy, Prashant Pandey</li>
<li class=""><strong>institution:</strong> Northeastern University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16407v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16407v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WarpSpeed implements eight concurrent GPU hash table designs with optimization techniques including fingerprint-based metadata and specialized GPU instructions. The library provides a unified benchmarking framework and rich API for modern GPU applications. Evaluation shows improved performance and scalability, offering practical guidance for efficient GPU data structure development.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Shift Parallelism: Low-Latency, High-Throughput LLM Inference for
Dynamic Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [parallelism, dynamic workloads, latency-throughput tradeoff, tensor parallelism, sequence parallelism, KV cache]</li>
<li class=""><strong>authors:</strong> Mert Hidayetoglu, Aurick Qiao, Michael Wyatt, Jeff Rasley, Yuxiong He, Samyam Rajbhandari</li>
<li class=""><strong>institution:</strong> Snowflake AI Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16495v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16495v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Shift Parallelism, a method that dynamically switches between tensor parallelism and sequence parallelism to optimize LLM inference. It achieves up to 1.51x faster response in interactive workloads and 50% higher throughput in batch workloads compared to TP-only solutions. The approach provides better latency-throughput tradeoffs for dynamic workloads without sacrificing performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Robust LLM Training Infrastructure at ByteDance</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [fault tolerance, fault diagnosis, cluster infrastructure, GPU management]</li>
<li class=""><strong>authors:</strong> Borui Wan, Gaohong Liu, Zuquan Song, Jun Wang, Yun Zhang, Guangming Sheng, Shuguang Wang, Houmin Wei, Chenyuan Wang, Weiqiang Lou, Xi Yang, Mofan Zhang, Kaihua Jiang, Cheng Ren, Xiaoyun Zhi, Menghan Yu, Zhe Nan, Zhuolin Zheng, Baoquan Zhong, Qinlong Wang, Huan Yu, Jinxin Chi, Wang Zhang, Yuhan Li, Zixian Du, Sida Zhao, Yongqiang Zhang, Jingzhe Tang, Zherui Liu, Chuan Wu, Yanghua Peng, Haibin Lin, Wencong Xiao, Xin Liu, Liang Xiang</li>
<li class=""><strong>institution:</strong> ByteDance, The University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16293v4" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16293v4</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ByteRobust is a large-scale GPU infrastructure management system designed for robust LLM training, focusing on failure detection and recovery through data-driven approaches. It leverages LLM training characteristics to enable high-capacity fault tolerance and prompt fault localization. The system achieves 97% ETTR for a three-month training job on 9,600 GPUs, significantly improving training stability and efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficient Pre-Training of LLMs via Topology-Aware Communication
Alignment on More Than 9600 GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM pre-training, communication patterns, network topology, GPU clusters, resource scheduling]</li>
<li class=""><strong>authors:</strong> Guoliang He, Youhe Jiang, Wencong Xiao, Kaihua Jiang, Shuguang Wang, Jun Wang, Zixian Du, Zhuo Jiang, Xinlei Zhang, Binhang Yuan, Eiko Yoneki</li>
<li class=""><strong>institution:</strong> University of Cambridge, ByteDance Seed, HKUST</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15940v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15940v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents Arnold, a topology-aware scheduling system that aligns LLM communication patterns with data center network topology to optimize training performance. The system reduces communication group spread by up to 1.67x and improves end-to-end training performance by 10.6% when scaling to over 9600 GPUs, demonstrating significant efficiency gains for large-scale LLM pre-training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] RLinf: Flexible and Efficient Large-scale Reinforcement Learning via
Macro-to-Micro Flow Transformation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [reinforcement learning, system optimization, workflow transformation, performance optimization, distributed training]</li>
<li class=""><strong>authors:</strong> Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, Zixiao Huang, Mingjie Wei, Yuqing Xie, Ke Yang, Bo Dai, Zhexuan Xu, Xiangyuan Wang, Xu Fu, Zhihao Liu, Kang Chen, Weilin Liu, Gang Liu, Boxun Li, Jianlei Yang, Zhi Yang, Guohao Dai, Yu Wang</li>
<li class=""><strong>institution:</strong> Tsinghua University, Peking University, UC Berkeley, Beihang University, Shanghai Jiaotong University, Infinigence AI, Zhongguancun Academy</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15965v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15965v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RLinf introduces a macro-to-micro flow transformation (M2Flow) paradigm that automatically decomposes and recomposes RL workflows for optimized execution. The system employs context switching, elastic pipelining, and profiling-guided scheduling to enhance training efficiency. Evaluations show RLinf achieves 1.1x-2.13x speedup over state-of-the-art systems in reinforcement learning tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] PCCL: Photonic circuit-switched collective communication for distributed
ML</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [collective communication, photonic circuits, network reconfiguration, distributed ML, GPU clusters]</li>
<li class=""><strong>authors:</strong> Abhishek Vijaya Kumar, Arjun Devraj, Rachee Singh</li>
<li class=""><strong>institution:</strong> Cornell University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15450v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15450v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PCCL introduces a photonic circuit-switched collective communication library that dynamically reconfigures network topology to match communication patterns, eliminating congestion and dilation in distributed ML. The hardware-agnostic optimization framework balances reconfiguration delays with performance gains. Evaluation shows up to 3× speedup over state-of-the-art algorithms on 128 GPUs, translating to 1.3× faster end-to-end training throughput.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Conditional Prior-based Non-stationary Channel Estimation Using
Accelerated Diffusion Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [diffusion models, channel estimation, wireless communication, non-stationary channels, MIMO systems]</li>
<li class=""><strong>authors:</strong> Muhammad Ahmed Mohsin, Ahsan Bilal, Muhammad Umer, Asad Aali, Muhammad Ali Jamshed, Dean F. Hougen, John M. Cioffi</li>
<li class=""><strong>institution:</strong> Stanford University, University of Oklahoma, University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.15182v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.15182v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a conditional prior diffusion method for non-stationary channel estimation that uses historical channel observations to guide the denoising process. The approach employs cross-time attention and SNR-matched initialization to accelerate inference while maintaining accuracy. Experimental results show superior performance over traditional baselines across all SNR ranges with strong high-SNR fidelity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless
and GPU-Based Training Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [serverless computing, distributed machine learning, cost-performance analysis, training architectures, communication optimization]</li>
<li class=""><strong>authors:</strong> Amine Barrak, Fabio Petrillo, Fehmi Jaafar</li>
<li class=""><strong>institution:</strong> Oakland University, École de technologie supérieure (ETS), University of Quebec at Chicoutimi</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.14920v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.14920v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares serverless distributed ML architectures (SPIRT, MLLess, AllReduce, ScatterReduce) with GPU-based training using CNN models on CIFAR-10. The study evaluates training time, cost, communication overhead, and accuracy under consistent conditions. Results show GPU-based training achieves fastest convergence while serverless frameworks offer cost advantages for lightweight models, with optimizations like gradient accumulation improving serverless performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU acceleration, Zero-Knowledge Proofs, performance analysis, Number-Theoretic Transform, Multi-Scalar Multiplication]</li>
<li class=""><strong>authors:</strong> Tarunesh Verma, Yichao Yuan, Nishil Talati, Todd Austin</li>
<li class=""><strong>institution:</strong> University of Michigan</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22684v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22684v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents ZKProphet, a comprehensive performance study of Zero-Knowledge Proofs on GPUs that identifies NTT as the new bottleneck after MSM optimizations. The analysis reveals under-utilization of GPU resources and limited instruction-level parallelism in arithmetic operations. The work provides optimization guidelines including parameter tuning and alternative data representations to scale ZKP performance on modern GPU architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Julia GraphBLAS with Nonblocking Execution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GraphBLAS, nonblocking execution, Julia programming, multi-stage programming, DAG optimization]</li>
<li class=""><strong>authors:</strong> Pascal Costanza, Timothy G. Mattson, Raye Kimmerer, Benjamin Brock</li>
<li class=""><strong>institution:</strong> University of Bristol, National Energy Research Scientific Computing Center, Lawrence Berkeley National Laboratory, Intel</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.14211v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.14211v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements nonblocking GraphBLAS execution in Julia using multi-stage programming to generate and compile symbolic code representations at runtime. The approach enables aggressive optimization of operation DAGs through fusion, parallelization, and other transformations. Julia&#x27;s language features significantly simplify implementing nonblocking execution, demonstrating promising results for GraphBLAS performance optimization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FLAME: A Serving System Optimized for Large-Scale Generative
Recommendation with Efficiency</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [generative recommendation, serving system, CPU-GPU heterogeneous computing, memory optimization, kernel fusion, request orchestration]</li>
<li class=""><strong>authors:</strong> Xianwen Guo, Bin Huang, Xiaomeng Wu, Guanlin Wu, Fangjian Li, Shijia Wang, Qiang Xiao, Chuanjiang Luo, Yong Li</li>
<li class=""><strong>institution:</strong> Netease Cloud Music</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.22681v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.22681v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FLAME introduces a CPU-GPU heterogeneous serving system with three key modules: Proximal Data Accelerator for memory optimization, Fused Kernel Engine for computation acceleration, and Dynamic Stream Orchestrator for request coordination. The system achieves significant performance improvements including 1.9x throughput gain, 1.7x latency reduction, and 4.6x-6.1x computation speedup. Comprehensive evaluations demonstrate FLAME effectively supports large-scale online deployment of generative recommendation models with remarkable system performance improvements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] GPU Programming for AI Workflow Development on AWS SageMaker: An
Instructional Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [GPU programming, AWS SageMaker, parallel computing, AI workflow development, HPC profiling]</li>
<li class=""><strong>authors:</strong> Sriram Srinivasan, Hamdan Alabsi, Rand Obeidat, Nithisha Ponnala, Azene Zenebe</li>
<li class=""><strong>institution:</strong> Bowie State University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.13703v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.13703v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an instructional course design that teaches GPU programming and parallel computing concepts using AWS SageMaker for developing AI agents. The course employed experiential learning with cloud-based GPU instances and tools like TensorBoard for performance analysis. Results showed AWS provided an effective platform for practical GPU programming, experiential learning enhanced technical proficiency, and the course strengthened students&#x27; problem-solving skills in compute-intensive AI workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Taming Serverless Cold Starts Through OS Co-Design</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [serverless], [serverless computing, cold start optimization, OS co-design, snapshot/restore, memory management]</li>
<li class=""><strong>authors:</strong> Ben Holmes, Baltasar Dinis, Lana Honcharuk, Joshua Fried, Adam Belay</li>
<li class=""><strong>institution:</strong> MIT CSAIL</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.14292v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.14292v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents Spice, an execution engine that integrates with the OS to optimize serverless snapshot/restore mechanisms, addressing OS-level limitations rather than storage speed. It restores kernel state without costly replay and introduces efficient memory mapping primitives to minimize page faults. The system achieves near-warm performance on cold restores, reducing latency significantly compared to existing process-based and VM-based approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Scaling Up Throughput-oriented LLM Inference Applications on
Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [throughput-oriented inference, opportunistic GPU clusters, pervasive context management, resource allocation, context reuse]</li>
<li class=""><strong>authors:</strong> Thanh Son Phung, Douglas Thain</li>
<li class=""><strong>institution:</strong> University of Notre Dame</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.13201v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.13201v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes pervasive context management to enable efficient LLM inference on heterogeneous opportunistic GPU clusters by reusing computational contexts across dynamically allocated resources. This throughput-oriented approach allows pooling available resources over time instead of requiring static allocations. Evaluation shows this method reduces execution time by 98.1% compared to traditional approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] AI Factories: It&#x27;s time to rethink the Cloud-HPC divide</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [AI Factories, HPC-cloud convergence, dual-stack approach, cloud-native technologies, supercomputing]</li>
<li class=""><strong>authors:</strong> Pedro Garcia Lopez, Daniel Barcelona Pons, Marcin Copik, Torsten Hoefler, Eduardo Quiñones, Maciej Malawski, Peter Pietzutch, Alberto Marti, Thomas Ohlson Timoudas, Aleksander Slominski</li>
<li class=""><strong>institution:</strong> URV, ETH Zurich, BSC, Sano &amp; AGH, ICL, OpenNebula, RISE, IBM Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12849v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12849v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a dual-stack approach integrating HPC and cloud-native technologies in supercomputers to bridge the performance-usability divide. It advocates combining HPC&#x27;s raw computing power with cloud-native tools like Kubernetes for better AI service deployment. The convergence aims to create accessible AI Factories that support both high-performance computing and user-friendly AI applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code
Translation in HPC</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [finetuning], [parallel code translation, LLM evaluation, compiler-guided repair]</li>
<li class=""><strong>authors:</strong> Tomer Bitan, Tal Kadosh, Erel Kaplan, Shira Meiri, Le Chen, Peter Morales, Niranjan Hasabnis, Gal Oren</li>
<li class=""><strong>institution:</strong> Technion, Ben-Gurion University, Argonne National Laboratory, Code Metal, Stanford University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12136v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12136v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces UniPar, a framework for evaluating LLM-based parallel code translation between serial code, CUDA, and OpenMP. The methodology combines fine-tuning, hyperparameter optimization, and compiler-guided repair to improve translation performance. Results show that UniPar improves compilation rates from 46% to 69% and functional correctness from 15% to 33% compared to off-the-shelf models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Towards the Distributed Large-scale k-NN Graph Construction by Graph
Merge</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [k-NN graph construction, graph merge, distributed computing, nearest neighbor search, indexing graph]</li>
<li class=""><strong>authors:</strong> Cheng Zhang, Wan-Lei Zhao, Shihai Xiao, Jiajie Yao, Xuecang Zhang</li>
<li class=""><strong>institution:</strong> Xiamen University, Huawei Technologies Ltd.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11697v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11697v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes efficient graph merge algorithms (Two-way Merge and Multi-way Merge) for distributed large-scale k-NN graph construction. The method enables building billion-scale graphs in parallel across multiple nodes, achieving high-quality results with significant time savings. Experimental results show a billion-scale k-NN graph can be constructed in approximately 17 hours using only three nodes while maintaining similar search performance to original indexing graphs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Parallel/Distributed Tabu Search for Scheduling Microprocessor Tasks in
Hybrid Flowshop</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [tabu search, parallel computing, distributed computing, hybrid flow shop, multiprocessor tasks]</li>
<li class=""><strong>authors:</strong> Adam Janiak, Damian Kowalczyk, Maciej Lichtenstein</li>
<li class=""><strong>institution:</strong> Wrocław University of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11396v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11396v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a parallel/distributed tabu search algorithm for scheduling multiprocessor tasks in hybrid flow shop environments to minimize makespan. The algorithm employs parallel and distributed mechanisms for neighborhood evaluation and effectively balances heterogeneous network environments. The approach addresses NP-hard scheduling problems by leveraging distributed computing power to find good solutions efficiently.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Chameleon: Taming Dynamic Operator Sequences for Memory-Intensive LLM
Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [memory optimization, swap-based training, dynamic operator sequences, eager mode]</li>
<li class=""><strong>authors:</strong> Zibo Wang, Yuhang Zhou, Zhibin Wang, Shipeng Li, Xinjing Huang, Chendong Cai, Bingxu Mu, Yuqing Sun, Zhiheng Hu, Bin She, Shu You, Guanghuan Fang, Rong Gu, Wanchun Dou, Guihai Chen, Chen Tian</li>
<li class=""><strong>institution:</strong> Nanjing University, Huawei Technologies Co., Ltd</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11076v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11076v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Chameleon introduces a lightweight online profiler and optimized swap policy execution to handle dynamic operator sequences in LLM training. It reduces profiling overhead by 84.25% and enables training models up to 4x larger than hardware memory. The system improves performance by up to 38.94% compared to recomputation or high-degree parallelism methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] GFS: A Preemption-aware Scheduling Framework for GPU Clusters with
Predictive Spot Instance Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU scheduling, spot instances, preemption management, resource allocation, demand forecasting]</li>
<li class=""><strong>authors:</strong> Jiaang Duan, Shenglin Xu, Shiyou Qian, Dingyu Yang, Kangjin Wang, Chenzhi Liao, Yinghao Yu, Qin Hua, Hanwen Hu, Qi Wang, Wenchao Wu, Dongqing Bao, Tianyu Lu, Jian Cao, Guangtao Xue, Guodong Yang, Liping Zhang, Gang Chen</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Zhejiang University, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.11134v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.11134v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GFS introduces a preemption-aware scheduling framework with predictive spot instance management for GPU clusters. It uses demand forecasting, dynamic spot quota allocation, and preemptive scheduling policies to balance high-priority and low-priority tasks. The framework reduces LP task eviction rates by 33.0%, cuts queuing delays by 44.1%, and increases GPU allocation rates by up to 22.8% in production environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Coordinated Reinforcement Learning Prefetching Architecture for
Multicore Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [hardware prefetching, multicore systems, reinforcement learning, memory optimization, coordinated learning]</li>
<li class=""><strong>authors:</strong> Mohammed Humaid Siddiqui, Fernando Guzman, Yufei Wu, Ruishu Ann</li>
<li class=""><strong>institution:</strong> Fairleigh Dickinson University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.10719v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.10719v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes CRL-Pythia, a coordinated reinforcement learning-based prefetching architecture that enables cross-core information sharing and cooperative prefetching decisions in multicore systems. This approach significantly reduces redundant prefetch requests (up to 20%) and improves learning convergence across cores. Experimental results show CRL-Pythia achieves approximately 12% IPC improvement for bandwidth-constrained workloads while maintaining moderate hardware overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FastTrack: GPU-Accelerated Tracking for Visual SLAM</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU acceleration, visual SLAM, tracking optimization, CUDA implementation, stereo feature matching, local map tracking]</li>
<li class=""><strong>authors:</strong> Kimia Khabiri, Parsa Hosseininejad, Shishir Gopinath, Karthik Dantu, Steven Y. Ko</li>
<li class=""><strong>institution:</strong> Simon Fraser University, University at Buffalo</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.10757v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.10757v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents FastTrack, a GPU-accelerated tracking module for visual-inertial SLAM systems that offloads computationally intensive components like stereo feature matching and local map tracking to the GPU using CUDA. The implementation based on ORB-SLAM3 demonstrates up to 2.8× performance improvement on both desktop and embedded platforms when evaluated with EuRoC and TUM-VI datasets. The approach carefully balances parallelization benefits against data transfer costs to achieve significant tracking speedups.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] MinatoLoader: Accelerating Machine Learning Training Through Efficient
Data Preprocessing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [data loader, data preprocessing, GPU utilization, batch construction, PyTorch]</li>
<li class=""><strong>authors:</strong> Rahma Nouaji, Stella Bitchebe, Ricardo Macedo, Oana Balmau</li>
<li class=""><strong>institution:</strong> McGill University, INESC TEC, University of Minho</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.10712v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.10712v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MinatoLoader introduces an efficient data loader that prioritizes fast-to-preprocess samples and processes slower ones in parallel to reduce GPU idleness. It improves training time by up to 7.5× and increases GPU utilization from 46.4% to 90.45% while maintaining model accuracy and enabling faster convergence.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Characterizing the Efficiency of Distributed Training: A Power,
Performance, and Thermal Perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, GPU clusters, parallelism strategies, power consumption, thermal behavior, system optimization]</li>
<li class=""><strong>authors:</strong> Seokjin Go, Joongun Park, Spandan More, Hanjiang Wu, Irene Wang, Aaron Jezghani, Tushar Krishna, Divya Mahajan</li>
<li class=""><strong>institution:</strong> Georgia Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.10371v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.10371v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper comprehensively characterizes LLM training efficiency across multiple GPU platforms and parallelism strategies, analyzing hardware utilization, power consumption, and thermal behavior. The study reveals that performance depends on complex interactions between hardware, system topology, and model execution rather than just hardware scaling. Key findings show scale-up systems can outperform scale-out in communication-bound scenarios, while certain parallelism combinations cause bandwidth underutilization and thermal throttling issues.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] SynergAI: Edge-to-Cloud Synergy for Architecture-Driven High-Performance
Orchestration for AI Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [AI inference, edge-to-cloud orchestration, performance-aware scheduling, architecture-aware deployment, Kubernetes]</li>
<li class=""><strong>authors:</strong> Foteini Stathopoulou, Aggelos Ferikoglou, Manolis Katsaragakis, Dimosthenis Masouros, Sotirios Xydis, Dimitrios Soudris</li>
<li class=""><strong>institution:</strong> National Technical University of Athens</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12252v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12252v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SynergAI introduces a framework for performance- and architecture-aware AI inference serving across heterogeneous edge-to-cloud infrastructures, combining offline and online decision-making policies for intelligent workload scheduling. It dynamically allocates workloads across diverse hardware architectures to minimize QoS violations. Evaluation shows it achieves 2.4x average reduction in QoS violations compared to state-of-the-art solutions through optimized architecture-aware deployments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] TrEnv: Transparently Share Serverless Execution Environments Across
Different Functions and Nodes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [serverless computing, execution environment sharing, memory optimization, cold start reduction, LLM agents]</li>
<li class=""><strong>authors:</strong> Jialiang Huang, Teng Ma, Zheng Liu, Sixing Lin, Kang Chen, Jinlei Jiang, Xia Liao, Yingdi Shan, Yongwei Wu, Ning Zhang, Mengting Lu, Tao Ma, Haifeng Gong, Mingxing Zhang</li>
<li class=""><strong>institution:</strong> Tsinghua University, Alibaba Group, Zhejiang University, Intel</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.09525v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.09525v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TrEnv introduces a serverless platform that transparently shares execution environments across functions and nodes using repurposable sandboxes and memory templates. It reduces startup latency and memory usage for both container- and VM-based environments, particularly optimized for LLM agent workloads. Evaluation shows up to 7× lower P99 latency and 61% memory savings compared to state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Optimizing the Variant Calling Pipeline Execution on Human Genomes Using
GPU-Enabled Machines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [variant calling, GPU optimization, machine learning, job scheduling, genomic data processing]</li>
<li class=""><strong>authors:</strong> Ajay Kumar, Praveen Rao, Peter Sanders</li>
<li class=""><strong>institution:</strong> The University of Missouri, Karlsruhe Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.09058v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.09058v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a machine learning-based approach to optimize variant calling pipeline execution on GPU-enabled machines. The method uses ML to predict execution times of pipeline stages and generates optimal execution plans inspired by flexible job shop scheduling. Experimental results show 2X speedup over greedy approaches and 1.6X speedup over dynamic resource allocation methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and
Dynamic Parallelism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [heterogeneous GPU clusters, fine-grained parallelism, dynamic parallelism, attention computation, load balancing]</li>
<li class=""><strong>authors:</strong> Zizhao Mo, Jianxiong Liao, Huanle Xu, Zhi Zhou, Chengzhong Xu</li>
<li class=""><strong>institution:</strong> University of Macau, Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.08309v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.08309v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hetis introduces a fine-grained and dynamic parallelism approach for LLM serving in heterogeneous GPU clusters, selectively parallelizing compute-intensive operations and dynamically distributing attention computations at head granularity. It employs an online load dispatching policy to balance network latency, computational load, and memory intensity. Evaluation shows Hetis improves throughput by up to 2.25× and reduces latency by 1.49× compared to existing systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Design and Implementation of Code Completion System Based on LLM and
CodeBERT Hybrid Subsystem</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [code completion, hybrid model, CodeBERT, GPT-3.5, deep learning]</li>
<li class=""><strong>authors:</strong> Bingbing Zhang, Ziyu Lin, Yingxin Su</li>
<li class=""><strong>institution:</strong> Xiamen Institute of Technology, Google LLC, University of California Davis</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.08215v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.08215v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid model combining CodeBERT and GPT-3.5 for code completion tasks, leveraging CodeBERT&#x27;s code understanding and GPT-3.5&#x27;s generation capabilities. The hybrid system demonstrates superior performance in accuracy, code quality, and efficiency compared to benchmarks. Robustness testing confirms the model&#x27;s reliability and effectiveness in software development applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] HYLU: Hybrid Parallel Sparse LU Factorization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [sparse linear systems, LU factorization, parallel computing, numerical kernels, hybrid solver]</li>
<li class=""><strong>authors:</strong> Xiaoming Chen</li>
<li class=""><strong>institution:</strong> Institute of Computing Technology, Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07690v4" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07690v4</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HYLU introduces a hybrid parallel sparse LU factorization solver that integrates multiple numerical kernels to adapt to various matrix sparsity patterns. It demonstrates significant performance improvements over Intel MKL PARDISO, achieving geometric mean speedups of 1.95x for one-time solving and 2.40x for repeated solving on benchmark matrices. The solver is designed for multi-core shared-memory architectures and addresses the challenge of efficiently handling diverse sparse linear systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with
FaaS-hosted MCP Services</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Agentic AI, workflow patterns, MCP servers, FaaS deployment, performance evaluation]</li>
<li class=""><strong>authors:</strong> Shiva Sai Krishna Anand Tokal, Vaibhav Jha, Anand Eswaran, Praveen Jayachandran, Yogesh Simmhan</li>
<li class=""><strong>institution:</strong> Indian Institute of Science, IBM India Research Lab</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07595v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07595v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AgentX, a novel agentic workflow pattern composed of stage designer, planner, and executor agents, and introduces FaaS-based deployment approaches for MCP servers. Empirical evaluation shows AgentX achieves competitive or better success rates compared to ReAct and Magentic One patterns, while highlighting trade-offs in latency and cost for different deployment models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] MoE-Compression: How the Compression Error of Experts Affects the
Inference Accuracy of MoE Model?</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [model compression, Mixture of Experts, error-bounded lossy compression, SZ3, CuSZp, expert offloading]</li>
<li class=""><strong>authors:</strong> Songkai Ma, Zhaorui Zhang, Sheng Di, Benben Liu, Xiaodong Yu, Xiaoyi Lu, Dan Wang</li>
<li class=""><strong>institution:</strong> Hong Kong Polytechnic University, Argonne National Laboratory, The University of Hong Kong, Stevens Institute of Technology, University of California Merced</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07727v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07727v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using error-bounded lossy compression algorithms like SZ3 and CuSZp to compress non-activated experts in MoE models, reducing data transfer overhead during inference. The study finds that compression errors in shallow layers cause minimal accuracy degradation, middle-layer errors significantly impair accuracy, while deep-layer errors can sometimes improve inference performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Towards Scalable Proteomics: Opportunistic SMC Samplers on HTCondor</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Sequential Monte Carlo, HTCondor, opportunistic computing, proteomics, Bayesian inference]</li>
<li class=""><strong>authors:</strong> Matthew Carter, Lee Devlin, Alexander Philips, Edward Pyzer-Knapp, Paul Spirakis, Simon Maskell</li>
<li class=""><strong>institution:</strong> University of Liverpool, Xyme</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.08020v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.08020v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces an opportunistic computing framework using HTCondor to deploy Sequential Monte Carlo samplers for scalable proteomics inference. It features a Coordinator-Manager-Follower architecture that reduces synchronization overhead in heterogeneous environments. Results show the approach achieves accurate inference with weak scaling, generating more samples as resources increase under fixed time constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Astra: A Multi-Agent System for GPU Kernel Performance Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU kernel optimization, multi-agent LLM system, CUDA optimization, performance improvement, SGLang]</li>
<li class=""><strong>authors:</strong> Anjiang Wei, Tianran Sun, Yogesh Seenichamy, Hang Song, Anne Ouyang, Azalia Mirhoseini, Ke Wang, Alex Aiken</li>
<li class=""><strong>institution:</strong> Stanford University, Shanghai Jiao Tong University, Nanjing University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07506v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07506v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Astra introduces the first LLM-based multi-agent system for GPU kernel optimization that starts from existing CUDA implementations and uses specialized agents collaborating through iterative code generation, testing, profiling, and planning. The system achieves an average 1.32x speedup on SGLang kernels using zero-shot prompting with OpenAI o4-mini. This demonstrates multi-agent LLM systems can autonomously apply various optimizations like loop transformations and memory access improvements to yield substantial performance gains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for
Efficient MoE LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Mixture of Experts, expert scheduling, memory optimization, prefetching, cache management]</li>
<li class=""><strong>authors:</strong> Yuning Zhang, Grant Pinkert, Nan Yang, Yanli Li, Dong Yuan</li>
<li class=""><strong>institution:</strong> The University of Sydney</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07379v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07379v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DuoServe-MoE proposes a dual-phase inference system that separates prefill and decode stages with tailored expert scheduling strategies. It uses a two-stream CUDA pipeline for prefill and a lightweight predictor for decode stage expert prefetching. Experiments show 1.42-7.54× latency improvement while maintaining only 15% peak memory usage of full model size.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Optimizing Task Scheduling in Fog Computing with Deadline Awareness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [fog computing, task scheduling, energy optimization, deadline awareness, reinforcement learning, golden eagle optimization]</li>
<li class=""><strong>authors:</strong> Mohammad Sadegh Sirjani, Mohammad Ahmad, Somayeh Sobati-Moghadam</li>
<li class=""><strong>institution:</strong> University of Texas at San Antonio, Ferdowsi University of Mashhad</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07378v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07378v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes RIGEO algorithm that combines Improved Golden Eagle Optimization for short-deadline tasks on low-traffic nodes and reinforcement learning for long-deadline tasks on high-traffic nodes. The method achieves up to 29% energy reduction, 86% response time improvement, and 19% fewer deadline violations compared to state-of-the-art approaches. The hybrid scheduling approach effectively balances energy efficiency and QoS requirements in fog computing environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Accelerating Frontier MoE Training with 3D Integrated Optics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [3D integrated photonics, mixture of experts, scale-up networks, high-bandwidth interconnects, GPU clusters]</li>
<li class=""><strong>authors:</strong> Mikhail Bernadskiy, Peter Carson, Thomas Graham, Taylor Groves, Ho John Lee, Eric Yeh</li>
<li class=""><strong>institution:</strong> Lightmatter</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15893v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15893v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using 3D integrated photonics to overcome copper interconnect limitations in AI training infrastructure. The method enables 8X scale-up capability and 2.7X faster training for trillion-parameter MoE models by providing higher bandwidth and longer reach across data center racks. The photonic solution demonstrates significant improvements in both performance and energy efficiency compared to traditional electrical interconnects.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Research on fault diagnosis and root cause analysis based on full stack
observability</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [fault diagnosis, root cause analysis, causal discovery, multi-modal fusion, AIOps]</li>
<li class=""><strong>authors:</strong> Jian Hou</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12231v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12231v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes KylinRCA framework that integrates temporal causal discovery and cross-modal graph learning for fault diagnosis. It achieves global root cause localization and provides auditable evidence chains through mask-based explanations. The framework offers an effective solution for fault diagnosis under full-stack observability environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [model orchestration, SLO optimization, heterogeneous instances, continuous batching, parallelism strategies]</li>
<li class=""><strong>authors:</strong> Mo Xuan, Zhang yue, Wu Weigang</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.06362v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.06362v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MaaSO introduces a SLO-aware orchestrator for MaaS platforms with three key modules: a profiler for instance performance characterization, a placer for optimizing heterogeneous configurations, and a distributor for SLO-aware request routing. The system improves SLO satisfaction by 15-30% and reduces response latency by 40-60% compared to existing approaches while lowering orchestration overhead. It addresses diverse LLM application requirements through intelligent instance configuration and request distribution.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FineServe: Precision-Aware KV Slab and Two-Level Scheduling for
Heterogeneous Precision LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [KV cache management, memory fragmentation, quantization, GPU sharing, scheduling]</li>
<li class=""><strong>authors:</strong> Kyungmin Bin, Seungbeom Choi, Jimyoung Son, Jieun Choi, Daseul Bae, Daehyeon Baek, Kihyo Moon, Minsung Jang, Hyojung Lee</li>
<li class=""><strong>institution:</strong> Samsung SDS</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.06261v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.06261v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FineServe proposes a precision-aware KV slab memory management technique and two-level scheduling framework for serving mixed-precision LLMs. The KV slab dynamically allocates KV cache based on quantization characteristics to reduce memory fragmentation, while the scheduler optimizes model placement and batch sizing. Experiments show FineServe achieves up to 2.2× higher SLO attainment and 1.8× higher throughput compared to state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across
Thousands of Computers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [distributed vector search, approximate nearest neighbor, graph index, key-value store, query latency, scalability]</li>
<li class=""><strong>authors:</strong> Philip Adams, Menghao Li, Shi Zhang, Li Tan, Qi Chen, Mingqin Li, Zengzhong Li, Knut Risvik, Harsha Vardhan Simhadri</li>
<li class=""><strong>institution:</strong> Microsoft, Microsoft Research Asia, Shopify</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.06046v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.06046v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DISTRIBUTEDANN presents a distributed vector search system that scales a single DISKANN graph index across thousands of machines using a distributed key-value store and in-memory ANN index. It achieves 26ms median query latency and processes over 100,000 queries per second on a 50-billion vector dataset, demonstrating 6x higher efficiency than traditional partitioning approaches. The system has been successfully deployed in Microsoft Bing, replacing conventional scale-out architectures for large-scale search applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Distributed Deep Learning using Stochastic Gradient Staleness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed deep learning, stochastic gradient staleness, parallel backpropagation, data parallelism, convergence analysis]</li>
<li class=""><strong>authors:</strong> Viet Hoang Pham, Hyo-Sung Ahn</li>
<li class=""><strong>institution:</strong> Posts and Telecommunications Institute of Technology (PTIT), Gwangju Institute of Science and Technology (GIST)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.05679v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.05679v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a distributed training method combining data parallelism and fully decoupled parallel backpropagation to accelerate deep learning. The approach uses multiple computational units to process more training data per iteration while avoiding locking issues in backpropagation. Theoretical convergence guarantees are provided and empirical evaluations on CIFAR-10 demonstrate significant improvements in training efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficient Fault Localization in a Cloud Stack Using End-to-End
Application Service Topology</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [fault localization, cloud computing, root cause detection, performance anomalies, service topology, metric selection]</li>
<li class=""><strong>authors:</strong> Dhanya R Mathews, Mudit Verma, Pooja Aggarwal, J. Lakshmi</li>
<li class=""><strong>institution:</strong> Indian Institute of Science, IBM Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.05511v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.05511v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a topology-aware root cause detection (TA-RCD) approach that uses end-to-end application service topology to select informative metrics for efficient fault localization in cloud stacks. The method incorporates service topology into root cause detection algorithms to improve accuracy. Evaluation shows the proposed approach achieves at least 2x better performance than state-of-the-art RCD algorithms in Top-3 and Top-5 recall metrics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Scaling Performance of Large Language Model Pretraining</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, data parallelism, scaling performance, GPU compute capacity, cluster infrastructure]</li>
<li class=""><strong>authors:</strong> Alexander Interrante-Grant, Carla Varela-Rosa, Suhaas Narayan, Chris Connelly, Albert Reuther</li>
<li class=""><strong>institution:</strong> MIT Lincoln Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.05258v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.05258v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates scaling performance in large language model pretraining pipelines, focusing on distributed training techniques and data parallelism optimization. The research was conducted using hundreds of Nvidia H100 GPU nodes on the TX-GAIN supercomputing cluster. The work provides practical recommendations for fully leveraging GPU compute capacity when scaling up LLM training across large datasets and model sizes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] veScale: Consistent and Efficient Tensor Programming with Eager-Mode
SPMD</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [SPMD, distributed training, eager execution, tensor programming, random number generation, performance optimization]</li>
<li class=""><strong>authors:</strong> Youjie Li, Cheng Wan, Zhiqi Lin, Hongyu Zhu, Jiacheng Yang, Ziang Song, Xinyi Di, Jiawei Wu, Huiyao Shu, Wenlei Bao, Yanghua Peng, Haibin Lin, Li-Wen Chang</li>
<li class=""><strong>institution:</strong> ByteDance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07003v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07003v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> veScale introduces an eager-mode training system that fully embraces the SPMD paradigm to simplify distributed tensor programming. It addresses consistency issues through a novel distributed RNG algorithm and improves performance by reducing PyTorch overhead and enhancing communication efficiency. The system achieves up to 2.2x speedup over state-of-the-art training systems while reducing code complexity by 78.4% and maintaining single-device-equivalent results.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Discovering Software Parallelization Points Using Deep Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [software parallelization, loop classification, deep learning, genetic algorithm, code tokenization]</li>
<li class=""><strong>authors:</strong> Izavan dos S. Correia, Henrique C. T. Santos, Tiago A. E. Ferreira</li>
<li class=""><strong>institution:</strong> Federal Rural University of Pernambuco, Federal Institute of Pernambuco</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16215v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16215v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using deep neural networks (DNN and CNN) to classify programming loops for parallelization potential, generating training data via genetic algorithms. The CNN model achieved slightly better performance than DNN, with both showing similar variability. The study demonstrates deep learning&#x27;s feasibility in automating parallelizable structure identification for software optimization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing
for Energy-Efficient LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [energy efficiency, frequency scaling, request routing, SLO-aware serving, control theory]</li>
<li class=""><strong>authors:</strong> Jiahuan Yu, Aryan Taneja, Junfeng Lin, Minjia Zhang</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04827v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04827v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VoltanaLLM introduces a feedback-driven frequency controller and state-space router that co-design GPU frequency scaling and request routing in prefill/decode disaggregated architectures. The system achieves up to 36.3% energy savings while maintaining near-perfect SLO attainment rates across multiple LLMs and real-world datasets. This demonstrates effective energy-efficient LLM serving through phase-specific control and intelligent routing decisions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous
GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [diffusion model inference, heterogeneous GPU systems, parallel computing, workload balancing]</li>
<li class=""><strong>authors:</strong> Han Liang, Jiahui Zhou, Zicheng Zhou, Xiaoxi Zhang, Xu Chen</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04719v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04719v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> STADI introduces a hybrid scheduler with fine-grained temporal and spatial parallelism for diffusion model inference on heterogeneous GPUs. It uses computation-aware step allocation and elastic patch parallelism to balance workloads across different GPU capabilities. The method reduces inference latency by up to 45% and improves resource utilization compared to existing approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Massively-Parallel Implementation of Inextensible Elastic Rods Using
Inter-block GPU Synchronization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU computing, elastic rods simulation, inter-block synchronization, real-time simulation, medical simulation]</li>
<li class=""><strong>authors:</strong> Przemyslaw Korzeniowski, Niels Hald, Fernando Bello</li>
<li class=""><strong>institution:</strong> Imperial College London</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04277v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04277v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a massively-parallel GPU implementation of inextensible elastic rods using inter-block synchronization, enabling multiple physics time-steps per kernel launch. The method achieves significant speedups (up to 40x faster) over CPU implementations while maintaining computational efficiency across different element counts. This allows for accurate real-time simulation at haptic interactive rates required for medical applications like catheter/guidewire simulation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Counterfactual simulations for large scale systems with burnout
variables</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [counterfactual simulation, burnout variables, parallel computing, online advertising, MapReduce]</li>
<li class=""><strong>authors:</strong> Benjamin Heymann</li>
<li class=""><strong>institution:</strong> ENSAE, Criteo AI LAB</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04038v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04038v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces uncertainty relaxation algorithms that enable efficient parallel computation for counterfactual estimation in systems with burnout variables. This approach significantly improves scalability compared to traditional sequential processing methods. The method is particularly applicable to online advertising platforms where budget constraints create complex coupling effects between campaigns.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for
High-Performance Distributed Training Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [checkpointing], [distributed training, gradient compression, differential checkpointing, failure recovery, performance optimization]</li>
<li class=""><strong>authors:</strong> Chenxuan Yao, Yuchong Hu, Feifan Liu, Zhengyu Liu, Dan Feng</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04084v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04084v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LowDiff proposes an efficient frequent checkpointing framework that reuses compressed gradients as differential checkpoints and employs batched gradient writes with dynamic tuning. It achieves high checkpointing frequency up to per iteration with minimal runtime overhead through layer-wise gradient reusing and asynchronous persistence strategies. Experimental results demonstrate less than 3.1% performance overhead while enabling rapid failure recovery in distributed training systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Combining Performance and Productivity: Accelerating the Network Sensing
Graph Challenge with GPUs and Commodity Data Science Software</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [Graph Challenge, GPU acceleration, RAPIDS, cuDF, cupy, GraphBLAS, network sensing, data science]</li>
<li class=""><strong>authors:</strong> Siddharth Samsi, Dan Campbell, Emanuel Scoullos, Oded Green</li>
<li class=""><strong>institution:</strong> NVIDIA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03653v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03653v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an alternative implementation of the Network Sensing Graph Challenge using NVIDIA&#x27;s RAPIDS ecosystem and data science frameworks. The authors demonstrate that off-the-shelf tools like cuDF and cupy can achieve significant speedups over CPU-based implementations without requiring specialized HPC code. They report GPU acceleration improvements ranging from 147x to 2185x across different NVIDIA GPU architectures compared to Pandas on CPU.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] DPQuant: Efficient and Differentially-Private Model Training via Dynamic
Quantization Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [differential privacy, quantization, deep learning, model training, efficiency optimization]</li>
<li class=""><strong>authors:</strong> Yubo Gao, Renbo Tu, Gennady Pekhimenko, Nandita Vijaykumar</li>
<li class=""><strong>institution:</strong> University of Toronto</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03472v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03472v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DPQuant introduces a dynamic quantization framework that adaptively selects layers to quantize each epoch using probabilistic sampling and loss-aware prioritization. This approach reduces quantization variance amplified by DP-SGD noise injection while preserving privacy guarantees. The method achieves near Pareto-optimal accuracy-compute trade-offs with up to 2.21× throughput improvements and less than 2% accuracy drop across various models and datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Prob-GParareal: A Probabilistic Numerical Parallel-in-Time Solver for
Differential Equations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [probabilistic numerical methods, parallel-in-time solvers, Gaussian processes, uncertainty quantification, differential equations]</li>
<li class=""><strong>authors:</strong> Guglielmo Gattiglio, Lyudmila Grigoryeva, Massimiliano Tamborrino</li>
<li class=""><strong>institution:</strong> University of Warwick, University of St. Gallen</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03945v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03945v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Prob-GParareal, a probabilistic extension of the GParareal algorithm that uses Gaussian processes to model correction functions and quantify uncertainty in parallel-in-time solutions of differential equations. Theoretical analysis shows computational complexity and error bounds, while numerical experiments demonstrate accuracy on benchmark ODE systems including chaotic, stiff, and bifurcation problems. The method also accommodates probabilistic initial conditions and maintains compatibility with classical numerical solvers, bridging a gap in probabilistic counterparts to established PinT methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Semi-decentralized Federated Time Series Prediction with Client
Availability Budgets</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [federated learning], [time series prediction, client selection, availability budgets, semi-decentralized learning, trajectory data]</li>
<li class=""><strong>authors:</strong> Yunkai Bao, Reza Safarzadeh, Xin Wang, Steve Drew</li>
<li class=""><strong>institution:</strong> University of Calgary</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03660v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03660v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FedDeCAB, a semi-decentralized client selection method that uses probabilistic rankings to handle client availability constraints in federated learning. It enables offline clients to obtain partial model parameters from neighbors for joint optimization, reducing communication overhead. Experiments on real-world trajectory datasets demonstrate effectiveness under heterogeneous data distribution and dynamic client availability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] CloudFormer: An Attention-based Performance Prediction for Public Clouds
with Unknown Workload</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [performance prediction, cloud computing, transformer model, virtual machine interference, system metrics]</li>
<li class=""><strong>authors:</strong> Amirhossein Shahbazinia, Darong Huang, Luis Costero, David Atienza</li>
<li class=""><strong>institution:</strong> EPFL (École Polytechnique Fédérale de Lausanne)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03394v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03394v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CloudFormer proposes a dual-branch Transformer model that jointly models temporal dynamics and system-level interactions to predict VM performance degradation in public clouds. The model achieves state-of-the-art performance with 7.8% MAE and demonstrates robust generalization across diverse workloads without scenario-specific tuning. Experimental results show it substantially outperforms existing methods by at least 28% in predictive accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] The High Cost of Keeping Warm: Characterizing Overhead in Serverless
Autoscaling Policies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [serverless], [autoscaling, performance overhead, resource efficiency, cloud computing, control plane design]</li>
<li class=""><strong>authors:</strong> Leonid Kondrashov, Boxi Zhou, Hancheng Wang, Dmitrii Ustiugov</li>
<li class=""><strong>institution:</strong> Nanyang Technological University, Nanjing University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03104v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03104v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper designs an open-source serverless system that replicates commercial platform behaviors and evaluates autoscaling policies using real-world workloads. It finds significant computational overhead (10-40% CPU cycles) from instance churn and excessive memory allocation (2-10x) due to scaling policies. The study demonstrates that reducing these overheads causes performance degradation, highlighting the need for better autoscaling strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale
Training of LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [failure recovery, checkpoint-free, scale-independent, fault-tolerance]</li>
<li class=""><strong>authors:</strong> Haijun Zhang, Jinxiang Wang, Zhenhua Yu, Yanyong Zhang, Xuejie Ji, Kaining Mao, Jun Zhang, Yaqing Zhang, Ting Wu, Fei Jie, Xiemin Huang, Zhifang Cai, Junhua Cheng, Shuwei Wang, Wei Li, Xiaoming Bao, Hua Xu, Shixiong Zhao, Jun Li, Hongwei Sun, Ziyang Zhang, Yi Xiong, Chunsheng Li</li>
<li class=""><strong>institution:</strong> iFLYTEK AI Engineering Institute, University of Science and Technology of China, Huawei Technologies Co., Ltd</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03047v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03047v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlashRecovery introduces a fast failure recovery system with three core innovations: real-time failure detection, scale-independent task restart, and checkpoint-free single-step restoration. The system achieves optimal recovery time objectives by eliminating traditional checkpoint overhead and maintaining consistent recovery times across cluster scales. Experimental results demonstrate recovery of 4,800-device training clusters within 150 seconds.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Mycroft: Tracing Dependencies in Collective Communication Towards
Reliable LLM Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [collective communication, distributed tracing, root cause analysis, reliability, fault detection]</li>
<li class=""><strong>authors:</strong> Yangtao Deng, Lei Zhang, Qinlong Wang, Xiaoyun Zhi, Xinlei Zhang, Zhuo Jiang, Haohan Xu, Lei Wang, Zuquan Song, Gaohong Liu, Yang Bai, Shuguang Wang, Wencong Xiao, Jianxi Ye, Minlan Yu, Hong Xu</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, ByteDance, Harvard University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03018v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03018v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mycroft introduces a lightweight distributed tracing system that tracks collective communication states and leverages internal dependencies to resolve reliability issues in LLM training. It achieves rapid anomaly detection within 15 seconds in 90% of cases and root cause identification within 20 seconds in 60% of cases. The system has been successfully deployed at ByteDance for over six months, demonstrating effectiveness through extensive fault injection experiments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to
Break the GPU Memory Wall</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [offloading, GPU memory, optimizer states, I/O optimization, multi-tier storage]</li>
<li class=""><strong>authors:</strong> Avinash Maurya, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory, Rochester Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02480v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02480v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MLP-Offload introduces a multi-level, multi-path offloading engine that optimizes LLM training by efficiently managing optimizer states across storage tiers with cache-aware and concurrency-controlled strategies. It addresses I/O bottlenecks during backward and update phases, particularly in resource-constrained setups. Evaluations show it achieves 2.5× faster training iterations compared to state-of-the-art methods for models up to 280B parameters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] KubeIntellect: A Modular LLM-Orchestrated Agent Framework for End-to-End
Kubernetes Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Kubernetes management, natural language interface, modular agents, code generation, workflow orchestration]</li>
<li class=""><strong>authors:</strong> Mohsen Seyedkazemi Ardebili, Andrea Bartolini</li>
<li class=""><strong>institution:</strong> University of Bologna</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02449v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02449v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KubeIntellect presents an LLM-powered framework that uses modular agents orchestrated by a supervisor to enable natural language interaction with Kubernetes APIs. The system integrates memory checkpoints, human-in-the-loop clarification, and dynamic task sequencing while generating tools via a secure Code Generator Agent. Evaluation shows 93% tool synthesis success rate and 100% reliability across 200 queries, demonstrating effective infrastructure management through interpretable, extensible LLM-driven systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] An Efficient and Adaptive Watermark Detection System with Tile-based
Error Correction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [watermark detection, error correction, GPU optimization, tiling techniques, Reed-Solomon codes, resource-aware scheduling]</li>
<li class=""><strong>authors:</strong> Xinrui Zhong, Xinze Feng, Jingwei Zuo, Fanjiang Ye, Yi Mu, Junfeng Guo, Heng Huang, Myungjin Lee, Yuke Wang</li>
<li class=""><strong>institution:</strong> Rice University, University of Illinois Urbana-Champaign, University of Maryland, Cisco Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02447v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02447v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> QRMark proposes an efficient watermark detection system combining QR code-inspired error correction with tiling techniques and GPU optimization strategies. It uses Reed-Solomon error correction to maintain accuracy while implementing resource-aware stream allocation and workload interleaving. The system achieves 2.43× speedup over sequential baselines while preserving detection robustness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficient Pyramidal Analysis of Gigapixel Images on a Decentralized
Modest Computer Cluster</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [gigapixel image analysis, pyramidal processing, load balancing, decentralized computing, performance optimization]</li>
<li class=""><strong>authors:</strong> Marie Reinbigler, Rishi Sharma, Rafael Pires, Elisabeth Brunet, Anne-Marie Kermarrec, Catalin Fetita</li>
<li class=""><strong>institution:</strong> SAMOVAR, Inria Saclay, Télécom SudParis, IP Paris, EPFL</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02440v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02440v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PyramidAI introduces a pyramidal approach for gigapixel image analysis that starts with low-resolution processing and progressively focuses on regions of interest at higher resolutions. The method reduces processed data by up to 2.65× while maintaining accuracy on single computers. When deployed on a decentralized cluster of 12 modest computers, it cuts analysis time from over an hour to just minutes, demonstrating practical efficiency for large-scale image analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Batch Query Processing and Optimization for Agentic Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [batch query processing, agentic workflows, optimization, KV-cache sharing, GPU utilization]</li>
<li class=""><strong>authors:</strong> Junyi Shen, Noppanat Wadlom, Yao Lu</li>
<li class=""><strong>institution:</strong> National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02121v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02121v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Halo introduces batch query processing and optimization for agentic LLM workflows by representing workflows as structured query plan DAGs and performing plan-level optimization to minimize redundant execution. The system achieves up to 18.6x speedup for batch inference and 4.7x throughput improvement in online serving while maintaining output quality. It enables efficient agentic workflows through adaptive batching, KV-cache sharing, and compute-communication overlap.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Optimal Parallel Scheduling under Concave Speedup Functions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [parallel computing, resource allocation, concave speedup functions, cloud computing, edge computing, AI workloads]</li>
<li class=""><strong>authors:</strong> Chengzhang Li, Peizhong Ju, Atilla Eryilmaz, Ness Shroff</li>
<li class=""><strong>institution:</strong> The Ohio State University, University of Kentucky</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01811v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01811v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes the SmartFill algorithm based on a Consistent Derivative Ratio Rule and General Water-Filling method for optimal parallel job scheduling under general concave speedup functions. It demonstrates that SmartFill outperforms prior approaches like heSRPT by selectively allocating resources rather than distributing to all active jobs. The method provides closed-form solutions for regular functions and efficient computation for non-regular cases.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Continuous Energy Ising Machine Leveraging Difference-of-Convex
Programming</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [Ising machine, combinatorial optimization, difference-of-convex programming, GPU computing, continuous relaxation]</li>
<li class=""><strong>authors:</strong> Debraj Banerjee, Santanu Mahapatra, Kunal Narayan Chaudhury</li>
<li class=""><strong>institution:</strong> Indian Institute of Science</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01928v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01928v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a continuous energy Ising machine that relaxes binary spins to continuous variables and uses difference-of-convex programming to efficiently solve combinatorial optimization problems. The method requires only single matrix-vector multiplication per iteration with convergence guarantees. Implementation across various GPU platforms demonstrates consistent outperformance over existing solvers from small to ultra-large problem sizes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] HiCR, an Abstract Model for Distributed Heterogeneous Programming</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [distributed heterogeneous systems, runtime systems, programming model, hardware abstraction, plugin architecture]</li>
<li class=""><strong>authors:</strong> Sergio Miguel Martin, Luca Terracciano, Kiril Dichev, Noah Baumann, Jiashu Lin, Albert-Jan Yzelman</li>
<li class=""><strong>institution:</strong> Huawei Zurich Research Center, HiSilicon Technologies</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01425v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01425v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HiCR presents an abstract model for distributed heterogeneous programming that defines minimal operations for hardware discovery, kernel execution, and memory management without implementation specifics. It employs a plugin-based approach to handle device-specific details while maintaining portability across platforms. The model enables applications to run on diverse systems without significant refactoring while supporting various parallel programming paradigms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] LongCat-Flash Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training, LLM inference], [Mixture-of-Experts, computational efficiency, agentic capabilities, scaling framework, open-source]</li>
<li class=""><strong>authors:</strong> Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han, Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen, Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong, Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu, Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan, Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang, Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang, Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu, Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, Liang Shi, Lianhui Ma, Lin Qiu, Lingbin Kong, Lingtong Si, Linkun Lyu, Linsen Guo, Liqi Yang, Lizhi Yan, Mai Xia, Man Gao, Manyuan Zhang, Meng Zhou, Mengxia Shen, Mingxiang Tuo, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pingwei Sun, Qi Gu, Qianyun Li, Qingyuan Li, Qiong Huang, Qiyuan Duan, Ran Meng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shizhe Wu, Shuai Liang, Shuo Wang, Suogui Dang, Tao Fang, Tao Li, Tefeng Chen, Tianhao Bai, Tianhao Zhou, Tingwen Xie, Wei He, Wei Huang, Wei Liu, Wei Shi, Wei Wang, Wei Wu, Weikang Zhao, Wen Zan, Wenjie Shi, Xi Nan, Xi Su, Xiang Li, Xiang Mei, Xiangyang Ji, Xiangyu Xi, Xiangzhou Huang, Xianpeng Li, Xiao Fu, Xiao Liu, Xiao Wei, Xiaodong Cai, Xiaolong Chen, Xiaoqing Liu, Xiaotong Li, Xiaowei Shi, Xiaoyu Li, Xili Wang, Xin Chen, Xing Hu, Xingyu Miao, Xinyan He, Xuemiao Zhang, Xueyuan Hao, Xuezhi Cao, Xunliang Cai, Xurui Yang, Yan Feng, Yang Bai, Yang Chen, Yang Yang, Yaqi Huo, Yerui Sun, Yifan Lu, Yifan Zhang, Yipeng Zang, Yitao Zhai, Yiyang Li, Yongjing Yin, Yongkang Lv, Yongwei Zhou, Yu Yang, Yuchen Xie, Yueqing Sun, Yuewen Zheng, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunfang Tai, Yunke Zhao, Zeyang Yu, Zhao Zhang, Zhaohua Yang, Zhenchao Zhang, Zhikang Xia, Zhiye Zou, Zhizhao Zeng, Zhongda Su, Zhuofan Chen, Zijian Zhang, Ziwen Wang, Zixu Jiang, Zizhe Zhao, Zongyu Wang, Zunhai Su</li>
<li class=""><strong>institution:</strong> Meituan</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01322v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01322v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LongCat-Flash introduces a 560B parameter MoE model with Zero-computation Experts and Shortcut-connected MoE for dynamic computation allocation and improved inference efficiency. It achieves training on 20T+ tokens in 30 days and competitive performance, especially in agentic tasks. The model is open-sourced to advance community research.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] LiquidGEMM: Hardware-Efficient W4A8 GEMM Kernel for High-Performance LLM
Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [quantization, GEMM kernel, LLM inference, W4A8, hardware efficiency]</li>
<li class=""><strong>authors:</strong> Huanqi Hu, Bowen Xiao, Shixuan Sun, Jianian Yin, Zhexi Zhang, Xiang Luo, Chengquan Jiang, Weiqi Xu, Xiaoying Jia, Xin Liu, Minyi Guo</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, ByteDance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01229v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01229v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LiquidGEMM introduces a hardware-efficient W4A8 GEMM kernel with LiquidQuant for fast dequantization and an implicit fine-grained pipeline that overlaps operations. It achieves up to 2.90x speedup over state-of-the-art W4A8 kernels and up to 4.94x system-level speedup, enhancing LLM serving performance significantly.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] LobRA: Multi-tenant Fine-tuning over Heterogeneous Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [finetuning], [multi-tenant fine-tuning, LoRA adapters, heterogeneous data, sequence length variation, workload balance]</li>
<li class=""><strong>authors:</strong> Sheng Lin, Fangcheng Fu, Haoyang Li, Hao Ge, Xuanyu Wang, Jiawen Niu, Yaofeng Tu, Bin Cui</li>
<li class=""><strong>institution:</strong> Peking University, Shanghai Jiao Tong University, ZTE Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01193v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01193v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LobRA introduces a framework for multi-tenant fine-tuning using heterogeneous FT replicas and workload-aware data dispatch to handle sequence length variation and skewness. It significantly reduces GPU time by 45.03%-60.67% for joint fine-tuning tasks. The approach optimizes resource usage and parallel configurations to improve efficiency in processing multiple fine-tuning requests.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World
Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [speculative decoding, dynamic adaptation, KLD stability, large-batch serving, latency optimization]</li>
<li class=""><strong>authors:</strong> Mingyu Yang, Jae-Young Choi, Kihyo Moon, Minsung Jang, Eunjoo Jeon</li>
<li class=""><strong>institution:</strong> Samsung SDS</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01083v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01083v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DSDE introduces a training-free framework that uses Kullback-Leibler divergence variance as a stability signal to dynamically adjust speculation length during LLM inference. This approach achieves competitive latency and superior robustness across diverse workloads, especially in low-acceptance-rate scenarios. The method validates post-hoc signals as valuable components for building more robust LLM inference systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Accelerating Latency-Critical Applications with AI-Powered
Semi-Automatic Fine-Grained Parallelization on SMT Processors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [parallel computing, SMT processors, AI-powered advisor, fine-grained parallelization, latency-critical applications]</li>
<li class=""><strong>authors:</strong> Denis Los, Igor Petushkov</li>
<li class=""><strong>institution:</strong> Moscow Institute of Physics and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.00883v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.00883v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Aira, an AI-powered parallelization adviser that uses LLMs to automate fine-grained parallelization of latency-critical applications on SMT processors. The system extends AI coding agents with tools for hotspot detection, dependency analysis, and performance simulation. Experimental results show 17% geomean performance gain when applying Aira with the Relic parallel framework to real-world benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] HADIS: Hybrid Adaptive Diffusion Model Serving for Efficient
Text-to-Image Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [diffusion models, model serving, query routing, resource allocation, cascade configuration]</li>
<li class=""><strong>authors:</strong> Qizheng Yang, Tung-I Chen, Siyu Zhao, Ramesh K. Sitaraman, Hui Guan</li>
<li class=""><strong>institution:</strong> University of Massachusetts Amherst</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.00642v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.00642v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HADIS introduces a hybrid adaptive diffusion model serving system that optimizes cascade model selection, query routing, and resource allocation using rule-based prompt routing and offline profiling. It improves response quality by up to 35% and reduces latency violation rates by 2.7-45× compared to state-of-the-art systems through efficient handling of complex queries and dynamic resource management.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for
KV Cache</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [KV cache compression, memory optimization, attention mechanism, lossy compression]</li>
<li class=""><strong>authors:</strong> Bo Jiang, Taolue Yang, Youyuan Liu, Chengming Zhang, Xubin He, Sian Jin</li>
<li class=""><strong>institution:</strong> Temple University, University of Houston</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.00579v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.00579v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KVComp introduces a lossy compression framework specifically designed for KV cache in transformer-based LLMs, employing novel compression algorithms and system architecture co-design. The method achieves 47-83% higher memory reduction than existing approaches with minimal accuracy degradation. It also demonstrates high execution throughput and can outperform cuBLAS-based attention kernels through reduced data movement.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] COMET: A Framework for Modeling Compound Operation Dataflows with
Explicit Collectives</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [dataflow optimization, compound operations, collective communication, machine learning accelerators, performance modeling]</li>
<li class=""><strong>authors:</strong> Shubham Negi, Manik Singhal, Aayush Ankit, Sudeep Bhoja, Kaushik Roy</li>
<li class=""><strong>institution:</strong> Purdue University, d-Matrix, Meta</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.00599v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.00599v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> COMET introduces a novel framework for modeling and optimizing dataflows of compound operations in ML accelerators, explicitly accounting for collective communication across spatial clusters. It provides latency and energy cost models that handle both GEMM and non-GEMM operations within compound structures. The approach achieves significant speedups (up to 3.46×) for operations like GEMM-LayerNorm and self-attention compared to unfused baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [collective communication, NVLink optimization, multi-GPU communication, bandwidth improvement, load balancing]</li>
<li class=""><strong>authors:</strong> Ao Shen, Rui Zhang, Junping Zhao</li>
<li class=""><strong>institution:</strong> Ant Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15882v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15882v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlexLink proposes a collective communication framework that aggregates heterogeneous interconnects (NVLink, PCIe, RDMA NICs) using a two-stage adaptive load balancing strategy. This approach dynamically partitions communication traffic across available links to prevent faster interconnects from being throttled by slower ones. The system achieves up to 27% bandwidth improvement for collective operators like AllReduce and AllGather while maintaining lossless compatibility with NCCL APIs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Odyssey: Adaptive Policy Selection for Resilient Distributed Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [fault tolerance, distributed training, performance optimization, adaptive policy selection]</li>
<li class=""><strong>authors:</strong> Yuhang Zhou, Zhibin Wang, Peng Jiang, Haoran Xia, Junhe Lu, Qianyu Jiang, Rong Gu, Hengxi Xu, Xinjing Huang, Guanghuan Fang, Zhiheng Hu, Jingyi Zhang, Yongjin Cai, Jian He, Chen Tian</li>
<li class=""><strong>institution:</strong> Nanjing University, Huawei</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.21613v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.21613v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Odyssey proposes an adaptive fault-tolerant system that intelligently selects optimal recovery strategies through unified performance modeling and execution plan search. Experimental results show it maintains within 11.00% performance gap between post-recovery and failure-free training while achieving up to 1.355x higher throughput compared to state-of-the-art methods. The system preserves model convergence and efficient memory usage during distributed training of large language models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] ReLATE: Learning Efficient Sparse Encoding for High-Performance Tensor
Decomposition</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [sparse tensor decomposition, reinforcement learning, tensor encoding optimization, performance optimization]</li>
<li class=""><strong>authors:</strong> Ahmed E. Helal, Fabio Checconi, Jan Laukemann, Yongseok Soh, Jesmin Jahan Tithi, Fabrizio Petrini, Jee Choi</li>
<li class=""><strong>institution:</strong> Intel Corporation, University of Erlangen-Nürnberg, University of Oregon</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.00280v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.00280v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ReLATE introduces a reinforcement learning framework that autonomously discovers efficient sparse tensor encodings through interaction with the tensor decomposition environment. It uses hybrid model-free and model-based learning with action masking mechanisms to ensure correct encoding. The method consistently outperforms expert-designed formats, achieving up to 2× speedup with geometric-mean improvements of 1.4-1.46× across diverse datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] An Optimistic Gradient Tracking Method for Distributed Minimax
Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed optimization, minimax optimization, gradient tracking, consensus protocol, convergence analysis]</li>
<li class=""><strong>authors:</strong> Yan Huang, Jinming Xu, Jiming Chen, Karl Henrik Johansson</li>
<li class=""><strong>institution:</strong> KTH Royal Institute of Technology, Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.21431v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.21431v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DOGT, a distributed optimistic gradient tracking method for minimax optimization that approximates centralized optimistic approaches locally. It achieves linear convergence for strongly convex-strongly concave functions and remains robust to function heterogeneity. With accelerated consensus, ADOGT achieves optimal convergence rates and communication complexity, validated through numerical experiments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency
with Speculative Decoding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [mixture-of-experts, speculative decoding, offloading, CPU-GPU orchestration, attention kernel, throughput optimization]</li>
<li class=""><strong>authors:</strong> Zhibin Wang, Zhonghui Zhang, Yuhang Zhou, Zibo Wang, Mo Zhou, Peng Jiang, Weilin Cai, Chengying Huan, Rong Gu, Sheng Zhong, Chen Tian</li>
<li class=""><strong>institution:</strong> Nanjing University, The Hong Kong University of Science and Technology (Guangzhou)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.21706v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.21706v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SpecMoEOff, a method that uses speculative decoding to hide offloading latency and increase expert workload in Mixture-of-Experts inference. It develops CPU chunked attention kernels and automatic hyperparameter tuning to optimize CPU-GPU orchestration. Experimental results show up to 2.5x throughput improvement over state-of-the-art MoE offloading techniques.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Learning to Shard: RL for Co-optimizing the Parallelism Degrees and
Per-operator Sharding Dimensions in Distributed LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [distributed inference, parallelism optimization, reinforcement learning, sharding dimensions, NPU clusters]</li>
<li class=""><strong>authors:</strong> Ruokai Yin, Sattwik Deb Mishra, Xuan Zuo, Hokchhay Tann, Preyas Shah, Apala Guha</li>
<li class=""><strong>institution:</strong> Yale University, Microsoft Azure</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.00217v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.00217v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Learn to Shard, an RL-based approach that co-optimizes both parallelism degrees and per-operator sharding dimensions for distributed LLM inference. The method uses an attention-based policy over elite strategies to efficiently explore the large search space. Evaluations on H100 clusters with MoE models up to 1.6T parameters show throughput improvements of up to 3.5x over metaheuristic baselines and 1.06x over Megatron heuristics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Fast and Scalable Mixed Precision Euclidean Distance Calculations Using
GPU Tensor Cores</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU computing, tensor cores, mixed precision, Euclidean distance, similarity search]</li>
<li class=""><strong>authors:</strong> Brian Curless, Michael Gowanlock</li>
<li class=""><strong>institution:</strong> Northern Arizona University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.21230v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.21230v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FaSTED, a fast Euclidean distance algorithm using GPU tensor cores with mixed precision (FP16-32) computation. It achieves 2.5-51× speedup over state-of-the-art methods while maintaining accuracy loss below 0.06% compared to FP64 baseline. The algorithm enables efficient similarity searches through hierarchical data reuse and optimized memory utilization across global memory, shared memory, and registers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] On the Optimization of Methods for Establishing Well-Connected
Communities</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [community detection, parallel algorithms, graph analytics, high-performance computing, connectivity optimization]</li>
<li class=""><strong>authors:</strong> Mohammad Dindoost, Oliver Alvarado Rodriguez, Bartosz Bryg, Minhyuk Park, George Chacko, Tandy Warnow, David A. Bader</li>
<li class=""><strong>institution:</strong> New Jersey Institute of Technology, University of Illinois Urbana-Champaign</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02590v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02590v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents optimized parallel implementations of Well-Connected Clusters and Connectivity Modifier algorithms using the HPE Chapel programming language. The implementations achieve substantial performance improvements and scalability on modern multicore architectures, enabling community detection on massive graphs with over 2 billion edges. The software integration into Arkouda/Arachne framework provides practical connectivity-preserving clustering solutions at web scale, demonstrated by processing the Open-Alex dataset in minutes using 128 cores.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] TinyServe: Query-Aware Cache Selection for Efficient LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [KV cache optimization, sparse attention, query-aware selection, CUDA kernels, memory efficiency]</li>
<li class=""><strong>authors:</strong> Dong Liu, Yanxuan Yu</li>
<li class=""><strong>institution:</strong> Yale University, Columbia University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12211v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12211v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TinyServe introduces a query-aware page selection mechanism that uses bounding-box metadata to selectively load relevant KV cache blocks, reducing memory access overhead. The system integrates page scoring, sparse memory access, and masked attention in a fused CUDA kernel. Experiments show up to 3.4x speedup and over 2x memory savings with negligible accuracy loss, demonstrating efficient LLM serving on resource-constrained hardware.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Collaborative Evolution of Intelligent Agents in Large-Scale
Microservice Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [multi-agent systems, microservice architecture, service optimization, graph representation learning, evolutionary game theory]</li>
<li class=""><strong>authors:</strong> Yilin Li, Song Han, Sibo Wang, Ming Wang, Renzi Meng</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, Northeastern University, Rice University, Trine University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.20508v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.20508v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a multi-agent collaborative evolution mechanism for intelligent service optimization in large-scale microservice systems, using graph representation learning and evolutionary game-driven policy optimization. The method enables agents to perceive system structural changes and dynamically adjust strategies through selection-mutation processes. Experimental results demonstrate improved governance efficiency and operational stability compared to existing approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU kernel optimization, hardware-aware LLMs, performance engineering, swizzling patterns, memory access patterns]</li>
<li class=""><strong>authors:</strong> Arya Tschand, Muhammad Awad, Ryan Swann, Kesavan Ramakrishnan, Jeffrey Ma, Keith Lowery, Ganesh Dasika, Vijay Janapa Reddi</li>
<li class=""><strong>institution:</strong> Harvard University, AMD, Stanford University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.20258v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.20258v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SwizzlePerf introduces hardware-aware LLMs that automatically generate spatial optimizations for GPU kernels by leveraging memory access patterns and architecture specifications. The system achieves up to 2.06x speedup and 70% L2 hit rate improvement on diverse ML and science kernels. It reduces optimization time from weeks to minutes compared to manual expert engineering.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Predictable LLM Serving on GPU Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [GPU multi-tenancy, PCIe-aware placement, dynamic MIG, tail latency optimization, SLO compliance]</li>
<li class=""><strong>authors:</strong> Erfan Darzi, Shreeanant Bharadwaj, Sree Bhargavi Balija</li>
<li class=""><strong>institution:</strong> Harvard University, MIT, Northeastern University, University of California San Diego</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.20274v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.20274v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a host-level controller that combines dynamic MIG reconfiguration, PCIe-aware placement, and lightweight guardrails to mitigate noisy-neighbor interference in GPU clusters. It reduces SLO miss-rate by ≈32% and improves p99 latency by ≈15% with minimal throughput cost. The approach demonstrates effectiveness for LLM serving with vLLM, improving TTFT p99 by 10-15% without controller modifications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] New Tools, Programming Models, and System Support for
Processing-in-Memory Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [memory], [processing-in-memory, DRAM architecture, hardware/software co-design, programming models, system optimization]</li>
<li class=""><strong>authors:</strong> Geraldo F. Oliveira</li>
<li class=""><strong>institution:</strong> ETH Zürich</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.19868v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.19868v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This dissertation introduces four major contributions for processing-in-memory architectures: DAMOV methodology for characterizing data movement bottlenecks, MIMDRAM for flexible resource allocation in DRAM, Proteus for reducing latency of bulk bitwise operations, and DaPPA programming framework for easier programmability. These tools and frameworks collectively address key challenges in PIM adoption by improving efficiency, flexibility, and programmability of DRAM-based processing-in-memory systems. The work demonstrates significant advancements in hardware/software co-design for memory-centric computing architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] IsoSched: Preemptive Tile Cascaded Scheduling of Multi-DNN via Subgraph
Isomorphism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [DNN scheduling, preemptive scheduling, tile spatial scheduling, subgraph isomorphism, multi-DNN execution]</li>
<li class=""><strong>authors:</strong> Boran Zhao, Zihang Yuan, Yanbin Hu, Haiming Zhai, Haoruo Zhang, Wenzhe Zhao, Tian Xia, Pengju Ren</li>
<li class=""><strong>institution:</strong> Xi&#x27;an Jiaotong University, Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.12208v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.12208v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> IsoSched introduces a preemptive scheduling framework for multi-DNN workloads using tile spatial scheduling to reduce DRAM overhead. It formulates scheduling as an integer-linear program and subgraph isomorphism problem, employing Monte Carlo Tree Search and compressed matrix encoding for efficiency. The framework outperforms existing approaches in latency-bound throughput, speedup, and energy efficiency while achieving higher critical task satisfaction rates.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Taming the Chaos: Coordinated Autoscaling for Heterogeneous and
Disaggregated LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [autoscaling, heterogeneous hardware, disaggregated architecture, GPU utilization, service level objectives]</li>
<li class=""><strong>authors:</strong> Rongzhi Li, Ruogu Du, Zefang Chu, Sida Zhao, Chunlei Han, Zuocheng Shi, Yiwen Shao, Huanle Han, Long Huang, Zherui Liu, Shufan Liu</li>
<li class=""><strong>institution:</strong> ByteDance, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.19559v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.19559v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HeteroScale introduces a coordinated autoscaling framework with topology-aware scheduling and metric-driven policies for LLM inference in disaggregated architectures. It uses a single robust metric to jointly scale prefill and decode pools, maintaining architectural balance. The system significantly improves GPU utilization by 26.6 percentage points and saves hundreds of thousands of GPU-hours daily while meeting service objectives.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Aegis: Taxonomy and Optimizations for Overcoming Agent-Environment
Failures in LLM Agents</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [LLM agents, environment optimization, failure taxonomy, agent-environment interaction, system optimization]</li>
<li class=""><strong>authors:</strong> Kevin Song, Anand Jayarajan, Yaoyao Ding, Qidong Su, Zhanda Zhu, Sihang Liu, Gennady Pekhimenko</li>
<li class=""><strong>institution:</strong> University of Toronto, Vector Institute, University of Waterloo</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.19504v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.19504v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Aegis, a set of environment optimizations including observability enhancement, computation offloading, and speculative actions to address agent-environment interaction failures. By analyzing 142 agent traces across 5 benchmarks, the authors identify 6 failure modes and demonstrate that environment optimizations can improve agent success rates by 6.7-12.5% without modifying the underlying LLM. The work shows that optimizing the system environment is a complementary approach to improving agent reliability beyond just enhancing the agent models themselves.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts
Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Mixture-of-Experts, hybrid parallelism, adaptive optimization, Integer Linear Programming, inference efficiency]</li>
<li class=""><strong>authors:</strong> Haoran Lin, Xianzhi Yu, Kang Zhao, Han Bao, Zongyuan Zhan, Ting Hu, Wulong Liu, Zekun Yin, Xin Li, Weiguo Liu</li>
<li class=""><strong>institution:</strong> Shandong University, Huawei Noah&#x27;s Ark Lab</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.19373v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.19373v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HAP proposes a hybrid adaptive parallelism method that dynamically selects optimal parallel configurations for Mixture-of-Experts models by decomposing architectures into Attention and Expert modules and using Integer Linear Programming. The approach achieves significant speedups (1.57-1.77x) over traditional tensor parallelism across multiple GPU platforms while maintaining effectiveness across diverse MoE model configurations. This demonstrates HAP&#x27;s ability to adaptively optimize inference performance under varying computational constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] CARMA: Collocation-Aware Resource Manager with GPU Memory Estimator</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU resource management, deep learning training, task collocation, memory estimation, energy efficiency]</li>
<li class=""><strong>authors:</strong> Ehsan Yousefzadeh-Asl-Miandoab, Reza Karimzadeh, Bulat Ibragimov, Florina M. Ciorba, Pınar Tözün</li>
<li class=""><strong>institution:</strong> IT University of Copenhagen, University of Copenhagen, University of Basel</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.19073v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.19073v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CARMA introduces a collocation-aware GPU resource management system with GPUMemNet, an ML-based memory estimator, to prevent out-of-memory errors and minimize interference. It includes policies to cap GPU utilization and a recovery method for crashed tasks. Evaluation shows CARMA improves GPU utilization by 39.3%, reduces execution time by ~26.7%, and cuts energy use by ~14.2%.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via
Cluster-Level Collective Primitive</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [operator fusion, cluster-level communication, GPU optimization, collective primitives]</li>
<li class=""><strong>authors:</strong> Xinhao Luo, Zihan Liu, Yangjie Zhou, Shihan Fang, Ziyu Huang, Yu Feng, Chen Zhang, Shixuan Sun, Zhenzhe Zheng, Jingwen Leng, Minyi Guo</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Shanghai Qi Zhi Institute, National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.18850v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.18850v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces ClusterFusion, a framework that uses cluster-level communication primitives (ClusterReduce and ClusterGather) to expand operator fusion scope for LLM inference. By scheduling communication and computation jointly, it composes multiple decoding stages into single fused kernels. Evaluations show ClusterFusion achieves 1.61x speedup in end-to-end latency compared to state-of-the-art frameworks on H100 GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, speculative decoding, scheduling optimization, GPU utilization, rollout efficiency]</li>
<li class=""><strong>authors:</strong> Jingkai He, Tianjian Li, Erhu Feng, Dong Du, Qian Liu, Tao Liu, Yubin Xia, Haibo Chen</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, ByteDance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.18588v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.18588v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RhymeRL introduces two key innovations - HistoSpec for speculative decoding using historical rollout sequences and HistoPipe for workload balancing using historical distributions. The system achieves 2.6x performance improvement over existing methods while maintaining accuracy and RL paradigm integrity. It demonstrates scalability from dozens to thousands of GPUs in production environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Strata: Hierarchical Context Caching for Long Context Language Model
Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [hierarchical caching, KV cache optimization, GPU-assisted I/O, cache-aware scheduling]</li>
<li class=""><strong>authors:</strong> Zhiqiang Xie, Ziyi Xu, Mark Zhao, Yuwei An, Vikram Sharma Mailthody, Scott Mahlke, Michael Garland, Christos Kozyrakis</li>
<li class=""><strong>institution:</strong> Stanford University, NVIDIA, Shanghai Jiao Tong University, University of Colorado Boulder, Carnegie Mellon University, University of Michigan</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.18572v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.18572v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Strata introduces a hierarchical context caching framework with GPU-assisted I/O to address KV cache fragmentation and cache-aware scheduling to balance compute with I/O latency. It achieves up to 5x lower Time-To-First-Token and 3.75x speedup over existing systems on long-context benchmarks while maintaining short-context performance. The system is built on SGLang and has been deployed in production environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Managing Multi Instance GPUs for High Throughput and Energy Savings</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU partitioning, dynamic scheduling, memory estimation, energy efficiency, throughput optimization]</li>
<li class=""><strong>authors:</strong> Abhijeet Saraha, Yuanbo Li, Chris Porter, Santosh Pande</li>
<li class=""><strong>institution:</strong> Georgia Institute of Technology, IBM Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.18556v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.18556v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops dynamic partitioning and scheduling schemes for Multi-Instance GPUs using techniques like memory estimation, partition fusion/fission, and process restart. The approach achieves significant throughput improvements (up to 6.20x for general workloads, 1.59x for ML workloads) and energy savings (up to 5.93x for general workloads, 1.12x for ML workloads) on A100 GPUs, including notable gains for LLM inference workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FSA: An Alternative Efficient Implementation of Native Sparse Attention
Kernel</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [sparse attention, GPU optimization, large language models, computational efficiency, kernel implementation]</li>
<li class=""><strong>authors:</strong> Ran Yan, Youhe Jiang, Zhuoming Chen, Haohui Mai, Beidi Chen, Binhang Yuan</li>
<li class=""><strong>institution:</strong> HKUST, Carnegie Mellon University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.18224v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.18224v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Flash Sparse Attention (FSA), an alternative kernel implementation that enables efficient Native Sparse Attention computation for LLMs with varied numbers of query heads in GQA groups. FSA achieves significant performance improvements over vanilla NSA implementation, including up to 3.5x kernel-level latency reduction and substantial speedups in both training and inference phases while maintaining comparable accuracy to full attention.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Experiences with Model Context Protocol Servers for Science and High
Performance Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [Model Context Protocol, scientific workflows, LLM-powered agents, research cyberinfrastructure, distributed computing]</li>
<li class=""><strong>authors:</strong> Haochen Pan, Ryan Chard, Reid Mello, Christopher Grams, Tanjin He, Alexander Brace, Owen Price Skelly, Will Engler, Hayden Holbrook, Song Young Oh, Maxime Gonthier, Michael Papka, Ben Blaiszik, Kyle Chard, Ian Foster</li>
<li class=""><strong>institution:</strong> University of Chicago, Argonne National Laboratory, University of Illinois Chicago</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.18489v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.18489v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes using Model Context Protocol (MCP) as a unifying interface to enable LLM-powered agents to discover and orchestrate scientific workflows across heterogeneous research infrastructure. They implement thin MCP servers over existing services like Globus, computing facilities, and domain-specific tools. The approach demonstrates successful agent-led workflow coordination in computational chemistry, bioinformatics, and quantum chemistry while identifying challenges in evaluation and trust for autonomous scientific workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert
Partition and Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [Mixture of Experts, LLM inference, expert partitioning, computation dropping, neuron-level reconstruction, load balancing]</li>
<li class=""><strong>authors:</strong> Weilin Cai, Le Qin, Shwai He, Junwei Cui, Ang Li, Jiayi Huang</li>
<li class=""><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou), University of Maryland, College Park</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.18376v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.18376v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DualSparse-MoE introduces post-training expert partitioning to coordinate tensor-level and neuron-level sparsity in MoE models without retraining. The system combines dynamic computation dropping with static neuron reconstruction to improve inference efficiency. Experimental results show this approach achieves significant computational speedups with minimal accuracy loss (0.08%-0.28% degradation at 25% drop rate) and 1.41x speedup through load-imbalance aware expert parallelism.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Views: A Hardware-friendly Graph Database Model For Storing Semantic
Information</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [graph database, hardware acceleration, semantic processing, near-memory computing, data structure]</li>
<li class=""><strong>authors:</strong> Yanjun Yang, Adrian Wheeldon, Yihan Pan, Alex Serb</li>
<li class=""><strong>institution:</strong> University of Edinburgh, Literal Labs</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.18123v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.18123v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Views, a hardware-friendly graph database model with optimized data structures for efficient storage and retrieval. It demonstrates equivalence to traditional graph representations while enabling semantic reasoning and cognitive modeling. The approach addresses computational bottlenecks in graph databases through hardware-aware design.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Scalable Engine and the Performance of Different LLM Models in a SLURM
based HPC architecture</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [SLURM, HPC, containerized microservices, dynamic resource scheduling, concurrent requests, latency, throughput]</li>
<li class=""><strong>authors:</strong> Anderson de Lima Luiz, Shubham Vijay Kurlekar, Munir Georges</li>
<li class=""><strong>institution:</strong> Technische Hochschule Ingolstadt, AImotion Bavaria</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.17814v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.17814v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a SLURM-based HPC architecture for scalable LLM inference using dynamic resource scheduling and containerized microservices. Experiments with Llama models show small models handle 128 concurrent requests with sub-50ms latency, while larger models saturate at just 2 concurrent users with over 2-second latency. The architecture demonstrates minimal overhead and reliable scaling for both batch and interactive settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Practical GPU Choices for Earth Observation: ResNet-50 Training
Throughput on Integrated, Laptop, and Cloud Accelerators</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [GPU benchmarking, deep learning, remote sensing, ResNet-50, land use classification]</li>
<li class=""><strong>authors:</strong> Ritvik Chaturvedi</li>
<li class=""><strong>institution:</strong> University of Maryland</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.18206v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.18206v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements a containerized ResNet-50 pipeline for land use classification using Sentinel-2 imagery, benchmarking training performance across three GPU types. The study shows up to 2x training speed improvements on NVIDIA RTX 3060 and Tesla T4 GPUs compared to Apple M3 Pro while maintaining high accuracy. Results demonstrate the feasibility of using consumer and free cloud GPUs for scalable geospatial deep learning applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters
at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Mixture-of-Experts, adapter serving, memory optimization, expert routing, fine-tuning]</li>
<li class=""><strong>authors:</strong> Ge Shi, Hanieh Sadri, Qian Wang, Yu Zhang, Ying Xiong, Yong Zhang, Zhenan Fan</li>
<li class=""><strong>institution:</strong> Huawei Technologies Canada, Huawei Cloud</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.17624v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.17624v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ExpertWeave introduces a system for efficiently serving multiple expert-specialized fine-tuned adapters concurrently over a shared MoE base model. It uses a virtual-memory-assisted expert weight manager and fused kernel for batched rerouting to reduce memory footprint and improve throughput. Evaluations show it enables serving multiple adapters on a single accelerator with minimal latency overhead while maintaining model accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained
Elastic Long-Context LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [prefix caching, memory pooling, load balancing, cache management, long-context LLM]</li>
<li class=""><strong>authors:</strong> Bingyang Wu, Zili Zhang, Yinmin Zhong, Guanzhe Huang, Yibo Zhu, Xuanzhe Liu, Xin Jin</li>
<li class=""><strong>institution:</strong> Peking University, StepFun</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.17219v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.17219v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TokenLake proposes a unified segment-level prefix cache pool with a declarative cache interface and heavy-hitter-aware load balancing algorithm to improve cache efficiency. It achieves better cache load balance, deduplication, and defragmentation while minimizing communication overhead. Evaluations show TokenLake improves throughput by up to 2.6× and hit rate by 2.1× compared to state-of-the-art solutions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] PICO: Performance Insights for Collective Operations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [collective operations, performance benchmarking, HPC, AI training, MPI, NCCL]</li>
<li class=""><strong>authors:</strong> Saverio Pasqualoni, Lorenzo Piarulli, Daniele De Sensi</li>
<li class=""><strong>institution:</strong> Sapienza, University of Rome</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.16809v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.16809v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces PICO, a lightweight extensible framework for benchmarking collective operations in HPC and AI systems. It addresses limitations in existing tools by providing detailed profiling while ensuring reproducibility. The framework helps identify communication bottlenecks in large-scale distributed computing environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Neuromorphic Simulation of Drosophila Melanogaster Brain Connectome on
Loihi 2</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [neuromorphic computing, brain simulation, connectome mapping, Loihi 2, Drosophila melanogaster, biological neural networks]</li>
<li class=""><strong>authors:</strong> Felix Wang, Bradley H. Theilman, Fred Rothganger, William Severa, Craig M. Vineyard, James B. Aimone</li>
<li class=""><strong>institution:</strong> Sandia National Laboratories</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.16792v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.16792v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements the complete Drosophila melanogaster brain connectome with 140K neurons and 50M synapses on Intel&#x27;s Loihi 2 neuromorphic hardware, overcoming hardware constraints through novel mapping solutions. The neuromorphic simulation achieves orders-of-magnitude speedup compared to conventional hardware, with performance advantages increasing under sparse activity conditions. This demonstrates neuromorphic platforms&#x27; capability to accelerate biologically realistic brain simulations, enabling advances in neuro-inspired AI and computational neuroscience.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] On the Duality of Task and Actor Programming Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [task-based programming, actor-based programming, performance optimization, distributed systems, parallel computing]</li>
<li class=""><strong>authors:</strong> Rohan Yadav, Joseph Guman, Sean Treichler, Michael Garland, Alex Aiken, Fredrik Kjolstad, Michael Bauer</li>
<li class=""><strong>institution:</strong> Stanford University, NVIDIA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.16522v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.16522v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper demonstrates that task-based and actor-based programming models are duals of each other and develops optimization techniques that enable task-based systems to achieve performance competitive with actor-based systems. The techniques were applied to Realm and Legion task-based runtimes, reducing Realm&#x27;s overheads by 1.7-5.3x and improving Legion&#x27;s strong scaling by 1.3-5.0x. The work bridges the performance gap between task-based and actor-based systems while maintaining task-based programming&#x27;s productivity advantages.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Systematic Characterization of LLM Quantization: A Performance, Energy,
and Quality Perspective</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [quantization, performance-energy-quality tradeoffs, online characterization, GPU architectures, post-training quantization, distributed execution]</li>
<li class=""><strong>authors:</strong> Tianyao Shi, Yi Ding</li>
<li class=""><strong>institution:</strong> Purdue University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.16712v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.16712v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops qMeter, an automated framework for characterizing LLM quantization, and systematically evaluates 11 post-training quantization methods across different model sizes and GPU architectures. The study reveals complex tradeoffs between performance, energy, and quality that are highly dependent on tasks, methods, workload characteristics, and hardware. The findings highlight important interactions between quantization techniques and system-level factors like parallelism and GPU architecture under realistic serving conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across
Multiple Formats</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Infrastructure as Code, benchmark evaluation, multi-format IaC, LLM-based generation, cloud deployment, prompt engineering]</li>
<li class=""><strong>authors:</strong> Sam Davidson, Li Sun, Bhavana Bhasker, Laurent Callot, Anoop Deoras</li>
<li class=""><strong>institution:</strong> Amazon Web Services</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.05303v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.05303v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Multi-IaC-Bench, a benchmark dataset for evaluating LLMs in generating and modifying Infrastructure as Code across AWS CloudFormation, Terraform, and CDK formats. It demonstrates that while LLMs achieve high syntactic validity (&gt;95%), they face challenges in semantic alignment and complex patterns. The study emphasizes the importance of prompt engineering and retry mechanisms for successful IaC generation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI&#x27;s
Open-Weight Mixture of Experts Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Mixture-of-Experts, deployment efficiency, energy consumption, VRAM optimization, throughput analysis]</li>
<li class=""><strong>authors:</strong> Deepak Kumar, Divakar Yadav, Yash Patel</li>
<li class=""><strong>institution:</strong> Illinois Institute of Technology, University of Wisconsin-Milwaukee, Lawrence Technological University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.16700v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.16700v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study conducts a single-GPU deployment analysis comparing GPT-OSS-20B (a Mixture-of-Experts model) against dense baselines Qwen3-32B and Yi-34B. The evaluation measures time-to-first-token, decode throughput, VRAM usage, and energy consumption. Results show GPT-OSS-20B achieves higher decode throughput, lower energy consumption, and reduced VRAM usage despite higher TTFT, demonstrating MoE&#x27;s deployment advantages for efficient inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] ASIC-Agent: An Autonomous Multi-Agent System for ASIC Design with
Benchmark Evaluation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [ASIC design automation, multi-agent system, hardware design workflow, RTL generation, verification, OpenLane hardening, Caravel integration]</li>
<li class=""><strong>authors:</strong> Ahmed Allam, Youssef Mansour, Mohamed Shalan</li>
<li class=""><strong>institution:</strong> The American University in Cairo</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.15940v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.15940v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents ASIC-Agent, an autonomous multi-agent system that enhances LLMs with specialized sub-agents for RTL generation, verification, hardening and integration in ASIC design workflows. The system operates within a sandbox environment with hardware design tools and leverages a vector database containing documentation and community insights. Results show ASIC-Agent successfully automates various ASIC design tasks and significantly accelerates the design workflow when powered by Claude 4 Sonnet.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Mitigating context switching in densely packed Linux clusters with
Latency-Aware Group Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [scheduling], [Linux scheduler, context switching, cluster management, Kubernetes, serverless computing, cgroups]</li>
<li class=""><strong>authors:</strong> Al Amjad Tawfiq Isstaif, Evangelia Kalyvianaki, Richard Mortier</li>
<li class=""><strong>institution:</strong> University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.15703v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.15703v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CFS-Latency-Aware Group Scheduling (CFS-LAGS), a modified Linux kernel scheduler that reduces context switching overhead in densely packed clusters by prioritizing task completion over per-task fairness. The approach enables faster draining of contended CPU run queues, significantly decreasing context switching time. Evaluation shows the method achieves equivalent performance with 28% smaller cluster size compared to standard scheduling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO
Serving and Fast Scaling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multi-SLO scheduling, dynamic scaling, P/D disaggregated architecture, KV cache transfer, device-to-device weight transfer]</li>
<li class=""><strong>authors:</strong> Zahra Yousefijamarani, Xinglu Wang, Qian Wang, Morgan Lindsay Heisler, Taha Shabani, Niloofar Gholipour, Parham Yassini, Hong Chang, Kan Chen, Qiantao Zhang, Xiaolong Bai, Jiannan Wang, Ying Xiong, Yong Zhang, Zhenan Fan</li>
<li class=""><strong>institution:</strong> Huawei Technologies Canada Co., Ltd., Huawei Technologies Co., Ltd., Simon Fraser University, École de technologie supérieure</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.15919v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.15919v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HyperFlexis introduces a unified LLM serving system that jointly optimizes scheduling and scaling through multi-SLO-aware scheduling and device-to-device weight transfers. The system achieves up to 4.44× higher SLO attainment, 65.82% lower latency, and maintains cost parity with state-of-the-art baselines. These improvements address variable request demands and support both collocated and disaggregated prefill/decode architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficient Mixed-Precision Large Language Model Inference with TurboMind</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [mixed-precision inference, KV cache optimization, hardware-aware optimization, attention mechanism, GEMM optimization]</li>
<li class=""><strong>authors:</strong> Li Zhang, Youhe Jiang, Guoliang He, Xin Chen, Han Lv, Qian Yao, Fangcheng Fu, Kai Chen</li>
<li class=""><strong>institution:</strong> Shanghai AI Laboratory, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.15601v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.15601v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces TurboMind&#x27;s mixed-precision inference techniques featuring two novel pipelines: a GEMM pipeline with offline weight packing and online acceleration, and an attention pipeline supporting arbitrary precision combinations. The approach includes hardware-aware weight packing, adaptive head alignment, instruction-level parallelism, and KV memory loading optimization. Evaluations show up to 61% lower latency and 156% higher throughput compared to existing frameworks across diverse LLMs and GPU architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Declarative Data Pipeline for Large Scale ML Services</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [declarative data pipeline, Apache Spark, machine learning integration, modular framework, performance optimization]</li>
<li class=""><strong>authors:</strong> Yunzhao Yang, Runhui Wang, Xuanqing Liu, Adit Krishnan, Yefan Tao, Yuqian Deng, Kuangyou Yao, Peiyuan Sun, Henrik Johnson, Aditi sinha, Davor Golac, Gerald Friedland, Usman Shakeel, Daryl Cooke, Joe Sullivan, Chris Kong</li>
<li class=""><strong>institution:</strong> Amazon Web Services</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.15105v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.15105v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a declarative data pipeline architecture that integrates machine learning capabilities into Apache Spark using modular computation units called Pipes. The framework achieves significant improvements including 50% development efficiency gains, 500x scalability improvement, and 10x throughput enhancement while maintaining high CPU utilization. The approach demonstrates how clear component boundaries and standardized interfaces can balance system performance with maintainability in large-scale ML services.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] On the Effectiveness of Graph Reordering for Accelerating Approximate
Nearest Neighbor Search on GPU</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [graph reordering, memory optimization, GPU acceleration, approximate nearest neighbor search]</li>
<li class=""><strong>authors:</strong> Yutaro Oguri, Mai Nishimura, Yusuke Matsui</li>
<li class=""><strong>institution:</strong> The University of Tokyo, OMRON SINIC X Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.15436v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.15436v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a systematic framework for evaluating graph reordering strategies to optimize memory layouts for graph-based approximate nearest neighbor search on GPUs. The proposed method transforms scattered memory access patterns into sequential ones through node relocation, achieving up to 15% QPS improvement while maintaining search accuracy. The results demonstrate that memory layout optimization works orthogonally to existing algorithmic innovations in graph-based ANNS systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] TOAST: Fast and scalable auto-partitioning based on principled static
analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [auto-partitioning, distributed training, static analysis, Monte Carlo Tree Search, model parallelism, sharding strategy]</li>
<li class=""><strong>authors:</strong> Sami Alabed, Dominik Grewe, Norman Alexander Rink, Masha Samsikova, Timur Sitdikov, Agnieszka Swietlik, Dimitrios Vytiniotis, Daniel Belov</li>
<li class=""><strong>institution:</strong> Google DeepMind, Isomorphic Labs</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.15010v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.15010v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TOAST combines novel static compiler analysis with Monte Carlo Tree Search to efficiently explore partitioning strategies for large ML models. The system identifies tensor dimensions requiring identical sharding and resolves partitioning conflicts to construct an optimized decision space. It significantly outperforms existing auto-partitioners across diverse hardware and models while discovering superior solutions through fully automated processing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Leveraging Hardware-Aware Computation in Mixed-Precision Matrix
Multiply: A Tile-Centric Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [mixed-precision, matrix multiplication, hardware optimization, task-based runtime, high-performance computing]</li>
<li class=""><strong>authors:</strong> Qiao Zhang, Rabab Alomairy, Dali Wang, Zhuowei Gu, Qinglei Cao</li>
<li class=""><strong>institution:</strong> Saint Louis University, Massachusetts Institute of Technology, King Abdullah University of Science and Technology, Oak Ridge National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.14848v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.14848v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an adaptive mixed-precision GEMM framework using tile-level precision control and the PaRSEC runtime system for workload balancing. The method achieves scalable performance across multiple supercomputing architectures including Fugaku, A100 DGX, and Frontier. Results demonstrate improved computational efficiency and accuracy by bridging algorithmic innovations with hardware capabilities.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] MOHAF: A Multi-Objective Hierarchical Auction Framework for Scalable and
Fair Resource Allocation in IoT Ecosystems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [resource allocation, auction mechanisms, multi-objective optimization, IoT, distributed systems, hierarchical clustering]</li>
<li class=""><strong>authors:</strong> Kushagra Agrawal, Polat Goktas, Anjan Bandopadhyay, Debolina Ghosh, Junali Jasmine Jena, Mahendra Kumar Gourisaria</li>
<li class=""><strong>institution:</strong> KIIT Deemed to be University, University College Dublin, Manipal University Jaipur</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.14830v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.14830v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MOHAF proposes a distributed resource allocation framework using hierarchical clustering and greedy submodular optimization with dynamic pricing. It achieves superior allocation efficiency (0.263) and perfect fairness (Jain&#x27;s index = 1.000) compared to baseline methods. The framework demonstrates near-linear scalability and effectively balances cost, QoS, energy efficiency, and fairness in IoT ecosystems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] SSSP-Del: Fully Dynamic Distributed Algorithm for Single-Source Shortest
Path</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [dynamic graph algorithms, distributed computing, shortest path problem, vertex-centric processing, asynchronous algorithms]</li>
<li class=""><strong>authors:</strong> Parshan Javanrood, Matei Ripeanu</li>
<li class=""><strong>institution:</strong> University of British Columbia</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.14319v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.14319v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces SSSP-Del, a vertex-centric asynchronous distributed algorithm that maintains single-source shortest paths in dynamic graphs by processing streams of edge insertions and deletions. The algorithm operates in shared-nothing architecture and propagates distance changes efficiently across distributed systems. Experimental evaluation demonstrates its effectiveness in handling large-scale graphs while providing low-latency query results and maintaining solution stability under high-throughput topology updates.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Power Stabilization for AI Training Datacenters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [power stabilization, GPU clusters, datacenter infrastructure, power management, synchronous training, power swings]</li>
<li class=""><strong>authors:</strong> Esha Choukse, Brijesh Warrier, Scot Heath, Luz Belmont, April Zhao, Hassan Ali Khan, Brian Harry, Matthew Kappel, Russell J. Hewett, Kushal Datta, Yu Pei, Caroline Lichtenberger, John Siegler, David Lukofsky, Zaid Kahn, Gurpreet Sahota, Andy Sullivan, Charles Frederick, Hien Thai, Rebecca Naughton, Daniel Jurnove, Justin Harp, Reid Carper, Nithish Mahalingam, Srini Varkala, Alok Gautam Kumbhare, Satyajit Desai, Venkatesh Ramamurthy, Praneeth Gottumukkala, Girish Bhatia, Kelsey Wildstone, Laurentiu Olariu, Ileana Incorvaia, Alex Wetmore, Prabhat Ram, Melur Raghuraman, Mohammed Ayna, Mike Kendrick, Ricardo Bianchini, Aaron Hurst, Reza Zamani, Xin Li, Michael Petrov, Gene Oden, Rory Carmichael, Tom Li, Apoorv Gupta, Pratikkumar Patel, Nilesh Dattani, Lawrence Marwong, Rob Nertney, Hirofumi Kobayashi, Jeff Liott, Miro Enev, Divya Ramakrishnan, Ian Buck, Jonah Alben</li>
<li class=""><strong>institution:</strong> Microsoft, OpenAI, NVIDIA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.14318v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.14318v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper addresses power stabilization challenges in large-scale AI training datacenters by proposing a multi-pronged approach across software, GPU hardware, and infrastructure layers. The solutions are tested using real hardware and Microsoft&#x27;s cloud power simulator, demonstrating effective mitigation of power swings that could otherwise damage grid infrastructure. The work enables safe scaling of AI training workloads while maintaining grid stability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A High Performance GPU CountSketch Implementation and Its Application to
Multisketching and Least Squares Problems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU computing, CountSketch, multisketching, least squares solver, randomized linear algebra]</li>
<li class=""><strong>authors:</strong> Andrew J. Higgins, Erik G. Boman, Ichitaro Yamazaki</li>
<li class=""><strong>institution:</strong> Sandia National Laboratories</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.14209v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.14209v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops an efficient GPU implementation of CountSketch and applies it to multisketching by combining CountSketch with Gaussian sketches. The implementation demonstrates significant performance improvements, achieving up to 77% faster solving times for multisketched least squares problems compared to normal equations. The method maintains better numerical stability while introducing only an O(1) multiplicative factor in the relative residual norm.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint
Caching and Resource-Aware Graph Partitioning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [graph neural networks, distributed training, graph partitioning, caching, communication optimization, multi-GPU]</li>
<li class=""><strong>authors:</strong> Xianfeng Song, Yi Zou, Zheng Shi</li>
<li class=""><strong>institution:</strong> Based on author names (Xianfeng Song, Yi Zou, Zheng Shi), specific institutions cannot be determined from provided content. Email suffixes not available.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.13716v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.13716v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CaPGNN introduces a joint adaptive caching algorithm using CPU/GPU memory and resource-aware graph partitioning to optimize parallel GNN training. The framework reduces redundant inter-GPU communication and balances computational workloads across heterogeneous GPUs. Experiments show up to 96% communication reduction and 12.7× training acceleration compared to state-of-the-art methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Equinox: Holistic Fair Scheduling in Serving Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [fairness scheduling, LLM serving, resource allocation, prediction framework, GPU utilization]</li>
<li class=""><strong>authors:</strong> Zhixiang Wei, James Yen, Jingyi Chen, Ziyang Zhang, Zhibai Huang, Chen Chen, Xingzi Yu, Yicheng Gu, Chenggang Wu, Yun Wang, Mingyuan Xia, Jie Wu, Hao Wang, Zhengwei Qi</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, UltraRISC Shanghai, China Telecom, Stevens Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.16646v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.16646v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Equinox introduces a dual-counter fairness framework with predictive MoPE components to enable holistic scheduling for LLM serving. The system achieves improved throughput, reduced latency, and higher fairness while maintaining high GPU utilization. Evaluations demonstrate significant performance gains over existing approaches like VTC across production workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Optimizing Allreduce Operations for Heterogeneous Architectures with
Multiple Processes per GPU</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Allreduce optimization, GPU communication, heterogeneous architectures, multi-CPU acceleration, MPI, NCCL, RCCL]</li>
<li class=""><strong>authors:</strong> Michael Adams, Amanda Bienz</li>
<li class=""><strong>institution:</strong> University of New Mexico</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.13397v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.13397v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes optimizations for GPU-aware all-reduce operations by extending lane-aware reductions to GPUs and utilizing multiple CPU cores per GPU to accelerate communication. The approach achieves significant speedups of up to 2.45x for large MPI all-reduces on NVIDIA A100 GPUs. The optimization method also demonstrates improved performance on NVIDIA&#x27;s and AMD&#x27;s collective communication libraries across multiple supercomputers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts
Architectures on HPC Platforms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [Mixture-of-Experts, scalable training, cross-platform optimization, hybrid parallelism, HPC systems]</li>
<li class=""><strong>authors:</strong> Yueming Yuan, Ahan Gupta, Jianping Li, Sajal Dash, Feiyi Wang, Minjia Zhang</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign, Oak Ridge National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.13337v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.13337v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> X-MoE introduces a novel training system with padding-free MoE training, redundancy-bypassing dispatch, and hybrid parallelism to overcome scalability limitations in expert-specialized MoE architectures. The system achieves 10x larger model scaling (545B parameters) on AMD GPUs while maintaining high throughput, demonstrating superior performance on non-NVIDIA HPC platforms like Frontier.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Harnessing the Full Potential of RRAMs through Scalable and Distributed
In-Memory Computing with Integrated Error Correction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [in-memory computing, RRAM, error correction, distributed computing, matrix computation, energy efficiency]</li>
<li class=""><strong>authors:</strong> Huynh Q. N. Vo, Md Tawsif Rahman Chowdhury, Paritosh Ramanan, Murat Yildirim, Gozde Tutuncuoglu</li>
<li class=""><strong>institution:</strong> Oklahoma State University, Wayne State University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.13298v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.13298v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MELISO+ introduces a distributed in-memory computing framework with two-tier error correction for RRAM devices, enabling large-scale matrix computations. This approach reduces arithmetic errors by over 90% while improving energy efficiency by 3-5 orders of magnitude and reducing latency 100-fold. The system allows lower-precision RRAM devices to outperform high-precision alternatives in accuracy, energy, and latency metrics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] DIT: Dimension Reduction View on Optimal NFT Rarity Meters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [NFT rarity, dimension reduction, blockchain, performance metrics, unidimensional scaling]</li>
<li class=""><strong>authors:</strong> Dmitry Belousov, Yury Yanovich</li>
<li class=""><strong>institution:</strong> Moscow Institute of Physics and Technology, Skolkovo Institute of Science and Technology, HSE University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.12671v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.12671v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a dimension reduction approach for NFT rarity assessment using non-metric weighted unidimensional scaling. It introduces Dissimilarity in Trades (DIT) as a computationally efficient performance measure and develops a new rarity meter that outperforms existing methods on the ROAR benchmark. The method effectively captures NFT value relationships through trade dissimilarity analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficient GPU-Centered Singular Value Decomposition Using the
Divide-and-Conquer Method</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU computing, singular value decomposition, divide-and-conquer method, linear algebra, matrix factorization]</li>
<li class=""><strong>authors:</strong> Shifang Liu, Huiyuan Li, Hongjiao Sheng, Haoyuan Gui, Xiaoyu Zhang</li>
<li class=""><strong>institution:</strong> Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.11467v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.11467v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a GPU-centered SVD algorithm using a novel bidiagonal divide-and-conquer method that eliminates CPU-GPU data transfers by performing all computations on GPU. The approach optimizes BLAS utilization and enables asynchronous CPU-GPU execution. Experimental results show significant speedups up to 1293.64x compared to existing solvers on AMD and NVIDIA GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI
Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [I/O optimization, energy efficiency, distributed storage, GPU acceleration, data loading]</li>
<li class=""><strong>authors:</strong> Hasibul Jamil, MD S Q Zulkar Nine, Tevfik Kosar</li>
<li class=""><strong>institution:</strong> University at Buffalo (SUNY), Tennessee Technological University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.11035v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.11035v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EMLIO introduces a service-based I/O architecture with lightweight daemons on storage nodes that serialize, batch, and stream data using TCP with out-of-order prefetching, integrated with GPU-accelerated preprocessing. It achieves up to 8.6× faster I/O and 10.9× lower energy consumption compared to state-of-the-art loaders across various network environments. The system maintains consistent performance and energy profiles regardless of network distance, providing a scalable solution for energy-aware I/O in large-scale AI training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Mixed-Precision Performance Portability of FFT-Based GPU-Accelerated
Algorithms for Block-Triangular Toeplitz Matrices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [mixed-precision computing, performance portability, GPU acceleration, FFT algorithms, Toeplitz matrices, HPC workflows]</li>
<li class=""><strong>authors:</strong> Sreeram Venkat, Kasia Swirydowicz, Noah Wolfe, Omar Ghattas</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin, Advanced Micro Devices, Inc.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.10202v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.10202v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a performance-portable framework using hipify to enable CUDA-based FFTMatvec applications to run seamlessly on AMD GPUs. It introduces dynamic mixed-precision optimization with Pareto front analysis to determine optimal precision configurations for target error tolerances. The approach achieves excellent performance scaling to 4,096 GPUs on Frontier supercomputer while maintaining application code unchanged through rocBLAS library integrations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Verify Distributed Deep Learning Model Implementation Refinement with
Iterative Relation Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [distributed deep learning, model refinement, static verification, GraphGuard, iterative relation inference, bug detection]</li>
<li class=""><strong>authors:</strong> Zhanghan Wang, Ding Ding, Hang Zhu, Haibin Lin, Aurojit Panda</li>
<li class=""><strong>institution:</strong> ByteDance, New York University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.09505v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.09505v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents GraphGuard, a static verification approach using iterative relation inference to check refinement between sequential and distributed deep learning models. The method proves whether sequential model outputs can be reconstructed from distributed implementations and scales to large models like GPT and Llama-3. It provides actionable bug localization output and successfully identifies implementation bugs in distributed model deployments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication
and Expert Swap</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [MoE training, token deduplication, expert swap, communication optimization, load balancing]</li>
<li class=""><strong>authors:</strong> Wenxiang Lin, Xinglin Pan, Lin Zhang, Shaohuai Shi, Xuan Wang, Xiaowen Chu</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology, Shenzhen; The Hong Kong University of Science and Technology (Guangzhou); The Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.09591v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.09591v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HierMoE accelerates MoE model training through hierarchical token deduplication to reduce communication traffic and expert swap to balance GPU workloads. The system achieves 1.55× to 3.32× faster communication and 1.18× to 1.27× faster end-to-end training compared to state-of-the-art systems. These improvements are demonstrated on DeepSeek-V3 and Qwen3-30B-A3B models using a 32-GPU cluster.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] P/D-Device: Disaggregated Large Language Model between Cloud and Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [cloud-device collaboration, token generation optimization, prefill phase acceleration, throughput improvement, latency reduction]</li>
<li class=""><strong>authors:</strong> Yibo Jin, Yixu Xu, Yue Chen, Chengbin Wang, Tao Wang, Jiaqi Huang, Rongfei Zhang, Yiming Dong, Yuting Yan, Ke Cheng, Yingjie Zhu, Shulan Wang, Qianqian Tang, Shuaishuai Meng, Guanxin Cheng, Ze Wang, Shuyan Miao, Ketao Wang, Wen Liu, Yifan Yang, Tong Zhang, Anran Wang, Chengzhou Lu, Tiantian Dong, Yongsheng Zhang, Zhe Wang, Hefei Guo, Hongjie Liu, Wei Lu, Zhengyong Zhang</li>
<li class=""><strong>institution:</strong> Huawei Technologies Co., Ltd.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.09035v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.09035v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes P/D-Device, a disaggregated LLM serving scheme that splits computation between cloud and devices to optimize resource usage. By having the cloud assist with prefill phase processing and implementing a speed controller for token delivery, the method reduces TTFT by at least 60% and increases cloud throughput by up to 15x while maintaining smooth TPOT performance. Experimental results demonstrate significant improvements in both latency and throughput metrics compared to alternative approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Survey on Parallel Text Generation: From Parallel Decoding to
Diffusion Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [parallel text generation, autoregressive generation, non-autoregressive generation, diffusion models, inference acceleration]</li>
<li class=""><strong>authors:</strong> Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip S. Yu, Aiwei Liu</li>
<li class=""><strong>institution:</strong> Peking University, University of Illinois Chicago, Tsinghua University, XPENG, Alibaba Group, The Hong Kong University of Science and Technology (Guangzhou)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.08712v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.08712v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This survey systematically categorizes parallel text generation methods into autoregressive-based and non-autoregressive-based paradigms to overcome the sequential bottleneck in traditional LLM inference. It analyzes theoretical trade-offs between speed, quality, and efficiency across different techniques. The study concludes by identifying open challenges and future research directions for improving inference performance through parallel generation approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor
Search</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU acceleration, graph indexing, approximate nearest neighbor search, parallel processing, large-scale datasets]</li>
<li class=""><strong>authors:</strong> Zhonggen Li, Xiangyu Ke, Yifan Zhu, Bocheng Yu, Baihua Zheng, Yunjun Gao</li>
<li class=""><strong>institution:</strong> Zhejiang University, Singapore Management University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.08744v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.08744v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Tagore, a GPU-accelerated library for efficient graph indexing in approximate nearest neighbor search. It proposes GNN-Descent for k-NN graph initialization, a universal pruning procedure called CFS with GPU kernels, and an asynchronous GPU-CPU-disk framework for large datasets. Experiments show Tagore achieves 1.32x-112.79x speedup while maintaining index quality across real-world datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Cluster Topology-Driven Placement of Experts Reduces Network Traffic in
MoE Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [mixture of experts, model placement, network traffic reduction, integer linear programming, cluster topology]</li>
<li class=""><strong>authors:</strong> Danil Sivtsov, Aleksandr Katrutsa, Ivan Oseledets</li>
<li class=""><strong>institution:</strong> AIRI, Skoltech, Avito</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.09229v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.09229v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an integer linear programming (ILP) approach for topology-aware placement of Mixture-of-Experts (MoE) models in inference clusters. The method optimizes expert placement to minimize network transmissions by considering cluster topology as a weighted graph. Experimental results show the ILP-based strategy achieves lower network traffic compared to competitors for both small-scale (DeepSeekMoE 16B) and large-scale (DeepSeek-R1 671B) models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Reinforcement Learning-Driven Task Scheduling Algorithm for
Multi-Tenant Distributed Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [reinforcement learning, task scheduling, multi-tenant systems, Proximal Policy Optimization, resource management, fairness optimization]</li>
<li class=""><strong>authors:</strong> Xiaopei Zhang, Xingang Wang, Xin Wang</li>
<li class=""><strong>institution:</strong> University of California, Los Angeles, Institute of Automation Chinese Academy of Sciences, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.08525v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.08525v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a reinforcement learning-based task scheduling algorithm using Proximal Policy Optimization to optimize task latency, resource utilization, and tenant fairness in multi-tenant distributed systems. The method models scheduling as a Markov decision process and demonstrates superior performance across multiple metrics compared to existing approaches. Experimental results show the proposed framework achieves strong stability and generalization while improving scheduling efficiency under complex conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Towards Efficient and Practical GPU Multitasking in the Era of LLM</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU multitasking, resource management, performance isolation, temporal sharing, memory sharing]</li>
<li class=""><strong>authors:</strong> Jiarong Xing, Yifan Qiao, Simon Mo, Xingqi Cui, Gur-Eyal Sela, Yang Zhou, Joseph Gonzalez, Ion Stoica</li>
<li class=""><strong>institution:</strong> UC Berkeley, Rice University, UC Davis</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.08448v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.08448v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a GPU resource management layer analogous to CPU operating systems to enable efficient multitasking. It identifies limitations in current GPU sharing approaches and emphasizes the need for both temporal sharing and memory sharing. The authors argue this paradigm shift is crucial for improving GPU utilization in modern AI workloads like LLMs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Vector-Centric Machine Learning Systems: A Cross-Stack Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [retrieval-augmented generation, vector search, recommender systems, cross-stack optimization, hardware acceleration, system efficiency]</li>
<li class=""><strong>authors:</strong> Wenqi Jiang</li>
<li class=""><strong>institution:</strong> ETH Zurich</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.08469v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.08469v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This thesis proposes a cross-stack approach to optimize vector-centric machine learning systems like RAG and recommender systems. It introduces solutions spanning algorithms, systems, and hardware layers including PipeRAG, RAGO, Chameleon, FANNS, Falcon, MicroRec and FleetRec. The work demonstrates significant efficiency improvements across the computing stack by co-designing algorithms with underlying hardware architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Extremely Scalable Distributed Computation of Contour Trees via
Pre-Simplification</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [contour trees, distributed computation, topological data analysis, pre-simplification, memory optimization, scientific visualization]</li>
<li class=""><strong>authors:</strong> Mingzhe Li, Hamish Carr, Oliver Rübel, Bei Wang, Gunther H. Weber</li>
<li class=""><strong>institution:</strong> University of Utah, University of Leeds, Lawrence Berkeley National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.08433v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.08433v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a pre-simplification strategy to reduce memory overhead in distributed hierarchical contour tree computations. The method enables construction of extremely large contour trees with over half a trillion nodes. Results show the approach achieves enhanced scalability, building the largest known contour tree in under 15 minutes on a 550-billion-element dataset.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Coordinated Power Management on Heterogeneous Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [performance prediction, power management, heterogeneous systems, collaborative filtering, GPU computing]</li>
<li class=""><strong>authors:</strong> Zhong Zheng, Zhiling Lan, Xingfu Wu, Valerie E. Taylor, Michael E. Papka</li>
<li class=""><strong>institution:</strong> University of Illinois Chicago, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.07605v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.07605v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents OPEN, a framework combining offline and online phases for performance prediction in CPU-GPU heterogeneous systems. It uses lightweight online profiling with collaborative filtering to achieve up to 98.29% prediction accuracy while reducing profiling costs. This enables efficient power-aware performance modeling for better runtime decisions in HPC environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] On the Efficiency of Dynamic Transaction Scheduling in Blockchain
Sharding</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [blockchain sharding, dynamic transaction scheduling, competitive ratio, latency analysis, NP-hardness]</li>
<li class=""><strong>authors:</strong> Ramesh Adhikari, Costas Busch, Miroslav Popovic</li>
<li class=""><strong>institution:</strong> Augusta University, University of Novi Sad</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.07472v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.07472v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes dynamic scheduling algorithms for blockchain sharding systems where transactions arrive online and access multiple shards. The authors analyze both stateless and stateful models, proving competitive ratios for latency and showing NP-hardness of approximation. Their algorithms achieve near-optimal performance within poly-log factors of the best possible bounds.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] An Experimental Exploration of In-Memory Computing for Multi-Layer
Perceptrons</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [processing-in-memory, in-memory computing, neural networks, multilayer perceptrons, UPMEM, performance evaluation]</li>
<li class=""><strong>authors:</strong> Pedro Carrinho, Hamid Moghadaspour, Oscar Ferraz, João Dinis Ferreira, Yann Falevoz, Vitor Silva, Gabriel Falcao</li>
<li class=""><strong>institution:</strong> International Iberian Nanotechnology Laboratory, Instituto de Telecomunicações, University of Coimbra, UPMEM</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.07317v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.07317v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper explores processing-in-memory (PiM) acceleration for multilayer perceptrons using the UPMEM PiM system. The implementation achieves up to 259× better performance for large batch inference compared to CPU and demonstrates competitive execution times with low-power GPUs when using WRAM. Results show PiM effectively alleviates data movement bottlenecks in neural network inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU
Libraries</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU libraries, kernel fusion, C++ API, SRAM optimization, compile-time code generation]</li>
<li class=""><strong>authors:</strong> Oscar Amoros, Albert Andaluz, Johnny Nunez, Antonio J. Pena</li>
<li class=""><strong>institution:</strong> Universitat Politecnica de Catalunya, NVIDIA Computing SL, Barcelona Supercomputing Center</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.07071v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.07071v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a C++ API methodology using C++17 metaprogramming to automatically generate fused GPU kernels that combine multiple operations at compile time. The approach enables both horizontal and vertical kernel fusion without requiring custom compilers or pre-compiled kernels. Experimental results show performance improvements from 2x to over 1000x compared to traditional GPU libraries while maintaining high-level programmability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Kairos: Low-latency Multi-Agent Serving with Shared LLMs and Excessive
Loads in the Public Cloud</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [multi-agent systems, LLM serving, latency optimization, workflow orchestration, priority scheduling, memory-aware dispatching]</li>
<li class=""><strong>authors:</strong> Jinyuan Chen, Jiuchen Shi, Quan Chen, Minyi Guo</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.06948v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.06948v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Kairos proposes a multi-agent orchestration system with workflow-aware priority scheduling and memory-aware dispatching to optimize LLM serving. It reduces end-to-end latency by 17.8% to 28.4% compared to state-of-the-art methods through improved request scheduling and resource management for shared LLMs under excessive loads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in
Digital Twin Ecosystems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [Multi-Agent Reinforcement Learning, Digital Twins, Network-Aware Path Finding, Centralized Training Decentralized Execution, 6G Networks]</li>
<li class=""><strong>authors:</strong> Arman Dogru, R. Irem Bor-Yaliniz, Nimal Gamini Senarath</li>
<li class=""><strong>institution:</strong> Huawei Canada Advanced Research Center</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.06767v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.06767v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PANAMA introduces a network-aware multi-agent reinforcement learning framework using priority asymmetry and CTDE architecture for path finding in digital twin ecosystems. The method demonstrates superior performance in accuracy, speed and scalability compared to existing benchmarks. It enables optimized data-sharing strategies between application and network providers while advancing synergy between digital twins, wireless networks and AI-driven automation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] KV Cache Compression for Inference Efficiency in LLMs: A Review</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [KV cache compression, memory optimization, inference efficiency]</li>
<li class=""><strong>authors:</strong> Yanyu Liu, Jingying Fu, Sixiang Liu, Yitian Zou, You Fu, Jiehan Zhou, Shouhua Zhang</li>
<li class=""><strong>institution:</strong> Shandong University of Science and Technology, University of Oulu</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.06297v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.06297v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This review systematically examines KV cache optimization techniques including selective token strategies, quantization and attention compression. These methods significantly reduce memory usage while maintaining inference speed, addressing the memory bottleneck in large language models. The paper also identifies future research directions such as hybrid optimization and software-hardware co-design for improved inference efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Performant Unified GPU Kernels for Portable Singular Value Computation
Across Hardware and Precision</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU computing, singular value decomposition, Julia programming, performance optimization, hardware portability]</li>
<li class=""><strong>authors:</strong> Evelyne Ringoot, Rabab Alomairy, Valentin Churavy, Alan Edelman</li>
<li class=""><strong>institution:</strong> Massachusetts Institute of Technology, University of Mainz, University of Augsburg</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.06339v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.06339v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a portable GPU-accelerated SVD implementation in Julia using a two-stage QR reduction algorithm. The method leverages Julia&#x27;s multiple dispatch and metaprogramming to create unified kernels supporting various GPU architectures and precisions. The implementation outperforms most existing libraries for large matrices while achieving 80-90% of cuSOLVER&#x27;s performance, demonstrating that portability doesn&#x27;t sacrifice computational efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML
Next To Your Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [data engineering, AI/ML workloads, Python programming, elastic scalability, secure sandbox, performance optimization]</li>
<li class=""><strong>authors:</strong> Brandon Baker, Elliott Brossard, Chenwei Xie, Zihao Ye, Deen Liu, Yijun Xie, Arthur Zwiegincew, Nitya Kumar Sharma, Gaurav Jain, Eugene Retunsky, Mike Halcrow, Derek Denny-Brown, Istvan Cseri, Tyler Akidau, Yuxiong He</li>
<li class=""><strong>institution:</strong> Snowflake, Inc</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.05904v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.05904v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Snowpark extends Snowflake&#x27;s data cloud by enabling Python-based data engineering and AI/ML workloads directly within its infrastructure. It uses secure sandboxes for isolation and implements optimizations like package caching and workload scheduling. The system demonstrates efficient large-scale data processing while maintaining security and governance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] EC2MoE: Adaptive End-Cloud Pipeline Collaboration Enabling Scalable
Mixture-of-Experts Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [mixture-of-experts, end-cloud collaboration, pipeline optimization, expert scheduling, low-rank compression]</li>
<li class=""><strong>authors:</strong> Zheming Yang, Yunqing Hu, Sheng Sun, Wen Ji</li>
<li class=""><strong>institution:</strong> Institute of Computing Technology, Chinese Academy of Sciences, Institute of AI for Industries, University of Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.06024v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.06024v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EC2MoE proposes an adaptive end-cloud pipeline collaboration framework for scalable MoE inference, featuring a hardware-aware group gate network and route-aware pipeline scheduling. The system achieves 2.2-5.1× throughput improvement and 53-67% latency reduction while maintaining accuracy. It demonstrates good scalability under dynamic load and network conditions through optimized expert routing and computation distribution.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers
Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [load balancing, sequence parallelism, distributed training, diffusion transformers, workload optimization]</li>
<li class=""><strong>authors:</strong> Kai Zhang, Peng Wang, Sai Bi, Jianming Zhang, Yuanjun Xiong</li>
<li class=""><strong>institution:</strong> Adobe</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.06001v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.06001v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KnapFormer introduces an online load balancer that combines sequence parallelism with workload balancing by solving a global knapsack problem to redistribute tokens across GPUs. It minimizes workload variance and communication overhead, achieving less than 1% discrepancy and 2-3x speedup in training diffusion models like FLUX on mixed-resolution and image-video data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Simulating LLM training workloads for heterogeneous compute and network
infrastructure</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, heterogeneous computing, training simulation, performance prediction, workload partitioning]</li>
<li class=""><strong>authors:</strong> Sumit Kumar, Arjun Temura, Naman Sharma, Ramanjeet Singh, Meet Dadhania, Praveen Tammana, Satananda Burla, Abed Mohammad Kamaluddin, Rinku Shah</li>
<li class=""><strong>institution:</strong> IIIT-Delhi, IIT Hyderabad, Marvell Technology Inc.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.05370v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.05370v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a heterogeneity-aware distributed LLM training simulator that addresses limitations of existing homogeneous simulators. It introduces design components like non-uniform workload partitioning to model heterogeneous compute and network infrastructure. Initial results demonstrate how heterogeneity impacts computation and communication times during LLM training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Theseus: A Distributed and Scalable GPU-Accelerated Query Processing
Platform Optimized for Efficient Data Movement</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [distributed query processing, GPU acceleration, data movement optimization, memory management, asynchronous execution]</li>
<li class=""><strong>authors:</strong> Felipe Aramburú, William Malpica, Kaouther Abrougui, Amin Aramoon, Romulo Auccapuclla, Claude Brisson, Matthijs Brobbel, Colby Farrell, Pradeep Garigipati, Joost Hoozemans, Supun Kamburugamuve, Akhil Nair, Alexander Ocsa, Johan Peltenburg, Rubén Quesada López, Deepak Sihag, Ahmet Uyar, Dhruv Vats, Michael Wendt, Jignesh M. Patel, Rodrigo Aramburú</li>
<li class=""><strong>institution:</strong> Voltron Data, Carnegie Mellon University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.05029v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.05029v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Theseus introduces a distributed GPU-accelerated query engine with specialized asynchronous control mechanisms for efficient data movement across memory tiers. It employs fixed-size page-locked host memory allocations to enhance throughput and reduce fragmentation. The system demonstrates significant performance improvements, outperforming Databricks Photon by up to 4× and handling 100 TB scale benchmarks with minimal hardware nodes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Tesserae: Scalable Placement Policies for Deep Learning Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [deep learning, GPU cluster, placement policies, graph matching, job migration, job packing, resource utilization]</li>
<li class=""><strong>authors:</strong> Song Bian, Saurabh Agarwal, Md. Tareq Mahmood, Shivaram Venkataraman</li>
<li class=""><strong>institution:</strong> University of Wisconsin-Madison</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.04953v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.04953v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Tesserae formulates placement constraints as graph matching problems to design scalable policies for minimizing job migration overheads and improving job packing in GPU clusters. The system achieves up to 1.62x improvement in average job completion time and up to 1.15x improvement in makespan compared to existing schedulers. This demonstrates that graph-based approaches can effectively address placement challenges in deep learning workload scheduling.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Optimizing Microgrid Composition for Sustainable Data Centers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [microgrid optimization, sustainable data centers, renewable energy, energy storage, co-simulation, carbon emissions]</li>
<li class=""><strong>authors:</strong> Julius Irion, Philipp Wiesner, Jonathan Bader, Odej Kao</li>
<li class=""><strong>institution:</strong> TU Berlin</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.04284v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.04284v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an optimization framework that extends the Vessim co-simulator with NREL&#x27;s SAM renewable energy models to analyze microgrid compositions for data centers. The framework uses multi-horizon black-box optimization to evaluate the trade-offs between operational and embodied carbon emissions. It helps data center operators make informed decisions about renewable energy integration and energy storage sizing for improved sustainability and reliability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in
Cloud Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [IoT data scheduling, cloud computing, reinforcement learning, ant colony optimization, resource optimization, QoS-aware scheduling]</li>
<li class=""><strong>authors:</strong> Noor Islam S. Mohammad</li>
<li class=""><strong>institution:</strong> New York University Tandon School of Engineering</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.04334v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.04334v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid scheduling algorithm combining deep reinforcement learning and ant colony optimization for IoT data scheduling in cloud environments. The method achieves adaptive task allocation and global resource optimization, reducing response time by 18.4%, improving resource utilization by 12.7%, and decreasing energy consumption by 9.3% compared to baseline approaches while maintaining SLA compliance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] High-Performance and Power-Efficient Emulation of Matrix Multiplication
using INT8 Matrix Engines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [matrix multiplication, INT8 matrix engines, mixed-precision computing, performance optimization, power efficiency]</li>
<li class=""><strong>authors:</strong> Yuki Uchino, Katsuhisa Ozaki, Toshiyuki Imamura</li>
<li class=""><strong>institution:</strong> RIKEN Center for Computational Science, Shibaura Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.03984v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.03984v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes novel emulation methods for single- and double-precision matrix multiplication using low-precision INT8 matrix engines. The approach achieves significant performance improvements over conventional methods, with 3.0x speedup for SGEMM and 1.4x for DGEMM on GH200 Grace Hopper Superchip. The method also demonstrates substantial power efficiency gains of 154% for SGEMM and 43% for DGEMM compared to native implementations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Two-dimensional Sparse Parallelism for Large Scale Deep Learning
Recommendation Model Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [recommendation systems, distributed training, embedding tables, model parallelism, data parallelism, optimization algorithms]</li>
<li class=""><strong>authors:</strong> Xin Zhang, Quanyu Zhu, Liangbei Xu, Zain Huda, Wang Zhou, Jin Fang, Dennis van der Staay, Yuxi Hu, Jade Nie, Jiyan Yang, Chunzhi Yang</li>
<li class=""><strong>institution:</strong> Meta, Inc.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.03854v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.03854v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a two-dimensional sparse parallelism approach combining data parallelism with model parallelism to address scalability challenges in large-scale deep learning recommendation model training. It introduces momentum-scaled row-wise AdaGrad to maintain model performance while improving efficiency. Experiments show the method achieves near-linear training speed scaling up to 4K GPUs while maintaining performance parity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Block: Balancing Load in LLM Serving with Context, Knowledge and
Predictive Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM serving, load balancing, predictive scheduling, distributed systems, auto-provisioning]</li>
<li class=""><strong>authors:</strong> Wei Da, Evangelia Kalyvianaki</li>
<li class=""><strong>institution:</strong> University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.03611v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.03611v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Block introduces a distributed predictive scheduling framework that leverages contextual information from LLM inference requests to optimize load balancing. It uses predictable characteristics like response lengths and hardware performance to make scheduling decisions. Evaluation shows Block improves serving capacity by up to 16.7% and reduces P99 tail latency by up to 49.5% compared to heuristic schedulers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Frontier: Simulating the Next Generation of LLM Inference Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [simulation, disaggregated architectures, Mixture-of-Experts, expert parallelism, system optimization]</li>
<li class=""><strong>authors:</strong> Yicheng Feng, Xin Tan, Kin Hang Sew, Yimin Jiang, Yibo Zhu, Hong Xu</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, StepFun</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.03148v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.03148v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Frontier introduces a high-fidelity simulator specifically designed for next-generation LLM inference systems, supporting both co-located and disaggregated architectures with native MoE capabilities. It provides refined operator models and enables simulation of complex workflows like cross-cluster expert routing and advanced pipelining. The simulator empowers researchers to efficiently design and optimize large-scale LLM inference systems without expensive hardware experimentation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in
Agentic Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [provenance, agentic workflows, LLM agents, transparency, reproducibility, reliability]</li>
<li class=""><strong>authors:</strong> Renan Souza, Amal Gueroudji, Stephen DeWitt, Daniel Rosendo, Tirthankar Ghosal, Robert Ross, Prasanna Balaprakash, Rafael Ferreira da Silva</li>
<li class=""><strong>institution:</strong> Oak Ridge National Laboratory, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.02866v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.02866v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PROV-AGENT extends W3C PROV with Model Context Protocol to capture agent interactions and metadata in workflows. The system enables near real-time provenance tracking across edge, cloud, and HPC environments. It supports critical provenance queries and reliability analysis for AI agents in scientific workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] xDeepServe: Model-as-a-Service on Huawei CloudMatrix384</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [model serving system, disaggregated architecture, SuperPod infrastructure, MoE models, communication library, scalable scheduling]</li>
<li class=""><strong>authors:</strong> Ao Xiao, Bangzheng He, Baoquan Zhang, Baoxing Huai, Bingji Wang, Bo Wang, Bo Xu, Boyi Hou, Chan Yang, Changhong Liu, Cheng Cui, Chenyu Zhu, Cong Feng, Daohui Wang, Dayun Lin, Duo Zhao, Fengshao Zou, Fu Wang, Gangqiang Zhang, Gengyuan Dan, Guanjie Chen, Guodong Guan, Guodong Yang, Haifeng Li, Haipei Zhu, Haley Li, Hao Feng, Hao Huang, Hao Xu, Hengrui Ma, Hengtao Fan, Hui Liu, Jia Li, Jiang Liu, Jiang Xu, Jie Meng, Jinhan Xin, Junhao Hu, Juwei Chen, Lan Yu, Lanxin Miao, Liang Liu, Linan Jing, Lu Zhou, Meina Han, Mingkun Deng, Mingyu Deng, Naitian Deng, Nizhong Lin, Peihan Zhao, Peng Pan, Pengfei Shen, Ping Li, Qi Zhang, Qian Wang, Qin ZhC Qingrong Xia, Qingyi Zhang, Qunchao Fu, Ren Guo, Ruimin Gao, Shaochun Li, Sheng Long, Shentian Li, Shining Wan, Shuai Shen, Shuangfu Zeng, Shuming Jing, Siqi Yang, Song Zhang, Tao Xu, Tianlin Du, Ting Chen, Wanxu Wu, Wei Jiang, Weinan Tong, Weiwei Chen, Wen Peng, Wenli Zhou, Wenquan Yang, Wenxin Liang, Xiang Liu, Xiaoli Zhou, Xin Jin, Xinyu Duan, Xu Li, Xu Zhang, Xusheng Chen, Yalong Shan, Yang Gan, Yao Lu, Yi Deng, Yi Zheng, Ying Xiong, Yingfei Zheng, Yiyun Zheng, Yizhou Shan, Yong Gao, Yong Zhang, Yongqiang Yang, Yuanjin Gong, Yue Yu, Yuetao Chen, Yukun Zhu, Yulong He, Yusu Zhao, Yuyan Wu, Zenan Zhang, Zhaojin Zhuo, Zhaoyang Ji, Zhefeng Wang, Zheng Wang, Zhenan Fan, Zhenhua Yang, Zhenli Sheng, Zhibin Yu, Zhigang Ji, Zhihao Ren, Zhipeng Bian, Zhixia Liu, Zhiyu Dong, Zhonghua Li, Zhou Yu, Zhuoming Shen, Zhuwei Peng, Zi Ye, Zihao Xiang, Zimin Fu, Zixuan Zhang</li>
<li class=""><strong>institution:</strong> Huawei</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.02520v5" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.02520v5</a></li>
<li class=""><strong>Simple LLM Summary:</strong> xDeepServe introduces a disaggregated architecture called Transformerless that decomposes transformer models into modular units executed independently on NPUs. The system implements disaggregated prefill-decode and MoE-attention designs supported by XCCL communication library leveraging CloudMatrix384&#x27;s global shared memory. This approach enables scalable inference across hundreds of NPUs while maintaining performance through independent scaling of compute and memory resources.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FlashCommunication V2: Bit Splitting and Spike Reserving for Any Bit
Communication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [communication optimization, quantization, distributed training, GPU communication, bit splitting, spike reserving]</li>
<li class=""><strong>authors:</strong> Qingyuan Li, Bo Zhang, Hui Kang, Tianhao Xu, Yulei Qian, Yuchen Xie, Lin Ma</li>
<li class=""><strong>institution:</strong> Meituan, NVIDIA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.03760v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.03760v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlashCommunication V2 introduces bit splitting and spike reserving techniques to enable efficient cross-GPU communication at arbitrary bit widths. The method achieves up to 3.2× speedup in AllReduce and 2× in All2All communication while maintaining acceptable accuracy losses, significantly improving distributed training performance for large language models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] On Effectiveness of Graph Neural Network Architectures for Network
Digital Twins (NDTs)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [graph neural networks, network digital twins, network management, 6G networks, performance prediction]</li>
<li class=""><strong>authors:</strong> Iulisloi Zacarias, Oussama Ben Taarit, Admela Jukan</li>
<li class=""><strong>institution:</strong> Technische Universität Braunschweig</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.02373v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.02373v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an AI-based Network Digital Twin using multi-layered knowledge graphs and graph neural networks to predict network performance metrics. The authors evaluated four GNN architectures on RIPE Atlas measurement data and found GraphTransformer achieved the best performance. The work demonstrates GNNs&#x27; effectiveness for proactive network management in next-generation networks like 6G.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] TeraNoC: A Multi-Channel 32-bit Fine-Grained, Hybrid Mesh-Crossbar NoC
for Efficient Scale-up of 1000+ Core Shared-L1-Memory Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [network-on-chip, hybrid interconnect, shared-memory clusters, many-core architecture, low-latency communication]</li>
<li class=""><strong>authors:</strong> Yichao Zhang, Zexin Fu, Tim Fischer, Yinrong Li, Marco Bertuletti, Luca Benini</li>
<li class=""><strong>institution:</strong> ETH Zurich, University of Bologna</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.02446v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.02446v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TeraNoC presents a hybrid mesh-crossbar on-chip interconnect that combines the scalability of 2D-meshes with the low latency of crossbars. The design enables efficient scaling of shared-memory clusters to 1024 cores while maintaining high bandwidth and low power consumption. Compared to crossbar-only implementations, TeraNoC reduces die area by 37.8% and improves area efficiency by up to 98.7%.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] VeOmni: Scaling Any Modality Model Training with Model-Centric
Distributed Recipe Zoo</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [omni-modal training, distributed training, 3D parallelism, mixture-of-experts, scalability]</li>
<li class=""><strong>authors:</strong> Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, Xin Liu</li>
<li class=""><strong>institution:</strong> ByteDance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.02317v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.02317v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VeOmni introduces a model-centric distributed training framework that decouples communication from computation, enabling efficient 3D parallelism for omni-modal LLMs. The framework achieves high throughput (2,800 tokens/sec/GPU) and scales to 160K context lengths on 128 GPUs. This demonstrates superior efficiency and scalability for training large multimodal models with minimal code changes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Machine Learning-Driven Performance Analysis of Compressed Communication
in Aerial-RIS Networks for Future 6G Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [6G networks, Aerial RIS, NOMA, CoMP, compressed communication, autoencoder, spectral efficiency]</li>
<li class=""><strong>authors:</strong> Muhammad Farhan Khan, Muhammad Ahmed Mohsin, Zeeshan Alam, Muhammad Saad, Muhammad Waqar</li>
<li class=""><strong>institution:</strong> University College Cork, Stanford University, University of New Brunswick, NUST Pakistan, Edge Hill University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.01911v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.01911v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a novel system integrating UAV-assisted RIS, NOMA, and CoMP with machine learning autoencoder for compressed feedback communication. This approach significantly improves spectral efficiency and reduces outage probability in 6G networks. Simulation results demonstrate enhanced bandwidth utilization and overall network performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Prefill-Decode Aggregation or Disaggregation? Unifying Both for
Goodput-Optimized LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [LLM serving, prefill-decode aggregation, prefill-decode disaggregation, service level objectives, goodput optimization, latency shifting, scheduling mechanisms]</li>
<li class=""><strong>authors:</strong> Chao Wang, Pengfei Zuo, Zhangyu Chen, Yunkai Liang, Zhou Yu, Ming-Chang Yang</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Huawei Cloud, Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.01989v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.01989v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TaiChi proposes a unified system combining prefill-decode aggregation and disaggregation with differentiated GPU instances and configurable sliders. It introduces latency shifting and specialized scheduling mechanisms to optimize resource allocation. The system achieves up to 77% goodput improvement under balanced SLOs compared to state-of-the-art approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Energy-Predictive Planning for Optimizing Drone Service Delivery</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [drone delivery, energy prediction, path optimization, Bi-LSTM, heuristic optimization]</li>
<li class=""><strong>authors:</strong> Guanting Ren, Babar Shahzaad, Balsam Alkouz, Abdallah Lakhdari, Athman Bouguettaya</li>
<li class=""><strong>institution:</strong> University of Sydney</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.01671v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.01671v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an Energy-Predictive Drone Service framework using Bi-LSTM models to predict drone energy consumption and arrival times. It develops heuristic optimization for path planning and recharging schedules. Experimental results show improved delivery efficiency through better energy management and reduced waiting times.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Learning Unified System Representations for Microservice Tail Latency
Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [microservice performance, tail latency prediction, graph neural networks, resource modeling, system monitoring]</li>
<li class=""><strong>authors:</strong> Wenzhuo Qian, Hailiang Zhao, Tianlv Chen, Jiayi Chen, Ziqi Wang, Kingsum Chow, Shuiguang Deng</li>
<li class=""><strong>institution:</strong> Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.01635v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.01635v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes USRFNet, a deep learning network that separately models traffic-side features using GNNs and resource-side features using gMLP modules, then fuses them into unified system representations. It demonstrates substantial improvements in predicting window-level P95 tail latency for microservice systems compared to state-of-the-art baselines. The method effectively handles heterogeneous data and captures both service dependencies and resource dynamics for accurate performance prediction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Parallel Algorithm for Finding Robust Spanners in Large Social
Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [parallel algorithms, robust spanners, social networks, GPU acceleration, network resilience]</li>
<li class=""><strong>authors:</strong> Arindam Khanda, Satyaki Roy, Prithwiraj Roy, Sajal K. Das</li>
<li class=""><strong>institution:</strong> Missouri University of Science and Technology, University of Alabama in Huntsville, Global Action Alliance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.01485v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.01485v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a novel scoring technique and parallel CUDA implementation for identifying robust spanner nodes in large social networks. The method achieves comparable spanning capacity to benchmark algorithms while providing superior robustness against network disruptions. GPU implementation demonstrates 244× speedup over traditional approaches, enabling efficient analysis of large-scale networks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in
Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [carbon footprint, neural scaling laws, sustainability, GPU optimization, batch size scaling]</li>
<li class=""><strong>authors:</strong> Lei Jiang, Fan Chen</li>
<li class=""><strong>institution:</strong> Indiana University Bloomington</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.06524v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.06524v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes CarbonScaling, an analytical framework that extends neural scaling laws to incorporate carbon emissions from LLM training. It integrates models for neural scaling, GPU evolution, parallelism optimization, and carbon estimation. Results show hardware scaling benefits smaller models but has diminishing returns for large LLMs, while training optimizations like batch size scaling can improve carbon efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] PiKV: KV Cache Management System for Mixture of Experts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [KV cache management, Mixture of Experts, distributed systems, memory optimization, GPU acceleration]</li>
<li class=""><strong>authors:</strong> Dong Liu, Yanxuan Yu, Ben Lengerich, Ying Nian Wu, Xuhong Wang</li>
<li class=""><strong>institution:</strong> Yale University, Columbia University, University of Wisconsin-Madison, University of California - Los Angeles, Shanghai AI Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.06526v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.06526v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PiKV introduces a parallel distributed KV cache management system specifically designed for Mixture of Experts architectures. It employs expert-sharded KV storage, intelligent routing to reduce token-to-KV access, and adaptive scheduling to retain query-relevant entries. The system significantly reduces memory usage and communication overhead while maintaining performance through integrated compression modules.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Compression-Induced Communication-Efficient Large Model Training and
Inferencing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [phantom parallelism, energy efficiency, tensor parallelism, model compression, communication optimization]</li>
<li class=""><strong>authors:</strong> Sudip K. Seal, Maksudul Alam, Jorge Ramirez, Sajal Dash, Hao Lu</li>
<li class=""><strong>institution:</strong> Oak Ridge National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.00960v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.00960v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces phantom parallelism as an energy-efficient alternative to traditional tensor parallelism for training large neural networks. The method derives new forward/backward propagation operators and implements them in a custom training pipeline. Experimental results show ~50% energy reduction while achieving comparable model performance with smaller models on fewer GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Adacc: An Adaptive Framework Unifying Compression and Activation
Recomputation for LLM Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [memory optimization, activation recomputation, data compression, adaptive scheduling, mixed integer linear programming]</li>
<li class=""><strong>authors:</strong> Ping Chen, Zhuohong Deng, Ping Li, Shuibing He, Hongzi Zhu, Yi Zheng, Zhefeng Wang, Baoxing Huai, Minyi Guo</li>
<li class=""><strong>institution:</strong> Zhejiang University, Shanghai Jiao Tong University, Huawei Cloud, Zhejiang Lab</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.00806v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.00806v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Adacc proposes an adaptive framework that dynamically combines activation recomputation and data compression at tensor level using layer-specific compression algorithms and MILP-based scheduling. It addresses GPU memory constraints in LLM training while preserving model accuracy. Experimental results show 1.01-1.37× training throughput improvement over state-of-the-art frameworks with comparable accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Optimal Scheduling Algorithms for LLM Inference: Theory and Practice</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [scheduling algorithms, throughput optimization, Service Level Objectives, Time To First Token, Time Between Token]</li>
<li class=""><strong>authors:</strong> Agrim Bari, Parikshit Hegde, Gustavo de Veciana</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.01002v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.01002v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper develops theoretical scheduling frameworks and practical algorithms (RAD and SLAI) for LLM inference systems with two-phase computation structure. It proposes dynamic resource allocation and deadline-aware prioritization strategies. Experimental results show SLAI reduces median TTFT by 53% and increases serving capacity by 26% while meeting latency constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Tetris: Efficient Intra-Datacenter Calls Packing for Large Conferencing
Services</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [load balancing, call packing, media processing, CPU utilization optimization, datacenter efficiency]</li>
<li class=""><strong>authors:</strong> Rohan Gandhi, Ankur Mallick, Ken Sueda, Rui Liang</li>
<li class=""><strong>institution:</strong> Microsoft</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.00426v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.00426v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Tetris proposes a multi-step framework for efficient call packing in datacenters that optimizes initial call assignments using historical data and periodically migrates calls from hot servers using linear optimization. The system reduces participant numbers on hot Media Processor servers by at least 2.5× compared to existing approaches, addressing CPU utilization imbalance caused by variable call characteristics and bursty arrivals. This improves performance while reducing hosting costs for large-scale conferencing services.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Quality-of-Service Aware LLM Routing for Edge Computing with Multiple
Experts</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [edge computing, quality-of-service, deep reinforcement learning, request routing, heterogeneous graph attention network]</li>
<li class=""><strong>authors:</strong> Jin Yang, Qiong Wu, Zhiying Feng, Zhi Zhou, Deke Guo, Xu Chen</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University, The Hong Kong University of Science and Technology, National University of Defense Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.00234v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.00234v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a deep reinforcement learning-based QoS-aware routing framework for LLM services at the edge, using dynamic state abstraction with heterogeneous graph attention networks and action impact estimation. The method significantly improves average QoS and computing resource efficiency compared to existing baselines, effectively handling dynamic workloads while preventing latency violations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Consistent Point Matching</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [medical imaging, point matching, consistency heuristic, anatomical landmark localization, multi-resolution descriptor]</li>
<li class=""><strong>authors:</strong> Halid Ziya Yerebakan, Gerardo Hermosillo Valadez</li>
<li class=""><strong>institution:</strong> Siemens Medical Solutions</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.23609v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.23609v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper enhances point-matching robustness in medical images by incorporating a consistency heuristic across multiple resolution levels. The method operates efficiently on standard CPU hardware without requiring machine learning models or training data. It achieves state-of-the-art performance on the Deep Lesion Tracking dataset and enables high-precision navigation between medical images.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Towards a Testbed for Scalable FaaS Platforms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [serverless], [FaaS, testbed, scalability, architecture, performance evaluation]</li>
<li class=""><strong>authors:</strong> Trever Schirmer, David Bermbach</li>
<li class=""><strong>institution:</strong> Technische Universität Berlin</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.23431v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.23431v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents HyperFaaS, a research-focused testbed for scalable FaaS platforms with a tree-like architecture using load balancers and worker nodes. This testbed enables rapid evaluation of different architectural designs and technologies under massive load. The goal is to improve understanding of how platform architecture impacts performance in serverless computing environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units
with Precision Recovery</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [matrix multiplication, numerical precision, hardware acceleration, performance optimization]</li>
<li class=""><strong>authors:</strong> Weicheng Xue, Baisong Xu, Kai Yang, Yongxiang Liu, Dengdeng Fan, Pengxiang Xu, Yonghong Tian</li>
<li class=""><strong>institution:</strong> Peking University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.23387v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.23387v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SGEMM-cube enables FP32 GEMM computation on FP16-only NPUs by decomposing FP32 operands into FP16 values with tunable scaling and precision recovery techniques. The method achieves up to 77% of theoretical FP32-equivalent peak performance while maintaining numerical accuracy comparable to native FP32 implementations. The proposed term-wise accumulation scheme demonstrates superior numerical stability in low-exponent regimes compared to conventional FP32 GEMM.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU
Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM training, LLM inference, resource scheduling, co-location, pipeline parallelism, GPU utilization]</li>
<li class=""><strong>authors:</strong> Yufei Li, Zexin Li, Yinglun Zhu, Cong Liu</li>
<li class=""><strong>institution:</strong> University of California, Riverside</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.21276v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.21276v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LeMix proposes a unified scheduling system that co-locates LLM training and inference workloads using offline profiling, execution prediction, and runtime scheduling. It dynamically allocates resources based on workload characteristics to improve system efficiency. Evaluation shows LeMix achieves up to 3.53× higher throughput and 2.12× better SLO attainment compared to traditional separate setups.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Improving SpGEMM Performance Through Matrix Reordering and Cluster-wise
Computation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [sparse matrix multiplication, matrix reordering, hierarchical clustering, performance optimization, memory access patterns]</li>
<li class=""><strong>authors:</strong> Abdullah Al Raqibul Islam, Helen Xu, Dong Dai, Aydın Buluç</li>
<li class=""><strong>institution:</strong> University of North Carolina at Charlotte, Georgia Institute of Technology, University of Delaware, Lawrence Berkeley National Lab</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.21253v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.21253v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes hierarchical clustering for SpGEMM that combines row reordering and cluster-wise computation to improve memory access patterns. The method achieves 1.39x average speedup with low preprocessing costs while decoupling reordering from the clustered matrix format. The evaluation shows graph partitioning-based reordering provides better performance but with higher preprocessing time compared to other schemes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Sustainable AI Training via Hardware-Software Co-Design on NVIDIA, AMD,
and Emerging GPU Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [hardware-software co-design, GPU optimization, energy efficiency, performance-per-watt, memory optimization, kernel fusion, mixed precision]</li>
<li class=""><strong>authors:</strong> Yashasvi Makin, Rahul Maliakkal</li>
<li class=""><strong>institution:</strong> Meta Platforms Inc.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.13163v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.13163v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper explores hardware-software co-design techniques for sustainable AI training across NVIDIA, AMD, and emerging GPU architectures. It focuses on memory-level and kernel-level optimizations, specialized tensor cores, and software enhancements like mixed-precision arithmetic. The study demonstrates that coordinated hardware-software approaches can significantly improve training efficiency and reduce environmental impact without compromising performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Communication-Efficient Distributed Training for Collaborative Flat
Optima Recovery in Deep Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed training, communication efficiency, flat minima, generalization, DNN optimization]</li>
<li class=""><strong>authors:</strong> Tolga Dimlioglu, Anna Choromanska</li>
<li class=""><strong>institution:</strong> New York University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.20424v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.20424v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DPPF, a distributed training algorithm that incorporates a sharpness regularizer to guide workers toward flat minima while maintaining communication efficiency. It introduces Inverse Mean Valley as a sharpness measure and demonstrates both theoretically and empirically that DPPF achieves better generalization than local gradient methods and synchronous averaging. The method effectively balances pull-push dynamics to locate wider minima in the loss landscape.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Comparative Study of OpenMP Scheduling Algorithm Selection Strategies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [OpenMP, scheduling algorithms, reinforcement learning, expert-based methods, performance optimization, HPC systems]</li>
<li class=""><strong>authors:</strong> Jonas H. Müller Korndörfer, Ali Mohammed, Ahmed Eleliemy, Quentin Guilloteau, Reto Krummenacher, Florina M. Ciorba</li>
<li class=""><strong>institution:</strong> University of Basel, HPE HPC/AI EMEA Research Lab</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.20312v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.20312v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes and evaluates both expert-based and reinforcement learning approaches for dynamic scheduling algorithm selection in OpenMP applications. The study finds that RL methods can learn high-performing scheduling decisions but require significant exploration, while expert-based methods leverage prior knowledge with less exploration. Combining expert knowledge with RL achieves improved performance and adaptability across different application-system pairs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Ethereum Conflicts Graphed</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [Ethereum, blockchain, smart contracts, conflict graphs, parallel execution, transaction analysis]</li>
<li class=""><strong>authors:</strong> Dvir David Biton, Roy Friedman, Yaron Hay</li>
<li class=""><strong>institution:</strong> Technion - Israel Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.20196v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.20196v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper analyzes Ethereum blockchain data by tracing over 2 million blocks to study smart contract interactions and conflict graphs. Researchers developed calling graphs and examined read/write sets to understand transaction conflicts. They found that conflict graphs predominantly exhibit star-like configurations, revealing insights about parallelization potential in smart contract execution.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Advancing Compositional LLM Reasoning with Structured Task Relations in
Interactive Multimodal Communications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [compositional reasoning, task dependency graph, parameter partitioning, fine-tuning optimization, wireless networks]</li>
<li class=""><strong>authors:</strong> Xinye Cao, Hongcan Guo, Guoshun Nan, Jiaoyang Cui, Haoting Qian, Yihan Lin, Yilin Peng, Diyang Zhang, Yanzhao Hou, Huici Wu, Xiaofeng Tao, Tony Q. S. Quek</li>
<li class=""><strong>institution:</strong> Beijing University of Posts and Telecommunications, Singapore University of Technology and Design</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.21199v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.21199v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes ContextLoRA, a method that enables a single LLM to handle multiple interactive multimodal applications by constructing task dependency graphs and partitioning parameter matrices. It introduces ContextGear, a scheduling strategy to optimize training efficiency in resource-constrained environments. Experimental results demonstrate the superiority of the proposed approach across three benchmarks and a real-world wireless testbed.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Racing to Idle: Energy Efficiency of Matrix Multiplication on
Heterogeneous CPU and GPU Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [energy efficiency, matrix multiplication, heterogeneous computing, GPU performance, race to idle]</li>
<li class=""><strong>authors:</strong> Mufakir Qamar Ansari, Mudabir Qamar Ansari</li>
<li class=""><strong>institution:</strong> Unable to determine from provided information</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.20063v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.20063v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper empirically measures performance and energy consumption of large matrix multiplication on three different architectures within a consumer laptop. Using standard tools like Linux perf and nvidia-smi, they found the discrete GPU achieved 93.5x speedup and used only 2% of the CPU&#x27;s energy. The results demonstrate the &quot;race to idle&quot; principle and show discrete GPUs provide superior energy efficiency for HPC workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] $K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [log anomaly detection, unsupervised learning, typicality estimation, online detection, k-nearest neighbors]</li>
<li class=""><strong>authors:</strong> Weicong Chen, Vikash Singh, Zahra Rahmani, Debargha Ganguly, Mohsen Hariri, Vipin Chaudhary</li>
<li class=""><strong>institution:</strong> Case Western Reserve University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.20051v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.20051v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> K4 introduces an unsupervised framework for online log anomaly detection that transforms log embeddings into four-dimensional descriptors using k-NN statistics. The method achieves state-of-the-art performance with AUROC scores of 0.995-0.999 while being extremely fast, with training under 4 seconds and inference as low as 4 μs. It outperforms existing baselines by large margins and works without log parsing or retraining requirements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Parallel Hierarchical Agglomerative Clustering in Low Dimensions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [hierarchical clustering, parallel algorithms, approximation algorithms, computational complexity, centroid linkage, Ward&#x27;s linkage]</li>
<li class=""><strong>authors:</strong> MohammadHossein Bateni, Laxman Dhulipala, Willem Fletcher, Kishen N Gowda, D Ellis Hershkowitz, Rajesh Jayaram, Jakub Łącki</li>
<li class=""><strong>institution:</strong> Google Research, University of Maryland, Brown University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.20047v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.20047v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents efficient parallel algorithms for approximate hierarchical agglomerative clustering with non-monotone linkage functions like centroid and Ward&#x27;s distance in low dimensions. The algorithms leverage a structural result showing that the hierarchy height remains poly-logarithmic for constant approximations when dimensions are bounded. The work also establishes computational hardness for these clustering problems in arbitrary dimensions, demonstrating CC-hardness for centroid linkage when dimensions equal the number of points.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] CleANN: Efficient Full Dynamism in Graph-based Approximate Nearest
Neighbor Search</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [approximate nearest neighbor search, graph-based index, dynamic updates, vector databases]</li>
<li class=""><strong>authors:</strong> Ziyu Zhang, Yuanhao Wei, Joshua Engels, Julian Shun</li>
<li class=""><strong>institution:</strong> MIT CSAIL</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.19802v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.19802v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CleANN introduces a graph-based approximate nearest neighbor search system with three key components for efficient full dynamism: workload-aware linking, query-adaptive neighborhood consolidation, and semi-lazy memory cleaning. It maintains query quality comparable to static indexes while achieving 7-1200x throughput improvement on million-scale datasets with concurrent operations. The system effectively handles distribution shifts and deleted nodes without expensive global graph updates.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Accelerating Matrix Multiplication: A Performance Comparison Between
Multi-Core CPU and GPU</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [matrix multiplication, GPU acceleration, CUDA, OpenMP, parallel computing, performance comparison]</li>
<li class=""><strong>authors:</strong> Mufakir Qamar Ansari, Mudabir Qamar Ansari</li>
<li class=""><strong>institution:</strong> Unknown (no institutional information available in provided content)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.19723v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.19723v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements and compares three versions of matrix multiplication algorithms: sequential C++, OpenMP-parallelized CPU, and CUDA-optimized GPU implementations. The study demonstrates that GPU acceleration provides dramatic performance scaling, achieving up to 593x speedup over sequential and 45x over parallel CPU versions for large matrices. The findings highlight the significant performance benefits of many-core GPU architectures for data-parallel workloads even on consumer hardware.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] MegatronApp: Efficient and Comprehensive Management on Distributed LLM
Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, performance optimization, monitoring tools, parallel computing, training efficiency]</li>
<li class=""><strong>authors:</strong> Bohan Zhao, Guang Yang, Shuo Chen, Ruitao Liu, Tingrui Zhang, Yongchao He, Wei Xu</li>
<li class=""><strong>institution:</strong> Suanzhi Future, Shanghai Qi Zhi Institute</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.19845v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.19845v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MegatronApp introduces four composable modules (MegaScan, MegaFBD, MegaDPP, MegaScope) that enhance distributed LLM training by addressing performance optimization, diagnosis, and interpretability challenges. These tools provide lightweight extensions for tracing, resource allocation, dynamic scheduling, and visualization while maintaining compatibility with Megatron-LM. The framework improves training reliability and efficiency through synergistic integration of its specialized components.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Fast Parallel Median Filtering Algorithm Using Hierarchical Tiling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [median filtering, parallel algorithm, GPU computing, hierarchical tiling, sorting networks]</li>
<li class=""><strong>authors:</strong> Louis Sugy</li>
<li class=""><strong>institution:</strong> NVIDIA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.19926v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.19926v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a novel parallel median filtering algorithm using hierarchical tiling to minimize redundant computations, with two variants achieving O(k log(k)) and O(k) per-pixel complexity. The CUDA implementation demonstrates up to 5x speedup over state-of-the-art methods on modern GPUs, making it the fastest median filter for various data types and kernel sizes. The algorithm&#x27;s efficiency comes from leveraging separability and GPU-friendly design through data-oblivious and data-aware selection networks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficient and Scalable Agentic AI with Heterogeneous Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [AI agents, heterogeneous systems, MLIR, dynamic orchestration, TCO optimization]</li>
<li class=""><strong>authors:</strong> Zain Asgar, Michelle Nguyen, Sachin Katti</li>
<li class=""><strong>institution:</strong> Stanford University, Gimlet Labs Inc, Intel</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.19635v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.19635v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a system design for dynamically orchestrating AI agent workloads across heterogeneous compute infrastructure using MLIR-based compilation and cost-model-driven scheduling. The system decomposes agent execution graphs into granular operators and optimizes placement across CPUs and accelerators from different vendors. Preliminary results show heterogeneous combinations of older and newer hardware can achieve similar TCO as homogeneous latest-generation GPU clusters while extending infrastructure lifespan.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] GPUnion: Autonomous GPU Sharing on Campus</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU sharing, container-based execution, resource provider autonomy, checkpointing, migration, campus networks]</li>
<li class=""><strong>authors:</strong> Yufang Li, Yuanbo Zhang, Hanlong Liao, Guoming Tang, Deke Guo</li>
<li class=""><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou), Sun Yat-sen University, National University of Defense Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.18928v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.18928v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> GPUnion presents a campus-scale GPU sharing platform using container-based task dispatching, provider-first architecture, and resilient execution with automatic checkpointing and migration. The system demonstrates 30% GPU utilization improvement, 40% increase in interactive sessions, and 94% successful workload migration during provider departures. It enables voluntary participation while maintaining provider autonomy, challenging centralized resource allocation models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] RailX: A Flexible, Scalable, and Low-Cost Network Architecture for
Hyper-Scale LLM Training Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [network architecture, circuit switching, all-to-all topology, scalability, cost-efficiency]</li>
<li class=""><strong>authors:</strong> Yinxiao Feng, Tiancheng Chen, Yuchen Wei, Siyuan Shen, Shiju Wang, Wei Li, Kaisheng Ma, Torsten Hoefler</li>
<li class=""><strong>institution:</strong> Tsinghua University, ETH Zurich, Beihang University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.18889v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.18889v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RailX proposes a reconfigurable network architecture using intra-node direct connectivity and inter-node circuit switching with Hamiltonian Decomposition to organize rail-based rings into all-to-all topology. It achieves better scalability than existing networks with only 2-4 inter-node hops diameter. The system reduces network costs to less than 10% of Fat-Tree for injection/All-Reduce bandwidth while supporting over 100K chips with hyper bandwidth.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Deadline-Aware Joint Task Scheduling and Offloading in Mobile Edge
Computing Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [mobile edge computing, task scheduling, computation offloading, deadline-aware, online scheduling, service ratio]</li>
<li class=""><strong>authors:</strong> Ngoc Hung Nguyen, Van-Dinh Nguyen, Anh Tuan Nguyen, Nguyen Van Thieu, Hoang Nam Nguyen, Symeon Chatzinotas</li>
<li class=""><strong>institution:</strong> VinUniversity, Hanyang University, PHENIKAA University, University of Brescia, University of Luxembourg</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.18864v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.18864v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an optimal job scheduling algorithm with O(nlogn) complexity for deadline-aware task ordering and enables informed offloading decisions in mobile edge computing systems. It also develops an online approach with O(n) complexity to handle randomly arriving tasks. The proposed methods demonstrate effectiveness in improving service ratio and reducing scheduling costs through extensive numerical evaluations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Towards Designing an Energy Aware Data Replication Strategy for Cloud
Systems Using Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [data replication, reinforcement learning, cloud systems, energy efficiency, quality of service]</li>
<li class=""><strong>authors:</strong> Amir Najjar, Riad Mokadem, Jean-Marc Pierson</li>
<li class=""><strong>institution:</strong> Université de Toulouse, Institut de Recherche en Informatique de Toulouse</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.18459v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.18459v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a reinforcement learning-based data replication strategy for cloud systems that automatically adapts to workload changes and system characteristics. The method defines states, actions, and rewards to optimize the trade-off between provider profit and environmental impact while maintaining quality of service. The approach eliminates the need for manual threshold adjustments by system administrators through machine learning adaptation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] PPipe: Efficient Video Analytics Serving on Heterogeneous GPU Clusters
via Pool-Based Pipeline Parallelism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [video analytics, pipeline parallelism, heterogeneous GPU clusters, CNN inference, resource scheduling]</li>
<li class=""><strong>authors:</strong> Z. Jonny Kong, Qiang Xu, Y. Charlie Hu</li>
<li class=""><strong>institution:</strong> Purdue University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.18748v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.18748v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PPipe introduces pool-based pipeline parallelism for efficient video analytics serving on heterogeneous GPU clusters, using an MILP-based control plane and adaptive batching. The system exploits comparable inference latency across different GPU classes for various model layers. Evaluation shows PPipe achieves 41.1%-65.5% higher low-class GPU utilization and 32.2%-75.1% higher throughput compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Cloud Native System for LLM Inference Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [cloud native, Kubernetes, autoscaling, resource management, containerization, microservices]</li>
<li class=""><strong>authors:</strong> Minxian Xu, Junhan Liao, Jingfeng Wu, Yiyuan He, Kejiang Ye, Chengzhong Xu</li>
<li class=""><strong>institution:</strong> Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences; University of Macau</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.18007v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.18007v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using Cloud Native technologies like containerization and Kubernetes-based autoscaling to optimize LLM inference serving. The system dynamically adapts to workload fluctuations through microservices and dynamic scheduling. Results show improved resource efficiency, reduced latency, and enhanced throughput for high-demand LLM inference scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Unlock the Potential of Fine-grained LLM Serving via Dynamic Module
Scaling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [dynamic scaling, module replication, module migration, resource management, cost reduction]</li>
<li class=""><strong>authors:</strong> Jingfeng Wu, Yiyuan He, Minxian Xu, Xitong Gao, Kejiang Ye, Chengzhong Xu</li>
<li class=""><strong>institution:</strong> Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, University of Macau</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.18006v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.18006v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes CoCoServe, an elastic system that enables dynamic module-level scaling through replication and migration operations for LLM components. It develops an auto-scaling mechanism that optimizes resource allocation and performance at the module level. Evaluation shows CoCoServe reduces costs by 46%, decreases latency by 14%-75%, and achieves 1.16x-4x higher throughput compared to existing serving systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficient Column-Wise N<!-- -->:M<!-- --> Pruning on RISC-V CPU</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [weight pruning, RISC-V optimization, convolutional neural networks, XNNPACK modification, im2col fusion, AITemplate profiling]</li>
<li class=""><strong>authors:</strong> Chi-Wei Chu, Ding-Yong Hong, Jan-Jan Wu</li>
<li class=""><strong>institution:</strong> Institute of Information Science, Academia Sinica</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.17301v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.17301v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a column-wise N<!-- -->:M<!-- --> pruning strategy at tile level and modifies XNNPACK for efficient execution on RISC-V vector architecture. The approach fuses im2col with data packing operations and uses AITemplate profiling to optimize convolutional operators. The method achieves up to 4.0x ResNet inference throughput improvement while maintaining ImageNet top-1 accuracy within 2.1% of the dense baseline.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] BrownoutServe: SLO-Aware Inference Serving under Bursty Workloads for
MoE-based LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Mixture-of-Experts, dynamic workload adaptation, SLO-aware serving, bursty workloads, inference optimization]</li>
<li class=""><strong>authors:</strong> Jianmin Hu, Minxian Xu, Kejiang Ye, Chengzhong Xu</li>
<li class=""><strong>institution:</strong> Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Southern University of Science and Technology, University of Macau</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.17133v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.17133v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BrownoutServe introduces united experts that integrate knowledge from multiple experts and a dynamic brownout mechanism to adaptively adjust token processing. The framework achieves up to 2.07x throughput improvement compared to vLLM and reduces SLO violations by 90.28% while maintaining acceptable inference accuracy under bursty workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] BucketServe: Bucket-Based Dynamic Batching for Smart and Efficient LLM
Inference Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [dynamic batching, GPU memory optimization, sequence length grouping, SLO compliance, throughput improvement]</li>
<li class=""><strong>authors:</strong> Wanyi Zheng, Minxian Xu, Shengye Song, Kejiang Ye</li>
<li class=""><strong>institution:</strong> Southern University of Science and Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.17120v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.17120v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BucketServe introduces a bucket-based dynamic batching framework that groups requests by sequence length to minimize padding overhead and optimize GPU memory usage. It employs adaptive bucket splitting/merging and priority-aware scheduling to improve resource utilization and meet service level objectives. Experiments show BucketServe achieves up to 3.58x higher throughput and handles significantly more request load compared to existing systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] PathWeaver: A High-Throughput Multi-GPU System for Graph-Based
Approximate Nearest Neighbor Search</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [multi-GPU systems, approximate nearest neighbor search, graph-based algorithms, high-throughput computing]</li>
<li class=""><strong>authors:</strong> Sukjin Kim, Seongyeon Park, Si Ung Noh, Junguk Hong, Taehee Kwon, Hunseong Lim, Jinho Lee</li>
<li class=""><strong>institution:</strong> Seoul National University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.17094v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.17094v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PathWeaver introduces a multi-GPU framework with pipelining-based path extension, ghost staging, and direction-guided selection to accelerate graph-based approximate nearest neighbor search. The system reduces redundant computations and optimizes memory access across GPUs. Evaluations show it achieves up to 5.30× speedup over state-of-the-art multi-GPU ANNS frameworks while maintaining high recall rates.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Cooling Matters: Benchmarking Large Language Models and Vision-Language
Models on Liquid-Cooled Versus Air-Cooled H100 GPU Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [liquid cooling, air cooling, thermal management, energy efficiency, performance benchmarking, H100 GPU]</li>
<li class=""><strong>authors:</strong> Imran Latif, Muhammad Ali Shafique, Hayat Ullah, Alex C. Newkirk, Xi Yu, Arslan Munir</li>
<li class=""><strong>institution:</strong> Johnson Controls, Kansas State University, Florida Atlantic University, Lawrence Berkeley National Laboratory, Brookhaven National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.16781v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.16781v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study benchmarks LLMs and VLMs on liquid-cooled versus air-cooled H100 GPU systems using GPU Burn, Weights &amp; Biases, and IPMItool. Results show liquid-cooled systems maintain lower GPU temperatures (41-50°C vs 54-72°C) and achieve 17% higher performance with better energy efficiency. The findings demonstrate liquid cooling&#x27;s advantages for thermal stability and computational performance in AI data centers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] AcceleratedKernels.jl: Cross-Architecture Parallel Algorithms from a
Unified, Transpiled Codebase</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [parallel computing, GPU acceleration, transpilation, heterogeneous computing, high-performance computing]</li>
<li class=""><strong>authors:</strong> Andrei-Leonard Nicusan, Dominik Werner, Simon Branford, Simon Hartley, Andrew J. Morris, Kit Windows-Yule</li>
<li class=""><strong>institution:</strong> University of Birmingham</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.16710v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.16710v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AcceleratedKernels.jl introduces a backend-agnostic Julia library using transpilation to target multiple hardware accelerators while maintaining performance comparable to C implementations. The framework enables CPU-GPU co-processing and achieves world-class sorting throughputs on HPC clusters. Direct GPU interconnects provide significant speedups, making communication-heavy tasks economically viable only with GPUDirect technology.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Autonomous Dominant Resource Fairness for Blockchain Ecosystems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [blockchain, resource allocation, smart contracts, dominant resource fairness, gas efficiency]</li>
<li class=""><strong>authors:</strong> Serdar Metin</li>
<li class=""><strong>institution:</strong> No institutional affiliation identified (based on personal email address)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.16350v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.16350v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Autonomous Dominant Resource Fairness, a smart contract adaptation of Precomputed Dominant Resource Fairness for blockchain-based multi-resource allocation. The algorithm avoids loop iterations to operate within blockchain gas limits while handling heterogeneous resource demands. Experimental results demonstrate the method can efficiently manage hundreds of resource types for unlimited users with low gas costs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Collaborative Inference and Learning between Edge SLMs and Cloud LLMs: A
Survey of Algorithms, Execution, and Open Challenges</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference, LLM training, edge computing], [edge-cloud collaboration, distributed adaptation, speculative decoding, parameter alignment, bidirectional distillation, privacy-preserving methods]</li>
<li class=""><strong>authors:</strong> Senyao Li, Haozhao Wang, Wenchao Xu, Rui Zhang, Song Guo, Jingling Yuan, Xian Zhong, Tianwei Zhang, Ruixuan Li</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology, Hong Kong University of Science and Technology, Wuhan University of Technology, Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.16731v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.16731v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This survey systematically explores collaborative inference and learning strategies between cloud LLMs and edge SLMs through task assignment, task division, and mixture-based approaches. It covers distributed adaptation techniques including parameter alignment, pruning, and bidirectional distillation for model optimization. The work establishes a foundational framework for efficient, scalable, and trustworthy edge-cloud intelligence through system-algorithm co-design.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for
Efficient Large-Scale Model Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [GPU memory allocation, memory fragmentation, spatio-temporal planning, PyTorch allocator, large-scale model training]</li>
<li class=""><strong>authors:</strong> Zixiao Huang, Junhao Hu, Hao Lin, Chunyang Zhu, Yueran Tang, Quanlu Zhang, Zhen Guo, Zhenhua Li, Shengen Yan, Zhenhua Zhu, Guohao Dai, Yu Wang</li>
<li class=""><strong>institution:</strong> Tsinghua University, Infinigence AI, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.16274v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.16274v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> STWeaver introduces a novel GPU memory allocator that combines offline planning with online allocation to reduce memory fragmentation in large-scale model training. It leverages spatio-temporal regularities to create near-optimal allocation plans while handling dynamic models. The approach reduces fragmentation ratio by 79.2% on average and improves training performance by up to 32.5% with negligible overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficient Routing of Inference Requests across LLM Instances in
Cloud-Edge Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [routing algorithm, cloud-edge computing, multi-objective optimization, NSGA-II, request distribution]</li>
<li class=""><strong>authors:</strong> Shibo Yu, Mohammad Goudarzi, Adel Nadjaran Toosi</li>
<li class=""><strong>institution:</strong> Monash University, The University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.15553v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.15553v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an NSGA-II-based routing algorithm to distribute LLM inference requests across cloud-edge environments, optimizing response quality, time, and cost. The method adapts to request heterogeneity and node diversity, achieving up to 95.2% improvement in response time and 34.9% in cost reduction compared to baselines. Results demonstrate its effectiveness for scalable LLM deployments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] An ML-Driven Participant Selection Technique for Federated
Recommendation System in Edge-Cloud Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [federated learning], [federated recommendation systems, participant selection, multi-armed bandit, reinforcement learning, edge-cloud computing]</li>
<li class=""><strong>authors:</strong> Jintao Liu, Mohammad Goudarzi, Adel Nadjaran Toosi</li>
<li class=""><strong>institution:</strong> Monash University, The University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.15233v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.15233v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a multi-armed bandit-based participant selection method for federated recommendation systems that optimizes client performance reputation, data utility, and system efficiency. Experimental results show the approach accelerates convergence by 32-50% and reduces training time by up to 46% while maintaining or improving recommendation metrics. This demonstrates adaptive client sampling can significantly enhance efficiency and fairness in federated learning deployments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] AMPED: Accelerating MTTKRP for Billion-Scale Sparse Tensor Decomposition
on Multiple GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [sparse tensor decomposition, MTTKRP, multi-GPU, load balancing, partitioning]</li>
<li class=""><strong>authors:</strong> Sasindu Wijeratne, Rajgopal Kannan, Viktor Prasanna</li>
<li class=""><strong>institution:</strong> University of Southern California, DEVCOM Army Research Office</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.15121v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.15121v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AMPED introduces a multi-GPU parallel algorithm with partitioning and dynamic load balancing to accelerate MTTKRP computations for billion-scale sparse tensors. The approach effectively distributes computation across multiple GPUs to minimize idle time and overcome single GPU limitations. Experimental results show a 5.1× geometric mean speedup over state-of-the-art GPU baselines using 4 GPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [benchmark, kernel generation, multi-platform, LLM evaluation, deep learning kernels]</li>
<li class=""><strong>authors:</strong> Zhongzhen Wen, Yinghui Zhang, Zhong Li, Zhongxin Liu, Linna Xie, Tian Zhang</li>
<li class=""><strong>institution:</strong> Nanjing University, Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.17773v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.17773v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces MultiKernelBench, a comprehensive benchmark for evaluating LLMs in generating deep learning kernels across multiple hardware platforms. It features a modular backend abstraction layer and proposes a category-aware one-shot prompting method. The evaluation reveals significant performance variations across tasks and platforms, demonstrating the effectiveness of targeted prompting strategies while highlighting poor generalization to less-exposed hardware platforms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Mayura: Exploiting Similarities in Motifs for Temporal Co-Mining</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [temporal graphs, motif mining, co-mining, GPU acceleration, performance optimization]</li>
<li class=""><strong>authors:</strong> Sanjay Sri Vallabh Singapuram, Ronald Dreslinski, Nishil Talati</li>
<li class=""><strong>institution:</strong> University of Michigan</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.14813v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.14813v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mayura introduces a novel framework for temporal motif co-mining that exploits structural and temporal similarities across motifs using a Motif-Group Tree (MG-Tree) to reduce redundant computations. The system achieves significant performance improvements with average speed-ups of 2.4× on CPU and 1.7× on GPU compared to state-of-the-art individual motif mining approaches, while maintaining exact results for critical applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Characterizing Communication Patterns in Distributed Large Language
Model Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [distributed inference, communication patterns, parallelization strategies, tensor parallelism, pipeline parallelism, performance analysis]</li>
<li class=""><strong>authors:</strong> Lang Xu, Kaushik Kandadi Suresh, Quentin Anthony, Nawras Alnaasan, Dhabaleswar K. Panda</li>
<li class=""><strong>institution:</strong> The Ohio State University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.14392v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.14392v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper combines profiling measurements and analytical models to characterize communication behavior in distributed LLM inference. It finds tensor parallelism provides better response times for short sequences but incurs high network overhead, while pipeline parallelism reduces data transfer but increases latency. The study provides practical recommendations for selecting parallelization schemes in production LLM services.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement
Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [CUDA optimization, reinforcement learning, contrastive learning, GPU computing, automated code optimization]</li>
<li class=""><strong>authors:</strong> Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li, Chris Shum</li>
<li class=""><strong>institution:</strong> DeepReinforce</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.14111v7" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.14111v7</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CUDA-L1 introduces a contrastive reinforcement learning framework that transforms LLMs into effective CUDA optimizers using speedup-based rewards. The method achieves significant performance improvements across multiple GPU architectures, with average speedups of 3.12x on A100 and demonstrates portability to other GPUs. It autonomously discovers optimization techniques, combines them strategically, and identifies performance bottlenecks without requiring human expertise.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] DistFlow: A Fully Distributed RL Framework for Scalable and Efficient
LLM Post-Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [distributed reinforcement learning, large language models, scalability, multi-controller architecture, GPU efficiency]</li>
<li class=""><strong>authors:</strong> Zhixin Wang, Tianyi Zhou, Liming Liu, Ao Li, Jiarui Hu, Dian Yang, Yinhui Lu, Jinlong Hou, Siyuan Feng, Yuan Cheng, Yuan Qi</li>
<li class=""><strong>institution:</strong> Shanghai Innovation Institute, Fudan University, Zhejiang University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.13833v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.13833v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DistFlow introduces a fully distributed RL framework that eliminates centralized controllers by assigning data transfer and execution tasks to all workers. This enables near-linear scalability up to 1024 GPUs and decouples resource configuration from execution logic. Experiments demonstrate up to 7x throughput improvement over state-of-the-art frameworks while maintaining excellent scalability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic
MPSoC</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [neuromorphic computing, DNN inference, edge AI, SpiNNaker2, multi-core scheduling, PyTorch deployment]</li>
<li class=""><strong>authors:</strong> Matthias Jobst, Tim Langer, Chen Liu, Mehmet Alici, Hector A. Gonzalez, Christian Mayr</li>
<li class=""><strong>institution:</strong> TU Dresden, SpiNNcloud Systems GmbH</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.13736v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.13736v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an end-to-end DNN inference framework that extends OctopuScheduler to enable PyTorch model deployment on SpiNNaker2 neuromorphic MPSoCs. The framework incorporates quantization and lowering steps to efficiently schedule complex DNNs across the chip&#x27;s 152 processing elements. It demonstrates successful edge-based execution of large-scale models including transformers while optimizing memory usage and inter-processor communication.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Checkmate: Zero-Overhead Model Checkpointing via Network Gradient
Replication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [checkpointing], [model checkpointing, gradient replication, fault tolerance, distributed training, DNN training]</li>
<li class=""><strong>authors:</strong> Ankit Bhardwaj, Weiyang Wang, Jeremy Carin, Adam Belay, Manya Ghobadi</li>
<li class=""><strong>institution:</strong> Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.13522v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.13522v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Checkmate introduces a zero-overhead checkpointing system that leverages existing network gradients during data-parallel training to create checkpoints without interrupting training. It uses a multicast abstraction to simultaneously deliver gradients to a CPU-based shadow cluster that maintains checkpoint state. The system achieves per-iteration checkpointing with comparable throughput to no-checkpoint baselines, significantly reducing repeated work during failures by 80-97.1%.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Leveraging Multi-Instance GPUs through moldable task scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU scheduling, moldable tasks, MIG, resource allocation, makespan minimization]</li>
<li class=""><strong>authors:</strong> Jorge Villarrubia, Luis Costero, Francisco D. Igual, Katzalin Olcoz</li>
<li class=""><strong>institution:</strong> Universidad Complutense de Madrid</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.13601v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.13601v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes FAR, a 3-phase moldable task scheduling algorithm that leverages NVIDIA MIG technology through dynamic GPU reconfigurations. The algorithm combines classical moldability methods with novel heuristics tailored to MIG constraints and achieves makespan improvements over state-of-the-art approaches. Experimental results show the method achieves near-optimal performance with makespan no worse than 1.22x of optimum for real benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Autonomous Resource Management in Microservice Systems via Reinforcement
Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [reinforcement learning, microservice architecture, resource allocation, system optimization, throughput]</li>
<li class=""><strong>authors:</strong> Yujun Zou, Nia Qi, Yingnan Deng, Zhihao Xue, Ming Gong, Wuyang Zhang</li>
<li class=""><strong>institution:</strong> University of California Berkeley, Georgia Institute of Technology, Rose-Hulman Institute of Technology, University of Pennsylvania, University of Massachusetts Amherst</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.12879v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.12879v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a reinforcement learning-based method for autonomous resource management in microservice systems. The approach uses intelligent scheduling algorithms that continuously optimize resource allocation through agent-environment interactions. Experimental results demonstrate significant improvements in response speed, throughput, and resource utilization while reducing energy consumption compared to traditional static allocation methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] PolyServe: Efficient Multi-SLO Serving at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM serving, multi-SLO scheduling, token generation latency, request batching, auto-scaling, tail latency management]</li>
<li class=""><strong>authors:</strong> Kan Zhu, Haiyang Shi, Le Xu, Jiaxin Shan, Arvind Krishnamurthy, Baris Kasikci, Liguang Xie</li>
<li class=""><strong>institution:</strong> University of Washington, ByteDance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.17769v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.17769v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PolyServe proposes a multi-SLO scheduling policy that groups requests by latency requirements and routes them to appropriate servers while maintaining load gradients for auto-scaling. It enables cross-SLO instance sharing and uses profiling with dynamic chunking to manage tail latency. The system achieves 1.23x goodput gain and up to 92.5% of optimal goodput compared to existing policies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO
Problems at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [QUBO optimization, solver benchmarking, GPU acceleration, simulated annealing, large-scale computation]</li>
<li class=""><strong>authors:</strong> Pei-Kun Yang</li>
<li class=""><strong>institution:</strong> I-Shou University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.17770v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.17770v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper benchmarks five software-based QUBO solvers (Neal, PyTorch CPU/GPU, JAX, SciPy) on large-scale problems using simulated annealing with varying convergence thresholds. PyTorch demonstrated the best balance between solution quality and scalability, handling up to 45,000 variables efficiently with GPU acceleration, while Neal achieved lower energies but was limited to smaller problems due to memory constraints. The study provides practical guidance for selecting QUBO solvers based on problem scale and computational resources.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] BootSeer: Analyzing and Mitigating Initialization Bottlenecks in
Large-Scale LLM Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [startup overhead, container image loading, dependency installation, checkpoint resumption, system optimization]</li>
<li class=""><strong>authors:</strong> Rui Li, Xiaoyun Zhi, Jinxin Chi, Menghan Yu, Lixin Huang, Jia Zhu, Weilun Zhang, Xing Ma, Wenjia Liu, Zhicheng Zhu, Daowen Luo, Zuquan Song, Xin Yin, Chao Xiang, Shuguang Wang, Wencong Xiao, Gene Cooperman</li>
<li class=""><strong>institution:</strong> ByteDance, Northeastern University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.12619v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.12619v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BootSeer introduces a system-level optimization framework addressing three key startup bottlenecks in LLM training: container image loading, runtime dependency installation, and model checkpoint resumption. It employs techniques including hot block record-and-prefetch, dependency snapshotting, and striped HDFS-FUSE. The system achieved 50% reduction in startup overhead when deployed in production environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Rel-HNN: Split Parallel Hypergraph Neural Network for Learning on
Relational Databases</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [hypergraph neural network, relational databases, multi-GPU training, split-parallel algorithm]</li>
<li class=""><strong>authors:</strong> Md. Tanvir Alam, Md. Ahasanul Alam, Md Mahmudur Rahman, Md. Mosaddek Khan</li>
<li class=""><strong>institution:</strong> University of Dhaka, Brac University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.12562v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.12562v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Rel-HNN, a hypergraph neural network framework that models attribute-value pairs as nodes and tuples as hyperedges to capture fine-grained relational semantics. It introduces a split-parallel training algorithm using multi-GPU execution for scalability. Experiments show Rel-HNN outperforms existing methods in classification/regression tasks and achieves up to 3.18x speedup over single-GPU training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed
Storage</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [SpMV optimization, sparse matrix storage, GPU acceleration]</li>
<li class=""><strong>authors:</strong> Junqing Lin, Jingwei Sun, Mingge Lu, Guangzhong Sun</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.12205v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.12205v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes EC-SpMV, a GPU-optimized approach for sparse matrix-vector multiplication in sparse LLMs. It introduces hierarchical block extraction and a compressed sparse format (EC-CSR) using delta indexing. The method achieves up to 6.44× speedup over existing SpMV libraries and reduces storage overhead by 55.4% compared to CSR format.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] NineToothed: A Triton-Based High-Level Domain-Specific Language for
Machine Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [domain-specific language, parallel programming, tensor-oriented metaprogramming, code generation, Triton]</li>
<li class=""><strong>authors:</strong> Jiacheng Huang, Zimin Li, Yinghui Li, Haojie Wang</li>
<li class=""><strong>institution:</strong> Qiyuan Lab, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.11978v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.11978v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> NineToothed introduces a high-level domain-specific language with serial semantics that automatically transforms serial code into parallel code using tensor-oriented metaprogramming. The system employs an arrange-and-apply paradigm to express tiled computations without managing low-level details. Evaluation shows NineToothed significantly simplifies kernel development while maintaining performance comparable to Triton.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Incentivised Orchestrated Training Architecture (IOTA): A Technical
Primer for Release</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [decentralized training, blockchain incentives, pipeline parallelism, activation compression, distributed systems]</li>
<li class=""><strong>authors:</strong> Felix Quinque, Alan Aboudib, Szymon Fonau, Rodrigo Lopez Portillo Alcocer, Brian McCrindle, Steffen Cruz</li>
<li class=""><strong>institution:</strong> Macrocosmos AI</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.17766v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.17766v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> IOTA introduces a decentralized training architecture that enables collaborative LLM pretraining across distributed miners using data- and pipeline-parallel approaches with activation compression. The system implements granular incentive mechanisms and fair attribution schemes to reward contributors proportionally while detecting exploits. This approach allows scaling model sizes beyond single-machine VRAM constraints while maintaining training efficiency in adversarial environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] BlockBPE: Parallel BPE Tokenization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [tokenization, GPU acceleration, BPE, batch processing, parallel computing]</li>
<li class=""><strong>authors:</strong> Amos You</li>
<li class=""><strong>institution:</strong> UC Berkeley</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.11941v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.11941v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BlockBPE introduces a parallel GPU implementation of byte-pair encoding tokenization that eliminates Regex pre-tokenization to enable highly parallelized token merges. This approach reduces time complexity from O(n log n) to O(nd) and achieves up to 2x higher throughput than existing tokenizers like tiktoken and HuggingFace Tokenizers in high-batch inference workloads. The method demonstrates significant performance improvements for GPU-based batch inference while maintaining acceptable generation quality.</li>
</ul>
</li>
<li class="">
<p><em><em>[arXiv2510] A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to
IDA</em> and BTS</em>*</p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [GPU parallel computing, depth-first search, heuristic batching, neural network heuristics, IDA*, BTS, cost-bounded search]</li>
<li class=""><strong>authors:</strong> Ehsan Futuhi, Nathan R. Sturtevant</li>
<li class=""><strong>institution:</strong> University of Alberta, Alberta Machine Intelligence Institute (Amii)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.11916v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.11916v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a parallel CPU-GPU framework for cost-bounded depth-first search that batches GPU computations to enhance algorithms like IDA* and BTS. It maintains optimality guarantees while efficiently utilizing neural network heuristics through batched evaluations. Experiments on Rubik&#x27;s Cube and sliding tile puzzles demonstrate significant performance improvements with GPU batching in DFS.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Arctic Inference with Shift Parallelism: Fast and Efficient Open Source
Inference System for Enterprise AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Shift Parallelism, speculative decoding, SwiftKV, embedding optimization, vLLM plugin]</li>
<li class=""><strong>authors:</strong> Samyam Rajbhandari, Mert Hidayetoglu, Aurick Qiao, Ye Wang, Juncheng Yang, Jeff Rasley, Michael Wyatt, Yuxiong He</li>
<li class=""><strong>institution:</strong> Snowflake AI Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.11830v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.11830v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Arctic Inference introduces Shift Parallelism, a dynamic strategy that adapts to traffic patterns while integrating speculative decoding and compute reduction techniques. It achieves significant speedups in request completion and generation throughput compared to existing systems. The system delivers cost-effective inference for enterprise AI and is now available as open-source.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] ELK: Exploring the Efficiency of Inter-core Connected AI Chips with Deep
Learning Compiler Techniques</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [deep learning compiler, inter-core connected AI chips, memory optimization, performance modeling]</li>
<li class=""><strong>authors:</strong> Yiqi Liu, Yuqi Xue, Noelle Crawford, Jilong Xue, Jian Huang</li>
<li class=""><strong>institution:</strong> UIUC, Microsoft Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.11506v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.11506v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Elk introduces a deep learning compiler framework that optimizes performance on inter-core connected AI chips by jointly managing compute, communication, and I/O trade-offs through inductive operator scheduling and cost-aware memory allocation. It achieves 94% of ideal roofline performance on average and enables efficient execution of large DL models while supporting architecture design space exploration for new ICCA chips.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed
Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [spatiotemporal graph neural networks, distributed training, memory optimization]</li>
<li class=""><strong>authors:</strong> Seth Ockerman, Amal Gueroudji, Tanwi Mallick, Yixuan He, Line Pouchard, Robert Ross, Shivaram Venkataraman</li>
<li class=""><strong>institution:</strong> University of Wisconsin-Madison, Argonne National Laboratory, Arizona State University, Sandia National Laboratories</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.11683v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.11683v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PGT-I, a memory-efficient distributed training framework for spatiotemporal graph neural networks. It proposes index-batching and distributed-index-batching techniques that dynamically construct snapshots at runtime to reduce memory overhead. The approach achieves up to 89% memory reduction and 11.78x speedup, enabling training on the full PeMS dataset without graph partitioning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Quantifying the Energy Consumption and Carbon Emissions of LLM Inference
via Simulations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [energy consumption, carbon emissions, GPU power modeling, simulation framework, carbon-aware scheduling]</li>
<li class=""><strong>authors:</strong> Miray Özcan, Philipp Wiesner, Philipp Weiß, Odej Kao</li>
<li class=""><strong>institution:</strong> Minerva University, Technische Universität Berlin</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.11417v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.11417v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a simulation framework that extends LLM inference simulation with GPU power modeling and integrates it with energy system co-simulation to quantify carbon emissions. The framework analyzes how inference parameters affect energy demand and demonstrates up to 69.2% renewable offset potential through carbon-aware scheduling, providing foundations for sustainable LLM deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data
Placement</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [RAG acceleration, mixed precision, data placement, KV cache optimization]</li>
<li class=""><strong>authors:</strong> Danying Ge, Jianhua Gao, Yixue Yang, Weixing Ji</li>
<li class=""><strong>institution:</strong> Beijing Normal University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20878v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20878v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HA-RAG proposes a hotness-aware optimization system that uses mixed-precision compression for frequently accessed KV chunks and intelligent data placement strategies. This approach reduces disk I/O and memory access overhead while improving data access efficiency. Experimental results show HA-RAG achieves up to 10.49× speedup in Time-To-First-Token with negligible accuracy loss compared to TurboRAG.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Compositional Monte Carlo Tree Diffusion for Extendable Planning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [diffusion models, trajectory planning, Monte Carlo Tree Search, compositional planning, long-horizon tasks]</li>
<li class=""><strong>authors:</strong> Jaesik Yoon, Hyeonseo Cho, Sungjin Ahn</li>
<li class=""><strong>institution:</strong> KAIST, SAP, NYU</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21361v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21361v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Compositional Monte Carlo Tree Diffusion (C-MCTD), which extends MCTD by introducing three components for globally-aware planning across complete plan compositions. This framework enables reasoning beyond individual trajectories through online, distributed, and preplan composers. The approach addresses limitations in training trajectory lengths and achieves improved planning performance in complex long-horizon tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Symbolic Regression and Differentiable Fits in Beyond the Standard Model
Physics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [symbolic regression, particle physics, supersymmetric models, differentiable methods, neural network comparison]</li>
<li class=""><strong>authors:</strong> Shehu AbdusSalam, Steven Abel, Deaglan Bartlett, Miguel Crispim Romão</li>
<li class=""><strong>institution:</strong> Shahid Beheshti University, Durham University, CNRS, Sorbonne Université, University of Oxford</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20453v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20453v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper applies symbolic regression to derive analytical expressions for physical observables in Beyond Standard Model physics, specifically the Constrained Minimal Supersymmetric Standard Model. The symbolic expressions accurately represent Higgs mass, dark matter relic density, and muon magnetic moment contributions. The method enables efficient global parameter fitting using differentiable approaches and shows advantages over neural networks in robustness across parameter space.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficient Meningioma Tumor Segmentation Using Ensemble Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [ensemble learning, brain tumor segmentation, medical imaging, deep learning, computational efficiency]</li>
<li class=""><strong>authors:</strong> Mohammad Mahdi Danesh Pajouh, Sara Saeedi</li>
<li class=""><strong>institution:</strong> University of Calgary</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21040v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21040v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an ensemble learning approach combining three neural network architectures (SegResNet, attention-augmented SegResNet, and DDUNet) for meningioma tumor segmentation from MRI scans. The method achieves competitive Dice scores of 77.30% (ET), 76.37% (TC), and 73.9% (WT) while requiring only 20 training epochs. The results demonstrate that ensemble learning provides an effective and computationally efficient solution for brain tumor segmentation under hardware constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] MOBO-OSD: Batch Multi-Objective Bayesian Optimization via Orthogonal
Search Directions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-objective Bayesian optimization, Pareto optimal solutions, orthogonal search directions, batch optimization]</li>
<li class=""><strong>authors:</strong> Lam Ngo, Huong Ha, Jeffrey Chan, Hongyu Zhang</li>
<li class=""><strong>institution:</strong> RMIT University, Chongqing University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20872v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20872v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MOBO-OSD proposes a multi-objective Bayesian optimization method using orthogonal search directions to generate diverse Pareto optimal solutions through constrained subproblems. It incorporates Pareto Front Estimation for solution density and supports batch optimization for parallel evaluation. Experimental results show it consistently outperforms state-of-the-art algorithms across various benchmark functions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework
For Controllable Lyric Translation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [lyric translation, curriculum learning, reinforcement learning, unsupervised learning, controllable translation]</li>
<li class=""><strong>authors:</strong> Le Ren, Xiangjian Zeng, Qingqiang Wu, Ruoxuan Liang</li>
<li class=""><strong>institution:</strong> Xiamen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19967v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19967v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LyriCAR proposes a difficulty-aware curriculum reinforcement learning framework for unsupervised lyric translation, introducing adaptive curriculum strategies to guide model training. The method achieves state-of-the-art performance on EN-ZH translation tasks while reducing training steps by 40%. It effectively balances musical constraints like rhyme and cross-line coherence through progressive learning of complex challenges.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] AdaSPEC: Selective Knowledge Distillation for Efficient Speculative
Decoders</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [speculative decoding, knowledge distillation, token filtering, draft model, target model, token acceptance rate]</li>
<li class=""><strong>authors:</strong> Yuezhou Hu, Jiaxin Guo, Xinyu Feng, Tuo Zhao</li>
<li class=""><strong>institution:</strong> University of California, Berkeley, Tsinghua University, Georgia Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19779v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19779v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AdaSPEC introduces selective token filtering during knowledge distillation to improve speculative decoding. It uses a reference model to filter difficult tokens, enabling better draft model alignment on simpler tokens. The method achieves up to 15% higher token acceptance rates than DistillSpec across various tasks without compromising generation quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized
Linear Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [coordinate descent, elastic net, linear models, optimization algorithms, performance optimization]</li>
<li class=""><strong>authors:</strong> Yixiao Wang, Zishan Shao, Ting Jiang, Aditya Devarakonda</li>
<li class=""><strong>institution:</strong> Duke University, Wake Forest University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19999v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19999v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an enhanced cyclic coordinate descent method that uses Taylor expansion approximations to unroll vector recurrences, enabling more efficient batched computations for elastic net penalized linear models. The method achieves 3× faster training times compared to state-of-the-art solvers while maintaining convergence properties. Implementation in C++ with Eigen library demonstrates consistent performance improvements across diverse benchmark datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Not-a-Bandit: Provably No-Regret Drafter Selection in Speculative
Decoding for LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [speculative decoding, drafter selection, online learning, model acceleration, token acceptance]</li>
<li class=""><strong>authors:</strong> Hongyi Liu, Jiaji Huang, Zhen Jia, Youngsuk Park, Yu-Xiang Wang</li>
<li class=""><strong>institution:</strong> Rice University, Amazon Web Services, University of California San Diego</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20064v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20064v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes HedgeSpec, an online algorithm for draft model selection in speculative decoding that evaluates all draft models simultaneously without extra target model queries. The method achieves provable no-regret performance guarantees and exponential improvement over bandit-based approaches. Experimental results show substantial speedup improvements over state-of-the-art baselines across diverse domains, particularly for long reasoning chains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Survey on Cache Methods in Diffusion Models: Toward Efficient
Multi-Modal Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [diffusion models, cache methods, inference acceleration, feature reuse, computational efficiency]</li>
<li class=""><strong>authors:</strong> Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Tsinghua University, The Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19755v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19755v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This survey introduces Diffusion Caching as a training-free method that identifies and reuses computational redundancies in diffusion models through feature-level cross-step reuse and inter-layer scheduling. The approach reduces computational overhead without modifying model parameters while maintaining generation quality. The paper shows caching methods evolve from static reuse to dynamic prediction and can integrate with other acceleration techniques for efficient multimodal generation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Learning Upper Lower Value Envelopes to Shape Online RL: A Principled
Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [reinforcement learning, offline data, value envelopes, regret bounds, online RL]</li>
<li class=""><strong>authors:</strong> Sebastian Reboul, Hélène Halconruy, Randal Douc</li>
<li class=""><strong>institution:</strong> SAMOVAR, Télécom SudParis, Institut Polytechnique de Paris, Wiremind Cargo, Modal&#x27;X, Université Paris-Nanterre</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19528v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19528v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a principled two-stage framework that uses offline data to learn upper and lower value function bounds, then incorporates these envelopes into online reinforcement learning algorithms. This approach enables more flexible and tighter approximations than prior methods. Empirical results show substantial regret reductions compared to existing approaches like UCBVI.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Learning Noise-Resilient and Transferable Graph-Text Alignment via
Dynamic Quality Assessment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [graph-text alignment, multimodal learning, noise resilience, dynamic quality assessment, graph foundation models]</li>
<li class=""><strong>authors:</strong> Yuhang Liu, Minglai Shao, Zengyi Wo, Yunlong Chu, Bing Hao, Shengzhong Liu, Ruijie Wang, Jianxin Li</li>
<li class=""><strong>institution:</strong> Tianjin University, Baidu, Shanghai Jiao Tong University, Beihang University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19384v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19384v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ADAligner proposes a dynamic graph-text alignment framework that adaptively switches between many-to-many and one-to-one alignment strategies based on real-time quality assessment of supervision signals. It demonstrates superior performance across various graph learning tasks while maintaining robustness to noise and achieving 2-3× faster pre-training compared to existing methods. The approach establishes a scalable foundation for multimodal graph representation learning in real-world web environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Transforming Multi-Omics Integration with GANs: Applications in
Alzheimer&#x27;s and Cancer</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [multi-omics integration, generative adversarial networks, Alzheimer&#x27;s disease, cancer, synthetic data generation, biomarker discovery, drug repurposing]</li>
<li class=""><strong>authors:</strong> Md Selim Reza, Sabrin Afroz, Mostafizer Rahman, Md Ashad Alam</li>
<li class=""><strong>institution:</strong> Based on author names and content, specific institutions are not explicitly mentioned. Email suffixes would be needed for precise institutional affiliation.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19870v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19870v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Omics-GAN, a Generative Adversarial Network framework that generates synthetic multi-omics data to improve disease prediction accuracy. The method was validated on Alzheimer&#x27;s and cancer datasets, showing enhanced classification performance and preserved biological relationships. The approach also identified potential drug repurposing candidates, demonstrating utility in accelerating biomarker and drug discovery.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Enhancing Fractional Gradient Descent with Learned Optimizers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [fractional gradient descent, meta-learning, hyperparameter optimization, optimization algorithms]</li>
<li class=""><strong>authors:</strong> Jan Sobotka, Petr Šimánek, Pavel Kordík</li>
<li class=""><strong>institution:</strong> Czech Technical University in Prague</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18783v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18783v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes L2O-CFGD, a method that meta-learns dynamic hyperparameter schedules for Caputo Fractional Gradient Descent. This learned approach outperforms static hyperparameter configurations and achieves performance comparable to black-box meta-learned optimizers. The method provides insights into leveraging fractional calculus&#x27;s history-dependent properties for improved optimization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural
Networks Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [neural network compression, structured pruning, explainable AI, one-shot pruning, model efficiency]</li>
<li class=""><strong>authors:</strong> Baptiste Bauvin, Loïc Baret, Ola Ahmad</li>
<li class=""><strong>institution:</strong> Thales Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18636v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18636v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes C-SWAP, an explainability-aware structured pruning framework that uses causal relationships between model predictions and structures for one-shot neural network compression. The method progressively removes structures without performance degradation and requires no fine-tuning. Experiments on CNNs and Vision Transformers show substantial model size reduction with minimal performance impact, outperforming existing approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Lyapunov-Aware Quantum-Inspired Reinforcement Learning for
Continuous-Time Vehicle Control: A Feasibility Study</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [quantum reinforcement learning, Lyapunov stability, vehicle control, variational quantum circuits, adaptive cruise control]</li>
<li class=""><strong>authors:</strong> Nutkritta Kraipatthanapong, Natthaphat Thathong, Pannita Suksawas, Thanunnut Klunklin, Kritin Vongthonglua, Krit Attahakul, Aueaphum Aueawatthanaphisut</li>
<li class=""><strong>institution:</strong> Sirindhorn International Institute of Technology, Thammasat University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18852v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18852v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Lyapunov-Based Quantum Reinforcement Learning framework that integrates variational quantum circuits with stability-aware policy gradients for continuous-time vehicle control. The method combines quantum policy optimization with Lyapunov stability constraints to ensure safe decision-making in dynamic environments. Experimental results demonstrate successful integration of stability verification into quantum policy learning, maintaining bounded state evolution while validating the feasibility of safety-guaranteed quantum reinforcement learning architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion
Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [Vision Foundation Models, Latent Diffusion Models, Tokenizer Design, Variational Autoencoder, Multi-Scale Latent Fusion, Progressive Resolution Reconstruction]</li>
<li class=""><strong>authors:</strong> Tianci Bi, Xiaoyi Zhang, Yan Lu, Nanning Zheng</li>
<li class=""><strong>institution:</strong> Xi&#x27;an Jiaotong University, Microsoft Research Asia</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18457v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18457v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes VFM-VAE, a direct integration approach that uses Vision Foundation Models as tokenizers for Latent Diffusion Models, bypassing distillation methods. The method introduces Multi-Scale Latent Fusion and Progressive Resolution Reconstruction blocks to maintain semantic alignment while achieving high-quality reconstruction. The approach achieves 10x faster convergence and superior performance with gFID scores of 2.20 in 80 epochs and 1.62 in 640 epochs, establishing direct VFM integration as an effective paradigm for LDMs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with
Projected Gradient-Aligned Perturbations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [finetuning], [zeroth-order optimization, large language models, gradient estimation, memory efficiency, parameter-efficient fine-tuning]</li>
<li class=""><strong>authors:</strong> Zhendong Mi, Qitao Tan, Grace Li Zhang, Zhaozhuo Xu, Geng Yuan, Shaoyi Huang</li>
<li class=""><strong>institution:</strong> Stevens Institute of Technology, University of Georgia, Technical University of Darmstadt</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18228v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18228v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes P-GAP, a zeroth-order optimization method that estimates a low-dimensional gradient space and aligns perturbations with projected gradients to reduce variance and accelerate convergence. Experiments show P-GAP achieves up to 6-12% higher accuracy on classification and generation tasks while reducing training iterations by 81% and GPU hours by 70%. This demonstrates P-GAP enables fast, scalable, and resource-efficient fine-tuning of large language models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking
Enhanced Remasking for Diffusion Language Model</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [diffusion language models, code generation, sampling algorithm, adaptive acceleration, backtracking mechanism]</li>
<li class=""><strong>authors:</strong> Yihong Dong, Zhaoyu Ma, Xue Jiang, Zhiyuan Fan, Jiaru Qian, Yongmin Li, Jianha Xiao, Zhi Jin, Rongyu Cao, Binhua Li, Fei Huang, Yongbin Li, Ge Li</li>
<li class=""><strong>institution:</strong> Peking University, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18165v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18165v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Saber introduces a training-free sampling algorithm for diffusion language models that uses adaptive acceleration and backtracking mechanisms to improve code generation. The method achieves an average 1.9% improvement in Pass@1 accuracy and 251.4% inference speedup compared to existing DLM sampling approaches. This significantly narrows the performance gap between diffusion models and autoregressive models in code generation tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] One-step Diffusion Models with Bregman Density Ratio Matching</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [diffusion models, distillation, Bregman divergence, density-ratio matching, one-step generation]</li>
<li class=""><strong>authors:</strong> Yuanzhi Zhu, Eleftherios Tsonis, Lucas Degeorge, Vicky Kalogeiton</li>
<li class=""><strong>institution:</strong> LIX, École Polytechnique, CNRS, IPP, LIGM, École Nationale des Ponts et Chaussées</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16983v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16983v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Di-Bregman, a framework that formulates diffusion distillation as Bregman divergence-based density-ratio matching to accelerate sampling. The method connects various existing objectives through a unified theoretical lens and achieves improved one-step FID over reverse-KL distillation while maintaining visual fidelity comparable to teacher models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean
using Physics-informed Neural Network</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [physics-informed neural network, sea ice prediction, Arctic Ocean, machine learning, environmental modeling]</li>
<li class=""><strong>authors:</strong> Younghyun Koo, Maryam Rahnemoonfar</li>
<li class=""><strong>institution:</strong> Lehigh University, University of Colorado Boulder, National Snow and Ice Data Center</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.17756v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.17756v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a physics-informed neural network (PINN) approach using a Hierarchical Information-sharing U-net architecture to predict sea ice velocity and concentration in the Arctic Ocean. The model incorporates physical knowledge through physics loss and activation functions to improve physical consistency. Results show the PINN model outperforms purely data-driven approaches, particularly in challenging conditions like melting seasons and fast-moving ice regions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Improving Model Representation and Reducing KV Cache via Skip
Connections with First Value Heads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [KV cache reduction, skip connections, model efficiency, transformer architecture, mesa-optimization]</li>
<li class=""><strong>authors:</strong> Zhoutong Wu, Yuan Zhang, Yiming Dong, Chenheng Zhang, Cong Fang, Kun Yuan, Zhouchen Lin</li>
<li class=""><strong>institution:</strong> Peking University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16807v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16807v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SkipV1Former introduces skip connections from the first layer&#x27;s Value heads to deeper layers, reusing half of them to reduce KV cache size by approximately 25% while improving perplexity. This method enhances model representation by restoring compressed information and accelerates implicit mesa-optimization. It can be combined with other techniques like Group-Query Attention for further efficiency gains and supports uptraining existing models with minimal additional compute.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Mixed-Precision Quantization for Language Models: Techniques and
Prospects</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [mixed-precision quantization, model compression, post-training quantization, quantization-aware training, bit allocation]</li>
<li class=""><strong>authors:</strong> Mariam Rakka, Marios Fournarakis, Olga Krestinskaya, Jinane Bazzi, Khaled N. Salama, Fadi Kurdahi, Ahmed M. Eltawil, Mohammed E. Fouda</li>
<li class=""><strong>institution:</strong> University of California Irvine, King Abdullah University of Science and Technology, Wayve AI, RAIN AI</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16805v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16805v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This survey explores mixed-precision quantization techniques for language models, selectively allocating different bit precisions across model components to balance efficiency and accuracy. It compares various frameworks based on bit allocation strategies and analyzes their impact on perplexity and task performance. The work identifies key challenges and future directions including hardware-aware design and scalable optimization methods for large models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] TritonRL: Training LLMs to Think and Code Triton Without Cheating</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [Triton kernel generation, reinforcement learning, reward hacking detection, hierarchical rewards, automated kernel synthesis]</li>
<li class=""><strong>authors:</strong> Jiin Woo, Shaowei Zhu, Allen Nie, Zhen Jia, Yida Wang, Youngsuk Park</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, Amazon Web Services</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.17891v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.17891v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> TritonRL introduces a specialized LLM trained with supervised fine-tuning and reinforcement learning using verifiable rewards and hierarchical reward assignment to generate high-performance Triton kernels. The framework addresses data scarcity and reward hacking through robust verification mechanisms. Experiments show TritonRL achieves state-of-the-art correctness and speedup on KernelBench, outperforming other Triton-specific models.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Long-Context Attention Benchmark: From Kernel Efficiency to Distributed
Context Parallelism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [attention mechanism, distributed training, kernel optimization, context parallelism, benchmark evaluation]</li>
<li class=""><strong>authors:</strong> Tao Bu, Qiangang Wang, Bowen Zeng, Hanwen Sun, Yunpeng Huang, Chun Cao, Jingwei Xu</li>
<li class=""><strong>institution:</strong> Nanjing University, Zhejiang University, Peking University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.17896v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.17896v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a unified benchmark for evaluating long-context attention mechanisms, integrating both kernel-level optimizations and distributed context parallel strategies. The benchmark systematically assesses methods based on attention mask patterns and sequence length scalability across up to 96 GPUs. It provides reproducible comparisons and practical guidance for designing efficient attention mechanisms in large-scale LLM training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Fast and Compact Tsetlin Machine Inference on CPUs Using
Instruction-Level Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [Tsetlin Machine, CPU optimization, bitwise operations, early exit mechanism, literal reordering]</li>
<li class=""><strong>authors:</strong> Yefan Zeng, Shengyu Duan, Rishad Shafik, Alex Yakovlev</li>
<li class=""><strong>institution:</strong> Newcastle University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15653v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15653v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an optimized Tsetlin Machine implementation using instruction-level bitwise operations for compact model representation and faster inference. It introduces an early exit mechanism and literal reordering strategy to reduce unnecessary computations. Experimental results show up to 96.71% inference time reduction compared to conventional integer-based implementations while maintaining code density.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] KS-Net: Multi-layer network model for determining the rotor type from
motor parameters in interior PMSMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [machine learning, electric motors, rotor classification, IPMSM, deep learning]</li>
<li class=""><strong>authors:</strong> Kivanc Dogan, Ahmet Orhan</li>
<li class=""><strong>institution:</strong> Firat University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15688v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15688v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes KS-Net, a deep learning model for classifying rotor types in Interior Permanent Magnet Synchronous Motors using electromagnetic parameters. The model was evaluated against traditional machine learning algorithms and achieved 99.98% accuracy with only two misclassifications. The study demonstrates that data-driven approaches can effectively predict rotor shapes, offering a faster alternative to finite element method-based analyses for motor design optimization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Expert Merging in Sparse Mixture of Experts with Nash Bargaining</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [Mixture of Experts, Nash Bargaining, expert merging, model scaling, sparse networks]</li>
<li class=""><strong>authors:</strong> Dung V. Nguyen, Anh T. Nguyen, Minh H. Nguyen, Luc Q. Nguyen, Shiqi Jiang, Ethan Fetaya, Linh Duy Tran, Gal Chechik, Tan M. Nguyen</li>
<li class=""><strong>institution:</strong> National University of Singapore, Viettel AI, Hanoi University of Science and Technology, Bar Ilan University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16138v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16138v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces NAMEx, a novel framework that incorporates Nash Bargaining theory into expert merging for Sparse Mixture of Experts, enabling more balanced collaboration. The method also uses complex momentum to accelerate convergence with theoretical guarantees. Experiments show NAMEx consistently outperforms competing methods across various tasks and scales effectively to large models like Qwen1.5-MoE and DeepSeek-MoE.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Time-Embedded Algorithm Unrolling for Computational MRI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [ai for science], [computational MRI, algorithm unrolling, time-embedding, inverse problems, image reconstruction]</li>
<li class=""><strong>authors:</strong> Junno Yun, Yaşar Utku Alçalar, Mehmet Akçakaya</li>
<li class=""><strong>institution:</strong> University of Minnesota</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16321v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16321v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a time-embedded algorithm unrolling method for MRI reconstruction, incorporating iteration-dependent parameters inspired by AMP and diffusion models. The approach frames proximal operations and Onsager corrections as time-embedded networks with learnable parameters. Experiments on fastMRI demonstrate reduced artifacts and noise amplification, achieving state-of-the-art performance while maintaining computational efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Stochastic Optimization with Random Search</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [stochastic optimization, random search, variance reduction, zeroth-order optimization, finite-sum setting]</li>
<li class=""><strong>authors:</strong> El Mahdi Chayti, Taha El Bakkali El Kadi, Omar Saadi, Martin Jaggi</li>
<li class=""><strong>institution:</strong> EPFL, UM6P College of Computing</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15610v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15610v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper revisits random search methods for stochastic optimization, proposing a variance-reduced variant that leverages multiple samples to accelerate convergence. The analysis shows these methods work under weaker smoothness assumptions than previously considered, with stronger assumptions enabling improved guarantees. The approach relies on a translation invariance property to balance noise and reduce variance effectively.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only
Robotic Manipulation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [robotic manipulation, diffusion policy, vision-only, semantic-geometric fusion, imitation learning]</li>
<li class=""><strong>authors:</strong> Zehao Ni, Yonghao He, Lingfeng Qian, Jilei Mao, Fa Fu, Wei Sui, Hu Su, Junran Peng, Zhipeng Wang, Bin He</li>
<li class=""><strong>institution:</strong> D-ROBOTICS, National Key Laboratory of Autonomous Intelligent Unmanned Systems, University of Science and Technology Beijing, State Key Laboratory of Multimodal Artificial Intelligence System, Frontiers Science Center for Intelligent Autonomous Systems, Shanghai Institute of Intelligent Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15530v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15530v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> VO-DP proposes a vision-only diffusion policy method that fuses semantic features from DINOv2 and geometric features from VGGT using cross-attention for robotic manipulation. The method achieves performance comparable to point cloud-based methods in simulation and superior performance in real-world tasks while maintaining robustness to various environmental changes. The authors also open-source a training library supporting multiple visuomotor policies and simulators.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-28T02:33:32.000Z" itemprop="dateModified">Oct 28, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20251020-20251026"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251020-20251026</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/category/paper"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Paper</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-10-27" class="table-of-contents__link toc-highlight">2025-10-27</a></li><li><a href="#2025-10-28" class="table-of-contents__link toc-highlight">2025-10-28</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>