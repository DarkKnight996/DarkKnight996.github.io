<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251006-20251012" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251006-20251012 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20251006-20251012"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251006-20251012 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-10-06"><meta data-rh="true" property="og:description" content="2025-10-06"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20251006-20251012"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251006-20251012" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251006-20251012" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20251006-20251012","item":"https://darkknight996.github.io/daily/20251006-20251012"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.2b8e063a.js" defer="defer"></script>
<script src="/assets/js/main.d91e867f.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251006-20251012</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251006-20251012</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-06">2025-10-06<a href="#2025-10-06" class="hash-link" aria-label="Direct link to 2025-10-06" title="Direct link to 2025-10-06" translate="no">​</a></h2>
<ul>
<li class=""><strong>[arXiv2510] OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM
Training</strong>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [pipeline parallelism, memory optimization, scheduling optimization, activation offloading]</li>
<li class=""><strong>authors:</strong> Hongpei Li, Han Zhang, Huikang Liu, Dongdong Ge, Yinyu Ye</li>
<li class=""><strong>institution:</strong> Shanghai University of Finance and Economics, National University of Singapore, Shanghai Jiao Tong University, Stanford University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05186v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05186v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes OptPipe, a pipeline parallelism optimization method that formulates scheduling as a constrained optimization problem to balance memory usage and computation efficiency. It dynamically optimizes activation offloading and pipeline scheduling based on model structure and hardware configuration. Experimental results show up to 50% reduction in pipeline idle time and improved throughput under memory constraints.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-07">2025-10-07<a href="#2025-10-07" class="hash-link" aria-label="Direct link to 2025-10-07" title="Direct link to 2025-10-07" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Adaptive Protein Design Protocols and Middleware</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [protein design, high-performance computing, machine learning, adaptive protocols, computational biology]</li>
<li class=""><strong>authors:</strong> Aymen Alsaadi, Jonathan Ash, Mikhail Titov, Matteo Turilli, Andre Merzky, Shantenu Jha, Sagar Khare</li>
<li class=""><strong>institution:</strong> Rutgers University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.06396v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.06396v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> IMPRESS integrates machine learning with high-performance computing for computational protein design. It develops adaptive protocols and middleware that enable dynamic resource allocation and asynchronous workload execution. This approach improves protein design quality consistency and increases throughput efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Toward Systems Foundations for Agentic Exploration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [checkpointing], [agentic exploration, snapshot/restore mechanisms, fork semantics, external side-effects, native forking]</li>
<li class=""><strong>authors:</strong> Jiakai Xu, Tianle Zhou, Eugene Wu, Kostis Kaffes</li>
<li class=""><strong>institution:</strong> Columbia University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05556v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05556v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper benchmarks six snapshot/restore mechanisms and finds current tools like CRIU are too slow for agentic exploration. The authors identify three fundamental challenges: fork semantics, external side-effects, and native forking. They argue that existing systems cannot efficiently support LLM-powered agents that need to branch, backtrack and search across execution paths.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] MuFASA -- Asynchronous Checkpoint for Weakly Consistent Fully Replicated
Databases</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [checkpointing], [distributed databases, eventual consistency, asynchronous checkpointing, snapshot algorithms, replicated systems]</li>
<li class=""><strong>authors:</strong> Raaghav Ravishankar, Sandeep Kulkarni, Nitin H Vaidya</li>
<li class=""><strong>institution:</strong> Michigan State University, Georgetown University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.06404v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.06404v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MuFASA introduces an asynchronous checkpointing algorithm for weakly consistent fully replicated databases that creates strongly consistent snapshots with minimal overhead. The method requires only O(n) new messages and adds a single counter to existing messages. This approach helps identify anomalies in eventually consistent systems by focusing on snapshots around the time of occurrence.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] EARL: Efficient Agentic Reinforcement Learning Systems for Large
Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [agentic reinforcement learning, distributed training, dynamic parallelism, memory optimization]</li>
<li class=""><strong>authors:</strong> Zheyue Tan, Mustapha Abdullahi, Tuo Shi, Huining Yuan, Zelai Xu, Chao Yu, Boxun Li, Bo Zhao</li>
<li class=""><strong>institution:</strong> Aalto University, Tsinghua University, Infinigence-AI</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05943v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05943v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> EARL introduces a parallelism selector that dynamically adjusts model and training parallelism based on sequence length and system load, along with a data dispatcher for decentralized data exchange. These components enhance throughput, reduce out-of-memory failures, and enable stable large-scale training of agentic LLMs without context length penalties. The system effectively addresses bottlenecks in memory usage and cross-device data movement during agentic RL training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Safe and Compliant Cross-Market Trade Execution via Constrained RL and
Zero-Knowledge Audits</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [reinforcement learning, algorithmic trading, constrained Markov decision process, zero-knowledge proofs, compliance enforcement, safe AI]</li>
<li class=""><strong>authors:</strong> Ailiya Borjigin, Cong He</li>
<li class=""><strong>institution:</strong> Probe Group Pte. Ltd.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.04952v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.04952v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a cross-market algorithmic trading system using constrained reinforcement learning with proximal policy optimization and runtime action shielding. The system incorporates zero-knowledge compliance audits to verify constraint satisfaction without revealing proprietary information. Experimental results show the approach reduces implementation shortfall and variance while maintaining strict compliance across various stress scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] When Does Global Attention Help? A Unified Empirical Study on Atomistic
Graph Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [graph neural networks, global attention, atomistic graph learning, benchmarking framework, message passing neural networks, graph transformers]</li>
<li class=""><strong>authors:</strong> Arindam Chowdhury, Massimiliano Lupo Pasini</li>
<li class=""><strong>institution:</strong> Oak Ridge National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05583v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05583v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a unified benchmarking framework to systematically evaluate global attention mechanisms in atomistic graph learning. The study compares four model classes including MPNNs and graph transformers across seven datasets. Results show encoder-augmented MPNNs provide robust baselines, while fused local-global models benefit properties with long-range interactions, with attention mechanisms showing accuracy-compute trade-offs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data
Movement Forecasting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Mixture of Experts, data movement forecasting, expert selection, performance optimization, wafer-scale GPUs, trace analysis]</li>
<li class=""><strong>authors:</strong> Zhongkai Yu, Yue Guan, Zihao Yu, Chenyang Zhou, Shuyi Pei, Yangwook Kang, Yufei Ding, Po-An Tsai</li>
<li class=""><strong>institution:</strong> UCSD</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.05497v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.05497v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts comprehensive data movement profiling of large-scale MoE LLMs and identifies key patterns through temporal and spatial analysis. The authors propose architectural modifications based on their insights to optimize expert selection and data movement. Their approach achieves significant speedups (6.3× on DeepSeek V3 and 4.0× on Qwen3) in multi-unit serving systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] On-Package Memory with Universal Chiplet Interconnect Express (UCIe): A
Low Power, High Bandwidth, Low Latency and Low Cost Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [memory], [chiplet interconnect, on-package memory, UCIe, bandwidth density, low latency, power efficiency]</li>
<li class=""><strong>authors:</strong> Debendra Das Sharma, Swadesh Choudhary, Peter Onufryk, Rob Pelt</li>
<li class=""><strong>institution:</strong> Intel Corporation, AMD Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.06513v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.06513v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes enhancing UCIe with memory semantics to create power-efficient on-package memory solutions by reusing existing LPDDR6 and HBM memory through a logic die or enabling DRAM dies to natively support UCIe. The approaches achieve significantly higher bandwidth density (up to 10x), lower latency (up to 3x), lower power consumption (up to 3x), and reduced cost compared to existing HBM4 and LPDDR solutions, addressing memory bottlenecks in AI applications.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-08">2025-10-08<a href="#2025-10-08" class="hash-link" aria-label="Direct link to 2025-10-08" title="Direct link to 2025-10-08" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Vectorized FlashAttention with Low-cost Exponential Computation in
RISC-V Vector Processors</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [FlashAttention, RISC-V vector processors, exponential approximation, tiling strategies, performance optimization]</li>
<li class=""><strong>authors:</strong> Vasileios Titopoulos, Kosmas Alexandridis, Giorgos Dimitrakopoulos</li>
<li class=""><strong>institution:</strong> Democritus University of Thrace</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.06834v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.06834v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a vectorized implementation of FlashAttention for RISC-V processors using low-cost exponential approximations to simplify softmax computations. It employs tiling strategies to improve memory locality while avoiding ISA extensions. Experimental results demonstrate significant performance gains in attention layer processing with scalable vectorized implementations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] REACH: Reinforcement Learning for Adaptive Microservice Rescheduling in
the Cloud-Edge Continuum</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [cloud-edge continuum, microservice architecture, reinforcement learning, resource management, latency optimization]</li>
<li class=""><strong>authors:</strong> Xu Bai, Muhammed Tawfiqul Islam, Rajkumar Buyya, Adel N. Toosi</li>
<li class=""><strong>institution:</strong> University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.06675v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.06675v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> REACH uses reinforcement learning to dynamically reschedule microservices across cloud-edge infrastructure in real-time. The algorithm adapts to fluctuating resource availability and performance variations. Experimental results show REACH reduces end-to-end latency by 7.9-10% across benchmark applications while mitigating latency fluctuations.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-09">2025-10-09<a href="#2025-10-09" class="hash-link" aria-label="Direct link to 2025-10-09" title="Direct link to 2025-10-09" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Maple: A Multi-agent System for Portable Deep Learning across Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [multi-agent system, deep learning, GPU clusters, command line generation, natural language processing, distributed training]</li>
<li class=""><strong>authors:</strong> Molang Wu, Zhao Zhang</li>
<li class=""><strong>institution:</strong> Rutgers University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08842v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08842v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Maple introduces a multi-agent system that generates correct deep learning command lines from natural language input, addressing challenges in heterogeneous GPU cluster environments. The system achieves 92.0% accuracy across 567 test cases using multiple language models totaling 10B parameters. Results demonstrate Maple&#x27;s effectiveness in enabling portable and scalable distributed deep learning across diverse HPC systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D
Reconstruction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [edge computing], [reinforcement learning, multi-view 3D reconstruction, edge resource management, Q-learning, reliability]</li>
<li class=""><strong>authors:</strong> Motahare Mounesan, Sourya Saha, Houchao Gan, Md. Nurul Absur, Saptarshi Debroy</li>
<li class=""><strong>institution:</strong> City University of New York</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08839v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08839v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a reinforcement learning framework with two cooperative Q-learning agents for camera and server selection to manage edge resources in multi-view 3D reconstruction. The system operates online and learns policies through environmental interactions. Results demonstrate improved application reliability by effectively balancing latency and reconstruction quality in dynamic edge environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Man-Made Heuristics Are Dead. Long Live Code Generators!</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [automated policy generation, code synthesis, web caching, congestion control, Linux kernel integration]</li>
<li class=""><strong>authors:</strong> Rohit Dwivedula, Divyanshu Saxena, Aditya Akella, Swarat Chaudhuri, Daehyeok Kim</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08803v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08803v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PolicySmith, a framework that uses LLM-driven code generation to automatically synthesize optimal heuristics for system policies. The method replaces manual policy design with automated search techniques using generative models. Results show the generated policies outperform established baselines in web caching and can be directly integrated into the Linux kernel for congestion control.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Distributed Resource Selection for Self-Organising Cloud-Edge Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [distributed resource selection, cloud-edge systems, consensus-based mechanism, dynamic allocation, self-organising orchestration]</li>
<li class=""><strong>authors:</strong> Quentin Renau, Amjad Ullah, Emma Hart</li>
<li class=""><strong>institution:</strong> Edinburgh Napier University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08228v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08228v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a distributed consensus-based mechanism for resource selection in cloud-edge environments that uses local knowledge and inter-agent collaboration instead of central control. The approach achieves efficient resource allocation decisions while maintaining optimality and scalability. Results show the method provides rapid allocations up to 30 times faster than centralized heuristics, with computation time being the key factor influencing allocation decisions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving
with Layered Prefill</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM inference, stall-free scheduling, layered prefill, Mixture-of-Experts, energy efficiency, memory optimization]</li>
<li class=""><strong>authors:</strong> Gunjun Lee, Jiwon Kim, Jaiyoung Park, Younjoo Lee, Jung Ho Ahn</li>
<li class=""><strong>institution:</strong> Seoul National University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08055v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08055v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes layered prefill, a new scheduling paradigm that partitions transformer models into layer groups and interleaves prefill and decode operations across these groups. This approach eliminates redundant weight reloads in MoE models while maintaining stall-free decoding. Results show significant improvements in TTFT (up to 70%), end-to-end latency (41%), and energy efficiency (up to 22%) compared to chunked prefill methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] DYNAMIX: RL-based Adaptive Batch Size Optimization in Distributed
Machine Learning Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [reinforcement learning, batch size optimization, distributed machine learning, Proximal Policy Optimization, adaptive systems]</li>
<li class=""><strong>authors:</strong> Yuanjun Dai, Keqiang He, An Wang</li>
<li class=""><strong>institution:</strong> Case Western Reserve University, Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08522v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08522v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DYNAMIX uses reinforcement learning with Proximal Policy Optimization to dynamically adjust batch sizes in distributed machine learning systems. It employs multi-dimensional state representations including network metrics and resource utilization for decision-making. The method achieves up to 6.3% accuracy improvement and 46% training time reduction while scaling effectively to 32 nodes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] pyGinkgo: A Sparse Linear Algebra Operator Framework for Python</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [sparse linear algebra, high-performance computing, GPU acceleration, Python interface, Pybind11, NumPy compatibility, PyTorch compatibility]</li>
<li class=""><strong>authors:</strong> Keshvi Tuteja, Gregor Olenik, Roman Mishchuk, Yu-Hsiang Tsai, Markus Götz, Achim Streit, Hartwig Anzt, Charlotte Debus</li>
<li class=""><strong>institution:</strong> Karlsruhe Institute of Technology, Technical University of Munich</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08230v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08230v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> pyGinkgo provides a Python interface to the Ginkgo library using Pybind11, offering high-performance sparse linear algebra operations across multiple hardware backends. It demonstrates superior performance in sparse matrix-vector products and iterative solvers compared to existing Python libraries while maintaining compatibility with NumPy and PyTorch. The framework achieves performance parity with native C++ implementations while providing Python usability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Adaptive Execution Scheduler for DataDios SmartDiff</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [adaptive scheduler, differencing engine, latency optimization, memory management, Dask parallelism]</li>
<li class=""><strong>authors:</strong> Aryan Poduri</li>
<li class=""><strong>institution:</strong> DataDios</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.07811v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.07811v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an adaptive scheduler for a data differencing engine that dynamically tunes batch size and worker count within resource constraints to minimize tail latency. It uses a preflight profiler, online memory model, and guarded hill-climbing policy to safely select between in-memory and Dask-based execution backends. The scheduler achieves 23-28% lower p95 latency and 16-22% reduced peak memory compared to baseline methods while eliminating out-of-memory errors.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM
Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [hardware specialization, prefill-decode disaggregation, cost optimization, energy efficiency, systolic arrays, GDDR memory]</li>
<li class=""><strong>authors:</strong> Hengrui Zhang, Pratyush Patel, August Ning, David Wentzlaff</li>
<li class=""><strong>institution:</strong> Princeton University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08544v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08544v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SPAD, specialized hardware designs with separate Prefill Chips (larger systolic arrays, GDDR memory) and Decode Chips (reduced compute, high bandwidth) tailored for LLM inference phases. Compared to H100 baselines, SPAD achieves similar performance with 19-41% lower hardware costs and 2-17% lower TDP. The design maintains efficiency even under changing workloads through flexible chip reallocation.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-10">2025-10-10<a href="#2025-10-10" class="hash-link" aria-label="Direct link to 2025-10-10" title="Direct link to 2025-10-10" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Hierarchical Scheduling for Multi-Vector Image Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [image retrieval, hierarchical scheduling, multi-vector retrieval, efficiency optimization, similarity consistency]</li>
<li class=""><strong>authors:</strong> Maoliang Li, Ke Li, Yaoyang Liu, Jiayu Chen, Zihao Zheng, Yinjun Wu, Xiang Chen</li>
<li class=""><strong>institution:</strong> Peking University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08976v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08976v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes HiMIR, a hierarchical scheduling framework for multi-vector image retrieval that uses multiple granularities for better query-image alignment and reduces redundancy through cross-hierarchy similarity consistency. Experimental results show HiMIR achieves significant accuracy improvements while reducing computation by up to 3.5× compared to existing MVR systems. The method also includes automatic parameter configuration for practical deployment across diverse datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Slicing Is All You Need: Towards A Universal One-Sided Algorithm for
Distributed Matrix Multiplication</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [distributed matrix multiplication, one-sided communication, GPU-to-GPU communication, partitioning algorithms, PGAS framework]</li>
<li class=""><strong>authors:</strong> Benjamin Brock, Renato Golin</li>
<li class=""><strong>institution:</strong> Intel Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.08874v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.08874v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a universal one-sided algorithm for distributed matrix multiplication that supports all partitioning combinations using slicing-based index arithmetic. The method computes overlapping tile sets for local matrix multiplies and implements direct GPU-to-GPU communication via a C++ PGAS framework. Results show competitive performance with optimized libraries like PyTorch DTensor across various partitionings and replication factors.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-11">2025-10-11<a href="#2025-10-11" class="hash-link" aria-label="Direct link to 2025-10-11" title="Direct link to 2025-10-11" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude
Economy Networks via LLM-Enhanced Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [UAV trajectory optimization, resource allocation, reinforcement learning, vision-language models, low-altitude economy networks]</li>
<li class=""><strong>authors:</strong> Yang Li, Ruichen Zhang, Yinqiu Liu, Guangyuan Liu, Dusit Niyato, Abbas Jamalipour, Xianbin Wang, Dong In Kim</li>
<li class=""><strong>institution:</strong> Nanyang Technological University, University of Sydney, University of Electronic Science and Technology of China, Sungkyunkwan University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.10028v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.10028v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hierarchical optimization framework combining ARPO algorithm for resource allocation and LLM-enhanced reinforcement learning for UAV trajectory optimization. The LLM improves reward design offline without real-time latency. Numerical results show improved inference performance and communication efficiency in dynamic LAENet environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based
Model Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [speculative decoding, mixture-of-experts, expert prefetching, inference acceleration, computational optimization]</li>
<li class=""><strong>authors:</strong> Liangkun Chen, Zijian Wen, Tian Wu, Xiaoxi Zhang, Chuan Wu</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University, The University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.10302v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.10302v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SP-MoE introduces speculative expert prefetching and compute-communication pipelining to accelerate MoE-based model inference. It uses structural correspondence between draft and target models to prefetch experts ahead of verification. The method achieves 1.07-3.5× speedup over state-of-the-art approaches across various datasets and environments.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-12">2025-10-12<a href="#2025-10-12" class="hash-link" aria-label="Direct link to 2025-10-12" title="Direct link to 2025-10-12" translate="no">​</a></h2>
<ul>
<li class=""><strong>[arXiv2510] DCP: Addressing Input Dynamism In Long-Context Training via Dynamic
Context Parallelism</strong>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [dynamic context parallelism, long-context training, attention optimization, distributed training, computation balance]</li>
<li class=""><strong>authors:</strong> Chenyu Jiang, Zhenkun Cai, Ye Tian, Zhen Jia, Yida Wang, Chuan Wu</li>
<li class=""><strong>institution:</strong> The University of Hong Kong, Amazon Web Services</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.10620v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.10620v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DCP introduces a dynamic context parallel training framework with fine-grained blockwise partitioning to handle variable sequence lengths and attention patterns. It enables flexible mapping of data and computation blocks to devices, reducing communication overhead and improving load balance. Experimental results show significant speedup in attention computation (1.19x-3.77x) and end-to-end training performance (up to 1.46x) under different attention patterns.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-27T13:27:11.000Z" itemprop="dateModified">Oct 27, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20250929-20251005"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20250929-20251005</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20251013-20251019"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251013-20251019</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-10-06" class="table-of-contents__link toc-highlight">2025-10-06</a></li><li><a href="#2025-10-07" class="table-of-contents__link toc-highlight">2025-10-07</a></li><li><a href="#2025-10-08" class="table-of-contents__link toc-highlight">2025-10-08</a></li><li><a href="#2025-10-09" class="table-of-contents__link toc-highlight">2025-10-09</a></li><li><a href="#2025-10-10" class="table-of-contents__link toc-highlight">2025-10-10</a></li><li><a href="#2025-10-11" class="table-of-contents__link toc-highlight">2025-10-11</a></li><li><a href="#2025-10-12" class="table-of-contents__link toc-highlight">2025-10-12</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>