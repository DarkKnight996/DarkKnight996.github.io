<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20250901-20250907" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20250901-20250907 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20250901-20250907"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20250901-20250907 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-09-01"><meta data-rh="true" property="og:description" content="2025-09-01"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20250901-20250907"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250901-20250907" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250901-20250907" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20250901-20250907","item":"https://darkknight996.github.io/daily/20250901-20250907"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.3814fa89.js" defer="defer"></script>
<script src="/assets/js/main.2e42a93c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20250901-20250907</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20250901-20250907</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-01">2025-09-01<a href="#2025-09-01" class="hash-link" aria-label="Direct link to 2025-09-01" title="Direct link to 2025-09-01" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] LiquidGEMM: Hardware-Efficient W4A8 GEMM Kernel for High-Performance LLM
Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [quantization, GEMM kernel, LLM inference, W4A8, hardware efficiency]</li>
<li class=""><strong>authors:</strong> Huanqi Hu, Bowen Xiao, Shixuan Sun, Jianian Yin, Zhexi Zhang, Xiang Luo, Chengquan Jiang, Weiqi Xu, Xiaoying Jia, Xin Liu, Minyi Guo</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, ByteDance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01229v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01229v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LiquidGEMM introduces a hardware-efficient W4A8 GEMM kernel with LiquidQuant for fast dequantization and an implicit pipeline to overlap operations. It achieves up to 2.90x speedup over existing W4A8 kernels and 4.94x system-level speedup, outperforming NVIDIA TensorRT-LLM kernels.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Optimal Parallel Scheduling under Concave Speedup Functions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [parallel scheduling, concave speedup functions, resource allocation, cloud computing, edge computing, AI workloads]</li>
<li class=""><strong>authors:</strong> Chengzhang Li, Peizhong Ju, Atilla Eryilmaz, Ness Shroff</li>
<li class=""><strong>institution:</strong> The Ohio State University, University of Kentucky</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01811v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01811v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes the SmartFill algorithm for optimal parallel job scheduling under general concave speedup functions. It introduces the Consistent Derivative Ratio (CDR) Rule and General Water-Filling method to determine resource allocations. The approach outperforms prior methods like heSRPT across various concave speedup functions commonly found in AI workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] HiCR, an Abstract Model for Distributed Heterogeneous Programming</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [distributed heterogeneous systems, runtime systems, plugin-based approach, hardware abstraction, cross-platform execution]</li>
<li class=""><strong>authors:</strong> Sergio Miguel Martin, Luca Terracciano, Kiril Dichev, Noah Baumann, Jiashu Lin, Albert-Jan Yzelman</li>
<li class=""><strong>institution:</strong> Huawei Zurich Research Center, HiSilicon Technologies</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01425v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01425v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HiCR presents an abstract model for distributed heterogeneous programming that defines minimal operations for hardware discovery, kernel execution, and memory management through a plugin-based approach. The model enables applications to run across diverse platforms without refactoring by separating semantics from implementation details. This allows HiCR-based code to seamlessly execute on current and future systems while supporting various parallel programming paradigms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] LobRA: Multi-tenant Fine-tuning over Heterogeneous Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [finetuning], [multi-tenant fine-tuning, LoRA, sequence length heterogeneity, workload balancing, GPU efficiency]</li>
<li class=""><strong>authors:</strong> Sheng Lin, Fangcheng Fu, Haoyang Li, Hao Ge, Xuanyu Wang, Jiawen Niu, Yaofeng Tu, Bin Cui</li>
<li class=""><strong>institution:</strong> Peking University, Shanghai Jiao Tong University, ZTE Corporation</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01193v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01193v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LobRA introduces a framework for efficient multi-tenant fine-tuning using heterogeneous LoRA adapters to handle data sequence length variation and skewness. It deploys model replicas with tailored resource usage and balances workloads across them during training. Experiments show LobRA reduces GPU time for joint fine-tuning by 45.03%-60.67%.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-02">2025-09-02<a href="#2025-09-02" class="hash-link" aria-label="Direct link to 2025-09-02" title="Direct link to 2025-09-02" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] KubeIntellect: A Modular LLM-Orchestrated Agent Framework for End-to-End
Kubernetes Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [Kubernetes management, natural language interface, modular agents, code generation, workflow orchestration]</li>
<li class=""><strong>authors:</strong> Mohsen Seyedkazemi Ardebili, Andrea Bartolini</li>
<li class=""><strong>institution:</strong> University of Bologna</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02449v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02449v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KubeIntellect introduces an LLM-powered framework using modular agents and a supervisor to interpret natural language queries for comprehensive Kubernetes operations. The system features memory checkpoints, human-in-the-loop clarification, and dynamic task sequencing with a secure Code Generator Agent. Evaluation shows 93% tool synthesis success rate and 100% reliability across 200 queries, demonstrating effective infrastructure management through natural language interaction.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] An Efficient and Adaptive Watermark Detection System with Tile-based
Error Correction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [watermark detection, error correction, GPU optimization, tiling techniques]</li>
<li class=""><strong>authors:</strong> Xinrui Zhong, Xinze Feng, Jingwei Zuo, Fanjiang Ye, Yi Mu, Junfeng Guo, Heng Huang, Myungjin Lee, Yuke Wang</li>
<li class=""><strong>institution:</strong> Rice University, University of Illinois Urbana-Champaign, University of Maryland, Cisco Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02447v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02447v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> QRMark proposes an efficient watermark detection system combining QR Code-inspired error correction with tiling techniques and GPU optimization strategies. It uses Reed-Solomon error correction to maintain accuracy while employing resource-aware stream allocation and tile-based workload interleaving. The system achieves 2.43× speedup over baseline while preserving detection performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to
Break the GPU Memory Wall</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [memory offloading, multi-tier storage, I/O optimization, GPU memory, large language models]</li>
<li class=""><strong>authors:</strong> Avinash Maurya, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory, Rochester Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02480v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02480v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MLP-Offload proposes a multi-level, multi-path offloading engine that optimizes LLM training by offloading optimizer states across multiple storage tiers with cache efficiency and concurrency control. It addresses I/O bottlenecks during backward and update phases by utilizing unused remote storage bandwidth. The system achieves 2.5× faster training iterations compared to state-of-the-art methods for models up to 280B parameters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Efficient Pyramidal Analysis of Gigapixel Images on a Decentralized
Modest Computer Cluster</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [gigapixel image analysis, pyramidal processing, load balancing, decentralized computing]</li>
<li class=""><strong>authors:</strong> Marie Reinbigler, Rishi Sharma, Rafael Pires, Elisabeth Brunet, Anne-Marie Kermarrec, Catalin Fetita</li>
<li class=""><strong>institution:</strong> SAMOVAR, Inria Saclay, Télécom SudParis, IP Paris, EPFL</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02440v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02440v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces PyramidAI, a pyramidal approach that analyzes gigapixel images by starting at low resolutions and progressively focusing on regions of interest at higher resolutions. This method reduces processed data by up to 2.65x while maintaining accuracy. When deployed on a decentralized cluster of 12 modest computers, analysis time drops from over an hour to just minutes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] HydroGAT: Distributed Heterogeneous Graph Attention Transformer for
Spatiotemporal Flood Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [spatiotemporal flood prediction, graph neural networks, distributed training, heterogeneous graph attention, hydrological modeling]</li>
<li class=""><strong>authors:</strong> Aishwarya Sarkar, Autrin Hakimi, Xiaoqiong Chen, Hai Huang, Chaoqun Lu, Ibrahim Demir, Ali Jannesari</li>
<li class=""><strong>institution:</strong> Iowa State University, Tulane University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02481v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02481v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HydroGAT introduces a heterogeneous graph attention transformer that models every land and river pixel as nodes connected by hydrological flow directions, enabling simultaneous learning of spatiotemporal dependencies for flood prediction. The model achieves superior performance with NSE up to 0.97 and KGE up to 0.96 in hourly discharge forecasting while providing interpretable attention maps. A distributed training pipeline scales efficiently to 64 GPUs, demonstrating 15× speedup on the NERSC Perlmutter supercomputer.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Batch Query Processing and Optimization for Agentic Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [batch query processing, agentic workflows, KV-cache sharing, GPU optimization, DAG optimization]</li>
<li class=""><strong>authors:</strong> Junyi Shen, Noppanat Wadlom, Yao Lu</li>
<li class=""><strong>institution:</strong> National University of Singapore</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02121v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02121v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Halo introduces batch query processing and optimization for agentic LLM workflows by representing workflows as structured query plan DAGs and performing plan-level optimization to minimize redundant execution. The system achieves up to 18.6x speedup for batch inference and 4.7x throughput improvement under online serving while maintaining output quality. These gains are accomplished through adaptive batching, KV-cache sharing, and compute-communication overlap techniques.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] A Continuous Energy Ising Machine Leveraging Difference-of-Convex
Programming</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [Ising machine, combinatorial optimization, difference-of-convex programming, GPU acceleration, continuous relaxation]</li>
<li class=""><strong>authors:</strong> Debraj Banerjee, Santanu Mahapatra, Kunal Narayan Chaudhury</li>
<li class=""><strong>institution:</strong> Indian Institute of Science</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01928v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01928v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a continuous energy Ising machine that relaxes binary spins to continuous variables and uses a difference-of-convex programming approach with convergence guarantees. It implements efficient iterative algorithms requiring only single matrix-vector multiplication per iteration across GPU platforms. The method consistently outperforms existing solvers across problem sizes from 10³ to 10⁸ spins.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-03">2025-09-03<a href="#2025-09-03" class="hash-link" aria-label="Direct link to 2025-09-03" title="Direct link to 2025-09-03" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] CloudFormer: An Attention-based Performance Prediction for Public Clouds
with Unknown Workload</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [performance prediction, cloud computing, transformer, virtual machines, interference mitigation, system metrics]</li>
<li class=""><strong>authors:</strong> Amirhossein Shahbazinia, Darong Huang, Luis Costero, David Atienza</li>
<li class=""><strong>institution:</strong> EPFL (École Polytechnique Fédérale de Lausanne)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03394v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03394v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CloudFormer proposes a dual-branch Transformer model that jointly models temporal dynamics and system-level interactions to predict VM performance degradation in public clouds. The method achieves state-of-the-art performance with 7.8% MAE and demonstrates robust generalization across diverse workloads without scenario-specific tuning. It addresses performance interference challenges in multi-tenant cloud environments using high-resolution system metrics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] DPQuant: Efficient and Differentially-Private Model Training via Dynamic
Quantization Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [differential privacy, quantization, training efficiency, dynamic scheduling, privacy-preserving machine learning]</li>
<li class=""><strong>authors:</strong> Yubo Gao, Renbo Tu, Gennady Pekhimenko, Nandita Vijaykumar</li>
<li class=""><strong>institution:</strong> University of Toronto</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03472v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03472v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DPQuant introduces a dynamic quantization framework that adaptively selects layers for quantization each epoch using probabilistic sampling and loss-aware prioritization. The method reduces quantization variance amplified by DP-SGD noise injection while preserving privacy guarantees. Experimental results show DPQuant achieves near Pareto-optimal accuracy-compute trade-offs with up to 2.21× throughput improvements and less than 2% accuracy drop compared to static quantization baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Combining Performance and Productivity: Accelerating the Network Sensing
Graph Challenge with GPUs and Commodity Data Science Software</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU acceleration, graph processing, data science tools, GraphBLAS, RAPIDS ecosystem, performance benchmarking]</li>
<li class=""><strong>authors:</strong> Siddharth Samsi, Dan Campbell, Emanuel Scoullos, Oded Green</li>
<li class=""><strong>institution:</strong> NVIDIA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03653v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03653v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an alternative implementation of the Network Sensing Graph Challenge using NVIDIA&#x27;s RAPIDS ecosystem (cuDF and cupy) instead of traditional HPC code. The authors reformulate GraphBLAS computations using data science terminology and demonstrate significant speedups of 147x-2185x on various NVIDIA GPUs compared to CPU-based Pandas implementations. The work shows that commodity data science software can effectively accelerate complex graph processing workloads without specialized HPC programming.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale
Training of LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [failure recovery, checkpoint-free, scale-independent, fault-tolerance]</li>
<li class=""><strong>authors:</strong> Haijun Zhang, Jinxiang Wang, Zhenhua Yu, Yanyong Zhang, Xuejie Ji, Kaining Mao, Jun Zhang, Yaqing Zhang, Ting Wu, Fei Jie, Xiemin Huang, Zhifang Cai, Junhua Cheng, Shuwei Wang, Wei Li, Xiaoming Bao, Hua Xu, Shixiong Zhao, Jun Li, Hongwei Sun, Ziyang Zhang, Yi Xiong, Chunsheng Li</li>
<li class=""><strong>institution:</strong> iFLYTEK AI Engineering Institute, University of Science and Technology of China, Huawei Technologies Co., Ltd</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03047v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03047v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlashRecovery introduces a fast failure recovery system with three core innovations: real-time failure detection, scale-independent task restart, and checkpoint-free single-step recovery. The system achieves optimal RTO and RPO by eliminating traditional checkpointing overhead. Experimental results show it can restore training on 4,800 devices within 150 seconds with consistent recovery time across different cluster scales.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Mycroft: Tracing Dependencies in Collective Communication Towards
Reliable LLM Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [collective communication, distributed tracing, root cause analysis, reliability, fault detection]</li>
<li class=""><strong>authors:</strong> Yangtao Deng, Lei Zhang, Qinlong Wang, Xiaoyun Zhi, Xinlei Zhang, Zhuo Jiang, Haohan Xu, Lei Wang, Zuquan Song, Gaohong Liu, Yang Bai, Shuguang Wang, Wencong Xiao, Jianxi Ye, Minlan Yu, Hong Xu</li>
<li class=""><strong>institution:</strong> ByteDance, The Chinese University of Hong Kong, Harvard University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03018v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03018v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mycroft is a lightweight distributed tracing system that tracks collective communication states and leverages internal dependencies to resolve reliability issues in LLM training. It detects anomalies within 15 seconds in 90% of cases and identifies root causes within 20 seconds in 60% of cases. The system has been successfully deployed at ByteDance for over six months, demonstrating effective debugging capabilities through extensive fault injection experiments.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-04">2025-09-04<a href="#2025-09-04" class="hash-link" aria-label="Direct link to 2025-09-04" title="Direct link to 2025-09-04" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Counterfactual simulations for large scale systems with burnout
variables</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [counterfactual simulation, burnout variables, parallel computing, online advertising, MapReduce, auction systems]</li>
<li class=""><strong>authors:</strong> Benjamin Heymann</li>
<li class=""><strong>institution:</strong> ENSAE, Criteo AI LAB</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04038v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04038v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces uncertainty relaxation algorithms for efficient parallel computation of counterfactual simulations in large-scale systems with burnout variables. The method enables scalable estimation of alternative scenarios in online advertising platforms by overcoming sequential processing limitations. The approach significantly improves computational efficiency while maintaining accuracy in counterfactual analysis of auction systems with budget constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Massively-Parallel Implementation of Inextensible Elastic Rods Using
Inter-block GPU Synchronization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU computing, elastic rods simulation, medical simulation, parallel algorithms, CUDA, real-time systems]</li>
<li class=""><strong>authors:</strong> Przemyslaw Korzeniowski, Niels Hald, Fernando Bello</li>
<li class=""><strong>institution:</strong> Imperial College London</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04277v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04277v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a massively-parallel GPU implementation of inextensible elastic rods using inter-block synchronization, achieving significant speedups over CPU versions. The method enables multiple physics time-steps per kernel launch and maintains nearly constant computation time under certain constraints. This allows for accurate real-time simulation at haptic interactive rates for medical applications like catheter and guidewire simulation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for
High-Performance Distributed Training Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [checkpointing], [distributed training, gradient compression, failure recovery, differential checkpointing, performance optimization]</li>
<li class=""><strong>authors:</strong> Chenxuan Yao, Yuchong Hu, Feifan Liu, Zhengyu Liu, Dan Feng</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04084v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04084v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LowDiff proposes an efficient frequent checkpointing framework that reuses compressed gradients as differential checkpoints and employs batched gradient writes with dynamic tuning. The system achieves per-iteration checkpointing frequency with minimal runtime overhead by incorporating layer-wise gradient reusing and asynchronous persistence strategies. Experimental results demonstrate less than 3.1% performance overhead while enabling high-frequency checkpointing for distributed training systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Prob-GParareal: A Probabilistic Numerical Parallel-in-Time Solver for
Differential Equations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [probabilistic numerical solver, parallel-in-time methods, uncertainty quantification, Gaussian processes, differential equations]</li>
<li class=""><strong>authors:</strong> Guglielmo Gattiglio, Lyudmila Grigoryeva, Massimiliano Tamborrino</li>
<li class=""><strong>institution:</strong> University of Warwick, University of St. Gallen</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03945v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03945v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Prob-GParareal extends the GParareal algorithm using Gaussian processes to model correction functions, enabling uncertainty quantification in parallel-in-time solutions of differential equations. The method provides probabilistic forecasts while maintaining compatibility with classical solvers and handles probabilistic initial conditions. Theoretical analysis and numerical experiments demonstrate its accuracy and robustness across various benchmark problems including chaotic, stiff, and bifurcation systems.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-05">2025-09-05<a href="#2025-09-05" class="hash-link" aria-label="Direct link to 2025-09-05" title="Direct link to 2025-09-05" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] veScale: Consistent and Efficient Tensor Programming with Eager-Mode
SPMD</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [SPMD, distributed training, eager execution, tensor programming, random number generation, performance optimization]</li>
<li class=""><strong>authors:</strong> Youjie Li, Cheng Wan, Zhiqi Lin, Hongyu Zhu, Jiacheng Yang, Ziang Song, Xinyi Di, Jiawei Wu, Huiyao Shu, Wenlei Bao, Yanghua Peng, Haibin Lin, Li-Wen Chang</li>
<li class=""><strong>institution:</strong> ByteDance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07003v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07003v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> veScale introduces an eager-mode training system that fully embraces the SPMD paradigm to simplify distributed tensor programming. It addresses consistency issues through a novel distributed RNG algorithm and improves performance by reducing PyTorch overhead and enhancing communication efficiency. Evaluations show veScale achieves up to 2.2x speedup over state-of-the-art systems while reducing code complexity by 78.4% and maintaining single-device-equivalent results.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface
Visualization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed computing, 3D Gaussian splatting, scientific visualization, multi-GPU training, HPC systems]</li>
<li class=""><strong>authors:</strong> Mengjiao Han, Andres Sewell, Joseph Insley, Janet Knowles, Victor A. Mateevitsi, Michael E. Papka, Steve Petruzza, Silvio Rizzi</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory, Utah State University, University of Illinois Chicago</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.05216v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.05216v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a multi-GPU extension of the 3D Gaussian Splatting pipeline for scientific visualization, adapting a distributed training backend from Grendel-GS. The distributed optimization approach achieves 5.6× speedup on benchmark datasets and enables training on large datasets that exceed single-GPU capacity. This work provides a foundation for integrating 3D-GS into HPC-based scientific workflows for real-time visualization of complex simulations.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-06">2025-09-06<a href="#2025-09-06" class="hash-link" aria-label="Direct link to 2025-09-06" title="Direct link to 2025-09-06" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Distributed Deep Learning using Stochastic Gradient Staleness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed deep learning, stochastic gradient staleness, data parallelism, parallel backpropagation, convergence analysis]</li>
<li class=""><strong>authors:</strong> Viet Hoang Pham, Hyo-Sung Ahn</li>
<li class=""><strong>institution:</strong> Posts and Telecommunications Institute of Technology (PTIT), Gwangju Institute of Science and Technology (GIST)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.05679v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.05679v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a distributed training method combining data parallelism and fully decoupled parallel backpropagation to accelerate deep learning. The approach uses multiple computational units to process more training data per iteration while avoiding locking issues in backpropagation. Theoretical convergence guarantees are provided and empirical evaluations on CIFAR-10 demonstrate improved training efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU
Libraries</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU libraries, kernel fusion, C++ metaprogramming, SRAM optimization, CUDA]</li>
<li class=""><strong>authors:</strong> Oscar Amoros, Albert Andaluz, Johnny Nunez, Antonio J. Pena</li>
<li class=""><strong>institution:</strong> Universitat Politecnica de Catalunya, NVIDIA Computing SL, Barcelona Supercomputing Center</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.07071v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.07071v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a C++ API methodology using metaprogramming to automatically generate fused GPU kernels at compile time, enabling horizontal and vertical fusion for arbitrary operation sequences. The approach eliminates manual kernel development while maximizing GPU resource utilization and keeping intermediate data in on-chip memory. Experimental results show performance improvements from 2x to over 1000x compared to traditional GPU libraries while maintaining high-level programmability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Real-Time Analysis of Unstructured Data with Machine Learning on
Heterogeneous Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [particle physics, graph neural networks, GPU acceleration, FPGA implementation, real-time processing, LHCb experiment, heterogeneous architectures]</li>
<li class=""><strong>authors:</strong> Fotis I. Giasemis</li>
<li class=""><strong>institution:</strong> CERN</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.07423v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.07423v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a graph neural network pipeline for real-time charged particle track reconstruction at the LHCb experiment. The system was implemented end-to-end on GPUs within the first-level trigger and also accelerated on FPGAs. Results show improved performance compared to classical tracking algorithms while evaluating power consumption and processing speed across different hardware architectures.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-07">2025-09-07<a href="#2025-09-07" class="hash-link" aria-label="Direct link to 2025-09-07" title="Direct link to 2025-09-07" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across
Thousands of Computers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [distributed vector search, approximate nearest neighbor, graph index, key-value store, query latency, scalability]</li>
<li class=""><strong>authors:</strong> Philip Adams, Menghao Li, Shi Zhang, Li Tan, Qi Chen, Mingqin Li, Zengzhong Li, Knut Risvik, Harsha Vardhan Simhadri</li>
<li class=""><strong>institution:</strong> Microsoft, Microsoft Research Asia, Shopify</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.06046v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.06046v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DISTRIBUTEDANN presents a distributed vector search system that scales a single DISKANN graph index across thousands of machines using a distributed key-value store and in-memory ANN index. It achieves 26ms median query latency and processes over 100,000 queries per second on a 50-billion vector dataset, showing 6x higher efficiency than traditional partitioning approaches. The system has been successfully deployed in Microsoft Bing, replacing conventional scale-out architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Online Identification of IT Systems through Active Causal Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [causal learning, Gaussian process regression, system identification, active learning, intervention policy]</li>
<li class=""><strong>authors:</strong> Kim Hammar, Rolf Stadler</li>
<li class=""><strong>institution:</strong> KTH Royal Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02130v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02130v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents an active causal learning method that uses Gaussian process regression and rollout-based interventions to identify causal models of IT systems online. The approach is proven to be Bayesian-optimal and produces effective interventions. Experimental results show accurate system identification with minimal operational interference.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-28T10:10:27.000Z" itemprop="dateModified">Oct 28, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/category/daily"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Daily</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20250908-20250914"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20250908-20250914</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-09-01" class="table-of-contents__link toc-highlight">2025-09-01</a></li><li><a href="#2025-09-02" class="table-of-contents__link toc-highlight">2025-09-02</a></li><li><a href="#2025-09-03" class="table-of-contents__link toc-highlight">2025-09-03</a></li><li><a href="#2025-09-04" class="table-of-contents__link toc-highlight">2025-09-04</a></li><li><a href="#2025-09-05" class="table-of-contents__link toc-highlight">2025-09-05</a></li><li><a href="#2025-09-06" class="table-of-contents__link toc-highlight">2025-09-06</a></li><li><a href="#2025-09-07" class="table-of-contents__link toc-highlight">2025-09-07</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>