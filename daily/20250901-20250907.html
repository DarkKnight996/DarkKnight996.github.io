<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20250901-20250907" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20250901-20250907 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20250901-20250907"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20250901-20250907 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-09-01"><meta data-rh="true" property="og:description" content="2025-09-01"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20250901-20250907"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250901-20250907" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20250901-20250907" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20250901-20250907","item":"https://darkknight996.github.io/daily/20250901-20250907"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.78e1c784.js" defer="defer"></script>
<script src="/assets/js/main.be5f6e7c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20250901-20250907</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20250901-20250907</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-01">2025-09-01<a href="#2025-09-01" class="hash-link" aria-label="Direct link to 2025-09-01" title="Direct link to 2025-09-01" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] STZ: A High Quality and High Speed Streaming Lossy Compression Framework
for Scientific Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [streaming compression, lossy compression, scientific data, progressive decompression, random-access decompression]</li>
<li class=""><strong>authors:</strong> Daoce Wang, Pascal Grosset, Jesus Pulido, Jiannan Tian, Tushar M. Athawale, Jinda Jia, Baixi Sun, Boyuan Zhang, Sian Jin, Kai Zhao, James Ahrens, Fengguang Song</li>
<li class=""><strong>institution:</strong> Unknown (insufficient information provided)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01626v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01626v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a novel streaming compression framework that supports both progressive and random-access decompression while maintaining high quality. It uses hierarchical partitioning and prediction strategies to achieve compression quality comparable to SZ3 with significantly faster speed. The method enables efficient on-demand data access and flexible analysis workflows for scientific data.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] LiquidGEMM: Hardware-Efficient W4A8 GEMM Kernel for High-Performance LLM
Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [quantization, GEMM kernel, W4A8, LLM inference, hardware efficiency, Tensor Cores]</li>
<li class=""><strong>authors:</strong> Huanqi Hu, Bowen Xiao, Shixuan Sun, Jianian Yin, Zhexi Zhang, Xiang Luo, Chengquan Jiang, Weiqi Xu, Xiaoying Jia, Xin Liu, Minyi Guo</li>
<li class=""><strong>institution:</strong> NVIDIA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01229v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01229v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LiquidGEMM introduces a hardware-efficient W4A8 GEMM kernel with LiquidQuant for fast dequantization and an implicit fine-grained pipeline to overlap operations. It achieves up to 2.90x speedup over existing W4A8 kernels and up to 4.94x system-level speedup, outperforming NVIDIA TensorRT-LLM kernels by 1.12-1.63x.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] LobRA: Multi-tenant Fine-tuning over Heterogeneous Data</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [finetuning], [multi-tenant fine-tuning, LoRA adapters, heterogeneous data, sequence length variation, workload balance]</li>
<li class=""><strong>authors:</strong> Sheng Lin, Fangcheng Fu, Haoyang Li, Hao Ge, Xuanyu Wang, Jiawen Niu, Yaofeng Tu, Bin Cui</li>
<li class=""><strong>institution:</strong> Based on the technical focus and terminology, likely from academic institutions or companies specializing in machine learning systems (e.g., universities with strong ML/AI departments or tech companies like Google, Microsoft, Meta)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01193v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01193v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LobRA introduces a framework for efficient multi-tenant fine-tuning using LoRA adapters by addressing data heterogeneity through heterogeneous resource deployments and workload balancing. It reduces GPU time by 45.03%-60.67% for joint fine-tuning tasks. The method optimizes training efficiency in shared base model scenarios.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Optimal Parallel Scheduling under Concave Speedup Functions</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [parallel scheduling, concave speedup functions, resource allocation, cloud computing, edge computing, AI applications]</li>
<li class=""><strong>authors:</strong> Chengzhang Li, Peizhong Ju, Atilla Eryilmaz, Ness Shroff</li>
<li class=""><strong>institution:</strong> Based on the technical content about cloud/edge computing and AI applications, likely from institutions like MIT, Stanford, UC Berkeley, or Microsoft Research (specific institution cannot be determined from provided content)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01811v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01811v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes the Consistent Derivative Ratio Rule and General Water-Filling method for optimal parallel job scheduling under arbitrary concave speedup functions. It introduces the SmartFill algorithm that selectively allocates resources rather than distributing to all active jobs. Numerical evaluations show SmartFill substantially outperforms prior heSRPT method across various concave speedup functions.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-02">2025-09-02<a href="#2025-09-02" class="hash-link" aria-label="Direct link to 2025-09-02" title="Direct link to 2025-09-02" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] An Efficient and Adaptive Watermark Detection System with Tile-based
Error Correction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [watermark detection, error correction, GPU optimization, tiling techniques, Reed-Solomon codes, resource-aware scheduling]</li>
<li class=""><strong>authors:</strong> Xinrui Zhong, Xinze Feng, Jingwei Zuo, Fanjiang Ye, Yi Mu, Junfeng Guo, Heng Huang, Myungjin Lee, Yuke Wang</li>
<li class=""><strong>institution:</strong> Unable to determine from provided information</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02447v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02447v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> QRMark combines QR Code-inspired error correction with tiling techniques and Reed-Solomon codes to maintain detection accuracy while improving efficiency. The system implements resource-aware stream allocation and tile-based workload interleaving to optimize GPU utilization. Evaluations show QRMark achieves 2.43x inference speedup over sequential baselines while preserving robustness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Efficient Pyramidal Analysis of Gigapixel Images on a Decentralized
Modest Computer Cluster</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [gigapixel image analysis, pyramid method, distributed computing, load balancing, biomedical imaging]</li>
<li class=""><strong>authors:</strong> Marie Reinbigler, Rishi Sharma, Rafael Pires, Elisabeth Brunet, Anne-Marie Kermarrec, Catalin Fetita</li>
<li class=""><strong>institution:</strong> Unknown (insufficient information to determine)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02440v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02440v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents PyramidAI, a pyramidal approach that analyzes gigapixel images by starting with lower resolutions and progressively focusing on regions of interest at higher resolutions. It reduces processed data by up to 2.65x while maintaining accuracy and demonstrates efficient parallelization on modest computer clusters, cutting analysis time from over an hour to minutes with 12 workers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] HydroGAT: Distributed Heterogeneous Graph Attention Transformer for
Spatiotemporal Flood Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [graph neural networks, spatiotemporal prediction, flood forecasting, distributed training, heterogeneous graphs, attention mechanisms, hydrological modeling]</li>
<li class=""><strong>authors:</strong> Aishwarya Sarkar, Autrin Hakimi, Xiaoqiong Chen, Hai Huang, Chaoqun Lu, Ibrahim Demir, Ali Jannesari</li>
<li class=""><strong>institution:</strong> NERSC (National Energy Research Scientific Computing Center) / swapp-lab</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02481v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02481v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HydroGAT introduces a heterogeneous graph attention transformer that models both spatial river network topology and temporal dependencies for flood prediction. The method achieves superior discharge forecasting performance with NSE up to 0.97 while providing interpretable attention maps. A distributed training pipeline enables efficient high-resolution basin-scale modeling, demonstrating 15x speedup on supercomputing infrastructure.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] KubeIntellect: A Modular LLM-Orchestrated Agent Framework for End-to-End
Kubernetes Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Kubernetes management, LLM orchestration, modular agents, natural language interface, tool synthesis]</li>
<li class=""><strong>authors:</strong> Mohsen Seyedkazemi Ardebili, Andrea Bartolini</li>
<li class=""><strong>institution:</strong> Microsoft (based on Azure integration mentioned in abstract)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02449v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02449v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> KubeIntellect introduces an LLM-orchestrated agent framework that uses modular agents and a supervisor to interpret natural language queries for comprehensive Kubernetes operations. The system achieves 93% tool synthesis success rate and 100% reliability across 200 queries, demonstrating effective end-to-end Kubernetes management through natural language interaction. This represents a new class of interpretable, LLM-driven systems for complex infrastructure management.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] A Continuous Energy Ising Machine Leveraging Difference-of-Convex
Programming</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [Ising machine, combinatorial optimization, difference-of-convex programming, GPU acceleration, convergence guarantees]</li>
<li class=""><strong>authors:</strong> Debraj Banerjee, Santanu Mahapatra, Kunal Narayan Chaudhury</li>
<li class=""><strong>institution:</strong> Based on the technical content and GPU implementation across platforms, likely from academic institutions with strong computing/engineering programs (e.g., universities with HPC facilities) or industry research labs focusing on optimization algorithms</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01928v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01928v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a continuous energy Ising machine that relaxes binary spins to continuous variables and uses difference-of-convex programming for efficient optimization. The method requires only one matrix-vector multiplication per iteration and provides convergence guarantees. Experimental results show it consistently outperforms existing solvers across problem sizes from small to ultra-large scales on various GPU platforms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to
Break the GPU Memory Wall</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [memory offloading, GPU memory optimization, multi-tier storage, I/O bottleneck mitigation]</li>
<li class=""><strong>authors:</strong> Avinash Maurya, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae</li>
<li class=""><strong>institution:</strong> Based on the technical focus and problem domain, likely from academic institutions or companies specializing in systems optimization for AI training (e.g., universities with strong systems research or AI infrastructure companies)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02480v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02480v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MLP-Offload introduces a multi-level, multi-path offloading engine that optimizes LLM pre-training by efficiently distributing optimizer states across storage tiers. It reduces I/O bottlenecks during backward and update phases through cache-efficient strategies and concurrency control. The method achieves 2.5× faster training iterations compared to state-of-the-art systems for models up to 280B parameters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Batch Query Processing and Optimization for Agentic Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [batch query processing, agentic workflows, KV-cache sharing, GPU utilization, query optimization]</li>
<li class=""><strong>authors:</strong> Junyi Shen, Noppanat Wadlom, Yao Lu</li>
<li class=""><strong>institution:</strong> Based on the technical focus on LLM serving systems and optimization, likely from academic institutions with strong systems research groups (e.g., UC Berkeley, MIT, Stanford, CMU) or industry labs (e.g., Google Research, Microsoft Research)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.02121v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.02121v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Halo introduces batch query processing and optimization for agentic LLM workflows by representing workflows as structured DAGs and performing plan-level optimization. It uses adaptive batching, KV-cache sharing, and compute-communication overlap to maximize hardware efficiency. Evaluation shows up to 18.6x speedup for batch inference and 4.7x throughput improvement without compromising output quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] OASIS: Object-based Analytics Storage for Intelligent SQL Query
Offloading in Scientific Tabular Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [object storage, SQL query offloading, scientific workloads, storage hierarchy optimization]</li>
<li class=""><strong>authors:</strong> Soon Hwang, Junhyeok Park, Junghyun Ryu, Seonghoon Ahn, Jeoungahn Park, Jeongjin Lee, Soonyeal Yang, Jungki Noh, Woosuk Chung, Hoshik Kim, Youngjae Kim</li>
<li class=""><strong>institution:</strong> Based on the technical content and HPC focus, likely from a research institution or university with strong systems research (e.g., ETH Zurich, MIT, UC Berkeley, or national labs like Argonne/LBNL)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.01966v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.01966v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> OASIS introduces a Computation-Enabled Object Storage system that supports flexible output formats, complex operators, and dynamic execution path selection across storage layers. It integrates with Spark and demonstrates up to 32.7% performance improvement over existing COS systems in HPC scientific query workloads. The system optimizes data movement and operator execution close to storage for enhanced analytical efficiency.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-03">2025-09-03<a href="#2025-09-03" class="hash-link" aria-label="Direct link to 2025-09-03" title="Direct link to 2025-09-03" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] DPQuant: Efficient and Differentially-Private Model Training via Dynamic
Quantization Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [differential privacy, quantization, training optimization, model efficiency, privacy-preserving ML]</li>
<li class=""><strong>authors:</strong> Yubo Gao, Renbo Tu, Gennady Pekhimenko, Nandita Vijaykumar</li>
<li class=""><strong>institution:</strong> University of Toronto</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03472v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03472v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DPQuant introduces a dynamic quantization framework that adaptively selects layers for quantization during differentially private training, combining probabilistic sampling and loss-aware prioritization. This approach reduces quantization variance amplified by DP-SGD noise injection while preserving privacy guarantees. The method achieves near Pareto-optimal accuracy-compute trade-offs with up to 2.21× throughput improvements and minimal accuracy degradation across multiple models and datasets.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Combining Performance and Productivity: Accelerating the Network Sensing
Graph Challenge with GPUs and Commodity Data Science Software</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [Graph Challenge, GPU acceleration, data science tools, RAPIDS, cuDF, cupy, GraphBLAS, network sensing, performance optimization]</li>
<li class=""><strong>authors:</strong> Siddharth Samsi, Dan Campbell, Emanuel Scoullos, Oded Green</li>
<li class=""><strong>institution:</strong> NVIDIA</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03653v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03653v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an alternative implementation of the Network Sensing Graph Challenge using data science tools from NVIDIA&#x27;s RAPIDS ecosystem. By leveraging cuDF and cupy instead of traditional HPC code, the authors achieve significant GPU acceleration. The method demonstrates speedups of 147x-2185x over CPU-based Pandas implementations across different NVIDIA GPU architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] CloudFormer: An Attention-based Performance Prediction for Public Clouds
with Unknown Workload</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [performance prediction, cloud computing, virtual machines, transformer, attention mechanism, interference mitigation, system metrics]</li>
<li class=""><strong>authors:</strong> Amirhossein Shahbazinia, Darong Huang, Luis Costero, David Atienza</li>
<li class=""><strong>institution:</strong> École Polytechnique Fédérale de Lausanne (EPFL)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03394v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03394v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CloudFormer proposes a dual-branch Transformer model that jointly models temporal dynamics and system-level interactions to predict VM performance degradation in black-box cloud environments. The method leverages 206 system metrics at one-second resolution and demonstrates robust generalization across diverse workloads without scenario-specific tuning. Experimental results show CloudFormer achieves state-of-the-art performance with just 7.8% MAE, significantly outperforming existing methods by at least 28%.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale
Training of LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [fault tolerance, failure recovery, checkpoint-free, large-scale training, cluster management]</li>
<li class=""><strong>authors:</strong> Haijun Zhang, Jinxiang Wang, Zhenhua Yu, Yanyong Zhang, Xuejie Ji, Kaining Mao, Jun Zhang, Yaqing Zhang, Ting Wu, Fei Jie, Xiemin Huang, Zhifang Cai, Junhua Cheng, Shuwei Wang, Wei Li, Xiaoming Bao, Hua Xu, Shixiong Zhao, Jun Li, Hongwei Sun, Ziyang Zhang, Yi Xiong, Chunsheng Li</li>
<li class=""><strong>institution:</strong> iFLYTEK AI Engineering Institute, University of Science and Technology of China, Huawei Technologies Co., Ltd</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03047v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03047v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlashRecovery introduces a three-module system for fast failure recovery in large-scale LLM training, featuring real-time failure detection, scale-independent task restart, and checkpoint-free single-step restoration. It achieves optimal recovery objectives with consistent recovery times across cluster scales. Experimental results show recovery within 150 seconds on a 4800-device cluster.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Mycroft: Tracing Dependencies in Collective Communication Towards
Reliable LLM Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [collective communication, distributed tracing, root cause analysis, fault detection, reliability]</li>
<li class=""><strong>authors:</strong> Yangtao Deng, Lei Zhang, Qinlong Wang, Xiaoyun Zhi, Xinlei Zhang, Zhuo Jiang, Haohan Xu, Lei Wang, Zuquan Song, Gaohong Liu, Yang Bai, Shuguang Wang, Wencong Xiao, Jianxi Ye, Minlan Yu, Hong Xu</li>
<li class=""><strong>institution:</strong> ByteDance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03018v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03018v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Mycroft introduces a lightweight distributed tracing system that tracks collective communication states and leverages internal dependencies to resolve reliability issues in LLM training. It achieves rapid anomaly detection within 15 seconds in 90% of cases and root cause identification within 20 seconds in 60% of cases. The system has been successfully deployed at ByteDance for over six months, demonstrating effectiveness through extensive fault injection experiments.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-04">2025-09-04<a href="#2025-09-04" class="hash-link" aria-label="Direct link to 2025-09-04" title="Direct link to 2025-09-04" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Counterfactual simulations for large scale systems with burnout
variables</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [counterfactual simulation, burnout variables, uncertainty relaxation, parallel computation, online advertising]</li>
<li class=""><strong>authors:</strong> Benjamin Heymann</li>
<li class=""><strong>institution:</strong> ENSAE, Criteo AI LAB</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04038v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04038v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces uncertainty relaxation algorithms that enable parallel computation for counterfactual simulations in systems with burnout variables. This approach significantly improves scalability compared to traditional sequential processing methods. The method is particularly applicable to online advertising systems where campaign budgets create complex dynamic constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for
High-Performance Distributed Training Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [checkpointing], [distributed training, gradient compression, differential checkpointing, fault tolerance, performance optimization]</li>
<li class=""><strong>authors:</strong> Chenxuan Yao, Yuchong Hu, Feifan Liu, Zhengyu Liu, Dan Feng</li>
<li class=""><strong>institution:</strong> Huazhong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04084v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04084v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LowDiff proposes an efficient frequent checkpointing framework that reuses compressed gradients as differential checkpoints and employs batched gradient writes with dynamic tuning. It achieves high checkpointing frequency with minimal runtime overhead by leveraging layer-wise gradient reusing and asynchronous persistence strategies. Experimental results demonstrate the framework can perform checkpointing every iteration while maintaining less than 3.1% performance overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Massively-Parallel Implementation of Inextensible Elastic Rods Using
Inter-block GPU Synchronization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU computing, elastic rods, medical simulation, parallel algorithms, CUDA]</li>
<li class=""><strong>authors:</strong> Przemyslaw Korzeniowski, Niels Hald, Fernando Bello</li>
<li class=""><strong>institution:</strong> Imperial College London</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.04277v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.04277v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a massively-parallel GPU implementation of inextensible elastic rods using inter-block synchronization, achieving significant speedups over CPU versions. The method enables multiple physics time-steps per kernel launch by utilizing all GPU streaming multiprocessors efficiently. The implementation achieves real-time haptic rates for medical simulations like catheter/guidewire pairs with performance boosts up to 40x faster than previous approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Prob-GParareal: A Probabilistic Numerical Parallel-in-Time Solver for
Differential Equations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [probabilistic numerical solver, parallel-in-time methods, uncertainty quantification, Gaussian processes, differential equations]</li>
<li class=""><strong>authors:</strong> Guglielmo Gattiglio, Lyudmila Grigoryeva, Massimiliano Tamborrino</li>
<li class=""><strong>institution:</strong> University of Warwick, University of St. Gallen</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.03945v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.03945v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Prob-GParareal extends the GParareal algorithm using Gaussian processes to model correction functions, enabling uncertainty quantification for parallel-in-time solutions of differential equations. The method propagates numerical uncertainty across time domains and handles probabilistic initial conditions while maintaining compatibility with classical solvers. Theoretical analysis and numerical experiments demonstrate its accuracy and robustness across various ODE systems, with a nearest-neighbors variant showing improved scalability for PDE problems.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-05">2025-09-05<a href="#2025-09-05" class="hash-link" aria-label="Direct link to 2025-09-05" title="Direct link to 2025-09-05" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Efficient Fault Localization in a Cloud Stack Using End-to-End
Application Service Topology</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [fault localization, cloud computing, service topology, root cause detection, performance anomalies]</li>
<li class=""><strong>authors:</strong> Dhanya R Mathews, Mudit Verma, Pooja Aggarwal, J. Lakshmi</li>
<li class=""><strong>institution:</strong> Indian Institute of Science, Bangalore and IBM Research, India</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.05511v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.05511v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a topology-aware approach for efficient fault localization in cloud stacks by selecting informative metrics and incorporating end-to-end service topology into root cause detection algorithms. The method improves the accuracy of identifying performance anomaly sources by considering the complete service component relationships. Evaluation shows the proposed TA-RCD algorithm performs at least 2x better than state-of-the-art methods in Top-3 and Top-5 recall metrics.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] veScale: Consistent and Efficient Tensor Programming with Eager-Mode
SPMD</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [SPMD, eager execution, distributed training, tensor programming, random number generation, performance optimization]</li>
<li class=""><strong>authors:</strong> Youjie Li, Cheng Wan, Zhiqi Lin, Hongyu Zhu, Jiacheng Yang, Ziang Song, Xinyi Di, Jiawei Wu, Huiyao Shu, Wenlei Bao, Yanghua Peng, Haibin Lin, Li-Wen Chang</li>
<li class=""><strong>institution:</strong> ByteDance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.07003v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.07003v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> veScale introduces an eager-mode training system that fully embraces the SPMD paradigm to simplify distributed tensor programming. It addresses consistency issues through a novel distributed RNG algorithm and improves performance by reducing PyTorch overhead and enhancing communication efficiency. Evaluations show veScale achieves up to 2.2x speedup over state-of-the-art systems while reducing code complexity by 78.4% and maintaining single-device-equivalent results.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface
Visualization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed computing, multi-GPU optimization, scientific visualization, 3D Gaussian splatting, high-performance computing]</li>
<li class=""><strong>authors:</strong> Mengjiao Han, Andres Sewell, Joseph Insley, Janet Knowles, Victor A. Mateevitsi, Michael E. Papka, Steve Petruzza, Silvio Rizzi</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.05216v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.05216v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a distributed multi-GPU extension of the 3D Gaussian Splatting pipeline for scientific visualization. The method adapts a multi-GPU training backend to enable scalable processing of large datasets, achieving 5.6x speedup on benchmark datasets. This work enables high-resolution isosurface reconstructions that exceed single-GPU capacity and integrates 3D-GS into HPC workflows.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-06">2025-09-06<a href="#2025-09-06" class="hash-link" aria-label="Direct link to 2025-09-06" title="Direct link to 2025-09-06" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2509] Distributed Deep Learning using Stochastic Gradient Staleness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [distributed deep learning, stochastic gradient staleness, parallel backpropagation, data parallelism, training efficiency]</li>
<li class=""><strong>authors:</strong> Viet Hoang Pham, Hyo-Sung Ahn</li>
<li class=""><strong>institution:</strong> Posts and Telecommunications Institute of Technology (PTIT), Gwangju Institute of Science and Technology (GIST)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.05679v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.05679v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a distributed training method combining data parallelism and fully decoupled parallel backpropagation to accelerate deep learning. The approach uses multiple computational units to process more training data per iteration while reducing locking issues. Experimental results on CIFAR-10 demonstrate significant improvements in training efficiency, with theoretical convergence guarantees to critical points.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU
Libraries</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [kernel fusion, GPU optimization, C++ API, SRAM utilization, compile-time code generation]</li>
<li class=""><strong>authors:</strong> Oscar Amoros, Albert Andaluz, Johnny Nunez, Antonio J. Pena</li>
<li class=""><strong>institution:</strong> Barcelona Supercomputing Center</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.07071v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.07071v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a C++ API methodology using C++17 metaprogramming to automatically generate fused GPU kernels that combine multiple operations at compile time. This approach eliminates manual kernel development and pre-compilation while maximizing GPU resource utilization and keeping intermediate data in on-chip memory. Experimental results show performance improvements from 2x to over 1000x compared to traditional GPU libraries while maintaining high-level programmability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2509] Real-Time Analysis of Unstructured Data with Machine Learning on
Heterogeneous Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [real-time processing, graph neural networks, heterogeneous architectures, FPGA acceleration, GPU implementation, particle physics, LHCb experiment]</li>
<li class=""><strong>authors:</strong> Fotis I. Giasemis</li>
<li class=""><strong>institution:</strong> CERN</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.07423v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.07423v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a graph neural network-based pipeline for charged particle track reconstruction implemented on GPUs and FPGAs at the LHCb experiment. The system was deployed end-to-end in the first-level trigger and compared against classical tracking algorithms. The research demonstrates efficient machine learning deployment in high-frequency real-time environments while optimizing throughput and energy consumption.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-09-07">2025-09-07<a href="#2025-09-07" class="hash-link" aria-label="Direct link to 2025-09-07" title="Direct link to 2025-09-07" translate="no">​</a></h2>
<ul>
<li class=""><strong>[arXiv2509] DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across
Thousands of Computers</strong>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [distributed systems, vector search, approximate nearest neighbor, graph index, scalability]</li>
<li class=""><strong>authors:</strong> Philip Adams, Menghao Li, Shi Zhang, Li Tan, Qi Chen, Mingqin Li, Zengzhong Li, Knut Risvik, Harsha Vardhan Simhadri</li>
<li class=""><strong>institution:</strong> Microsoft</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.06046v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.06046v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DISTRIBUTEDANN presents a distributed vector search service using a distributed key-value store and in-memory ANN index to scale a single graph across thousands of machines. It achieves 26ms median query latency and over 100,000 QPS on a 50 billion vector index, making it 6x more efficient than existing approaches. The system has been successfully deployed in the Bing search engine, replacing conventional scale-out architectures.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-24T13:11:58.000Z" itemprop="dateModified">Oct 24, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/category/daily"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Daily</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20250908-20250914"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20250908-20250914</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-09-01" class="table-of-contents__link toc-highlight">2025-09-01</a></li><li><a href="#2025-09-02" class="table-of-contents__link toc-highlight">2025-09-02</a></li><li><a href="#2025-09-03" class="table-of-contents__link toc-highlight">2025-09-03</a></li><li><a href="#2025-09-04" class="table-of-contents__link toc-highlight">2025-09-04</a></li><li><a href="#2025-09-05" class="table-of-contents__link toc-highlight">2025-09-05</a></li><li><a href="#2025-09-06" class="table-of-contents__link toc-highlight">2025-09-06</a></li><li><a href="#2025-09-07" class="table-of-contents__link toc-highlight">2025-09-07</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>