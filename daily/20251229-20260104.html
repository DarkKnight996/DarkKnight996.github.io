<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251229-20260104" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251229-20260104 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20251229-20260104"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251229-20260104 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-12-29"><meta data-rh="true" property="og:description" content="2025-12-29"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20251229-20260104"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251229-20260104" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251229-20260104" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20251229-20260104","item":"https://darkknight996.github.io/daily/20251229-20260104"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.96be1c31.js" defer="defer"></script>
<script src="/assets/js/main.0304cc5d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260105-20260111"><span title="20260105-20260111" class="linkLabel_WmDU">20260105-20260111</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260112-20260118"><span title="20260112-20260118" class="linkLabel_WmDU">20260112-20260118</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251229-20260104</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251229-20260104</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-29">2025-12-29<a href="#2025-12-29" class="hash-link" aria-label="Direct link to 2025-12-29" title="Direct link to 2025-12-29" translate="no">​</a></h2>
<p><strong>cs.DC total: 16</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251229] LIME<!-- -->:Accelerating<!-- --> Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [pipeline parallelism, model offloading, fine-grained offline allocation scheduler, online memory adaptation, collaborative inference]</li>
<li class=""><strong>authors:</strong> Mingyu Sun, Xiao Zhang, Shen Qu, Yan Li, Mengbai Xiao, Yuan Yuan, Dongxiao Yu</li>
<li class=""><strong>institution:</strong> Shandong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21835" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21835</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes LIME, a collaborative system that uses interleaved pipeline parallelism and model offloading to enable lossless LLM inference across multiple memory-constrained edge devices. It introduces a fine-grained offline scheduler and online memory adaptation to optimize resource usage and minimize latency. Experiments show LIME achieves significant speedups over baselines on heterogeneous edge devices without compromising model accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deep-surrogate, two-stage design, mixture-of-experts, error-bounded lossy compression, quality prediction]</li>
<li class=""><strong>authors:</strong> Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Lukić, Franck Cappello</li>
<li class=""><strong>institution:</strong> University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21433" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21433</a></li>
<li class=""><strong>Simple LLM Summary:</strong> DeepCQ is a deep-surrogate framework that predicts lossy compression quality using a two-stage design and mixture-of-experts approach for time-evolving data. It generalizes across compressors, metrics, and datasets, achieving prediction errors under 10% and reducing computational overhead in scientific data analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Demystifying ARM SME to Optimize General Matrix Multiplications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [cache-aware partitioning, data packing, micro-kernels, multi-vector loads, tile registers, ARM SME]</li>
<li class=""><strong>authors:</strong> Chencheng Deng, Weiling Yang, Jianbin Fang, Dezun Dong</li>
<li class=""><strong>institution:</strong> National University of Defense Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21473" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21473</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents MpGEMM, an open-source library that optimizes General Matrix Multiplication (GEMM) by leveraging ARM&#x27;s Scalable Matrix Extension (SME) through techniques like cache-aware partitioning and specialized micro-kernels. It achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library on real-world workloads from DeepSeek and LLaMA.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [data spaces, cloud-edge continuum, edge computing, containerized microservices, AI/ML services, IoT testbed]</li>
<li class=""><strong>authors:</strong> Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda</li>
<li class=""><strong>institution:</strong> Spark Works Ltd, IONOS SE, Iquadrat Informática S.L.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21340" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21340</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a real-world use case of intelligent infrastructure monitoring implemented within a data space-enabled cloud-edge framework. It leverages edge computing, containerized microservices, and interoperable data sharing to address challenges like sensor integration and data privacy. The implementation demonstrates the transformative potential of combining AI, edge computing, and data spaces for building scalable and resilient smart city ecosystems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [LSTM, Random Forest, MQTT, InfluxDB, Streamlit, IoT monitoring, synthetic data, ASHRAE 2021]</li>
<li class=""><strong>authors:</strong> Krishna Chaitanya Sunkara, Rambabu Konakanchi</li>
<li class=""><strong>institution:</strong> Oracle, Charles Schwab</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21801" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21801</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents a smart IoT system that uses LSTM networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. It demonstrates high accuracy on synthetic data and shows that proactive leak management can prevent significant energy waste. The work establishes a proof-of-concept for sustainable data center operations, though it requires empirical validation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [e-graph, term rewriting, equality saturation, auto vectorize, auto distribution, auto schedule, non-uniform memory access (NUMA) abstraction, roofline model]</li>
<li class=""><strong>authors:</strong> Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang</li>
<li class=""><strong>institution:</strong> Canaan Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21571" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21571</a></li>
<li class=""><strong>Simple LLM Summary:</strong> nncase is an end-to-end compiler framework that uses an e-graph-based term rewriting engine to optimize LLM deployment across heterogeneous memory architectures. It unifies optimization through modules for auto vectorization, distribution, and scheduling. The evaluation shows it outperforms other frameworks and achieves performance comparable to hand-optimized solutions, demonstrating the viability of automated compilation for high-performance LLM inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Embedding Samples Dispatching for Recommendation Model Training in Edge Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [embedding cache, parameter server, edge computing, sample dispatching, HybridDis]</li>
<li class=""><strong>authors:</strong> Guopeng Li, Haisheng Tan, Chi Zhang, Hongqiu Ni, Zilong Wang, Xinyue Zhang, Yang Xu, Han Tian</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China, Hefei University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21615" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21615</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes ESD, a mechanism that optimizes the dispatch of input embedding samples to edge workers to minimize transmission costs in distributed DLRM training. It introduces HybridDis, a dispatch method combining an optimal and a heuristic algorithm to balance decision quality and resource use. Experiments show ESD reduces embedding transmission cost by up to 36.76% and achieves up to 1.74x training speedup.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol for Multicore Real-Time Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [real-time systems], [lock-free protocol, fault tolerance, resource sharing, multicore, worst-case response time analysis]</li>
<li class=""><strong>authors:</strong> Nan Chen, Xiaotian Dai, Tong Cheng, Alan Burns, Iain Bate, Shuai Zhao</li>
<li class=""><strong>institution:</strong> University of York, Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21701" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21701</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes LEFT-RS, a lock-free fault-tolerant resource sharing protocol for multicore real-time systems that allows concurrent read access to global resources and parallel entry into critical sections. It aims to improve efficiency and fault resilience by enabling tasks to complete earlier if others fail, while limiting overhead. The evaluation shows the method significantly outperforms existing approaches, achieving up to an 84.5% average improvement in schedulability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Proceedings First Workshop on Adaptable Cloud Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> TBD</li>
<li class=""><strong>authors:</strong> Giuseppe De Palma, Saverio Giallorenzo</li>
<li class=""><strong>institution:</strong> TBD</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22054" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22054</a></li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [vision transformer, cloud-device collaboration, patch-level importance scoring, dynamic scheduling, weighted ensembling]</li>
<li class=""><strong>authors:</strong> Linyi Jiang, Yifei Zhu, Hao Yin, Bo Li</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Tsinghua University, Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21730" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21730</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Hyperion is a cloud-device collaborative framework that enables low-latency inference on Ultra-HD video using vision transformers by identifying and transmitting critical image patches, dynamically adjusting transmission quality, and fusing edge and cloud results. It improves frame processing rate by up to 1.61× and accuracy by up to 20.2% compared to state-of-the-art baselines under dynamic network conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [mixture-of-experts (MoE), disaggregated expert parallelism (DEP), fine-grained task scheduling, FinDEP, task pipelining, optimization problem]</li>
<li class=""><strong>authors:</strong> Xinglin Pan, Shaohuai Shi, Wenxiang Lin, Yuxin Wang, Zhenheng Tang, Wei Wang, Xiaowen Chu</li>
<li class=""><strong>institution:</strong> The Hong Kong University of Science and Technology (Guangzhou), Harbin Institute of Technology, Shenzhen, Hong Kong Baptist University, The Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21487" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21487</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes FinDEP, a fine-grained task scheduling algorithm for disaggregated expert parallelism (DEP) to improve the inference throughput of MoE-based large language models. The method partitions computation and communication tasks and formulates an optimization problem to maximize task overlap. Experiments show FinDEP achieves up to 1.61x throughput improvement over prior methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [federated fine-tuning, adaptive aggregation, connection failures, data heterogeneity, convergence guarantee, LoRA]</li>
<li class=""><strong>authors:</strong> Yanmeng Wang, Zhiwen Dai, Shuai Wang, Jian Zhou, Fu Xiao, Tony Q. S. Quek, Tsung-Hui Chang</li>
<li class=""><strong>institution:</strong> Nanjing University of Posts and Telecommunications, The Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22035" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22035</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FedAuto, a federated fine-tuning framework that uses adaptive aggregation to handle unreliable network connections and heterogeneous client data without prior knowledge of network conditions. It provides a strong per-round convergence guarantee. Experiments show it outperforms existing methods in various failure scenarios for both full and parameter-efficient fine-tuning like LoRA.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [model parallelism, block placement, request routing, mixed integer linear programming, performance modeling, distributed inference]</li>
<li class=""><strong>authors:</strong> Tingyang Sun, Ting He, Bo Ji, Parimal Parag</li>
<li class=""><strong>institution:</strong> Pennsylvania State University, Virginia Tech, Indian Institute of Science</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21884" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21884</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents the first systematic study of resource allocation for distributed large language model inference, focusing on optimizing block placement and request routing. It develops performance models, formulates the problem as a mixed integer linear program, and provides efficient offline and online algorithms with performance guarantees. The proposed solution is shown to substantially reduce inference time compared to the state-of-the-art in geographically-distributed settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [agentic workflow, LLM-driven graph traversal, service dependency graph (SDG), hammock-block program dependence graph (PDG), root-cause analysis (RCA)]</li>
<li class=""><strong>authors:</strong> Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer</li>
<li class=""><strong>institution:</strong> University of Illinois at Urbana-Champaign, IBM Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22113" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22113</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PRAXIS, an orchestrator that uses an LLM-driven structured traversal over service dependency and program dependence graphs to diagnose code-related cloud incidents. It significantly improves root-cause analysis accuracy and reduces computational cost compared to prior methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] BLEST: Blazingly Efficient BFS using Tensor Cores</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [Binarised Virtual Slice Sets (BVSS), graph reordering, batched SpMSpV multiplication, kernel fusion, lazy vertex update]</li>
<li class=""><strong>authors:</strong> Deniz Elbek, Kamer Kaya</li>
<li class=""><strong>institution:</strong> Sabanci University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.21967" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.21967</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BLEST is a GPU-accelerated BFS framework that leverages Tensor Cores by reformulating the pull-based BFS pipeline using a bitmap-oriented structure and a batched sparse matrix-sparse vector multiplication pattern. It introduces techniques like Binarised Virtual Slice Sets for load balancing and employs graph reordering for memory efficiency. The experiments show that BLEST achieves significant speedups over state-of-the-art frameworks like BerryBees, Gunrock, and GSWITCH.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251229] FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [expert parallelism, data shuffling, transformation-communication fusion, pipelined communication engine, load-balancing]</li>
<li class=""><strong>authors:</strong> Zhuoran Zhu, Chunyang Zhu, Hao Lin, Xu Fu, Yiming Zhou, Quanlu Zhang, Zhenhua Li, Feng Qian, Chao Yu, Boxun Li, Guohao Dai, Yu Wang</li>
<li class=""><strong>institution:</strong> Tsinghua University, Infinigence AI, University of Southern California, Zhongguancun Academy, Shanghai Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22036" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22036</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces FUSCO, a communication library that fuses data transformation and communication to optimize distributed data shuffling for Mixture-of-Experts (MoE) model training. It addresses the layout mismatch between expert-major data and device-major communication by using a pipelined engine with lightweight planning. The results show that FUSCO significantly outperforms existing libraries like NCCL and DeepEP in both training and inference latency for MoE models.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 12</strong></p>
<ul>
<li class="">[arXiv251229] A Reinforcement Learning Approach to Synthetic Data Generation <a href="https://arxiv.org/pdf/2512.21395" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search <a href="https://arxiv.org/pdf/2512.21648" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model <a href="https://arxiv.org/pdf/2512.21540" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation <a href="https://arxiv.org/pdf/2512.21351" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning <a href="https://arxiv.org/pdf/2512.21446" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations <a href="https://arxiv.org/pdf/2512.21586" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] Generative Actor Critic <a href="https://arxiv.org/pdf/2512.21527" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning <a href="https://arxiv.org/pdf/2512.21412" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO <a href="https://arxiv.org/pdf/2512.21514" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities <a href="https://arxiv.org/pdf/2512.21717" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] A Comedy of Estimators: On KL Regularization in RL Training of LLMs <a href="https://arxiv.org/pdf/2512.21852" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] Meta-Learning-Based Handover Management in NextG O-RAN <a href="https://arxiv.org/pdf/2512.22022" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 12</strong></p>
<ul>
<li class="">[arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search <a href="https://arxiv.org/pdf/2512.21648" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning <a href="https://arxiv.org/pdf/2512.21446" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification <a href="https://arxiv.org/pdf/2512.21544" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] Discovering Sparse Recovery Algorithms Using Neural Architecture Search <a href="https://arxiv.org/pdf/2512.21563" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization <a href="https://arxiv.org/pdf/2512.21769" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] How Do Agents Perform Code Optimization? An Empirical Study <a href="https://arxiv.org/pdf/2512.21757" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism <a href="https://arxiv.org/pdf/2512.21452" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms <a href="https://arxiv.org/pdf/2512.21925" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction <a href="https://arxiv.org/pdf/2512.22007" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars <a href="https://arxiv.org/pdf/2512.22065" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling <a href="https://arxiv.org/pdf/2512.22066" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251229] Enabling Ultra-Fast Cardiovascular Imaging Across Heterogeneous Clinical Environments with a Generalist Foundation Model and Multimodal Database <a href="https://arxiv.org/pdf/2512.21652" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-30">2025-12-30<a href="#2025-12-30" class="hash-link" aria-label="Direct link to 2025-12-30" title="Direct link to 2025-12-30" translate="no">​</a></h2>
<p><strong>cs.DC total: 42</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251230] Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [adaptive GPU resource allocation, serverless computing, multi-agent systems, workload scheduling, O(N) algorithm]</li>
<li class=""><strong>authors:</strong> Guilin Zhang, Wulan Guo, Ziqi Tan</li>
<li class=""><strong>institution:</strong> George Washington University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22149" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22149</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an adaptive GPU resource allocation framework for multi-agent LLM systems in serverless environments. It uses an O(N) complexity algorithm to dynamically allocate resources based on workload, reducing latency by 85% compared to round-robin scheduling while maintaining throughput.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [edge-cloud collaboration, task decomposition, parallel execution, resource-aware routing, adaptive scheduling]</li>
<li class=""><strong>authors:</strong> Jiangwen Dong, Jiayu Li, Wanyu Lin</li>
<li class=""><strong>institution:</strong> The Hong Kong Polytechnic University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22137" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22137</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes HybridFlow, a framework that dynamically decomposes complex queries into interdependent subtasks and uses a learned router to adaptively assign them to edge or cloud LLMs based on utility and resource budgets. It demonstrates that this approach reduces inference time and token usage while maintaining competitive accuracy on several reasoning benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [spatial dataflow, tile-based compilation, MLIR, Triton, hardware representation, on-chip network, data reuse]</li>
<li class=""><strong>authors:</strong> Wei Li, Zhenyu Bai, Heru Wang, Pranav Dangi, Zhiqiang Zhang, Cheng Tan, Huiying Lan, Weng-Fai Wong, Tulika Mitra</li>
<li class=""><strong>institution:</strong> National University of Singapore, Arizona State University, Google, Lumai Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22168" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22168</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents TL, an end-to-end compiler framework that compiles tile-based programs for spatial dataflow accelerators by distributing tile instances across cores and optimizing data movement over the on-chip network. It achieves better performance than vendor libraries on kernels like GEMM and FlashAttention by increasing data reuse and reducing communications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] On Harnessing Idle Compute at the Edge for Foundation Model Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [selective hybrid tensor parallelism, parameter server, cost optimization model, decentralized training, edge computing]</li>
<li class=""><strong>authors:</strong> Leyang Xue, Meghana Madhyastha, Myungjin Lee, Amos Storkey, Randal Burns, Mahesh K. Marina</li>
<li class=""><strong>institution:</strong> The University of Edinburgh, Johns Hopkins University, Cisco Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22142" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22142</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Cleave, a new paradigm for decentralized foundation model training that uses selective hybrid tensor parallelism and a parameter server framework to partition training operations across edge devices. It matches cloud-based training performance by efficiently scaling to thousands of devices, handling memory limits and device heterogeneity, and outperforms existing edge-training methods in speed and failure recovery.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [GPU virtualization, benchmarking, container isolation, multi-tenancy, CUDA, performance evaluation, MIG, HAMi-core, BUD-FCSP]</li>
<li class=""><strong>authors:</strong> Jithin VG, Ditto PS</li>
<li class=""><strong>institution:</strong> Bud Ecosystem Inc</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22125" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22125</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces GPU-Virt-Bench, a comprehensive benchmarking framework with 56 metrics across 10 categories to evaluate software-based GPU virtualization systems. It demonstrates the framework by comparing solutions like HAMi-core and BUD-FCSP against ideal MIG behavior, providing critical performance insights for deploying GPU resources in multi-tenant environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [FPGA acceleration, High Level Synthesis (HLS), parameter quantization, layer fusion, input-points pruning, Uniform Random Sampling (URS), fixed-point implementation]</li>
<li class=""><strong>authors:</strong> Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn</li>
<li class=""><strong>institution:</strong> National University of Sciences and Technology, RPTU Kaiserslautern-Landau</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22139" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22139</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces HLS4PC, a parameterizable HLS framework for accelerating point-based 3D point cloud models on FPGAs. It applies hardware-aware compression techniques like quantization and pruning to create a lighter model variant, PointMLP-Lite, and demonstrates that the FPGA implementation achieves significantly higher throughput compared to prior works, GPUs, and CPUs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [Minimal Executable Program (MEP), Automatic Error Repair, Performance Pattern Inheritance, iterative optimization, cross-platform portability]</li>
<li class=""><strong>authors:</strong> Ruifan Chu, Anbang Wang, Xiuxiu Bai, Shuai Liu, Xiaoshe Dong</li>
<li class=""><strong>institution:</strong> School of Software Engineering, Xi’an Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22147" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22147</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an LLM-based framework for GPU kernel optimization that avoids expensive full application builds by using automatically generated Minimal Executable Programs (MEPs) for iterative optimization and evaluation. The framework integrates Automatic Error Repair and Performance Pattern Inheritance to fix faults and reuse effective strategies. The method achieves significant speedups on NVIDIA and DCU platforms and demonstrates practical, low-cost optimization without full-source dependencies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SoDA: An Efficient Interaction Paradigm for the Agentic Web</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Sovereign Digital Avatar (SoDA), orthogonal decoupling, Intent-Permission Handshake Mechanism, A2A protocols, dual-factor adaptive routing, Retrieval-Augmented Generation (RAG)]</li>
<li class=""><strong>authors:</strong> Zicai Cui, Zhouyuan Jian, Weiwen Liu, Weinan Zhang</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Shanghai Innovation Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22135" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22135</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes the Sovereign Digital Avatar (SoDA), a user-centric interaction paradigm that decouples storage, computation, and interaction to combat data lock-in and cognitive overload in the Agentic Web. It employs an Intent-Permission Handshake Mechanism for secure operation. Empirical results show SoDA significantly reduces token consumption and user cognitive load while improving information efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [structured pruning, multi-objective optimization, view-adaptive compression, distributed inference]</li>
<li class=""><strong>authors:</strong> Mahadev Sunil Kumar, Arnab Raha, Debayan Das, Gopakumar G, Amitava Mukherjee</li>
<li class=""><strong>institution:</strong> Accenture PLC, Intel Corporation, Indian Institute of Science, Amrita Vishwa Vidyapeetham, Birla Institute of Technology and Science</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22136" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22136</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents SlimEdge, a method for deploying distributed deep neural networks on constrained edge devices by integrating structured model pruning with multi-objective optimization to tailor network capacity to hardware constraints. It demonstrates the framework using a Multi-View CNN for 3D object recognition, adaptively compressing the model based on the contribution of individual views. The resulting models meet specified accuracy and memory bounds while achieving up to 5x faster inference across diverse hardware platforms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AiiDAlab: on the route to accelerate science</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [scientific workflow management], [AiiDAlab, AiiDA, web-based interface, computational workflows, provenance tracking, FAIR principles, electronic laboratory notebooks]</li>
<li class=""><strong>authors:</strong> Aliaksandr V.Yakutovich, Jusong Yu, Daniel Hollas, Edan Bainglass, Corsin Battaglia, Miki Bonacci, Lucas Fernandez Vilanova, Stephan Henne, Anders Kaestner, Michel Kenzelmann, Graham Kimbell, Jakob Lass, Fabio Lopes, Daniel G. Mazzone, Andres Ortega-Guerrero, Xing Wang, Nicola Marzari, Carlo A. Pignedoli, Giovanni Pizzi</li>
<li class=""><strong>institution:</strong> Empa-Swiss Federal Laboratories for Materials Science and Technology, Paul Scherrer Institute, École Polytechnique Fédérale de Lausanne, University of Bristol, ETH Zurich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22173" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22173</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents AiiDAlab, a web-based platform designed to simplify the execution and management of complex computational workflows on supercomputers. Its core method is providing an intuitive browser interface that abstracts technical complexities and automatically tracks full simulation provenance via the AiiDA engine. The main conclusion is that AiiDAlab has matured into a cross-disciplinary platform that accelerates scientific discovery by allowing researchers to focus on science rather than computational details, ensuring reproducibility and adherence to FAIR data principles.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] iOS as Acceleration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [distributed pipeline parallelism, model partitioning, mobile computing, iOS, thermal throttling, memory constraints]</li>
<li class=""><strong>authors:</strong> Alexander K. Chen</li>
<li class=""><strong>institution:</strong> Independent High School Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22180" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22180</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a proof-of-concept system that uses distributed pipeline parallelism to harness the compute power of iOS devices, partitioning model weights to work around their memory limitations. It demonstrates acceleration for tasks like modest model training and batch inference, concluding that ubiquitous mobile devices have significant potential to contribute to machine learning in resource-constrained environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [bit-flip fault localization, differential analysis, residual-path perturbation, loss-sensitivity profiling, transformer architectures]</li>
<li class=""><strong>authors:</strong> Muhammad Zeeshan Karamat, Sadman Saif, Christiana Chamon Garcia</li>
<li class=""><strong>institution:</strong> Virginia Tech</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22174" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22174</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces BitFlipScope, a software framework for localizing and recovering from bit-flip corruptions in Large Language Models. It uses differential analysis with a reference model or, when unavailable, residual-path perturbation and loss-sensitivity profiling to identify fault-affected regions. The framework enables lightweight performance recovery without fine-tuning, enhancing fault resilience for LLM deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] MatKV: Trading Compute for Flash Storage in LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [retrieval augmented generation, key-value vectors, prefill phase, materialization, flash storage]</li>
<li class=""><strong>authors:</strong> Kun-Woo Shin, Jay H. Park, Moonwook Oh, Yohan Jo, Jaeyoung Do, Sang-Won Lee</li>
<li class=""><strong>institution:</strong> Seoul National University, Samsung Electronics</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22195" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22195</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes MatKV, a method that precomputes and stores key-value vectors for RAG documents in flash storage to avoid recomputing them during inference. This approach reduces inference time and power consumption by half while maintaining accuracy. It also enables optimizations like overlapping KV loading with decoding and using lower-end GPUs for decoding.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Valori: A Deterministic Memory Substrate for AI Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [fixed-point arithmetic, Q16.16, state machine, vector embeddings, deterministic memory]</li>
<li class=""><strong>authors:</strong> Varshith Gudur</li>
<li class=""><strong>institution:</strong> Valori Kernel Project, Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22280" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22280</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Valori, a deterministic memory substrate for AI systems that replaces floating-point operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. It guarantees bit-identical memory states and search results across different hardware platforms. The authors conclude that deterministic memory is a necessary primitive for building trustworthy and auditable AI systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC/CFD], [GPU porting, unified memory, memory pool manager, portable programming model, strong scalability, weak scalability]</li>
<li class=""><strong>authors:</strong> Simone Bnà, Giuseppe Giaquinto, Ettore Fadiga, Tommaso Zanelli, Francesco Bottau</li>
<li class=""><strong>institution:</strong> Cineca Supercomputing Centre, Università degli Studi di Napoli Federico II</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22215" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22215</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents SPUMA, a minimally invasive approach for porting the OPENFOAM CFD software to NVIDIA and AMD GPUs using a portable programming model and a memory pool manager leveraging unified memory. Performance tests on pre-exascale clusters show good scalability and up to an 82% reduction in energy consumption compared to CPU-based simulations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Scalable Cloud-Native Architectures for Intelligent PMU Data Processing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [distributed stream processing, containerized microservices, elastic resource orchestration, edge-cloud hybrid architecture, machine learning for time-series analysis]</li>
<li class=""><strong>authors:</strong> Nachiappan Chockalingam, Akshay Deshpande, Lokesh Butra, Ram Sekhar Bodala, Nitin Saksena, Adithya Parthasarathy, Balakrishna Pothineni, Akash Kumar Agarwal</li>
<li class=""><strong>institution:</strong> IEEE, NTT Data, Amtrak, Albertsons Companies</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22231" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22231</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a scalable cloud-native architecture integrating AI, edge, and cloud computing for processing PMU data. It uses distributed stream processing, microservices, and elastic orchestration to enable low-latency analytics and anomaly detection. The approach achieves sub-second response times and provides a robust foundation for next-generation smart grid monitoring.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [mega-kernel, SM-level graph, cross-operator software pipelining, kernel fusion, CUDA, decentralized scheduling]</li>
<li class=""><strong>authors:</strong> Xinhao Cheng, Zhihao Zhang, Yu Zhou, Jianan Ji, Jinchen Jiang, Zepeng Zhao, Ziruo Xiao, Zihao Ye, Yingyi Huang, Ruihang Lai, Hongyi Jin, Bohan Hou, Mengdi Wu, Yixin Dong, Anthony Yip, Zihao Ye, Songting Wang, Wenqin Yang, Xupeng Miao, Tianqi Chen, Zhihao Jia</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, Tsinghua University, NVIDIA, University of Michigan, Purdue University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22219" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22219</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Mirage Persistent Kernel (MPK), a compiler and runtime system that automatically transforms multi-GPU model inference into a single, high-performance mega-kernel using an SM-level graph representation for fine-grained task scheduling. The system enables cross-operator software pipelining and kernel fusion, significantly reducing inference latency. The evaluation shows MPK outperforms existing kernel-per-operator systems by up to 1.7x, pushing LLM inference performance close to hardware limits.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Efficient Multi-Model Orchestration for Self-Hosted Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [Kubernetes, Helm, scale-to-zero, DistilBERT classifier, hybrid routing, keyword heuristics]</li>
<li class=""><strong>authors:</strong> Bhanu Prakash Vangala, Tanu Malik</li>
<li class=""><strong>institution:</strong> University of Missouri</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22402" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22402</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces &quot;Pick and Spin,&quot; a framework built on Kubernetes for orchestrating self-hosted LLMs. It uses a hybrid routing module with keyword heuristics and a DistilBERT classifier, along with adaptive scale-to-zero automation, to efficiently manage multiple models. The system achieves higher success rates, lower latency, and reduced GPU costs compared to static deployments, making enterprise-grade LLM performance more affordable on private infrastructure.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [text-to-sql, cloud compute cost, google bigquery, reasoning models, bytes processed, slot utilization, valid efficiency score (ves)]</li>
<li class=""><strong>authors:</strong> Saurabh Deochake, Debajyoti Mukhopadhyay</li>
<li class=""><strong>institution:</strong> SentinelOne, WIDiCoReL Research Lab</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22364" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22364</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an empirical study evaluating the cloud compute costs of SQL queries generated by six state-of-the-art LLMs on Google BigQuery. The core method involves measuring bytes processed, slot utilization, and estimated cost for 180 query executions on a 230GB dataset. The main conclusion is that reasoning models process significantly fewer bytes (44.5% less) and incur lower costs than standard models while maintaining high correctness, and that execution time is a poor proxy for cloud query cost.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative decoding, adaptive algorithm, multi-armed bandit, dynamic batch size]</li>
<li class=""><strong>authors:</strong> Rui Li, Zhaoning Zhang, Libo Zhang, Huaimin Wang, Xiang Fu, Zhiquan Lai</li>
<li class=""><strong>institution:</strong> National University of Defense Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22420" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22420</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Nightjar, a learning-based adaptive speculative decoding algorithm that dynamically adjusts the speculative length or disables speculation based on the real-time request load (batch size). This overcomes the limitation of fixed-length speculative decoding, which suffers from verification overhead under high loads. Experiments show Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Object Abstraction To Streamline Edge-Cloud-Native Application Development</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud-native computing], [Object-as-a-Service, edge-cloud continuum, serverless, FaaS, state management, orchestration, SLA-driven management]</li>
<li class=""><strong>authors:</strong> Pawissanutt Lertpongrujikorn</li>
<li class=""><strong>institution:</strong> University of North Texas</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22534" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22534</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This dissertation introduces the Object-as-a-Service (OaaS) paradigm, a unified approach to cloud-native development that abstracts infrastructure complexity by consolidating resource, state, and workflow management. Through empirical studies and prototypes like Oparaca and EdgeWeaver, it demonstrates that OaaS reduces development effort and improves performance compared to traditional serverless models, establishing a foundation for platforms that empower developers to focus on application logic.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Role-Based Fault Tolerance System for LLM RL Post-Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [role-based fault isolation, role-aware monitoring, non-disruptive recovery, dynamic point-to-point communication, UCX, warm standbys]</li>
<li class=""><strong>authors:</strong> Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin</li>
<li class=""><strong>institution:</strong> Zhejiang University, State Key Laboratory of Mathematical Engineering and Advanced Computing</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22492" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22492</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces RobustRL, a fault tolerance system for RL post-training of LLMs that uses a role-based approach to isolate and recover from GPU failures. Its method involves detecting failures with role-aware monitoring, restarting only the failed component (trainer or rollout) without disrupting the entire task, and reconnecting it using dynamic communication. The system significantly improves the Effective Training Time Ratio and reduces end-to-end training time compared to prior methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [disaggregated infrastructure, hardware-affinity workload mapping, fine-grained asynchrony, statefulness-aware computation, agentic RL]</li>
<li class=""><strong>authors:</strong> Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang</li>
<li class=""><strong>institution:</strong> HKUST, Alibaba Group, Tongyi Lab</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22560" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22560</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents RollArt, a distributed system designed to scale agentic reinforcement learning training by disaggregating the heterogeneous workload across specialized hardware. Its core methods include hardware-affinity workload mapping, fine-grained asynchrony, and statefulness-aware computation to mitigate synchronization overhead. The results show that RollArt improves training throughput and reduces end-to-end training time by 1.35-2.05× compared to monolithic baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [RDMA, best-effort transport, adaptive timeouts, Hadamard Transform, Erasure Coding, tail latency, collective communication]</li>
<li class=""><strong>authors:</strong> Ertza Warraich, Ali Imran, Annus Zulfiqar, Shay Vargaftik, Sonia Fahmy, Muhammad Shahbaz</li>
<li class=""><strong>institution:</strong> Purdue University, Broadcom, University of Michigan</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22743" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22743</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents OptiNIC, a domain-specific RDMA transport that eliminates retransmissions and in-order delivery, shifting loss recovery to the ML pipeline to reduce tail latency. It uses adaptive timeouts to trigger forward progress and retains standard congestion control. The evaluation shows OptiNIC significantly improves time-to-accuracy, throughput, and resilience for distributed ML workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [energy characterization, dynamic voltage and frequency scaling (DVFS), GPU underutilization, stage-level analysis]</li>
<li class=""><strong>authors:</strong> Mona Moghadampanah, Adib Rezaei Shahmirzadi, Farhana Amin, Dimitrios S. Nikolopoulos</li>
<li class=""><strong>institution:</strong> Virginia Tech</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22695" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22695</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper provides a stage-level energy characterization of multimodal large language model (MLLM) inference, identifying &quot;modality inflation&quot; as a key inefficiency. It demonstrates that dynamic voltage and frequency scaling (DVFS) is an effective optimization, offering energy savings with minimal performance impact for more efficient MLLM serving systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Two-Robot Computational Landscape: A Complete Characterization of Model Power in Minimal Mobile Robot Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [Look-Compute-Move (LCM), OBLOT, FSTA, FCOM, LUMI, FSYNCH, SSYNCH, ASYNCH, robot models, scheduler, simulation-free method]</li>
<li class=""><strong>authors:</strong> Naoki Kitamura, Yuichi Sudo, Koichi Wada</li>
<li class=""><strong>institution:</strong> The University of Osaka, Hosei University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22770" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22770</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a complete characterization of the computational power of two autonomous mobile robots across major models and schedulers using a novel simulation-free method. It concludes that the computational landscape for two robots fundamentally differs from the general case, with key findings including the equivalence of FSTA and LUMI under full synchrony and the orthogonality of FSTA and FCOM.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Argus: Token Aware Distributed LLM Inference Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [token-aware offloading, length-aware semantics, lyapunov optimization, integer nonlinear programming, edge-cloud system]</li>
<li class=""><strong>authors:</strong> Panlong Wu, Yifei Zhong, Danyang Chen, Ting Wang, Fangxin Wang</li>
<li class=""><strong>institution:</strong> CUHK(SZ)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22925" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22925</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents Argus, a token-aware distributed LLM inference framework for edge-cloud systems. Its core method includes a module to predict output token lengths and a Lyapunov-guided optimization module for task offloading, solved by a novel iterative algorithm. The conclusion is that Argus achieves robust and efficient performance in dynamic, heterogeneous environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] A Domain Decomposition-based Solver for Acoustic Wave propagation in Two-Dimensional Random Media</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [computational mathematics], [domain decomposition, stochastic Galerkin method, polynomial chaos expansion, Neumann-Neumann preconditioner, conjugate gradient]</li>
<li class=""><strong>authors:</strong> Sudhi Sharma Padillath Vasudevan</li>
<li class=""><strong>institution:</strong> Carleton University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23027" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23027</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a domain decomposition-based solver for acoustic wave propagation in random media, using an intrusive stochastic Galerkin method with polynomial chaos expansion. The method transforms the stochastic PDE into a deterministic system and employs a conjugate gradient solver with a two-level Neumann-Neumann preconditioner to handle high computational costs. The results demonstrate the solver&#x27;s efficient scalability with increasing problem size and number of random parameters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [quantization, mixture-of-experts (MoE), on-premise deployment, benchmarking, consumer-grade hardware]</li>
<li class=""><strong>authors:</strong> Alex Khalil, Guillaume Heilles, Maria Parraga, Simon Heilles</li>
<li class=""><strong>institution:</strong> UCLouvain, Universidad Espíritu Santo, DENEM Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23029" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23029</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper benchmarks the performance of a quantized, open-source Qwen3-30B MoE model deployed on a private server with consumer-grade hardware (NVIDIA RTX 5090). It finds that this on-premise setup can achieve performance comparable to commercial cloud services, offering SMBs a cost-effective and privacy-preserving alternative for LLM inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [osmotic learning, self-supervised learning, distributed representation learning, decentralized clustering, information diffusion, dynamic equilibrium]</li>
<li class=""><strong>authors:</strong> Mario Colosi, Reza Farahani, Maria Fazio, Radu Prodan, Massimo Villari</li>
<li class=""><strong>institution:</strong> University of Messina, University of Klagenfurt, University of Innsbruck</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23096" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23096</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Osmotic Learning (OSM-L), a self-supervised distributed learning paradigm that iteratively aligns local data representations to enable information diffusion and convergence into a shared latent knowledge without raw data exchange. The method functions as a decentralized clustering mechanism to uncover hidden structures in distributed data. Experimental results confirm its convergence and high accuracy in aligning local information while preserving contextual integrity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, graph neural networks, fairness, overlapping subgraphs, weighted aggregation, privacy-preserving estimation]</li>
<li class=""><strong>authors:</strong> Zihao Zhou, Shusen Yang, Fangyuan Zhao, Xuebin Ren</li>
<li class=""><strong>institution:</strong> Xi&#x27;an Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23235" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23235</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes FairGFL, a fairness-aware federated learning algorithm for graph data that addresses unfairness from imbalanced overlapping subgraphs. It uses a privacy-preserving weighted aggregation method and a composite loss regularizer to improve fairness while maintaining model utility. Experiments on benchmark datasets show it outperforms baselines in both utility and fairness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Optimal Configuration of API Resources in Cloud Native Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing], [black-box optimization, bayesian optimization, factor screening, Kubernetes, microservices, resource allocation]</li>
<li class=""><strong>authors:</strong> Eddy Truyen, Wouter Joosen</li>
<li class=""><strong>institution:</strong> KU Leuven</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23494" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23494</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper applies a black-box optimization framework to find optimal CPU and memory configurations for microservices in Kubernetes during the DevOps Release phase. It concludes that factor screening is useful for finding the optimal configuration with a limited budget, but Bayesian optimization without screening is better for finding a near-optimal solution.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Kubernetes autoscaling, AIOps, SLO-aware control, cost optimization, demand forecasting, multi-signal framework]</li>
<li class=""><strong>authors:</strong> Vinoth Punniyamoorthy, Bikesh Kumar, Sumit Saha, Lokesh Butra, Mayilsamy Palanigounder, Akash Kumar Agarwal, Kabilan Kannan</li>
<li class=""><strong>institution:</strong> IEEE, East West Bank, NTT Data, Albertsons</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23415" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23415</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an AIOps-driven autoscaling framework for Kubernetes that integrates SLO-aware and cost-conscious control with lightweight demand forecasting. The method uses a multi-signal approach to improve responsiveness and stability. The results show it reduces SLO violations by up to 31%, improves scaling response time by 24%, and lowers infrastructure cost by 18% compared to baseline Kubernetes autoscalers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [lyapunov optimization, deep reinforcement learning, edge-cloud partitioning, transformer layer decomposition, queue stability]</li>
<li class=""><strong>authors:</strong> Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer</li>
<li class=""><strong>institution:</strong> University of Innsbruck, Sharif University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23310" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23310</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Splitwise, a framework that uses Lyapunov-assisted deep reinforcement learning to dynamically partition LLM inference tasks between edge devices and the cloud at a fine-grained sub-layer level. It aims to jointly minimize latency and energy consumption while maintaining accuracy under variable network conditions. Experiments show it significantly reduces latency and energy use compared to existing methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Decoupling Adaptive Control in TeaStore</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [self-adaptive software systems, cloud-native computing], [MAPE-K control loop, microservices, Operator pattern, software architectural methods, modularity, system-wide consistency, planning]</li>
<li class=""><strong>authors:</strong> Eddy Truyen</li>
<li class=""><strong>institution:</strong> DistriNet, KU Leuven</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23495" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23495</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper examines how software architectural methods, the cloud-native Operator pattern, and legacy programming techniques can be used to decouple self-adaptive control logic from the TeaStore microservice application. It analyzes the trade-offs between fine-grained adaptation and system-wide control. The main conclusion is that these approaches can be combined into a multi-tiered architecture for self-adaptive microservices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Bitcoin-IPC: Scaling Bitcoin with a Network of Proof-of-Stake Subnets</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain scaling], [Bitcoin, Layer-2, Proof-of-Stake, subnet, SegWit, SWIFT messaging, virtual-byte cost]</li>
<li class=""><strong>authors:</strong> Marko Vukolić, Orestis Alpos, Jakov Mitrovski, Themis Papameletiou, Nikola Ristić, Dionysis Zindros</li>
<li class=""><strong>institution:</strong> Bitcoin Scaling Labs, Common Prefix</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23439" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23439</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Bitcoin-IPC, a protocol that scales Bitcoin by enabling the creation of permissionless Proof-of-Stake Layer-2 subnets whose stake is denominated in Bitcoin. Its design, inspired by SWIFT messaging and embedded in Bitcoin&#x27;s SegWit, routes value transfers through Bitcoin L1, reducing transaction cost by up to 23x and increasing throughput to over 160 tps without modifying Bitcoin L1.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [consistent hashing, rendezvous hashing, highest random weight, load balancing, minimal churn, cache locality]</li>
<li class=""><strong>authors:</strong> Yongjie Guan</li>
<li class=""><strong>institution:</strong> Zhejiang University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23434" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23434</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Local Rendezvous Hashing (LRH), a method that combines a token ring with a cache-local Highest Random Weight (HRW) election among a fixed window of distinct neighboring nodes to improve load balance. It achieves near-optimal load distribution with minimal key movement during failures and significantly outperforms multi-probe consistent hashing in lookup throughput while maintaining similar balance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Fancy Some Chips for Your TeaStore? Modeling the Control of an Adaptable Discrete System</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [modeling language for distributed systems], [Chips, control theory, component-based models, adaptable systems, BIP translation]</li>
<li class=""><strong>authors:</strong> Anna Gallone, Simon Bliudze, Sophie Cerf, Olga Kouchnarenko</li>
<li class=""><strong>institution:</strong> Université Marie et Louis Pasteur, CNRS UMR6174, Institut FEMTO-ST; Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 CRIStAL</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23496" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23496</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Chips, a modeling language that combines control theory with programming concepts to design and analyze robust, component-based systems. It demonstrates the language&#x27;s application through a case study of an adaptable TeaStore web application, showing how Chips facilitates systematic modeling and analysis of complex distributed systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] AdaptiFlow: An Extensible Framework for Event-Driven Autonomy in Cloud Microservices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [autonomic computing], [MAPE-K loop, event-driven adaptation, rule-based mechanism, decentralized adaptation, self-healing, self-protection, self-optimization]</li>
<li class=""><strong>authors:</strong> Brice Arléon Zemtsop Ndadji, Simon Bliudze, Clément Quinton</li>
<li class=""><strong>institution:</strong> Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23499" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23499</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents AdaptiFlow, a framework that provides abstraction layers for the Monitor and Execute phases of the MAPE-K loop to enable event-driven, decentralized autonomy in cloud microservices. It demonstrates that decentralized adaptation can be achieved with minimal service modification through localized decisions, bridging autonomic computing theory with cloud-native practice.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [signal processing], [power/energy consumption model, integer-friendly approximation, piecewise-polynomial, conflict-free data placement, mixed-radix streaming FFT, self-sorting FFT, fast Schur algorithm]</li>
<li class=""><strong>authors:</strong> Sergey Salishev</li>
<li class=""><strong>institution:</strong> Saint Petersburg State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22676" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22676</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This thesis develops signal-processing algorithms and hardware implementation schemes under constraints of minimal parallelism and memory to improve energy efficiency. It proposes models and methods including a power model for selecting optimal parallelism, approximation techniques to reduce lookup-table size, and conflict-free schedules for FFT computations. The results provide constructive theorems and design trade-offs for building efficient specialized accelerators for low-power computing.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Revisiting finite Abelian hidden subgroup problem and its distributed exact quantum algorithm</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [quantum algorithms], [amplitude amplification, Chinese Remainder Theorem, exact quantum algorithm, distributed quantum algorithm, hidden subgroup problem]</li>
<li class=""><strong>authors:</strong> Ziyuan Dong, Xiang Fan, Tengxun Zhong, Daowen Qiu</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.22959" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.22959</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper revisits the finite Abelian hidden subgroup problem, presenting a new exact quantum algorithm using amplitude amplification and a distributed version leveraging the Chinese Remainder Theorem. The distributed algorithm requires fewer qudits, lower query complexity, and no quantum communication. The authors also extend the distributed approach to some non-Abelian groups and develop a parallel classical algorithm with reduced query complexity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251230] Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, L0 constraint, probabilistic gates, sparsity, federated stochastic gradient descent]</li>
<li class=""><strong>authors:</strong> Krishna Harsha Kovelakuntla Huthasana, Alireza Olama, Andreas Lundell</li>
<li class=""><strong>institution:</strong> Åbo Akademi University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23071" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23071</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a federated learning method that enforces an L0 constraint on model parameters to achieve sparsity, using a reparameterization with probabilistic gates and continuous relaxation. The approach is shown to effectively reach a target parameter density under data and client heterogeneity with minimal performance loss, outperforming magnitude pruning-based methods in communication efficiency and statistical performance across various models and datasets.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 37</strong></p>
<ul>
<li class="">[arXiv251230] Unbiased Visual Reasoning with Controlled Visual Inputs <a href="https://arxiv.org/pdf/2512.22183" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks <a href="https://arxiv.org/pdf/2512.22186" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part I: Basic Concepts, Neural Networks, and Variants <a href="https://arxiv.org/pdf/2512.22190" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents <a href="https://arxiv.org/pdf/2512.22200" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] DiRL: An Efficient Post-Training Framework for Diffusion Language Models <a href="https://arxiv.org/pdf/2512.22234" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Masking Teacher and Reinforcing Student for Distilling Vision-Language Models <a href="https://arxiv.org/pdf/2512.22238" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Agentic Software Issue Resolution with Large Language Models: A Survey <a href="https://arxiv.org/pdf/2512.22256" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning <a href="https://arxiv.org/pdf/2512.22315" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents <a href="https://arxiv.org/pdf/2512.22322" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] PHANTOM: Physics-Aware Adversarial Attacks against Federated Learning-Coordinated EV Charging Management System <a href="https://arxiv.org/pdf/2512.22381" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing <a href="https://arxiv.org/pdf/2512.22455" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Memento-II: Learning by Stateful Reflective Memory <a href="https://arxiv.org/pdf/2512.22716" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents <a href="https://arxiv.org/pdf/2512.22733" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] ReDiF: Reinforced Distillation for Few Step Diffusion <a href="https://arxiv.org/pdf/2512.22802" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning <a href="https://arxiv.org/pdf/2512.22824" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning <a href="https://arxiv.org/pdf/2512.22857" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks <a href="https://arxiv.org/pdf/2512.22860" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks <a href="https://arxiv.org/pdf/2512.22876" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2512.22895" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning <a href="https://arxiv.org/pdf/2512.22910" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Heterogeneity in Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2512.22941" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] APO: Alpha-Divergence Preference Optimization <a href="https://arxiv.org/pdf/2512.22953" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning <a href="https://arxiv.org/pdf/2512.23087" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients <a href="https://arxiv.org/pdf/2512.23090" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms <a href="https://arxiv.org/pdf/2512.23097" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Evaluating Parameter Efficient Methods for RLVR <a href="https://arxiv.org/pdf/2512.23165" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing <a href="https://arxiv.org/pdf/2512.23244" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis <a href="https://arxiv.org/pdf/2512.23366" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis <a href="https://arxiv.org/pdf/2512.23419" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following <a href="https://arxiv.org/pdf/2512.23457" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance <a href="https://arxiv.org/pdf/2512.23461" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation <a href="https://arxiv.org/pdf/2512.23464" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation <a href="https://arxiv.org/pdf/2512.23480" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis <a href="https://arxiv.org/pdf/2512.23545" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning <a href="https://arxiv.org/pdf/2512.23617" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Training AI Co-Scientists Using Rubric Rewards <a href="https://arxiv.org/pdf/2512.23707" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning <a href="https://arxiv.org/pdf/2512.23515" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 20</strong></p>
<ul>
<li class="">[arXiv251230] HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology <a href="https://arxiv.org/pdf/2512.22188" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] DiRL: An Efficient Post-Training Framework for Diffusion Language Models <a href="https://arxiv.org/pdf/2512.22234" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Graph Attention-based Adaptive Transfer Learning for Link Prediction <a href="https://arxiv.org/pdf/2512.22252" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] LuxIA: A Lightweight Unitary matriX-based Framework Built on an Iterative Algorithm for Photonic Neural Network Training <a href="https://arxiv.org/pdf/2512.22264" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations <a href="https://arxiv.org/pdf/2512.22283" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators <a href="https://arxiv.org/pdf/2512.22307" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents <a href="https://arxiv.org/pdf/2512.22387" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Quantum Generative Models for Computational Fluid Dynamics: A First Exploration of Latent Space Learning in Lattice Boltzmann Simulations <a href="https://arxiv.org/pdf/2512.22672" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach <a href="https://arxiv.org/pdf/2512.22793" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning <a href="https://arxiv.org/pdf/2512.22824" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals <a href="https://arxiv.org/pdf/2512.23131" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction <a href="https://arxiv.org/pdf/2512.23175" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta <a href="https://arxiv.org/pdf/2512.23236" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants <a href="https://arxiv.org/pdf/2512.23312" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] SoulX-LiveTalk Technical Report <a href="https://arxiv.org/pdf/2512.23379" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis <a href="https://arxiv.org/pdf/2512.23424" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification <a href="https://arxiv.org/pdf/2512.23436" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation <a href="https://arxiv.org/pdf/2512.23464" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Space AI: Leveraging Artificial Intelligence for Space to Improve Life on Earth <a href="https://arxiv.org/pdf/2512.22399" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251230] Constraint programming model and biased random-key genetic algorithm for the single-machine coupled task scheduling problem with exact delays to minimize the makespan <a href="https://arxiv.org/pdf/2512.23150" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2026-01-01">2026-01-01<a href="#2026-01-01" class="hash-link" aria-label="Direct link to 2026-01-01" title="Direct link to 2026-01-01" translate="no">​</a></h2>
<p><strong>cs.DC total: 17</strong></p>
<ul>
<li class="">
<p><strong>[arXiv260101] PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [lossy compression, KV cache, quantization, matrix-vector multiplication, GPU memory bandwidth]</li>
<li class=""><strong>authors:</strong> Bo Jiang, Taolue Yang, Youyuan Liu, Xubin He, Sheng Di, Sian Jin</li>
<li class=""><strong>institution:</strong> Temple University, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24449" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24449</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PackKV, a framework that uses LLM-aware lossy compression techniques to reduce the memory footprint of the KV cache during long-context inference. It achieves significantly higher memory reduction and throughput compared to state-of-the-art quantization methods while maintaining accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Data Heterogeneity-Aware Client Selection for Federated Learning in Wireless Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [client selection, resource allocation, convex optimization, generalization error, data heterogeneity]</li>
<li class=""><strong>authors:</strong> Yanbing Yang, Huiling Zhu, Wenchi Cheng, Jingqing Wang, Changrun Chen, Jiangzhou Wang</li>
<li class=""><strong>institution:</strong> Xidian University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24286" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24286</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a joint client selection and resource allocation (CSRA) approach for federated learning in wireless networks, which uses convex optimization techniques to minimize latency and energy consumption while controlling generalization error caused by data heterogeneity. The method accounts for data heterogeneity among clients to improve efficiency. Simulation results show the proposed scheme achieves higher test accuracy with lower latency and energy consumption compared to baseline methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] A Granular Grassmannian Clustering Framework via the Schubert Variety of Best Fit</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [subspace clustering], [Schubert Variety of Best Fit, Grassmann manifold, Linde-Buzo-Grey algorithm, principal angles]</li>
<li class=""><strong>authors:</strong> Karim Salta, Michael Kirby, Chris Peterson</li>
<li class=""><strong>institution:</strong> Colorado State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23766" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23766</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a subspace clustering algorithm that uses a trainable prototype called a Schubert Variety of Best Fit (SVBF) instead of a subspace mean. This SVBF-LBG method, integrated into the Linde-Buzo-Grey pipeline, demonstrates improved cluster purity on synthetic and real-world data while preserving mathematical structure for further analysis.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Time-varying Mixing Matrix Design for Energy-efficient Decentralized Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [mixing matrix design, decentralized federated learning, energy consumption, time-varying topologies, multi-phase framework]</li>
<li class=""><strong>authors:</strong> Xusheng Zhang, Tuan Nguyen, Ting He</li>
<li class=""><strong>institution:</strong> University of Oxford, Pennsylvania State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24069" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24069</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a multi-phase design framework for time-varying mixing matrices to minimize the maximum per-node energy consumption in decentralized federated learning over wireless networks. The method trades off per-iteration energy use and convergence rate by activating optimized communication topologies. Evaluations show the solution effectively combines the low energy of sparse matrices and fast convergence of dense matrices.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Squeezing Edge Performance: A Sensitivity-Aware Container Management for Heterogeneous Tasks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [edge computing resource management], [container-based resource management, mixed-integer nonlinear programming, convex optimization, greedy refinement, queueing-based delay formulation]</li>
<li class=""><strong>authors:</strong> Yongmin Zhang, Pengyu Huang, Mingyi Dong, Jing Yao</li>
<li class=""><strong>institution:</strong> Central South University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23952" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23952</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a measurement-driven, two-stage container-based resource management scheme (CRMS) that uses a nonlinear performance model and convex optimization to minimize latency and power consumption on a single edge server. The method decomposes an NP-hard MINLP problem into tractable subproblems. Simulation results show CRMS reduces latency by over 14% and improves energy efficiency compared to baseline approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] RedunCut: Measurement-Driven Sampling and Accuracy Performance Modeling for Low-Cost Live Video Analytics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [dynamic model size selection, measurement-driven planner, data-driven performance model, sampling cost-benefit tradeoff, accuracy prediction]</li>
<li class=""><strong>authors:</strong> Gur-Eyal Sela, Kumar Krishna Agrawal, Bharathan Balaji, Joseph Gonzalez, Ion Stoica</li>
<li class=""><strong>institution:</strong> UC Berkeley, Amazon</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24386" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24386</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RedunCut is a system for live video analytics that reduces compute cost by using a measurement-driven planner to decide when to sample different model sizes and a data-driven model to predict per-segment accuracy. It dynamically selects the smallest suitable model for each video segment to meet a specified accuracy target. The system demonstrates cost reductions of 14-62% across diverse video workloads while maintaining accuracy and robustness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Zero-Trust Agentic Federated Learning for Secure IIoT Defense Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [federated learning, zero-trust architecture, TPM attestation, SHAP-weighted aggregation, Byzantine detection, adversarial training]</li>
<li class=""><strong>authors:</strong> Samaresh Kumar Singh, Joyjit Roy, Martin So</li>
<li class=""><strong>institution:</strong> Independent Researchers</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23809" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23809</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes ZTA-FL, a secure federated learning framework for IIoT that integrates TPM-based attestation, SHAP-weighted aggregation for Byzantine resilience, and on-device adversarial training. It demonstrates high detection accuracy and robustness against poisoning attacks while reducing communication overhead. The main conclusion is that the framework provides a defense-in-depth solution for secure, privacy-preserving collaborative intrusion detection in critical infrastructure.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Governing Cloud Data Pipelines with Agentic AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [policy-aware control architecture, bounded AI agents, adaptive resource reconfiguration, schema reconciliation, automated failure recovery, declarative policies]</li>
<li class=""><strong>authors:</strong> Aswathnarayan Muthukrishnan Kirubakaran, Adithya Parthasarathy, Nitin Saksena, Ram Sekhar Bodala, Akshay Deshpande, Suhas Malempati, Shiva Carimireddy, Abhirup Mazumder</li>
<li class=""><strong>institution:</strong> IEEE, Albertsons, Amtrak, Cato</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23737" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23737</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Agentic Cloud Data Engineering, a control architecture that integrates bounded AI agents to autonomously manage cloud data pipelines by analyzing telemetry and reasoning over governance policies. The system reduces recovery time by up to 45%, lowers operational costs by ~25%, and cuts manual intervention by over 70% compared to static orchestration, demonstrating its effectiveness for enterprise pipeline governance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python with Partitioning and Parallel Execution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [memory management], [zone-based garbage collection, concurrent mark-and-sweep, predictive memory mapping, dual-layer architecture, cache alignment]</li>
<li class=""><strong>authors:</strong> Abdulla M</li>
<li class=""><strong>institution:</strong> Abdullahlab-n (inferred from GitHub organization)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23768" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23768</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces VGC, a dual-layer garbage collector with an Active component for runtime concurrent mark-and-sweep and a Passive component for compile-time predictive memory mapping. This architecture reduces pause times by up to 30% and memory usage by up to 25% compared to traditional collectors. The main conclusion is that VGC provides a scalable and efficient memory management solution for Python, particularly for parallel and memory-intensive applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Security Without Detection: Economic Denial as a Primitive for Edge and IoT Defense</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [IoT/Edge Security], [Economic Denial Security (EDS), computational puzzles, decoy-driven interaction entropy, temporal stretching, bandwidth taxation, Stackelberg game, cost asymmetry]</li>
<li class=""><strong>authors:</strong> Samaresh Kumar Singh, Joyjit Roy</li>
<li class=""><strong>institution:</strong> IEEE</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.23849" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.23849</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Economic Denial Security (EDS), a detection-independent framework that makes attacks economically infeasible by composing four mechanisms (computational puzzles, decoy-driven interaction, temporal stretching, bandwidth taxation) to impose superlinear costs on attackers. It demonstrates that EDS significantly slows attacks, increases cost asymmetry, and reduces attack success with minimal overhead, providing a viable defense for resource-constrained IoT and edge systems where traditional detection fails.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Distributed Bilevel Optimization with Dual Pruning for Resource-limited Clients</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [distributed bilevel optimization, resource-adaptive framework, hypergradient estimator, dual pruning, convergence analysis]</li>
<li class=""><strong>authors:</strong> Mingyi Li, Xiao Zhang, Ruisheng Zheng, Hongjian Shi, Yuan Yuan, Xiuzhen Cheng, Dongxiao Yu</li>
<li class=""><strong>institution:</strong> Shandong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24667" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24667</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a resource-adaptive distributed bilevel optimization framework with a second-order free hypergradient estimator, enabling clients with limited resources to optimize submodels. The theoretical analysis shows the proposed methods achieve an asymptotically optimal convergence rate. Experiments demonstrate the effectiveness and computational efficiency of the approach.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Reliable and Resilient Collective Communication Library for LLM Training and Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [collective communication, connection migration, bandwidth-aware load redistribution, resilient collective algorithms, multi-NIC failover]</li>
<li class=""><strong>authors:</strong> Wei Wang, Nengneng Yu, Sixian Xiong, Zaoxing Liu</li>
<li class=""><strong>institution:</strong> University of Maryland, College Park</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.25059" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.25059</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents R²CCL, a fault-tolerant communication library for large-scale LLM training and inference. It exploits multi-NIC hardware to perform rapid connection migration and load redistribution to maintain progress under network failures. The evaluation shows it incurs low overhead and significantly outperforms existing baselines in recovery performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Understanding LLM Checkpoint/Restore I/O Strategies and Patterns</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [checkpoint/restore, I/O characterization, liburing, I/O aggregation, I/O coalescing, 3D parallelism]</li>
<li class=""><strong>authors:</strong> Mikaila J. Gossman, Avinash Maurya, Bogdan Nicolae, Jon C. Calhoun</li>
<li class=""><strong>institution:</strong> Clemson University, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24511" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24511</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper investigates I/O strategies for checkpointing large language models, focusing on using the kernel-accelerated liburing library and techniques like aggregation and coalescing to improve performance. It finds that file system-aware aggregation restores bandwidth and reduces metadata overhead. The proposed approach achieves significantly higher write throughput compared to state-of-the-art checkpointing engines like DataStates-LLM and TorchSnapshot.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Document Data Matching for Blockchain-Supported Real Estate</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [optical character recognition, natural language processing, verifiable credentials, blockchain, data matching]</li>
<li class=""><strong>authors:</strong> Henrique Lin, Tiago Dias, Miguel Correia</li>
<li class=""><strong>institution:</strong> INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Unlockit</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24457" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24457</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a system that uses OCR and NLP to extract and standardize data from real estate documents into verifiable credentials, with blockchain providing a decentralized trust layer. The prototype demonstrates automated data matching for inconsistency detection and reduces verification time. The framework shows potential to streamline transactions and enhance trust in the real estate sector.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [LLM-driven search, evolutionary search, instance-optimal heuristics, policy-mechanism separation, code generation]</li>
<li class=""><strong>authors:</strong> Rohit Dwivedula, Divyanshu Saxena, Sujay Yadalam, Daehyeok Kim, Aditya Akella</li>
<li class=""><strong>institution:</strong> The University of Texas at Austin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.25065" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.25065</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Vulcan, a framework that uses large language models (LLMs) combined with evolutionary search to automatically synthesize instance-optimal heuristics for system resource-management tasks like caching and memory tiering. It introduces LLM-friendly, task-agnostic interfaces to separate policy from mechanism, enabling efficient code generation. The synthesized heuristics outperform state-of-the-art human-designed algorithms by up to 69% in specific tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] Adaptive Resource Orchestration for Distributed Quantum Computing Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [quantum computing systems], [Modular Entanglement Hub (ModEn-Hub), quantum network orchestrator, hub-and-spoke photonic interconnect, teleportation-based non-local gates, parallel entanglement attempts, ebit cache, Monte Carlo simulation]</li>
<li class=""><strong>authors:</strong> Kuan-Cheng Chen, Felix Burt, Nitish K. Panigrahy, Kin K. Leung</li>
<li class=""><strong>institution:</strong> Imperial College London, Binghamton University SUNY</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24902" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24902</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes the Modular Entanglement Hub (ModEn-Hub) architecture, which uses a central hub with an orchestrator to manage entanglement generation and caching across distributed quantum processors. Through Monte Carlo simulation, the study shows this adaptive orchestration sustains high teleportation success rates (around 90%) across many QPUs, significantly outperforming a naive sequential baseline.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260101] AI-Driven Cloud Resource Optimization for Multi-Cluster Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [predictive learning, policy-aware decision-making, continuous feedback, cross-cluster telemetry, adaptive resource optimization]</li>
<li class=""><strong>authors:</strong> Vinoth Punniyamoorthy, Akash Kumar Agarwal, Bikesh Kumar, Abhirup Mazumder, Kabilan Kannan, Sumit Saha</li>
<li class=""><strong>institution:</strong> IEEE, Albertsons, East West Bank</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.24914" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.24914</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an AI-driven framework that uses predictive learning and continuous feedback to proactively optimize resource allocation across multi-cluster cloud environments. The method integrates cross-cluster telemetry analysis and policy-aware decision-making to balance performance, cost, and reliability. The prototype demonstrates improved resource efficiency and faster stabilization compared to conventional reactive approaches.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 32</strong></p>
<ul>
<li class="">[arXiv260101] Constraint Breeds Generalization: Temporal Dynamics as an Inductive Bias <a href="https://arxiv.org/pdf/2512.23916" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory <a href="https://arxiv.org/pdf/2512.23760" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Efficient Inference for Inverse Reinforcement Learning and Dynamic Discrete Choice Models <a href="https://arxiv.org/pdf/2512.24407" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Max-Entropy Reinforcement Learning with Flow Matching and A Case Study on LQR <a href="https://arxiv.org/pdf/2512.23870" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] FineFT: Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading <a href="https://arxiv.org/pdf/2512.23773" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics <a href="https://arxiv.org/pdf/2512.24445" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Prompt-Induced Over-Generation as Denial-of-Service: A Black-Box Attack-Side Benchmark <a href="https://arxiv.org/pdf/2512.23779" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] MaRCA: Multi-Agent Reinforcement Learning for Dynamic Computation Allocation in Large-Scale Recommender Systems <a href="https://arxiv.org/pdf/2512.24325" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] DRL-TH: Jointly Utilizing Temporal Graph Attention and Hierarchical Fusion for UGV Navigation in Crowded Environments <a href="https://arxiv.org/pdf/2512.24284" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment <a href="https://arxiv.org/pdf/2512.24040" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations <a href="https://arxiv.org/pdf/2512.24023" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] A Survey of AI Methods for Geometry Preparation and Mesh Generation in Engineering Simulation <a href="https://arxiv.org/pdf/2512.23719" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] GARDO: Reinforcing Diffusion Models without Reward Hacking <a href="https://arxiv.org/pdf/2512.24138" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions <a href="https://arxiv.org/pdf/2512.23770" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem <a href="https://arxiv.org/pdf/2512.24251" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning <a href="https://arxiv.org/pdf/2512.24532" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] From Perception to Punchline: Empowering VLM with the Art of In-the-wild Meme <a href="https://arxiv.org/pdf/2512.24555" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization <a href="https://arxiv.org/pdf/2512.24609" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization <a href="https://arxiv.org/pdf/2512.24615" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation <a href="https://arxiv.org/pdf/2512.24651" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting <a href="https://arxiv.org/pdf/2512.24702" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow <a href="https://arxiv.org/pdf/2512.24766" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Iterative Deployment Improves Planning Skills in LLMs <a href="https://arxiv.org/pdf/2512.24940" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control <a href="https://arxiv.org/pdf/2512.24955" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning <a href="https://arxiv.org/pdf/2512.25023" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Many Minds from One Model: Bayesian Transformers for Population Intelligence <a href="https://arxiv.org/pdf/2512.25063" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Scaling Open-Ended Reasoning to Predict the Future <a href="https://arxiv.org/pdf/2512.25070" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Fitted Q Evaluation Without Bellman Completeness via Stationary Weighting <a href="https://arxiv.org/pdf/2512.23805" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Stationary Reweighting Yields Local Convergence of Soft Fitted Q-Iteration <a href="https://arxiv.org/pdf/2512.23927" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Policy Mirror Descent with Temporal Difference Learning: Sample Complexity under Online Markov Data <a href="https://arxiv.org/pdf/2512.24056" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Robust Bayesian Dynamic Programming for On-policy Risk-sensitive Reinforcement Learning <a href="https://arxiv.org/pdf/2512.24580" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Sparse Offline Reinforcement Learning with Corruption Robustness <a href="https://arxiv.org/pdf/2512.24768" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 15</strong></p>
<ul>
<li class="">[arXiv260101] Learning Coupled System Dynamics under Incomplete Physical Constraints and Missing Data <a href="https://arxiv.org/pdf/2512.23761" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Joint Selection for Large-Scale Pre-Training Data via Policy Gradient-based Mask Learning <a href="https://arxiv.org/pdf/2512.24265" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Entropy-Aware Speculative Decoding Toward Improved LLM Reasoning <a href="https://arxiv.org/pdf/2512.23765" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] A Survey of AI Methods for Geometry Preparation and Mesh Generation in Engineering Simulation <a href="https://arxiv.org/pdf/2512.23719" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Enabling Physical AI at the Edge: Hardware-Accelerated Recovery of System Dynamics <a href="https://arxiv.org/pdf/2512.23767" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] DermaVQA-DAS: Dermatology Assessment Schema (DAS) &amp; Datasets for Closed-Ended Question Answering &amp; Segmentation in Patient-Generated Dermatology Images <a href="https://arxiv.org/pdf/2512.24340" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] SCP: Accelerating Discovery with a Global Web of Autonomous Scientific Agents <a href="https://arxiv.org/pdf/2512.24189" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots <a href="https://arxiv.org/pdf/2512.24673" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] FPGA Co-Design for Efficient N<!-- -->:M<!-- --> Sparse and Quantized Model Inference <a href="https://arxiv.org/pdf/2512.24713" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Big AI is accelerating the metacrisis: What can we do? <a href="https://arxiv.org/pdf/2512.24863" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing <a href="https://arxiv.org/pdf/2512.24896" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] q3-MuPa: Quick, Quiet, Quantitative Multi-Parametric MRI using Physics-Informed Diffusion Models <a href="https://arxiv.org/pdf/2512.23726" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] A multimodal Transformer for InSAR-based ground deformation forecasting with cross-site generalization across Europe <a href="https://arxiv.org/pdf/2512.23906" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] An Adaptive, Disentangled Representation for Multidimensional MRI Reconstruction <a href="https://arxiv.org/pdf/2512.24674" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260101] Are First-Order Diffusion Samplers Really Slower? A Fast Forward-Value Approach <a href="https://arxiv.org/pdf/2512.24927" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2026-01-15T02:51:23.000Z" itemprop="dateModified">Jan 15, 2026</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20251222-20251228"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251222-20251228</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20260105-20260111"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20260105-20260111</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-29" class="table-of-contents__link toc-highlight">2025-12-29</a></li><li><a href="#2025-12-30" class="table-of-contents__link toc-highlight">2025-12-30</a></li><li><a href="#2026-01-01" class="table-of-contents__link toc-highlight">2026-01-01</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>