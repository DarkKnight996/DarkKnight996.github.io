<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251020-20251026" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251020-20251026 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20251020-20251026"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251020-20251026 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-10-21"><meta data-rh="true" property="og:description" content="2025-10-21"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20251020-20251026"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251020-20251026" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251020-20251026" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20251020-20251026","item":"https://darkknight996.github.io/daily/20251020-20251026"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.5c13e80c.js" defer="defer"></script>
<script src="/assets/js/main.5489b0ec.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251020-20251026</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251020-20251026</h1></header><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-21">2025-10-21<a href="#2025-10-21" class="hash-link" aria-label="Direct link to 2025-10-21" title="Direct link to 2025-10-21" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long
Context Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, dynamic sparse attention, ultra-long context, computational efficiency]</li>
<li class=""><strong>authors:</strong> Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu</li>
<li class=""><strong>institution:</strong> Microsoft Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18830v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18830v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MTraining introduces a distributed methodology using dynamic sparse attention with three key components to enable efficient LLM training with ultra-long contexts. It successfully expanded Qwen2.5-3B&#x27;s context window from 32K to 512K tokens on 32 A100 GPUs. The approach achieves up to 6x higher training throughput while maintaining model accuracy across various benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM inference, edge computing, SLO-driven scheduling, dynamic rate control, utility maximization]</li>
<li class=""><strong>authors:</strong> Pan Zhou, Yiming Lei, Ling Liu, Xiaoqiong Xu, Ying Cai, Daji Ergu, Hongfang Yu, Yueyue Dai</li>
<li class=""><strong>institution:</strong> Southwest Minzu University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18544v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18544v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SLICE proposes an SLO-driven scheduling framework combining utility-maximizing request scheduling with dynamic iterative generation rate control for LLM inference on edge devices. It significantly improves SLO attainment compared to state-of-the-art systems like Orca and FastServe. Experimental results show up to 35x higher SLO attainment and 3.4x faster task completion times.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Comparative analysis of large data processing in Apache Spark using
Java, Python and Scala</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [Apache Spark, programming languages, performance comparison, data processing, ETL workflows, Apache Iceberg]</li>
<li class=""><strong>authors:</strong> Ivan Borodii, Illia Fedorovych, Halyna Osukhivska, Diana Velychko, Roman Butsii</li>
<li class=""><strong>institution:</strong> Ternopil Ivan Puluj National Technical University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19012v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19012v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study compares Apache Spark&#x27;s performance across Java, Python and Scala by executing ETL operations including data loading from CSV files, transformation and loading into Apache Iceberg tables. Results show Python performs best with small datasets (5MB), while Scala excels in complex operations combining multiple files. The programming language choice significantly impacts Spark processing efficiency, with optimal selection depending on data size and operation complexity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Distributed Framework for Causal Modeling of Performance Variability
in GPU Traces</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [GPU traces, performance variability, causal modeling, distributed framework, HPC]</li>
<li class=""><strong>authors:</strong> Ankur Lahiry, Ayush Pokharel, Banooqa Banday, Seth Ockerman, Amal Gueroudji, Mohammad Zaeed, Tanzima Z. Islam, Line Pouchard</li>
<li class=""><strong>institution:</strong> Texas State University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18300v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18300v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a distributed framework that partitions and processes GPU trace data concurrently using causal graph methods and parallel coordinating charts. The approach exposes performance variability and dependencies across execution flows in HPC systems. Experimental results show a 67% improvement in scalability for analyzing multiple traces independently.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Tokencake: A KV-Cache-centric Serving Framework for LLM-based
Multi-Agent Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [KV-Cache optimization, multi-agent systems, GPU memory management, scheduling algorithms]</li>
<li class=""><strong>authors:</strong> Zhuohang Bian, Feiyang Wu, Teng Ma, Youwei Zhuo</li>
<li class=""><strong>institution:</strong> Beihang University, Peking University, Alibaba</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18586v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18586v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Tokencake introduces a KV-Cache-centric serving framework that co-optimizes scheduling and memory management for LLM-based multi-agent applications. It uses dynamic memory partitioning and proactive offload mechanisms to handle space contention and time underutilization. Evaluation shows it reduces latency by over 47.06% and improves GPU memory utilization by up to 16.9% compared to vLLM.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-22">2025-10-22<a href="#2025-10-22" class="hash-link" aria-label="Direct link to 2025-10-22" title="Direct link to 2025-10-22" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Serverless GPU Architecture for Enterprise HR Analytics: A
Production-Scale BDaaS Implementation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [serverless GPU, TabNet, enterprise analytics, compliance, cost efficiency, interpretability]</li>
<li class=""><strong>authors:</strong> Guilin Zhang, Wulan Guo, Ziqi Tan, Srinivas Vippagunta, Suchitra Raman, Shreeshankar Chatterjee, Ju Lin, Shang Liu, Mary Schladenhauffen, Jeffrey Luo, Hailong Jiang</li>
<li class=""><strong>institution:</strong> George Washington University and Workday, Inc.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19689v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19689v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a serverless GPU architecture integrating TabNet for enterprise HR analytics, achieving significant improvements in throughput and latency while maintaining compliance. The design combines GPU acceleration with serverless elasticity to reduce costs and ensure interpretability. Results show 4.5x higher throughput and 90% lower cost per inference compared to Spark baselines, with minimal compliance overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Enabling Reconfiguration-Communication Overlap for Collective
Communication in Optical Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [optical networks, collective communication, network reconfiguration, distributed machine learning]</li>
<li class=""><strong>authors:</strong> Changbo Wu, Zhuolong Yu, Gongming Zhao, Hongli Xu</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19322v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19322v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SWOT introduces a demand-aware optical network framework that enables intra-collective reconfiguration and overlaps optical switch reconfigurations with ongoing transmissions. This approach dynamically aligns network resources with collective communication traffic patterns in distributed machine learning. Simulation results demonstrate significant performance improvements over traditional one-shot reconfiguration strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via
Hybrid Expert/Data Transmission</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [mixture-of-experts, expert parallelism, cross-datacenter training, communication optimization, hybrid transmission]</li>
<li class=""><strong>authors:</strong> Weihao Yang, Hao Huang, Donglei Wu, Ningke Li, Yanqi Pan, Qiyang Zheng, Wen Xia, Shiyi Li, Qiang Wang</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology, Shenzhen</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19470v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19470v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HybridEP proposes a framework that dynamically transforms expert placement and uses hybrid expert/data transmission to reduce communication overhead in cross-datacenter MoE training. It employs domain-based partitioning and parameter-efficient migration guided by a stream-based model. Experimental results show HybridEP achieves up to 5.6x speedup over existing systems under constrained bandwidth.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] RLBoost: Harvesting Preemptible Resources for Cost-Efficient
Reinforcement Learning on LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [reinforcement learning, preemptible resources, cost efficiency, rollout optimization, cluster scheduling, resource utilization]</li>
<li class=""><strong>authors:</strong> Yongji Wu, Xueshen Liu, Haizhong Zheng, Juncheng Gu, Beidi Chen, Z. Morley Mao, Arvind Krishnamurthy, Ion Stoica</li>
<li class=""><strong>institution:</strong> UC Berkeley, Google, CMU, University of Michigan</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19225v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19225v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RLBoost introduces a hybrid architecture that efficiently utilizes preemptible GPU resources for RL training on LLMs through adaptive rollout offload, pull-based weight transfer, and token-level response migration. This approach significantly improves training throughput by 1.51x-1.97x while reducing costs by 28%-49% compared to using only on-demand resources. The system effectively addresses resource under-utilization in RL workflows by leveraging the stateless nature of rollout stages.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework
For Controllable Lyric Translation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [lyric translation, curriculum learning, reinforcement learning, unsupervised learning, controllable translation]</li>
<li class=""><strong>authors:</strong> Le Ren, Xiangjian Zeng, Qingqiang Wu, Ruoxuan Liang</li>
<li class=""><strong>institution:</strong> Xiamen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19967v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19967v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LyriCAR proposes a difficulty-aware curriculum reinforcement learning framework for controllable lyric translation, using adaptive curriculum strategies to guide training with progressively complex challenges. The method achieves state-of-the-art performance in EN-ZH lyric translation while reducing training steps by nearly 40%. Experimental results demonstrate superior performance across both standard translation metrics and multi-dimensional reward scores.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Next-Generation Event-Driven Architectures: Performance, Scalability,
and Intelligent Orchestration Across Messaging Frameworks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [event-driven architectures, messaging frameworks, AI-enhanced orchestration, performance benchmarking, reinforcement learning, predictive scaling]</li>
<li class=""><strong>authors:</strong> Jahidul Arafat, Fariha Tasmin, Sanjaya Poudel</li>
<li class=""><strong>institution:</strong> Auburn University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.04404v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.04404v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces AIEO, an AI-enhanced event orchestration system using machine learning for predictive scaling and reinforcement learning for dynamic resource allocation. It comprehensively benchmarks 12 messaging frameworks across different workloads and demonstrates significant improvements in latency, resource utilization, and cost optimization. The research provides standardized benchmarking methodologies and intelligent orchestration solutions for next-generation distributed systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] AdaSPEC: Selective Knowledge Distillation for Efficient Speculative
Decoders</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [speculative decoding, knowledge distillation, token filtering, model alignment, efficiency optimization]</li>
<li class=""><strong>authors:</strong> Yuezhou Hu, Jiaxin Guo, Xinyu Feng, Tuo Zhao</li>
<li class=""><strong>institution:</strong> University of California, Berkeley, Tsinghua University, Georgia Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19779v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19779v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AdaSPEC introduces selective token filtering during knowledge distillation to improve speculative decoding efficiency. By filtering out difficult-to-fit tokens using a reference model, it enhances draft-target model alignment on simpler tokens. The method achieves up to 15% higher token acceptance rates than DistillSpec while maintaining generation quality across various tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Serverless GPU Architecture for Enterprise HR Analytics: A
Production-Scale BDaaS Implementation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [serverless GPU, TabNet, enterprise analytics, compliance, HR analytics, BDaaS, interpretability]</li>
<li class=""><strong>authors:</strong> Guilin Zhang, Wulan Guo, Ziqi Tan, Srinivas Vippagunta, Suchitra Raman, Shreeshankar Chatterjee, Ju Lin, Shang Liu, Mary Schladenhauffen, Jeffrey Luo, Hailong Jiang</li>
<li class=""><strong>institution:</strong> George Washington University and Workday, Inc.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19689v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19689v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a serverless GPU architecture integrated with TabNet for enterprise HR analytics, achieving significant improvements in throughput, latency, and cost compared to Spark baselines. The design ensures compliance through feature-mask interpretability while maintaining performance under load. The implementation provides a practical blueprint for secure and efficient analytics in regulated environments.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-24T13:39:50.000Z" itemprop="dateModified">Oct 24, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20251013-20251019"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251013-20251019</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/category/paper"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Paper</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-10-21" class="table-of-contents__link toc-highlight">2025-10-21</a></li><li><a href="#2025-10-22" class="table-of-contents__link toc-highlight">2025-10-22</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>