<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251020-20251026" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251020-20251026 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20251020-20251026"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251020-20251026 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-10-21"><meta data-rh="true" property="og:description" content="2025-10-21"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20251020-20251026"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251020-20251026" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251020-20251026" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20251020-20251026","item":"https://darkknight996.github.io/daily/20251020-20251026"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.15081a68.js" defer="defer"></script>
<script src="/assets/js/main.b6cc092e.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251020-20251026</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251020-20251026</h1></header><h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-21">2025-10-21<a href="#2025-10-21" class="hash-link" aria-label="Direct link to 2025-10-21" title="Direct link to 2025-10-21" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long
Context Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, dynamic sparse attention, ultra-long context, computational efficiency]</li>
<li class=""><strong>authors:</strong> Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu</li>
<li class=""><strong>institution:</strong> Microsoft Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18830v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18830v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MTraining introduces a distributed methodology using dynamic sparse attention with three key components to enable efficient LLM training with ultra-long contexts. It successfully expanded Qwen2.5-3B&#x27;s context window from 32K to 512K tokens on 32 A100 GPUs. The approach achieves up to 6x higher training throughput while maintaining model accuracy across various benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM inference, edge computing, SLO-driven scheduling, dynamic rate control, utility maximization]</li>
<li class=""><strong>authors:</strong> Pan Zhou, Yiming Lei, Ling Liu, Xiaoqiong Xu, Ying Cai, Daji Ergu, Hongfang Yu, Yueyue Dai</li>
<li class=""><strong>institution:</strong> Southwest Minzu University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18544v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18544v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SLICE proposes an SLO-driven scheduling framework combining utility-maximizing request scheduling with dynamic iterative generation rate control for LLM inference on edge devices. It significantly improves SLO attainment compared to state-of-the-art systems like Orca and FastServe. Experimental results show up to 35x higher SLO attainment and 3.4x faster task completion times.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Comparative analysis of large data processing in Apache Spark using
Java, Python and Scala</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [Apache Spark, programming languages, performance comparison, data processing, ETL workflows, Apache Iceberg]</li>
<li class=""><strong>authors:</strong> Ivan Borodii, Illia Fedorovych, Halyna Osukhivska, Diana Velychko, Roman Butsii</li>
<li class=""><strong>institution:</strong> Ternopil Ivan Puluj National Technical University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19012v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19012v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This study compares Apache Spark&#x27;s performance across Java, Python and Scala by executing ETL operations including data loading from CSV files, transformation and loading into Apache Iceberg tables. Results show Python performs best with small datasets (5MB), while Scala excels in complex operations combining multiple files. The programming language choice significantly impacts Spark processing efficiency, with optimal selection depending on data size and operation complexity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Distributed Framework for Causal Modeling of Performance Variability
in GPU Traces</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [GPU traces, performance variability, causal modeling, distributed framework, HPC]</li>
<li class=""><strong>authors:</strong> Ankur Lahiry, Ayush Pokharel, Banooqa Banday, Seth Ockerman, Amal Gueroudji, Mohammad Zaeed, Tanzima Z. Islam, Line Pouchard</li>
<li class=""><strong>institution:</strong> Texas State University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18300v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18300v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a distributed framework that partitions and processes GPU trace data concurrently using causal graph methods and parallel coordinating charts. The approach exposes performance variability and dependencies across execution flows in HPC systems. Experimental results show a 67% improvement in scalability for analyzing multiple traces independently.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Tokencake: A KV-Cache-centric Serving Framework for LLM-based
Multi-Agent Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [KV-Cache optimization, multi-agent systems, GPU memory management, scheduling algorithms]</li>
<li class=""><strong>authors:</strong> Zhuohang Bian, Feiyang Wu, Teng Ma, Youwei Zhuo</li>
<li class=""><strong>institution:</strong> Beihang University, Peking University, Alibaba</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18586v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18586v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Tokencake introduces a KV-Cache-centric serving framework that co-optimizes scheduling and memory management for LLM-based multi-agent applications. It uses dynamic memory partitioning and proactive offload mechanisms to handle space contention and time underutilization. Evaluation shows it reduces latency by over 47.06% and improves GPU memory utilization by up to 16.9% compared to vLLM.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-22">2025-10-22<a href="#2025-10-22" class="hash-link" aria-label="Direct link to 2025-10-22" title="Direct link to 2025-10-22" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Serverless GPU Architecture for Enterprise HR Analytics: A
Production-Scale BDaaS Implementation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [serverless GPU, TabNet, enterprise analytics, compliance, cost efficiency, interpretability]</li>
<li class=""><strong>authors:</strong> Guilin Zhang, Wulan Guo, Ziqi Tan, Srinivas Vippagunta, Suchitra Raman, Shreeshankar Chatterjee, Ju Lin, Shang Liu, Mary Schladenhauffen, Jeffrey Luo, Hailong Jiang</li>
<li class=""><strong>institution:</strong> George Washington University and Workday, Inc.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19689v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19689v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a serverless GPU architecture integrating TabNet for enterprise HR analytics, achieving significant improvements in throughput and latency while maintaining compliance. The design combines GPU acceleration with serverless elasticity to reduce costs and ensure interpretability. Results show 4.5x higher throughput and 90% lower cost per inference compared to Spark baselines, with minimal compliance overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Enabling Reconfiguration-Communication Overlap for Collective
Communication in Optical Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [optical networks, collective communication, network reconfiguration, distributed machine learning]</li>
<li class=""><strong>authors:</strong> Changbo Wu, Zhuolong Yu, Gongming Zhao, Hongli Xu</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19322v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19322v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SWOT introduces a demand-aware optical network framework that enables intra-collective reconfiguration and overlaps optical switch reconfigurations with ongoing transmissions. This approach dynamically aligns network resources with collective communication traffic patterns in distributed machine learning. Simulation results demonstrate significant performance improvements over traditional one-shot reconfiguration strategies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via
Hybrid Expert/Data Transmission</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [mixture-of-experts, expert parallelism, cross-datacenter training, communication optimization, hybrid transmission]</li>
<li class=""><strong>authors:</strong> Weihao Yang, Hao Huang, Donglei Wu, Ningke Li, Yanqi Pan, Qiyang Zheng, Wen Xia, Shiyi Li, Qiang Wang</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology, Shenzhen</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19470v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19470v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> HybridEP proposes a framework that dynamically transforms expert placement and uses hybrid expert/data transmission to reduce communication overhead in cross-datacenter MoE training. It employs domain-based partitioning and parameter-efficient migration guided by a stream-based model. Experimental results show HybridEP achieves up to 5.6x speedup over existing systems under constrained bandwidth.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] RLBoost: Harvesting Preemptible Resources for Cost-Efficient
Reinforcement Learning on LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [reinforcement learning, preemptible resources, cost efficiency, rollout optimization, cluster scheduling, resource utilization]</li>
<li class=""><strong>authors:</strong> Yongji Wu, Xueshen Liu, Haizhong Zheng, Juncheng Gu, Beidi Chen, Z. Morley Mao, Arvind Krishnamurthy, Ion Stoica</li>
<li class=""><strong>institution:</strong> UC Berkeley, Google, CMU, University of Michigan</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19225v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19225v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RLBoost introduces a hybrid architecture that efficiently utilizes preemptible GPU resources for RL training on LLMs through adaptive rollout offload, pull-based weight transfer, and token-level response migration. This approach significantly improves training throughput by 1.51x-1.97x while reducing costs by 28%-49% compared to using only on-demand resources. The system effectively addresses resource under-utilization in RL workflows by leveraging the stateless nature of rollout stages.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework
For Controllable Lyric Translation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [lyric translation, curriculum learning, reinforcement learning, unsupervised learning, controllable translation]</li>
<li class=""><strong>authors:</strong> Le Ren, Xiangjian Zeng, Qingqiang Wu, Ruoxuan Liang</li>
<li class=""><strong>institution:</strong> Xiamen University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19967v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19967v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LyriCAR proposes a difficulty-aware curriculum reinforcement learning framework for controllable lyric translation, using adaptive curriculum strategies to guide training with progressively complex challenges. The method achieves state-of-the-art performance in EN-ZH lyric translation while reducing training steps by nearly 40%. Experimental results demonstrate superior performance across both standard translation metrics and multi-dimensional reward scores.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Next-Generation Event-Driven Architectures: Performance, Scalability,
and Intelligent Orchestration Across Messaging Frameworks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [event-driven architectures, messaging frameworks, AI-enhanced orchestration, performance benchmarking, reinforcement learning, predictive scaling]</li>
<li class=""><strong>authors:</strong> Jahidul Arafat, Fariha Tasmin, Sanjaya Poudel</li>
<li class=""><strong>institution:</strong> Auburn University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.04404v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.04404v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces AIEO, an AI-enhanced event orchestration system using machine learning for predictive scaling and reinforcement learning for dynamic resource allocation. It comprehensively benchmarks 12 messaging frameworks across different workloads and demonstrates significant improvements in latency, resource utilization, and cost optimization. The research provides standardized benchmarking methodologies and intelligent orchestration solutions for next-generation distributed systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] AdaSPEC: Selective Knowledge Distillation for Efficient Speculative
Decoders</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [speculative decoding, knowledge distillation, token filtering, model alignment, efficiency optimization]</li>
<li class=""><strong>authors:</strong> Yuezhou Hu, Jiaxin Guo, Xinyu Feng, Tuo Zhao</li>
<li class=""><strong>institution:</strong> University of California, Berkeley, Tsinghua University, Georgia Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19779v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19779v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AdaSPEC introduces selective token filtering during knowledge distillation to improve speculative decoding efficiency. By filtering out difficult-to-fit tokens using a reference model, it enhances draft-target model alignment on simpler tokens. The method achieves up to 15% higher token acceptance rates than DistillSpec while maintaining generation quality across various tasks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Serverless GPU Architecture for Enterprise HR Analytics: A
Production-Scale BDaaS Implementation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [serverless GPU, TabNet, enterprise analytics, compliance, HR analytics, BDaaS, interpretability]</li>
<li class=""><strong>authors:</strong> Guilin Zhang, Wulan Guo, Ziqi Tan, Srinivas Vippagunta, Suchitra Raman, Shreeshankar Chatterjee, Ju Lin, Shang Liu, Mary Schladenhauffen, Jeffrey Luo, Hailong Jiang</li>
<li class=""><strong>institution:</strong> George Washington University and Workday, Inc.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19689v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19689v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a serverless GPU architecture integrated with TabNet for enterprise HR analytics, achieving significant improvements in throughput, latency, and cost compared to Spark baselines. The design ensures compliance through feature-mask interpretability while maintaining performance under load. The implementation provides a practical blueprint for secure and efficient analytics in regulated environments.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-23">2025-10-23<a href="#2025-10-23" class="hash-link" aria-label="Direct link to 2025-10-23" title="Direct link to 2025-10-23" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Collective Communication for 100k+ GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [collective communication, NCCLX, large-scale GPU clusters, distributed training, communication efficiency]</li>
<li class=""><strong>authors:</strong> Min Si, Pavan Balaji, Yongzhou Chen, Ching-Hsiang Chu, Adi Gangidi, Saif Hasan, Subodh Iyengar, Dan Johnson, Bingzhe Liu, Jingliang Ren, Ashmitha Jeevaraj Shetty, Greg Steinbrecher, Xinfeng Xie, Yulun Wang, Bruce Wu, Jingyi Yang, Mingran Yang, Minlan Yu, Cen Zhao, Wes Bland, Denis Boyda, Suman Gumudavelli, Cristian Lumezanu, Rui Miao, Zhe Qu, Venkat Ramesh, Maxim Samoylov, Jan Seidel, Feng Tian, Qiye Tan, Shuqiang Zhang, Yimeng Zhao, Shengbao Zheng, Art Zhu, Hongyi Zeng</li>
<li class=""><strong>institution:</strong> Meta</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20171v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20171v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Meta developed NCCLX, a collective communication framework optimized for large-scale GPU clusters exceeding 100,000 GPUs. The framework addresses throughput and latency limitations in traditional methods for LLM workloads. Empirical evaluation on Llama4 demonstrates substantial improvements in communication efficiency for training and inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Morpheus: Lightweight RTT Prediction for Performance-Aware Load
Balancing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [RTT prediction, load balancing, Kubernetes, performance monitoring, edge computing]</li>
<li class=""><strong>authors:</strong> Panagiotis Giannakopoulos, Bart van Knippenberg, Kishor Chandra Joshi, Nicola Calabretta, George Exarchakos</li>
<li class=""><strong>institution:</strong> Eindhoven University of Technology, Thermo Fisher Scientific</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20506v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20506v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops lightweight RTT predictors using time-series monitoring data from Kubernetes GPU clusters to enable performance-aware load balancing. The approach achieves 95% accuracy with low overhead by selecting highly correlated metrics. Results show significant RTT reduction and resource efficiency improvements, demonstrating feasibility for production systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] GPU-Accelerated Primal Heuristics for Mixed Integer Programming</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [GPU acceleration, mixed integer programming, primal heuristics, feasibility pump, feasibility jump, fix-and-propagate]</li>
<li class=""><strong>authors:</strong> Akif Çördük, Piotr Sielski, Alice Boucher, Kumar Aatish</li>
<li class=""><strong>institution:</strong> Nvidia</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20499v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20499v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces GPU-accelerated primal heuristics for Mixed Integer Programming, combining several state-of-the-art methods like Feasibility Pump with GPU parallelization. The approach uses a GPU-accelerated PDLP as an approximate LP solver and a new probing cache for faster operations. Results show significant improvements in finding feasible solutions and objective quality on the MIPLIB2017 benchmark.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for
Scalable LLM Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [distributed training, ZeRO optimization, communication efficiency, asynchronous scheduling, memory optimization]</li>
<li class=""><strong>authors:</strong> Huawei Bai, Yifan Huang, Wenqi Shi, Ansheng You, Feifan Shao, Tengfei Han, Minghui Yu</li>
<li class=""><strong>institution:</strong> ByteDance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20111v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20111v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> AsyncHZP introduces an asynchronous hierarchical variant of ZeRO that adaptively reshards parameters across replica groups and uses multi-stream scheduling to overlap communication with computation. The method significantly reduces communication overhead while maintaining memory efficiency. Empirical results show it outperforms traditional ND parallelism and achieves state-of-the-art performance for both dense and MoE models at scale.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU hash tables, concurrent data structures, performance optimization, benchmarking framework]</li>
<li class=""><strong>authors:</strong> Hunter McCoy, Prashant Pandey</li>
<li class=""><strong>institution:</strong> Northeastern University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2509.16407v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2509.16407v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> WarpSpeed implements eight concurrent GPU hash table designs with optimization techniques like fingerprint-based metadata and specialized GPU instructions. The library provides a unified benchmarking framework and rich API for modern GPU applications. Evaluation shows improved performance and scalability across diverse workloads and real-world applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data
Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [Viterbi decoding, resource efficiency, FPGA acceleration, edge computing, adaptive algorithms]</li>
<li class=""><strong>authors:</strong> Ziheng Deng, Xue Liu, Jiantong Jiang, Yankai Li, Qingxu Deng, Xiaochun Yang</li>
<li class=""><strong>institution:</strong> Northeastern University, The University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19301v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19301v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FLASH Viterbi introduces a fast and adaptive Viterbi decoding operator using non-recursive divide-and-conquer with pruning and parallelization techniques. It also presents FLASH-BS Viterbi with dynamic beam search for memory efficiency and includes FPGA-based hardware accelerators. The algorithms demonstrate superior performance in decoding time and memory efficiency while maintaining adaptability for modern data systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FlashMP: Fast Discrete Transform-Based Solver for Preconditioning
Maxwell&#x27;s Equations on GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU computing, Maxwell&#x27;s equations, preconditioning, discrete transforms, domain decomposition, parallel efficiency]</li>
<li class=""><strong>authors:</strong> Haoyuan Zhang, Yaqian Gao, Xinxin Zhang, Jialin Li, Runfeng Jin, Yidong Chen, Feng Zhang, Wu Yuan, Wenpeng Ma, Shan Liang, Jian Zhang, Zhonghua Lu</li>
<li class=""><strong>institution:</strong> Chinese Academy of Sciences, University of Chinese Academy of Sciences, Tsinghua University, Xinyang Normal University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2508.07193v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2508.07193v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlashMP introduces a novel GPU-based preconditioning system using discrete transforms to solve Maxwell&#x27;s equations efficiently. It employs domain decomposition for multi-GPU scalability and achieves significant speedups over existing libraries. Evaluations show up to 16x reduction in iteration counts and 2.5x-4.9x speedups with high parallel efficiency on large GPU clusters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] RailS: Load Balancing for All-to-All Communication in Distributed
Mixture-of-Experts Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [load balancing, all-to-all communication, mixture-of-experts, distributed training, network optimization]</li>
<li class=""><strong>authors:</strong> Heng Xu, Zhiwei Yu, Chengze Du, Ying Zhou, Letian Li, Haojie Wang, Weiqiang Cheng, Jialong Li</li>
<li class=""><strong>institution:</strong> Shenzhen University of Advanced Technology, Tsinghua University, Chinese University of Hong Kong, China Mobile</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19262v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19262v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> RailS introduces a distributed load-balancing framework that leverages Rail topology symmetry and local LPT scheduling to optimize all-to-all communication in MoE training. It activates parallel rails for multipath transmission and achieves near-optimal load balance. The system improves bandwidth by 20%-78% and reduces iteration time by 18%-40% for Mixtral workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Symbiosis: Multi-Adapter Inference and Fine-Tuning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [finetuning], [parameter-efficient fine-tuning, multi-adapter inference, resource optimization, model sharing]</li>
<li class=""><strong>authors:</strong> Saransh Gupta, Umesh Deshpande, Travis Janssen, Swami Sundararaman</li>
<li class=""><strong>institution:</strong> IBM Research</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2507.03220v3" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2507.03220v3</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Symbiosis introduces a base-model-as-a-service approach that enables sharing frozen base model layers across multiple fine-tuning or inference processes. It uses split-execution to decouple client-specific adapters from shared base layers, allowing independent resource management and mixing of PEFT methods. The system demonstrates efficient simultaneous fine-tuning of 20 Gemma2-27B adapters on 8 GPUs while improving GPU utilization and preserving user privacy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Transferable Graph Learning for Transmission Congestion Management via
Busbar Splitting</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [graph neural networks, network topology optimization, congestion management, power systems, transfer learning]</li>
<li class=""><strong>authors:</strong> Ali Rajaei, Peter Palensky, Jochen L. Cremer</li>
<li class=""><strong>institution:</strong> Delft University of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20591v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20591v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a graph neural network approach for network topology optimization via busbar splitting to manage transmission congestion. The method uses heterogeneous edge-aware message passing to predict effective splitting actions, achieving significant speed-up while maintaining solution quality. Results demonstrate AC-feasible solutions within one minute with only 2.3% optimality gap on large-scale systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Symbolic Regression and Differentiable Fits in Beyond the Standard Model
Physics</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [symbolic regression, particle physics, supersymmetry, differentiable methods, neural networks]</li>
<li class=""><strong>authors:</strong> Shehu AbdusSalam, Steven Abel, Deaglan Bartlett, Miguel Crispim Romão</li>
<li class=""><strong>institution:</strong> Shahid Beheshti University, Durham University, CNRS, Sorbonne Université, University of Oxford</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20453v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20453v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper applies symbolic regression to derive analytical expressions for physical observables in Beyond Standard Model physics. The method produces accurate differentiable fits that enable efficient parameter estimation compared to conventional sampling approaches. Symbolic regression demonstrates superior global robustness over neural networks while achieving comparable performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Competition is the key: A Game Theoretic Causal Discovery Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [causal discovery, reinforcement learning, game theory, finite-sample guarantees, DDQN, GES, GraN-DAG]</li>
<li class=""><strong>authors:</strong> Amartya Roy, Souvik Chakraborty</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Delhi, Robert Bosch GmbH</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20106v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20106v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a game-theoretic reinforcement learning framework for causal discovery where a DDQN agent competes against baseline algorithms like GES or GraN-DAG. The method provides provable finite-sample guarantees and consistently outperforms existing approaches on both synthetic and real-world benchmarks. It achieves scalability to large graphs while unifying strong empirical performance with rigorous theoretical foundations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A Survey on Cache Methods in Diffusion Models: Toward Efficient
Multi-Modal Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [diffusion models, cache-based acceleration, inference optimization, feature reuse, computational redundancy]</li>
<li class=""><strong>authors:</strong> Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Tsinghua University, The Hong Kong University of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.19755v2" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.19755v2</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This survey introduces Diffusion Caching as a training-free method that identifies and reuses computational redundancies in diffusion models through feature-level cross-step reuse and inter-layer scheduling. The approach reduces computational overhead without modifying model parameters while maintaining generation quality. The paper shows caching methods have evolved from static reuse to dynamic prediction and can integrate with other acceleration techniques for efficient multimodal generation.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-26">2025-10-26<a href="#2025-10-26" class="hash-link" aria-label="Direct link to 2025-10-26" title="Direct link to 2025-10-26" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Learning to Schedule: A Supervised Learning Framework for Network-Aware
Scheduling of Data-Intensive Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [network-aware scheduling, supervised learning, job completion time prediction, Kubernetes, Spark workloads, telemetry data]</li>
<li class=""><strong>authors:</strong> Sankalpa Timilsina, Susmit Shannigrahi</li>
<li class=""><strong>institution:</strong> Tennessee Technological University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21419v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21419v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a supervised learning-based scheduler that predicts job completion times using real-time network and host metrics to make optimal placement decisions. The system was evaluated on a geo-distributed Kubernetes cluster running Spark workloads and achieved 34-54% higher accuracy in node selection compared to the default Kubernetes scheduler. The key innovation is demonstrating supervised learning for real-time network-aware scheduling in multi-site clusters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Accelerating Mobile Inference through Fine-Grained CPU-GPU Co-Execution</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [mobile inference, CPU-GPU co-execution, synchronization optimization, execution time prediction]</li>
<li class=""><strong>authors:</strong> Zhuojin Li, Marco Paolieri, Leana Golubchik</li>
<li class=""><strong>institution:</strong> University of Southern California</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21081v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21081v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a fine-grained CPU-GPU co-execution approach for mobile deep learning inference, using OpenCL shared virtual memory for lightweight synchronization and machine learning models for execution time prediction. The method achieves significant speedups of up to 1.89x for linear layers and 1.75x for convolutional layers on mobile devices. The approach effectively reduces inference latency by leveraging both CPU and GPU resources while minimizing synchronization overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] JSTprove: Pioneering Verifiable AI for a Trustless Future</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [verifiable AI, zero-knowledge machine learning, inference verification, privacy-preserving AI, cryptographic proofs, model transparency]</li>
<li class=""><strong>authors:</strong> Jonathan Gold, Tristan Freiberg, Haruna Isah, Shirin Shahabi</li>
<li class=""><strong>institution:</strong> Inference Labs Inc., Polyhedra Network</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21024v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21024v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> JSTprove introduces a zkML toolkit that simplifies generating and verifying proofs for AI inference without exposing sensitive data. The system hides cryptographic complexity behind a user-friendly interface while maintaining auditability. It serves as both a practical engineering tool and a foundation for future verifiable AI deployments in critical applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep
Learning Training Workloads</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [GPU memory estimation, CPU-based analysis, resource management, dynamic program analysis]</li>
<li class=""><strong>authors:</strong> Jiabo Shi, Dimitrios Pezaros, Yehia Elkhatib</li>
<li class=""><strong>institution:</strong> University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21048v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21048v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> xMem proposes a CPU-only dynamic analysis framework to accurately estimate peak GPU memory requirements for deep learning training workloads without consuming GPU resources. The method achieves 91% reduction in median relative error and 75% reduction in OOM probability compared to state-of-the-art solutions. This enables significant improvements in memory conservation potential and safer GPU sharing in cluster environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Lincoln AI Computing Survey (LAICS) and Trends</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [AI accelerators, performance analysis, power consumption, market trends, computing architectures]</li>
<li class=""><strong>authors:</strong> Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally, Jeremy Kepner</li>
<li class=""><strong>institution:</strong> MIT Lincoln Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.20931v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.20931v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper updates the Lincoln AI Computing Survey by collecting and analyzing performance and power consumption data of commercial AI accelerators. It plots these metrics on scatter graphs to identify market trends and introduces a new categorization of computing architectures. The survey highlights how generative AI models are driving computational demands and architectural innovations in AI hardware.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] On Thin Ice: Towards Explainable Conservation Monitoring via Attribution
and Perturbations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [computer vision, explainable AI, conservation monitoring, object detection, model interpretability, wildlife monitoring]</li>
<li class=""><strong>authors:</strong> Jiayi Zhou, Günel Aghakishiyeva, Saagar Arya, Julian Dale, James David Poling, Holly R. Houliston, Jamie N. Womble, Gregory D. Larsen, David W. Johnston, Brinnae Bent</li>
<li class=""><strong>institution:</strong> Duke University, University of Agder, University of Cambridge, U.S. National Park Service, Alaska Spatial Science</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.21689v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.21689v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper applies post-hoc explainability methods including gradient-based activation mapping and perturbation techniques to a Faster R-CNN seal detector trained on aerial imagery. The explanations successfully localize on seal features and reveal systematic model errors like confusing seals with black ice. The approach demonstrates how explainable AI can build trust and provide actionable insights for conservation monitoring applications.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-29T03:41:31.000Z" itemprop="dateModified">Oct 29, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20251013-20251019"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251013-20251019</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20251027-20251102"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251027-20251102</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-10-21" class="table-of-contents__link toc-highlight">2025-10-21</a></li><li><a href="#2025-10-22" class="table-of-contents__link toc-highlight">2025-10-22</a></li><li><a href="#2025-10-23" class="table-of-contents__link toc-highlight">2025-10-23</a></li><li><a href="#2025-10-26" class="table-of-contents__link toc-highlight">2025-10-26</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>