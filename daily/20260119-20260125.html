<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20260119-20260125" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20260119-20260125 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20260119-20260125"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20260119-20260125 | DarkKnight Note"><meta data-rh="true" name="description" content="2026-01-19"><meta data-rh="true" property="og:description" content="2026-01-19"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20260119-20260125"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20260119-20260125" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20260119-20260125" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20260119-20260125","item":"https://darkknight996.github.io/daily/20260119-20260125"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.ebf0f0f8.js" defer="defer"></script>
<script src="/assets/js/main.e57753a9.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260105-20260111"><span title="20260105-20260111" class="linkLabel_WmDU">20260105-20260111</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260112-20260118"><span title="20260112-20260118" class="linkLabel_WmDU">20260112-20260118</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20260119-20260125"><span title="20260119-20260125" class="linkLabel_WmDU">20260119-20260125</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260126-20260201"><span title="20260126-20260201" class="linkLabel_WmDU">20260126-20260201</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260202-20260208"><span title="20260202-20260208" class="linkLabel_WmDU">20260202-20260208</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20260119-20260125</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20260119-20260125</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2026-01-19">2026-01-19<a href="#2026-01-19" class="hash-link" aria-label="Direct link to 2026-01-19" title="Direct link to 2026-01-19" translate="no">​</a></h2>
<p><strong>cs.DC total: 4</strong></p>
<ul>
<li class="">
<p><strong>[arXiv260119] Konflux: Optimized Function Fusion for Serverless Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [serverless computing], [function fusion, FaaS, cost optimization, latency optimization, emulation]</li>
<li class=""><strong>authors:</strong> Niklas Kowallik, Trever Schirmer, David Bermbach</li>
<li class=""><strong>institution:</strong> TU Berlin</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11156" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11156</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents Konflux, a system that analyzes all possible function fusion configurations for serverless applications by emulating the FaaS platform locally. This approach eliminates the need for costly production benchmarking. The main conclusion is that only a limited set of fusion configurations are optimal, and these are heavily dependent on the specific cloud pricing model.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260119] AFLL: Real-time Load Stabilization for MMO Game Servers Based on Circular Causality Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [backpropagation, gradient descent, adaptive feedback loop, real-time learning, load stabilization, message throttling]</li>
<li class=""><strong>authors:</strong> Shinsuk Kang, Youngjae Kim</li>
<li class=""><strong>institution:</strong> Sogang University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.10998" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.10998</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AFLL, an adaptive feedback loop learning system that uses backpropagation to learn the causal relationship between server messages and client load in real-time, enabling predictive message throttling. It significantly reduces CPU usage, thread contention, and performance spikes in MMO game servers while maintaining zero learning overhead. The results demonstrate that circular causality learning enables practical real-time adaptation for latency-critical systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260119] A Survey of Real-Time Support, Analysis, and Advancements in ROS 2</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [real-time systems], [ROS 2, DDS, executors, timing analysis, publish-subscribe, micro-ROS]</li>
<li class=""><strong>authors:</strong> Daniel Casini, Jian-Jia Chen, Jing Li, Federico Reghenzani, Harun Teper</li>
<li class=""><strong>institution:</strong> Scuola Superiore Sant&#x27;Anna, TU Dortmund University, New Jersey Institute of Technology, Politecnico di Milano</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.10722" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.10722</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This survey paper provides a comprehensive overview of research efforts to analyze and enhance the real-time capabilities of the Robot Operating System 2 (ROS 2). It reviews key contributions on scheduling mechanisms, timing analysis for executors, communication delays, and community-driven runtime enhancements. The survey aims to guide researchers and practitioners in understanding and improving the timing predictability of ROS 2 for real-time robotic applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260119] Space-Optimal, Computation-Optimal, Topology-Agnostic, Throughput-Scalable Causal Delivery through Hybrid Buffering</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [causal delivery, SPS+FIFO, hybrid buffering, sender-buffering, receiver-buffering, topology-agnostic]</li>
<li class=""><strong>authors:</strong> Paulo Sérgio Almeida</li>
<li class=""><strong>institution:</strong> INESC TEC, University of Minho</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11487" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11487</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces a novel hybrid buffering algorithm for causal message delivery, combining sender-buffering to enforce Sender Permission to Send (SPS) and receiver-buffering to enforce FIFO order. This approach achieves effectively constant metadata size per message and amortized constant processing overhead, overcoming the limitations of prior sender-only or receiver-only buffering methods. It is presented as a topology-agnostic, throughput-scalable solution with space and computational optimality.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 17</strong></p>
<ul>
<li class="">[arXiv260119] Do explanations generalize across large reasoning models? <a href="https://arxiv.org/pdf/2601.11517" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model <a href="https://arxiv.org/pdf/2601.11143" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech <a href="https://arxiv.org/pdf/2601.11178" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs <a href="https://arxiv.org/pdf/2601.11061" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Policy-Based Deep Reinforcement Learning Hyperheuristics for Job-Shop Scheduling Problems <a href="https://arxiv.org/pdf/2601.11189" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Model-free policy gradient for discrete-time mean-field control <a href="https://arxiv.org/pdf/2601.11217" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Visual Marker Search for Autonomous Drone Landing in Diverse Urban Environments <a href="https://arxiv.org/pdf/2601.11078" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation <a href="https://arxiv.org/pdf/2601.11258" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Offline Reinforcement-Learning-Based Power Control for Application-Agnostic Energy Efficiency <a href="https://arxiv.org/pdf/2601.11352" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Factored Value Functions for Graph-Based Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2601.11401" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning <a href="https://arxiv.org/pdf/2601.10905" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Realistic Curriculum Reinforcement Learning for Autonomous and Sustainable Marine Vessel Navigation <a href="https://arxiv.org/pdf/2601.10911" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration <a href="https://arxiv.org/pdf/2601.10744" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search <a href="https://arxiv.org/pdf/2601.11037" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Toward Adaptive Grid Resilience: A Gradient-Free Meta-RL Framework for Critical Load Restoration <a href="https://arxiv.org/pdf/2601.10973" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration <a href="https://arxiv.org/pdf/2601.11144" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Reasoning Models Generate Societies of Thought <a href="https://arxiv.org/pdf/2601.10825" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 3</strong></p>
<ul>
<li class="">[arXiv260119] Differentially Private Subspace Fine-Tuning for Large Language Models <a href="https://arxiv.org/pdf/2601.11113" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] MHA2MLA-VLM: Enabling DeepSeek&#x27;s Economical Multi-Head Latent Attention across Vision-Language Models <a href="https://arxiv.org/pdf/2601.11464" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260119] Reasoning Models Generate Societies of Thought <a href="https://arxiv.org/pdf/2601.10825" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2026-01-21">2026-01-21<a href="#2026-01-21" class="hash-link" aria-label="Direct link to 2026-01-21" title="Direct link to 2026-01-21" translate="no">​</a></h2>
<p><strong>cs.DC total: 51</strong></p>
<ul>
<li class="">
<p><strong>[arXiv260121] Radio Labeling of Strong Prismatic Network With Star</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [graph theory, combinatorial optimization], [radio labeling, strong product, prismatic network, star graph, parallel algorithm]</li>
<li class=""><strong>authors:</strong> Liming Wang, Feng Li, Linlin Cui</li>
<li class=""><strong>institution:</strong> Qinghai Normal University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11624" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11624</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper studies the radio labeling problem for a strong prismatic network with a star, a combinatorial optimization model for wireless spectrum assignment. It presents theoretical results and a parallel algorithm to compute the labeling efficiently for large-scale networks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Cost-Aware Logging: Measuring the Financial Impact of Excessive Log Retention in Small-Scale Cloud Deployments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud cost optimization], [log retention, storage cost, operational usefulness, cost-effectiveness analysis, synthetic datasets]</li>
<li class=""><strong>authors:</strong> Jody Almaida Putra</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11584" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11584</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses synthetic log datasets to analyze the cost-effectiveness of different log retention windows in small-scale cloud deployments. It concludes that reducing retention from 90 to 14 days can lower storage costs by up to 78% while preserving over 97% of operationally useful logs, offering a lightweight framework for cost-aware configuration.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Hardware-Aware Reformulation of Convolutions for Efficient Execution on Specialized AI Hardware: A Case Study on NVIDIA Tensor Cores</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [hardware-aware reformulation, rewrite rules, width folding, structured filter expansion, tensor cores, semantic tuning]</li>
<li class=""><strong>authors:</strong> Ganesh Bikshandi</li>
<li class=""><strong>institution:</strong> Independent researcher (based on gmail address)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11608" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11608</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a hardware-aware reformulation method for CNNs using rewrite rules to restructure convolutions post-training, ensuring alignment with hardware constraints like those of NVIDIA Tensor Cores without modifying network weights. The approach, demonstrated through techniques like width folding and structured filter expansion, eliminates the need for inefficient zero-padding and lays the foundation for a systematic optimization strategy called semantic tuning.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] PerCache: Predictive Hierarchical Cache for RAG Applications on Mobile Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [hierarchical cache, KV cache reuse, semantic cache reuse, predictive caching, elastic scheduling]</li>
<li class=""><strong>authors:</strong> Kaiwei Liu, Liekang Zeng, Lilin Xu, Bufang Yang, Zhenyu Yan</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Columbia University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11553" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11553</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes PerCache, a predictive hierarchical cache system designed to reduce latency in mobile RAG applications by reusing intermediate computational results (like QKV cache) across queries and proactively populating the cache. It features a multi-stage matching architecture and adaptive resource management. Evaluations show it reduces latency by up to 34.4% compared to baselines and maintains performance under dynamic system loads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative decoding, edge computing, distributed inference, dynamic drafting, SLO-aware batching, verification interference, wasted drafting time]</li>
<li class=""><strong>authors:</strong> Xiangchen Li, Jiakun Fan, Qingyuan Wang, Dimitrios Spatharakis, Saeid Ghafouri, Hans Vandierendonck, Deepu John, Bo Ji, Ali R. Butt, Dimitrios S. Nikolopoulos</li>
<li class=""><strong>institution:</strong> Virginia Tech, University College Dublin, National Technical University of Athens, Queen’s University Belfast</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11652" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11652</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes WISP, a distributed LLM inference system that uses speculative decoding to offload drafting to edge devices and centralizes verification in the cloud. It addresses bottlenecks like wasted drafting time and verification interference with a dynamic drafting controller and SLO-aware batch scheduler. The system significantly improves capacity and goodput compared to centralized serving and prior distributed methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] PLA-Serve: A Prefill-Length-Aware LLM Serving System</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [prefill-length-aware scheduling, dual-queue design, CUDA Graph-based clustering, prefill-decode disaggregation, smart batching, temporal/spatial disaggregation]</li>
<li class=""><strong>authors:</strong> Jianshu She, Zonghang Li, Hongchao Du, Shangyu Wu, Wenhao Zheng, Eric Xing, Zhengzhong Liu, Huaxiu Yao, Jason Xue, Qirong Ho</li>
<li class=""><strong>institution:</strong> Mohamed bin Zayed University of Artificial Intelligence, The University of North Carolina at Chapel Hill</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11589" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11589</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PLA-Serve is a serving system that reduces LLM inference latency by disaggregating requests based on prompt length and using a length-aware smart batching mechanism. It employs a dual-queue design and CUDA Graph-based clustering to optimize scheduling for heterogeneous workloads. The system significantly reduces prefill latency and SLO violations compared to existing methods like SGLang.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Enhancing Model Context Protocol (MCP) with Context-Aware Server Collaboration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [Model Context Protocol (MCP), Shared Context Store (SCS), Context-Aware MCP (CA-MCP), multi-agent systems, LLM-driven coordination]</li>
<li class=""><strong>authors:</strong> Meenakshi Amulya Jayanti, X.Y. Han</li>
<li class=""><strong>institution:</strong> University of Chicago</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11595" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11595</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Context-Aware Model Context Protocol (CA-MCP) that enhances multi-agent LLM systems by introducing a Shared Context Store, allowing specialized servers to read from and write to a shared memory for better coordination. The method reduces the need for repeated LLM calls by offloading execution logic to context-aware servers. Experiments on benchmark datasets show that CA-MCP improves efficiency and reduces failures compared to the traditional stateless MCP approach.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] A Forward Simulation-Based Hierarchy of Linearizable Concurrent Objects</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [concurrent data structures], [linearizability, forward simulation, wait-freedom, lock-freedom, obstruction-freedom, bounded lattice, Herlihy-Wing queue, time-stamped queue]</li>
<li class=""><strong>authors:</strong> Chao Wang, Ruijia Li, Yang Zhou, Peng Wu, Yi Lv, Jianwei Liao, Jim Woodcock, Zhiming Liu</li>
<li class=""><strong>institution:</strong> Southwest University, Institute of Software (CAS), University of Chinese Academy of Sciences, State Key Laboratory of Intelligent Vehicle Technology, Aarhus University, University of York</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11646" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11646</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper systematically investigates the connection between linearizable concurrent objects and forward simulation, establishing a hierarchy based on this relation. It proves that sets of linearizable objects with various liveness properties form bounded join-semilattices or lattices under forward simulation. The work also provides an equivalent characterization of linearizability via forward simulation, which can be used for verification purposes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] EPD-Serve: A Flexible Multimodal EPD Disaggregation Inference Serving System On Ascend</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [stage-level disaggregation, dynamic orchestration, asynchronous feature prefetching, hierarchical grouped KV cache, multi-route scheduling, instance-level load balancing, spatial multiplexing]</li>
<li class=""><strong>authors:</strong> Fan Bai, Pai Peng, Zhengzhi Tang, Zhe Wang, Gong Chen, Xiang Lu, Yinuo Li, Huan Lin, Weizhe Lin, Yaoyuan Wang, Xiaosong Li</li>
<li class=""><strong>institution:</strong> Huawei Technologies Co., Ltd</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11590" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11590</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes EPD-Serve, a stage-level disaggregated inference serving system for multimodal models that decouples the pipeline into Encode, Prefill, and Decode stages for flexible deployment. It introduces mechanisms like asynchronous feature prefetching and hierarchical KV cache transmission to improve communication efficiency on Ascend hardware. The system significantly improves throughput under high concurrency while meeting strict latency constraints, demonstrating the effectiveness of disaggregation for multimodal inference optimization.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] GPU-Resident Inverted File Index for Streaming Vector Databases</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [inverted file index, slab-based allocation, validity bitmap, address translation table, streaming vector databases]</li>
<li class=""><strong>authors:</strong> Dongfang Zhao</li>
<li class=""><strong>institution:</strong> University of Washington</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11808" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11808</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SIVF, a GPU-resident inverted file index that uses slab-based memory management and a validity bitmap to enable in-place mutation directly in VRAM for streaming vector databases. It achieves up to 13,300× faster deletions and 36–105× higher ingestion throughput compared to static GPU IVF indices, with minimal memory overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Nixie: Efficient, Transparent Temporal Multiplexing for Consumer GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [temporal multiplexing, MLFQ-inspired scheduling, GPU memory management, Unified Virtual Memory (UVM)]</li>
<li class=""><strong>authors:</strong> Yechen Xu, Yifei Wang, Nathanael Ren, Yiran Chen, Danyang Zhuo</li>
<li class=""><strong>institution:</strong> Duke University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11743" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11743</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Nixie is a system service that enables transparent temporal multiplexing on consumer GPUs by coordinating memory allocation and kernel launches to efficiently manage CPU-GPU bandwidth and pinned memory. It uses a lightweight, MLFQ-inspired scheduler to prioritize latency-sensitive interactive jobs. Evaluations show Nixie improves interactive task latency by up to 3.8x and reduces CPU pinned memory usage by up to 66.8% compared to existing mechanisms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] HALO: Semantic-Aware Distributed LLM Inference in Lossy Edge Network</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [tensor parallelism, semantic-aware predictor, parallel execution, load-balancing scheduler, distributed inference, edge computing]</li>
<li class=""><strong>authors:</strong> Peirong Zheng, Wenchao Xu, Haozhao Wang, Jinyu Chen, Xuemin Shen</li>
<li class=""><strong>institution:</strong> The Hong Kong Polytechnic University, The Hong Kong University of Science and Technology, Huazhong University of Science and Technology, University of Waterloo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11676" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11676</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes HALO, a framework for distributed LLM inference in unreliable edge networks. Its core method uses a semantic-aware predictor and load-balancing scheduler to allocate less critical neuron computations to unstable devices, enabling relaxed synchronization. The main conclusion is that HALO achieves a 3.41x speedup over baselines in lossy networks while maintaining performance comparable to optimal conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Big Data Workload Profiling for Energy-Aware Cloud Resource Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud resource management], [workload profiling, virtual machine placement, energy-aware scheduling, SLA compliance, adaptive consolidation]</li>
<li class=""><strong>authors:</strong> Milan Parikh, Aniket Abhishek Soni, Sneja Mitinbhai Shah, Ayush Raj Jha</li>
<li class=""><strong>institution:</strong> Independent Researchers (IEEE)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11935" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11935</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a workload-aware scheduling framework that profiles CPU, memory, and I/O usage to guide energy-efficient virtual machine placement in cloud data centers. By combining historical logs with real-time telemetry, it predicts the energy and performance impact of placements for adaptive consolidation. The evaluation shows the framework achieves 15–20% energy savings while maintaining service-level agreement compliance for big data workloads like Hadoop and Spark.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU Disaggregation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [hybrid batching, disaggregated serving, adaptive resource management, CU masking, intra-GPU disaggregation]</li>
<li class=""><strong>authors:</strong> Amna Masood, Pratishtha Gaur, Nuwan Jayasena</li>
<li class=""><strong>institution:</strong> Advanced Micro Devices</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11822" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11822</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes RAPID-Serve, a technique that concurrently executes the prefill and decode phases of LLM inference on the same GPU(s) to meet latency SLOs while maintaining high throughput. It introduces Adaptive Resource Management, optionally leveraging fine-grained GPU partitioning like CU masking, to allocate compute resources at runtime. The method shows significant throughput improvements, especially under SLO constraints, compared to state-of-the-art approaches like hybrid batching and disaggregated serving.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Canonicalization of Batched Einstein Summations for Tuning Retrieval</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [batched einsum, graph canonicalization, tensor contractions, functional array operands, performance tuning]</li>
<li class=""><strong>authors:</strong> Kaushik Kulkarni, Andreas Klöckner</li>
<li class=""><strong>institution:</strong> University of Illinois at Urbana-Champaign</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12220" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12220</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents an algorithm to canonicalize batched Einstein summation (einsum) expressions by encoding them as colored graphs and applying graph canonicalization to derive a unique normal form. This enables the reuse of optimization knowledge and performance tuning across mathematically equivalent expressions. The evaluation shows a geomean speedup of 4.7x compared to JAX for tensor contractions from benchmark suites and an FEM solver.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Opportunistic Scheduling for Optimal Spot Instance Savings in the Cloud</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing, scheduling], [queuing theory, stochastic processes, knapsack problem, adaptive algorithm, spot instances]</li>
<li class=""><strong>authors:</strong> Neelkamal Bhuyan, Randeep Bhatia, Murali Kodialam, TV Lakshman</li>
<li class=""><strong>institution:</strong> Georgia Institute of Technology, Nokia Bell Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12266" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12266</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes an opportunistic scheduling framework for delay-sensitive jobs using spot and on-demand cloud instances to minimize cost. It uses queuing theory and optimization to derive optimal policies for different delay regimes, including a knapsack-based policy for high delays. The proposed adaptive algorithm is shown to achieve near-optimal cost savings while meeting delay constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] DaggerFFT: A Distributed FFT Framework Using Task Scheduling in Julia</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [task scheduling, distributed FFT, dynamic scheduling, Dagger, DArray, DTask, work stealing]</li>
<li class=""><strong>authors:</strong> Sana Taghipour Anvari, Julian Samaroo, Matin Raayai Ardakani, David Kaeli</li>
<li class=""><strong>institution:</strong> Northeastern University, Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12209" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12209</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents DaggerFFT, a distributed FFT framework in Julia that uses dynamic task scheduling and work stealing to manage computations as a task graph. It demonstrates that this approach outperforms static-distribution libraries, achieving speedups of up to 2.6x on CPU clusters and 1.35x on GPU clusters, and shows its practical integration into a geophysical fluid dynamics simulation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Power Aware Dynamic Reallocation For Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [disaggregation, power-aware scheduling, dynamic reallocation, prefill, decode, SLO attainment]</li>
<li class=""><strong>authors:</strong> Yiwei Jiang, Sangeeta Chowdhary, Nathaniel Morris, Rutwik Jain, Srilatha Manne, Sam Bayliss</li>
<li class=""><strong>institution:</strong> AMD Research and Advanced Development, University of Wisconsin-Madison</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12241" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12241</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes RAPID, a power-aware disaggregated inference framework that jointly manages GPU roles and power budgets to improve performance under strict power caps. It combines static and dynamic power reallocation with GPU reallocation to sustain goodput. The method achieves up to a 2x improvement in SLO attainment at peak load compared to static assignment, without increasing complexity or cost.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Computation-Bandwidth-Memory Trade-offs: A Unified Paradigm for AI Infrastructure</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [computation-bandwidth-memory trade-offs, AI Trinity, edge-cloud communication, distributed training, model inference]</li>
<li class=""><strong>authors:</strong> Yuankai Fan, Qizhen Weng, Xuelong Li</li>
<li class=""><strong>institution:</strong> Institute of Artificial Intelligence (TeleAI), China Telecom</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.11577" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.11577</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the &quot;AI Trinity&quot; paradigm, which treats computation, bandwidth, and memory as coequal pillars and proposes three fundamental trade-offs between them to optimize system performance. It demonstrates the effectiveness of this unified framework through applications in edge-cloud communication, distributed training, and model inference, providing a new conceptual foundation for scalable AI infrastructure.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] RIPPLE++: An Incremental Framework for Efficient GNN Inference on Evolving Graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [incremental inference, graph neural networks, dynamic graphs, streaming updates, incremental programming model, distributed deployment]</li>
<li class=""><strong>authors:</strong> Pranjal Naman, Parv Agarwal, Hrishikesh Haritas, Yogesh Simmhan</li>
<li class=""><strong>institution:</strong> Indian Institute of Science (IISc)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12347" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12347</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes RIPPLE++, a framework for efficient GNN inference on evolving graphs. It introduces a generalized incremental programming model to update embeddings by propagating changes only to affected neighborhoods, avoiding full recomputation. The method achieves significant speedups and lower latency compared to baselines in both single-machine and distributed settings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] On Resilient and Efficient Linear Secure Aggregation in Hierarchical Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [secure aggregation, hierarchical federated learning, information-theoretic security, unreliable communication, communication cost, randomness cost]</li>
<li class=""><strong>authors:</strong> Shudi Weng, Xiang Zhang, Yizhou Zhao, Giuseppe Caire, Ming Xiao, Mikael Skoglund</li>
<li class=""><strong>institution:</strong> KTH Royal Institute of Technology, Technical University of Berlin, Southwest University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12853" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12853</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper studies hierarchical secure aggregation in federated learning under unreliable communication links. It characterizes the minimum communication and randomness costs for robust aggregation and proposes an optimal protocol that achieves these fundamental limits.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Dynamic Detection of Inefficient Data Mapping Patterns in Heterogeneous OpenMP Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [heterogeneous computing], [dynamic analysis, OpenMP Tools Interface (OMPT), data mapping, performance profiling]</li>
<li class=""><strong>authors:</strong> Luke Marzen, Junhyung Shim, Ali Jannesari</li>
<li class=""><strong>institution:</strong> Iowa State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12713" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12713</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes dynamic analysis techniques, implemented in a tool called OMPDataPerf, to automatically detect and profile inefficient data transfer patterns in heterogeneous OpenMP applications. It uses the OpenMP Tools Interface (OMPT) to provide detailed traces and source code attribution for problematic data mappings. The main conclusion is that OMPDataPerf effectively identifies optimization opportunities with only a 5% geometric-mean runtime overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] ASAS-BridgeAMM: Trust-Minimized Cross-Chain Bridge AMM with Failure Containment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain security], [cross-chain bridge, automated market maker, failure containment, contained degradation, collateral haircuts, slippage bounds, withdrawal limits, Byzantine relayer model]</li>
<li class=""><strong>authors:</strong> Shengwei You, Aditya Joshi, Andrey Kuehlkamp, Jarek Nabrzyski</li>
<li class=""><strong>institution:</strong> University of Notre Dame</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12434" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12434</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces ASAS-BridgeAMM, a bridge-coupled automated market maker that uses a &quot;Contained Degradation&quot; mechanism to dynamically adjust parameters like collateral haircuts in response to adversarial signals, treating message latency as execution risk. It demonstrates a 73% reduction in worst-case bridge-induced insolvency compared to baseline architectures while maintaining transaction volume during stress periods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Efficient Local-to-Global Collaborative Perception via Joint Communication and Computation Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [local-to-global collaborative perception, roadside unit scheduling, data fusion, communication optimization, computation optimization]</li>
<li class=""><strong>authors:</strong> Hui Zhang, Yuquan Yang, Zechuan Gong, Xiaohua Xu, Dan Keun Sung</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China, Korea Advanced Institute of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12749" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12749</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a local-to-global collaborative perception framework (LGCP) that partitions a road into areas, assigns vehicle groups to each area for local perception, and uses a roadside unit to centrally schedule communications, fuse local results, and broadcast a global view. This method significantly reduces data transmission by an average of 44 times while maintaining or improving collaborative perception performance for autonomous driving.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] SWORD: A Secure LoW-Latency Offline-First Authentication and Data Sharing Scheme for Resource Constrained Distributed Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [authentication and data sharing], [proximity-based clustering, offline-first, blockchain, low-latency, secure authentication]</li>
<li class=""><strong>authors:</strong> Faisal Haque Bappy, Tahrim Hossain, Raiful Hasan, Kamrul Hasan, Mohamed Younis, Tariqul Islam</li>
<li class=""><strong>institution:</strong> University of Maryland Baltimore County, Kent State University, Tennessee State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12875" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12875</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SWORD, a scheme using proximity-based clustering to enable offline-first, low-latency authentication and data sharing for resource-constrained networks like IoT and IoV. It concludes that SWORD outperforms traditional blockchain solutions and matches the efficiency and latency of central-server-based approaches while being resilient to various security attacks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] SGCP: A Self-Organized Game-Theoretic Framework For Collaborative Perception</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [game theory, self-organization, clustering, potential game, distributed algorithms, collaborative perception, V2V communication]</li>
<li class=""><strong>authors:</strong> Zechuan Gong, Hui Zhang, Yuquan Yang, Wenyu Lu</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12524" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12524</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a decentralized, game-theoretic framework called SGCP for collaborative perception in autonomous driving, which uses self-organized clustering and a potential game for selective data transmission. It reduces communication overhead while improving perception accuracy and coverage compared to existing methods, as validated through co-simulation experiments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] From Design to Deorbit: A Solar-Electric Autonomous Module for Multi-Debris Remediation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [space systems], [NASA Evolutionary Xenon Thruster (NEXT), mechanical clamping system, Extended Kalman Filter (EKF), Delay/Disruption Tolerant Network (DTN), solar-electric propulsion]</li>
<li class=""><strong>authors:</strong> Om Mishra, Jayesh Patil, Sathwik Narkedimilli, G Srikantha Sharma, Ananda S, Manjunath K Vanahalli</li>
<li class=""><strong>institution:</strong> Indian Institute of Information Technology Dharwad, National University of Singapore, Hindustan Aeronautics Limited (HAL), U R Rao Satellite Center (ISRO)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12830" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12830</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a solar-electric autonomous module for removing multiple pieces of orbital debris. The core method integrates a mechanical clamping system for capture with a solar-powered ion thruster and autonomous navigation using an Extended Kalman Filter. High-fidelity simulations demonstrate successful deorbiting and establish a benchmark for renewable propulsion that reduces fuel reliance and extends mission life for debris cleanup.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Unleashing Efficient Asynchronous RL Post-Training via Staleness-Constrained Rollout Coordination</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [asynchronous reinforcement learning, data staleness, data skewness, global consistency protocol, rollout coordination, data servers]</li>
<li class=""><strong>authors:</strong> Haoyang Li, Sheng Lin, Fangcheng Fu, Yuming Zhou, Xiaodong Ji, Yanfeng Zhao, Lefeng Wang, Jie Jiang, Bin Cui</li>
<li class=""><strong>institution:</strong> Peking University, Shanghai Jiao Tong University, Tencent Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12784" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12784</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes StaleFlow, a system for efficient asynchronous RL post-training that jointly addresses data staleness and skewness. It introduces a global consistency protocol to control staleness and redesigns the architecture with data servers for flexible rollout coordination to mitigate skewness. Evaluations show StaleFlow achieves significantly higher throughput than state-of-the-art systems without compromising convergence.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] CooperLLM: Cloud-Edge-End Cooperative Federated Fine-tuning for LLMs via ZOO-based Gradient Correction</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [federated learning, zeroth-order optimization, gradient rectification, pipeline scheduling, adaptive compression]</li>
<li class=""><strong>authors:</strong> He Sun, Jinrui Zhou, Li Li, Mingjun Xiao</li>
<li class=""><strong>institution:</strong> University of Science and Technology of China, University of Macau</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12917" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12917</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes CooperLLM, a federated fine-tuning framework where mobile clients use lightweight zeroth-order optimization on private data, while a cloud server uses backpropagation on public data to generate guided gradient corrections. This cloud-assisted gradient rectification improves convergence speed and accuracy while preserving privacy. Experiments show the method significantly reduces on-device memory usage, accelerates convergence, and improves model accuracy compared to baseline methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] CPU-less parallel execution of lambda calculus in digital logic</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [parallel computing], [lambda calculus, digital logic, beta-reduction, tree-based representation, data flow, message passing, von Neumann bottleneck]</li>
<li class=""><strong>authors:</strong> Harry Fitchett, Charles Fox</li>
<li class=""><strong>institution:</strong> University of Lincoln</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13040" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13040</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a CPU-less parallel architecture by compiling lambda calculus directly into digital logic, using a tree-based representation with nodes corresponding to grammar forms and performing beta-reductions in parallel. It presents a proof-of-concept implementation and simulation results, suggesting the approach can be scaled to larger functional languages.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] OPTIMUM-DERAM: Highly Consistent, Scalable, and Secure Multi-Object Memory using RLNC</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [Random Linear Network Codes (RLNC), consistent hashing, blockchain oracle, Byzantine fault tolerance, atomic storage]</li>
<li class=""><strong>authors:</strong> Nicolas Nicolaou, Kishori M. Konwar, Moritz Grundei, Aleksandr Bezobchuk, Muriel Médard, Sriram Vishwanath</li>
<li class=""><strong>institution:</strong> Optimum, Georgia Tech</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13146" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13146</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes OPTIMUM-DERAM, a decentralized shared memory system that uses Random Linear Network Codes (RLNC) and a consistent hashing ring for object placement to improve scalability and performance. It demonstrates through experiments that this approach outperforms previous distributed shared memory solutions like the ABD algorithm.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Exploration on Highly Dynamic Graphs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [mobile agents, graph exploration, deterministic algorithm, 1-Interval Connectivity, Connectivity Time]</li>
<li class=""><strong>authors:</strong> Ashish Saxena, Kaushik Mondal</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology Ropar</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13047" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13047</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper studies the problem of graph exploration by mobile agents in two dynamic graph models. It proves an improved impossibility bound for exploration in the Connectivity Time model and presents a deterministic algorithm that uses a specific number of agents with 1-hop visibility and logarithmic memory.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] The Energy-Throughput Trade-off in Lossless-Compressed Source Code Storage</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [key-value store, lossless compression, data parallelism, energy efficiency, Pareto-optimal trade-offs]</li>
<li class=""><strong>authors:</strong> Paolo Ferragina, Francesco Tosoni</li>
<li class=""><strong>institution:</strong> Sant’Anna School of Advanced Studies</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13220" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13220</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper studies the design of a compressed key-value store for indexing large-scale source code datasets, evaluating the trade-offs between space, retrieval throughput, and energy consumption. The main conclusion is that different compression configurations yield distinct Pareto-optimal trade-offs, with high compression offering significant gains in throughput and energy efficiency, but scaling energy efficiency with data parallelism is challenging due to hardware non-energy-proportionality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Sutradhara: An Intelligent Orchestrator-Engine Co-design for Tool-based Agentic Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [orchestrator-engine co-design, tool-aware prompt splitting, streaming tool execution, orchestrator-aware cache management, KV cache, vLLM]</li>
<li class=""><strong>authors:</strong> Anish Biswas, Kanishk Goel, Jayashree Mohan, Alind Khare, Anjaly Parayil, Ramachandran Ramjee, Chetan Bansal</li>
<li class=""><strong>institution:</strong> Microsoft Research, M365 Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12967" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12967</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents Sutradhara, a co-designed system that integrates the agent orchestrator with the LLM serving engine to optimize tool-based agentic inference. Its key methods include overlapping tool execution with LLM prefill, streaming tool dispatch, and semantic cache management. The implementation on vLLM demonstrates reduced latency, showing that co-design can effectively address performance bottlenecks in agentic workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Enshrined Proposer Builder Separation in the presence of Maximal Extractable Value</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain consensus], [Proposer Builder Separation, Maximal Extractable Value, agent-based simulation, Gini coefficient, EIP-7732]</li>
<li class=""><strong>authors:</strong> Yitian Wang, Yebo Feng, Yingjiu Li, Jiahua Xu</li>
<li class=""><strong>institution:</strong> University College London, Nanyang Technological University, University of Oregon</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.12989" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.12989</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a formal framework combining mathematical analysis and agent-based simulations to evaluate the enshrined Proposer Builder Separation (ePBS) mechanism in Ethereum. It concludes that ePBS, despite separating block construction and proposal, significantly amplifies profit centralization and economic bias, exacerbating MEV-driven incentives.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Federated Learning Under Temporal Drift -- Mitigating Catastrophic Forgetting via Experience Replay</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, experience replay, catastrophic forgetting, temporal concept drift, FedAvg, client-side buffer]</li>
<li class=""><strong>authors:</strong> Sahasra Kokkula, Daniel David, Aaditya Baruah</li>
<li class=""><strong>institution:</strong> Columbia University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13456" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13456</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using client-side experience replay, where each client maintains a small buffer of past training samples, to mitigate catastrophic forgetting in Federated Learning under temporal concept drift. The method requires no changes to the server aggregation. Experiments on a seasonal drift simulation show that a simple replay buffer effectively restores model performance, preventing the severe accuracy drop seen with standard FedAvg.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Driving Computational Efficiency in Large-Scale Platforms using HPC Technologies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC workload analysis], [Monte Carlo simulations, task-parallel execution, CPU utilization, walltime utilization, I/O patterns, job accounting data]</li>
<li class=""><strong>authors:</strong> Alexander Martinez Mendez, Antonio J. Rubio-Montero, Carlos J. Barrios H., Hernán Asorey, Rafael Mayo-García, Luis A. Núñez</li>
<li class=""><strong>institution:</strong> Universidad Industrial de Santander, Centre for Energy, Environmental and Technological Research (CIEMAT)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13424" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13424</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes historical job accounting data from the EGI FedCloud platform to characterize the computational workloads and evaluate resource efficiency for the LAGO astroparticle physics project. The study identifies key workload types and metrics, revealing high CPU efficiency in core simulation tasks but inefficiencies from short test jobs. The findings provide data-driven recommendations for optimizing resource requests and workflow management to maximize scientific output.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Towards Scalable Federated Container Orchestration: The CODECO Approach</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Kubernetes, federated orchestration, semantic application models, partition-based federation, AI-assisted decision support, data-compute-network co-orchestration, hybrid governance]</li>
<li class=""><strong>authors:</strong> Rute C. Sofia, Josh Salomon, Ray Carrol, Luis Garcés-Erice, Peter Urbanetz, Jürgen Gesswein, Rizkallah Touma, Alejandro Espinosa, Luis M. Contreras, Vasileios Theodorou, George Papathanail, Georgios Koukis, Vassilis Tsaoussidis, Alberto del Rio, David Jimenez, Efterpi Paraskevoulakou, Panagiotis Karamolegkos, John Soldatos, Borja Dorado Nogales, Alejandro Tjaarda</li>
<li class=""><strong>institution:</strong> fortiss GmbH, Red Hat, IBM Research Europe - Zurich, Siemens AG, i2CAT Foundation, Telefónica, Intracom Telecom, Democritus University of Thrace / Athena Research Center, Universidad Politécnica de Madrid, University of Piraeus Research Centre, Netcompany, Universidad Carlos III de Madrid</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13351" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13351</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces CODECO, a federated orchestration framework that extends Kubernetes with semantic models, partition-based federation, and AI-assisted decision-making to manage applications across heterogeneous Edge-Cloud environments. It employs a hybrid governance model combining centralized policy with decentralized execution to support autonomy while maintaining global coherence. The main conclusion is that CODECO addresses the limitations of cloud-centric orchestration by enabling scalable, context-aware, and adaptive management in federated infrastructures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] RASC: Enhancing Observability &amp; Programmability in Smart Spaces</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [IoT systems], [RASC, RPC, Home Assistant, scheduling, observability, programmability]</li>
<li class=""><strong>authors:</strong> Anna Karanika, Kai-Siang Wang, Han-Ting Liang, Shalni Sundram, Indranil Gupta</li>
<li class=""><strong>institution:</strong> University of Illinois Urbana-Champaign</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13496" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13496</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the RASC (Request-Acknowledge-Start-Complete) abstraction to enhance observability and programmability for IoT actions in smart spaces. The method is implemented on top of existing RPC mechanisms and integrated into the Home Assistant framework. The evaluation shows that RASC meets latency SLOs for long actions and improves scheduling performance for home automations by 10%-55% over state-of-the-art counterparts.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] A Kubernetes custom scheduler based on reinforcement learning for compute-intensive pods</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [kubernetes, custom scheduler, reinforcement learning, deep q-network, compute-intensive pods]</li>
<li class=""><strong>authors:</strong> Hanlin Zhou, Huah Yong Chan, Shun Yao Zhang, Meie Lin, Jingfei Ni</li>
<li class=""><strong>institution:</strong> Universiti Sains Malaysia, Xiamen Institute of Software Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13579" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13579</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes two custom Kubernetes schedulers, SDQN and SDQN-n, based on Deep Q-Network reinforcement learning for scheduling compute-intensive pods. The schedulers outperform the default Kubernetes and other model-based schedulers, reducing average CPU utilization per node by 10% and over 20% respectively, demonstrating improved resource efficiency and energy savings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Automatic Adjustment of HPA Parameters and Attack Prevention in Kubernetes Using Random Forests</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Random Forest, Kubernetes HPA, HTTP status codes, honeypot pods]</li>
<li class=""><strong>authors:</strong> Hanlin Zhou, Huah Yong Chan, Jingfei Ni, Mengchun Wu, Qing Deng</li>
<li class=""><strong>institution:</strong> Universiti Sains Malaysia, Xiamen Institute of Software Technology, Manzhouli Customs, Jimei University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13515" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13515</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a method that uses the Random Forest algorithm to predict attacks and dynamically adjust the Horizontal Pod Autoscaler (HPA) parameters in Kubernetes, redirecting malicious traffic to honeypots. The approach effectively manages attack traffic, prevents excessive pod scaling, and reduces server errors under high load. The experiments highlight the importance of setting appropriate thresholds for HPA adjustments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] A Blockchain-Oriented Software Engineering Architecture for Carbon Credit Certification Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain and IoT integration], [blockchain, IoT smart metering, edge computing, smart contracts, permissioned blockchain, data aggregation]</li>
<li class=""><strong>authors:</strong> Matteo Vaccargiu, Azmat Ullah, Pierluigi Gallo</li>
<li class=""><strong>institution:</strong> University of Cagliari, University of Camerino, University of Palermo</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13772" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13772</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a blockchain-based software architecture for carbon credit certification, integrating real-time IoT data collection, edge-level aggregation, and secure on-chain storage via smart contracts. The system is demonstrated through a photovoltaic case study and aligns with European legislation and market standards. The architecture provides a structured pathway for generating verifiable carbon-credit records and supporting third-party verification.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] ContiguousKV: Accelerating LLM Prefill with Granularity-Aligned KV Cache Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [KV cache offloading, KV cache pruning, I/O optimization, asynchronous prefetching, attention-guided cache management]</li>
<li class=""><strong>authors:</strong> Jing Zou, Shangyu Wu, Hancong Duan, Qiao Li, Chun Jason Xue</li>
<li class=""><strong>institution:</strong> UESTC, MBZUAI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13631" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13631</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes ContiguousKV, a system that accelerates the LLM prefill phase by aligning KV cache pruning granularity with I/O operations to eliminate read amplification and using asynchronous prefetching to hide I/O latency. It achieves a 3.85x speedup over the state-of-the-art system IMPRESS while maintaining output quality.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [empirical study, failure analysis, deployment stack, tokenizer defects, configuration struggles, environmental incompatibility]</li>
<li class=""><strong>authors:</strong> Guangba Yu, Zirui Wang, Yujie Huang, Renyi Zhong, Yuedong Zhong, Yilun Wang, Michael R. Lyu</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Sun Yat-sen University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13655" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13655</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts a large-scale empirical study of 705 user-reported failures from open-source LLM ecosystems (DeepSeek, Llama, Qwen). It finds that the primary reliability bottleneck in user-managed deployment shifts from algorithmic defects to systemic fragility in the deployment stack, with root causes being homogeneous across different model series and escalating from fine-tuning to inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Device Association and Resource Allocation for Hierarchical Split Federated Learning in Space-Air-Ground Integrated Network</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hierarchical split federated learning, device association, resource allocation, split point selection, iterative optimization]</li>
<li class=""><strong>authors:</strong> Haitao Zhao, Xiaoyu Tang, Bo Xu, Jinlong Sun, Linghao Zhang</li>
<li class=""><strong>institution:</strong> Nanjing University of Posts and Telecommunications</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13817" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13817</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Hierarchical Split Federated Learning (HSFL) framework for the Space-Air-Ground Integrated Network (SAGIN) to address resource constraints and unbalanced data. The core method involves a joint optimization of device association, model split layer selection, and resource allocation, solved via an iterative algorithm. The simulation results show that the proposed approach effectively balances training efficiency and model accuracy in SAGIN.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Efficient Parallel <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">Δ</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(Δ+1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">Δ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>-Edge-Coloring</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [parallel algorithms], [PRAM, edge-coloring, Vizing&#x27;s theorem, parallel algorithm, time-processor tradeoff, arboricity]</li>
<li class=""><strong>authors:</strong> Michael Elkin, Ariel Khuzman</li>
<li class=""><strong>institution:</strong> Ben-Gurion University of the Negev</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13822" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13822</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents new parallel algorithms for the (Δ+1)-edge-coloring problem in the PRAM model, correcting a flaw in a prior analysis and achieving improved time and processor complexities. The main results include an algorithm with O(Δ⁴·log⁴ n) time and O(m·Δ) processors, as well as other trade-offs and improvements for graphs with small arboricity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Know Your Contract: Extending eIDAS Trust into Public Blockchains</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain, trust frameworks], [eIDAS, qualified electronic seal, smart contract, ECDSA P-256, CAdES, on-chain validation, regulatory compliance]</li>
<li class=""><strong>authors:</strong> Awid Vaziry, Christoph Wronka, Sandro Rodriguez Garzon, Axel Küpper</li>
<li class=""><strong>institution:</strong> Technische Universität Berlin, Baker Tilly</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13903" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13903</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an architecture that cryptographically binds smart contracts to qualified electronic seals under the EU&#x27;s eIDAS framework, creating a verifiable chain of trust from legal entities to blockchain addresses. It enables automated regulatory checks like Know Your Contract without new intermediaries. The main conclusion is that this integration provides a pathway for regulatory-compliant institutional DeFi, asset tokenization, and agentic commerce on public blockchains.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [sparse linear algebra, domain decomposition, halo exchange, adjoint-based differentiation, PyTorch autograd, multi-GPU scaling, iterative solvers]</li>
<li class=""><strong>authors:</strong> Mingyuan Chi</li>
<li class=""><strong>institution:</strong> Not explicitly provided; inferred from email domain as independent/github</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.13994" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.13994</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces torch-sla, a PyTorch library for GPU-accelerated, scalable, and differentiable sparse linear algebra. It addresses challenges in GPU acceleration, multi-GPU scaling via domain decomposition, and efficient gradient computation using adjoint solvers. The library enables end-to-end differentiable simulations with minimal memory overhead and supports large-scale problems like 400 million DOF linear solves.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] SecureSplit: Mitigating Backdoor Attacks in Split Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [split learning, backdoor attacks, dimensionality transformation, adaptive filtering, majority-based voting]</li>
<li class=""><strong>authors:</strong> Zhihao Dou, Dongfei Cui, Weida Wang, Anjun Gao, Yueyang Quan, Mengyao Ma, Viet Vo, Guangdong Bai, Zhuqing Liu, Minghong Fang</li>
<li class=""><strong>institution:</strong> Case Western Reserve University, Northeast Electric Power University, Fudan University, University of Louisville, University of North Texas, The University of Queensland, Swinburne University of Technology, City University of Hong Kong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14054" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14054</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SecureSplit, a defense mechanism for Split Learning that mitigates backdoor attacks. It uses a dimensionality transformation to highlight differences between benign and poisoned embeddings, followed by an adaptive filtering approach to remove the malicious ones. Experiments across multiple datasets and attack scenarios confirm its effectiveness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] &quot;Range as a Key&quot; is the Key! Fast and Compact Cloud Block Store Index with RASK</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud storage indexing], [range-as-a-key, log-structured leaf, range-aware split/merge, tree-structured index]</li>
<li class=""><strong>authors:</strong> Haoru Zhao, Mingkai Dong, Erci Xu, Zhongyu Wang, Haibo Chen</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14129" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14129</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes RASK, a tree-structured index for cloud block stores that directly indexes block ranges instead of individual blocks to save memory. It addresses challenges like range overlap using log-structured leaves and reduces fragmentation with range-aware split/merge mechanisms. Evaluation on production traces shows RASK reduces memory footprint by up to 98.9% and increases throughput by up to 31.0x compared to state-of-the-art indexes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260121] Multi-Partner Project: Multi-GPU Performance Portability Analysis for CFD Simulations at Scale</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [computational fluid dynamics, spectral elements, performance portability, multi-GPU, design space exploration, LUMI cluster]</li>
<li class=""><strong>authors:</strong> Panagiotis-Eleftherios Eleftherakis, George Anagnostopoulos, Anastassis Kapetanakis, Mohammad Umair, Jean-Yves Vet, Konstantinos Iliakis, Jonathan Vincent, Jing Gong, Akshay Patil, Clara García-Sánchez, Gerardo Zampino, Ricardo Vinuesa, Sotirios Xydis</li>
<li class=""><strong>institution:</strong> National Technical University of Athens, KTH Royal Institute of Technology, Hewlett Packard Enterprise (HPE), Technical University of Delft, University of Michigan</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14159" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14159</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the performance portability of the SOD2D spectral elements CFD framework across AMD and NVIDIA GPU architectures. It employs a multi-level, full-stack design space exploration to characterize performance. The study concludes that memory access optimizations cause significant performance variability (0.69x-3.91x speedup deviations), highlighting the limits of performance projections and the need for informed, multi-level tuning for scalable simulations.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 47</strong></p>
<ul>
<li class="">[arXiv260121] Hindsight Preference Replay Improves Preference-Conditioned Multi-Objective Reinforcement Learning <a href="https://arxiv.org/pdf/2601.11604" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Bielik 11B v3: Multilingual Large Language Model for European Languages <a href="https://arxiv.org/pdf/2601.11579" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Reinforcement Learning for Dynamic Workflow Optimization in CI/CD Pipelines <a href="https://arxiv.org/pdf/2601.11647" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment <a href="https://arxiv.org/pdf/2601.11574" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] AGGC: Adaptive Group Gradient Clipping for Stabilizing Large Language Model Training <a href="https://arxiv.org/pdf/2601.11864" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration <a href="https://arxiv.org/pdf/2601.11953" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] R<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning <a href="https://arxiv.org/pdf/2601.11960" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Extreme Value Policy Optimization for Safe Reinforcement Learning <a href="https://arxiv.org/pdf/2601.12008" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] UniMo: Unified Motion Generation and Understanding with Chain of Thought <a href="https://arxiv.org/pdf/2601.12126" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Aletheia: What Makes RLVR For Code Verifiers Tick? <a href="https://arxiv.org/pdf/2601.12186" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Speculative Sampling with Reinforcement Learning <a href="https://arxiv.org/pdf/2601.12212" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink NOMA Systems Using Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2601.12242" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation <a href="https://arxiv.org/pdf/2601.12401" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping <a href="https://arxiv.org/pdf/2601.12465" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Agentic Reasoning for Large Language Models <a href="https://arxiv.org/pdf/2601.12538" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models <a href="https://arxiv.org/pdf/2601.12641" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks <a href="https://arxiv.org/pdf/2601.12662" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Resource-Conscious RL Algorithms for Deep Brain Stimulation <a href="https://arxiv.org/pdf/2601.12699" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization <a href="https://arxiv.org/pdf/2601.12707" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Teaching Large Reasoning Models Effective Reflection <a href="https://arxiv.org/pdf/2601.12720" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Distribution-Centric Policy Optimization Dominates Exploration-Exploitation Trade-off <a href="https://arxiv.org/pdf/2601.12730" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction <a href="https://arxiv.org/pdf/2601.12762" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Communication Methods in Multi-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2601.12886" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] PaperGuide: Making Small Language-Model Paper-Reading Agents More Efficient <a href="https://arxiv.org/pdf/2601.12988" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Training instability in deep learning follows low-dimensional dynamical principles <a href="https://arxiv.org/pdf/2601.13160" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning <a href="https://arxiv.org/pdf/2601.13262" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning <a href="https://arxiv.org/pdf/2601.13284" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models <a href="https://arxiv.org/pdf/2601.13533" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Behavior Knowledge Merge in Reinforced Agentic Models <a href="https://arxiv.org/pdf/2601.13572" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2601.13657" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Reinforcement Learning for Opportunistic Routing in Software-Defined LEO-Terrestrial Systems <a href="https://arxiv.org/pdf/2601.13662" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering <a href="https://arxiv.org/pdf/2601.13752" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for Fiber Tractography <a href="https://arxiv.org/pdf/2601.13897" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning <a href="https://arxiv.org/pdf/2601.13942" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning <a href="https://arxiv.org/pdf/2601.13964" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based Multi-Objective Reinforcement Learning <a href="https://arxiv.org/pdf/2601.14092" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Toward Efficient Agents: Memory, Tool learning, and Planning <a href="https://arxiv.org/pdf/2601.14192" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Differentiated Pickup Point Offering for Emission Reduction in Last-Mile Delivery <a href="https://arxiv.org/pdf/2601.14196" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning <a href="https://arxiv.org/pdf/2601.14209" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Attention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment <a href="https://arxiv.org/pdf/2601.14228" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning <a href="https://arxiv.org/pdf/2601.14232" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Q-learning with Adjoint Matching <a href="https://arxiv.org/pdf/2601.14234" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression <a href="https://arxiv.org/pdf/2601.14238" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow <a href="https://arxiv.org/pdf/2601.14243" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data <a href="https://arxiv.org/pdf/2601.12053" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Primate-like perceptual decision making emerges through deep recurrent reinforcement learning <a href="https://arxiv.org/pdf/2601.12577" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated Reinforcement Learning <a href="https://arxiv.org/pdf/2601.13642" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 37</strong></p>
<ul>
<li class="">[arXiv260121] Overview of the SciHigh Track at FIRE 2025: Research Highlight Generation from Scientific Papers <a href="https://arxiv.org/pdf/2601.11582" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] NoiseFormer -- Noise Diffused Symmetric Attention Transformer <a href="https://arxiv.org/pdf/2601.11619" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers <a href="https://arxiv.org/pdf/2601.11641" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Reinforcement Learning for Dynamic Workflow Optimization in CI/CD Pipelines <a href="https://arxiv.org/pdf/2601.11647" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Speculative Decoding: Performance or Illusion? <a href="https://arxiv.org/pdf/2601.11580" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] DeepEvidence: Empowering Biomedical Discovery with Deep Knowledge Graph Research <a href="https://arxiv.org/pdf/2601.11560" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] A Proof of Concept for a Digital Twin of an Ultrasonic Fermentation System <a href="https://arxiv.org/pdf/2601.11723" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Large language models struggle with ethnographic text annotation <a href="https://arxiv.org/pdf/2601.12099" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Ordered Local Momentum for Asynchronous Distributed Learning under Arbitrary Delays <a href="https://arxiv.org/pdf/2601.12322" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Harmonizing the Arabic Audio Space with Data Scheduling <a href="https://arxiv.org/pdf/2601.12494" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory <a href="https://arxiv.org/pdf/2601.12557" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] A Cloud-based Multi-Agentic Workflow for Science <a href="https://arxiv.org/pdf/2601.12607" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] HERMES: A Unified Open-Source Framework for Realtime Multimodal Physiological Sensing, Edge AI, and Intervention in Closed-Loop Smart Healthcare Applications <a href="https://arxiv.org/pdf/2601.12610" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT <a href="https://arxiv.org/pdf/2601.12638" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Distilling Time Series Foundation Models for Efficient Forecasting <a href="https://arxiv.org/pdf/2601.12785" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation <a href="https://arxiv.org/pdf/2601.12904" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] PaperGuide: Making Small Language-Model Paper-Reading Agents More Efficient <a href="https://arxiv.org/pdf/2601.12988" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Probe and Skip: Self-Predictive Token Skipping for Efficient Long-Context LLM Inference <a href="https://arxiv.org/pdf/2601.13155" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models <a href="https://arxiv.org/pdf/2601.13358" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Patterning: The Dual of Interpretability <a href="https://arxiv.org/pdf/2601.13548" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Vulnerability of LLMs&#x27; Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions <a href="https://arxiv.org/pdf/2601.13590" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Machine learning based radiative parameterization scheme and its performance in operational reforecast experiments <a href="https://arxiv.org/pdf/2601.13592" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2601.13657" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache Compression for Long-Context LLM Inference <a href="https://arxiv.org/pdf/2601.13684" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Breaking the Data Barrier in Learning Symbolic Computation: A Case Study on Variable Ordering Suggestion for Cylindrical Algebraic Decomposition <a href="https://arxiv.org/pdf/2601.13731" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] A universal linearized subspace refinement framework for neural networks <a href="https://arxiv.org/pdf/2601.13989" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Group-Invariant Unsupervised Skill Discovery: Symmetry-aware Skill Representations for Generalizable Behavior <a href="https://arxiv.org/pdf/2601.14000" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment <a href="https://arxiv.org/pdf/2601.14022" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] &#x27;1&#x27;-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators <a href="https://arxiv.org/pdf/2601.14087" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Quantum Kernel Machine Learning for Autonomous Materials Science <a href="https://arxiv.org/pdf/2601.11775" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] On the Provable Suboptimality of Momentum SGD in Nonstationary Stochastic Optimization <a href="https://arxiv.org/pdf/2601.12238" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] BiCoLoR: Communication-Efficient Optimization with Bidirectional Compression and Local Training <a href="https://arxiv.org/pdf/2601.12400" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Artificial Intelligence in Materials Science and Engineering: Current Landscape, Key Challenges, and Future Trajectorie <a href="https://arxiv.org/pdf/2601.12554" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] onepot CORE -- an enumerated chemical space to streamline drug discovery, enabled by automated small molecule synthesis and AI <a href="https://arxiv.org/pdf/2601.12603" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Reorienting off-path Nudged Elastic Bands (RONEB) via Minimum Mode Following <a href="https://arxiv.org/pdf/2601.12630" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Pixelwise Uncertainty Quantification of Accelerated MRI Reconstruction <a href="https://arxiv.org/pdf/2601.13236" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260121] Unified Unbiased Variance Estimation for MMD: Robust Finite-Sample Performance with Imbalanced Data and Exact Acceleration under Null and Alternative Hypotheses <a href="https://arxiv.org/pdf/2601.13874" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2026-01-22">2026-01-22<a href="#2026-01-22" class="hash-link" aria-label="Direct link to 2026-01-22" title="Direct link to 2026-01-22" translate="no">​</a></h2>
<p><strong>cs.DC total: 13</strong></p>
<ul>
<li class="">
<p><strong>[arXiv260122] Beyond Denial-of-Service: The Puppeteer&#x27;s Attack for Fine-Grained Control in Ranking-Based Federated Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, federated rank learning, model poisoning attack, edge control attack, fine-grained control, byzantine-robust aggregation]</li>
<li class=""><strong>authors:</strong> Zhihao Chen, Zirui Gong, Jianting Ning, Yanjun Zhang, Leo Yu Zhang</li>
<li class=""><strong>institution:</strong> Fujian Normal University, Griffith University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14687" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14687</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces the Edge Control Attack (ECA), a novel fine-grained model poisoning attack against Federated Rank Learning (FRL). ECA manipulates ranking updates to precisely degrade a model&#x27;s accuracy to a target level while evading detection by mimicking normal convergence. The findings demonstrate that FRL remains vulnerable to advanced attacks, highlighting the need for stronger defenses.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260122] Agent Identity URI Scheme: Topology-Independent Naming and Capability-Based Discovery for Multi-Agent Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [agent:// URI scheme, DHT, capability-based discovery, PASETO tokens, trust root, sortable unique identifier]</li>
<li class=""><strong>authors:</strong> Roland R. Rodriguez Jr</li>
<li class=""><strong>institution:</strong> Independent Researcher</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14567" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14567</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes the agent:// URI scheme, a new naming system that decouples agent identity from network location using a trust root, capability path, and unique identifier. It enables decentralized, capability-based discovery via a DHT and uses cryptographic attestation for verification. The authors conclude that this scheme provides a stable, high-performance foundation for identity and discovery in multi-agent systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260122] Specifying and Verifying RDMA Synchronisation (Extended Version)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed computing], [RDMA, read-modify-write, TSO, formal verification, synchronization, locks]</li>
<li class=""><strong>authors:</strong> Guillaume Ambal, Max Stupple, Brijesh Dongol, Azalea Raad</li>
<li class=""><strong>institution:</strong> Imperial College London, University of Surrey</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14642" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14642</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a formal semantics for remote read-modify-write (RMW) instructions in RDMA over TSO, called RDMA^TSO_RMW, to enable the verification of synchronization abstractions like locks. It builds composable libraries and verifies three classes of remote locks, concluding that remote RMW operations are weak and only atomic against other remote RMWs.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260122] Exploiting Spot Instances for Time-Critical Cloud Workloads Using Optimal Randomized Strategies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud scheduling], [randomized algorithm, online scheduling, competitive ratio, spot instances, on-demand instances, deadline-aware]</li>
<li class=""><strong>authors:</strong> Neelkamal Bhuyan, Randeep Bhatia, Murali Kodialam, TV Lakshman</li>
<li class=""><strong>institution:</strong> Georgia Institute of Technology, Nokia Bell Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14612" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14612</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a novel randomized scheduling algorithm called ROSS for running time-critical jobs on a mix of cheap but unreliable spot instances and expensive on-demand instances in the cloud. It proves that ROSS achieves an optimal competitive ratio of √K for cost, significantly improving over deterministic policies. Evaluations on real-world cloud traces show it reduces costs by up to 30% compared to state-of-the-art methods while meeting hard deadlines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260122] Parallel Collaborative ADMM Privacy Computing and Adaptive GPU Acceleration for Distributed Edge Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [ADMM, Paillier homomorphic encryption, quantization, GPU acceleration, parallel encryption]</li>
<li class=""><strong>authors:</strong> Mengchun Xia, Zhicheng Dong, Donghong Cai, Fang Fang, Lisheng Fan, Pingzhi Fan</li>
<li class=""><strong>institution:</strong> Tibet University, Jinan University, Western University, Guangzhou University, Southwest Jiaotong University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14980" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14980</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a three-phase parallel collaborative ADMM privacy computing algorithm (3P-ADMM-PC2) for distributed edge networks, which uses Paillier homomorphic encryption and quantization to protect data privacy and adapts the computations for GPU acceleration. The experimental results show that the proposed method achieves mean square error performance close to non-private distributed ADMM while offering a significant computational speedup compared to CPU-based implementations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260122] AlertGuardian: Intelligent Alert Life-Cycle Management for Large-scale Cloud Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [large language models (LLMs), graph learning model, Retrieval Augmented Generation (RAG), multi-agent iterative feedback]</li>
<li class=""><strong>authors:</strong> Guangba Yu, Genting Mai, Rui Wang, Ruipeng Li, Pengfei Chen, Long Pan, Ruijie Xu</li>
<li class=""><strong>institution:</strong> Sun Yat-sen University, Tencent</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14912" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14912</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes AlertGuardian, a framework that combines large language models (LLMs) and lightweight graph models to manage the alert life-cycle in cloud systems through denoising, summarization, and rule refinement. It significantly reduces alert fatigue and improves fault diagnosis accuracy in real-world deployments. The system&#x27;s effectiveness is demonstrated by a high alert reduction ratio and acceptance of improved alert rules by engineers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260122] Application-level observability for adaptive Edge to Cloud continuum systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [OpenTelemetry, Prometheus, K3s, Chaos Mesh, Service-Level Objectives (SLOs), feedback control, microservices]</li>
<li class=""><strong>authors:</strong> Kaddour Sidi, Daniel Balouek, Baptiste Jonglez</li>
<li class=""><strong>institution:</strong> IMT Atlantique, Inria, LS2N</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14923" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14923</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an application-level observability framework for Edge-to-Cloud systems, integrating developer-driven instrumentation with SLO-aware feedback control using tools like OpenTelemetry and Prometheus. It demonstrates the framework&#x27;s effectiveness through a video processing use case, showing that it enables autonomous adaptation to maintain performance targets under variable workloads and faults. The preliminary results indicate improved scalability, fault tolerance, and responsiveness for adaptive, SLO-compliant applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260122] JAXMg: A multi-GPU linear solver in JAX</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [JAX, multi-GPU, linear solver, cuSOLVERMg, XLA Foreign Function Interface, Cholesky decomposition, eigendecomposition]</li>
<li class=""><strong>authors:</strong> Roeland Wiersema</li>
<li class=""><strong>institution:</strong> Flatiron Institute</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14466" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14466</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces JAXMg, a system that integrates multi-GPU dense linear algebra into JAX by interfacing with NVIDIA&#x27;s cuSOLVERMg library via the XLA Foreign Function Interface. It enables scalable Cholesky-based linear solves and symmetric eigendecompositions for large matrices that exceed single-GPU memory. The main conclusion is that this approach allows distributed GPU solvers to be used as JIT-compatible primitives within JAX, preserving composability and enabling multi-GPU execution in end-to-end scientific workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260122] Exploring Performance-Productivity Trade-offs in AMT Runtimes: A Task Bench Study of Itoyori, ItoyoriFBC, HPX, and MPI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [parallel computing], [Asynchronous Many-Task (AMT), Task Bench, Partitioned Global Address Space (PGAS), RDMA, work stealing, future-based synchronization, Minimum Effective Task Granularity (METG)]</li>
<li class=""><strong>authors:</strong> Torben R. Lahnor, Mia Reitz, Jonas Posner, Patrick Diehl</li>
<li class=""><strong>institution:</strong> University of Kassel, Fulda University of Applied Sciences, Los Alamos National Laboratory (LANL)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14608" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14608</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper uses the Task Bench framework to evaluate the performance and programmer productivity of parallel runtimes, including Itoyori, ItoyoriFBC, HPX, and MPI. It finds that MPI excels in regular workloads but is less productive, while Itoyori leads in communication-intensive performance and productivity, revealing a trade-off between efficiency and code simplicity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260122] Optimizing FaaS Platforms for MCP-enabled Agentic Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [FaaS, AWS Lambda, LangGraph, Model Context Protocol (MCP), serverless, agentic workflows, state management, DynamoDB, S3, function fusion]</li>
<li class=""><strong>authors:</strong> Varad Kulkarni, Vaibhav Jha, Nikhil Reddy, Yogesh Simmhan</li>
<li class=""><strong>institution:</strong> Indian Institute of Science, Bangalore</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14735" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14735</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents FAME, a serverless architecture that decomposes agentic workflows into modular FaaS functions (Planner, Actor, Evaluator) using LangGraph and orchestrates them with AWS Step Functions. It addresses statelessness by automating memory persistence with DynamoDB and optimizes performance through caching and MCP server wrappers. The evaluation shows significant improvements in latency, cost, and completion rates, demonstrating the viability of serverless platforms for scalable multi-agent AI workflows.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260122] On Distributed Quantum Computing with Distributed Fan-Out Operations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed quantum computing], [distributed fan-out operations, GHZ states, entangled pairs, circuit depth reduction, distributed quantum Fourier transform]</li>
<li class=""><strong>authors:</strong> Seng W. Loke</li>
<li class=""><strong>institution:</strong> Deakin University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.14734" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.14734</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes using distributed fan-out operations, which leverage multipartite GHZ states, as a primitive for distributed quantum computing. It compares circuits using only entangled pairs against those using distributed fan-out, showing that the latter can reduce circuit depth and potentially entanglement resource requirements. The main conclusion is that distributed GHZ states could serve as a fundamental building block for efficient distributed quantum operations if they can be realized practically.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260122] RadixMLP - Intra-batch Deduplication for Causal Transformers</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [prefix deduplication, prefix trie, gather-scatter, stateless inference, position-wise computation, ragged inference]</li>
<li class=""><strong>authors:</strong> Michael Feil, Julius Lipp</li>
<li class=""><strong>institution:</strong> Baseten, Independent</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.15013" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.15013</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces RadixMLP, a stateless technique that eliminates redundant computation in causal transformer inference by dynamically mapping a batch of sequences to a prefix trie, gathering shared segments for position-wise computation, and scattering results back at attention boundaries. It achieves speedups of 1.44–1.59× in real-world reranking benchmarks and up to 5× in synthetic tests with long shared prefixes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260122] DeepFedNAS: A Unified Framework for Principled, Hardware-Aware, and Predictor-Free Federated Neural Architecture Search</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, neural architecture search, supernet, pareto optimal, predictor-free search, hardware-aware optimization]</li>
<li class=""><strong>authors:</strong> Bostan Khan, Masoud Daneshtalab</li>
<li class=""><strong>institution:</strong> Mälardalen University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.15127" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.15127</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces DeepFedNAS, a two-phase federated neural architecture search framework. It uses a principled fitness function and a Pareto-optimal cache to guide supernet training, followed by a predictor-free search method for rapid subnet discovery. The method achieves state-of-the-art accuracy and a ~61x speedup in the search pipeline, making hardware-aware federated learning deployments practical.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 23</strong></p>
<ul>
<li class="">[arXiv260122] Towards Execution-Grounded Automated AI Research <a href="https://arxiv.org/pdf/2601.14525" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Case-Guided Sequential Assay Planning in Drug Discovery <a href="https://arxiv.org/pdf/2601.14710" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree <a href="https://arxiv.org/pdf/2601.14523" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Report for NSF Workshop on AI for Electronic Design Automation <a href="https://arxiv.org/pdf/2601.14541" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs <a href="https://arxiv.org/pdf/2601.14711" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks <a href="https://arxiv.org/pdf/2601.14652" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with Goal-Conditioned Reinforcement Learning <a href="https://arxiv.org/pdf/2601.14693" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning <a href="https://arxiv.org/pdf/2601.14716" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation <a href="https://arxiv.org/pdf/2601.14695" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Improving Regret Approximation for Unsupervised Dynamic Environment Generation <a href="https://arxiv.org/pdf/2601.14957" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Proximal Policy Optimization with Evolutionary Mutations <a href="https://arxiv.org/pdf/2601.14705" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] SearchGym: Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation <a href="https://arxiv.org/pdf/2601.14615" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] CI4A: Semantic Component Interfaces for Agents Empowering Web Automation <a href="https://arxiv.org/pdf/2601.14790" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Beyond Affinity: A Benchmark of 1D, 2D, and 3D Methods Reveals Critical Trade-offs in Structure-Based Drug Design <a href="https://arxiv.org/pdf/2601.14283" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study <a href="https://arxiv.org/pdf/2601.14888" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control <a href="https://arxiv.org/pdf/2601.15015" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem <a href="https://arxiv.org/pdf/2601.15038" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning <a href="https://arxiv.org/pdf/2601.15086" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning with Improved Network Embedding <a href="https://arxiv.org/pdf/2601.15131" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning <a href="https://arxiv.org/pdf/2601.15141" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data <a href="https://arxiv.org/pdf/2601.15158" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning <a href="https://arxiv.org/pdf/2601.15160" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models <a href="https://arxiv.org/pdf/2601.15165" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 14</strong></p>
<ul>
<li class="">[arXiv260122] Search over Self-Edit Strategies for LLM Adaptation <a href="https://arxiv.org/pdf/2601.14532" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Towards Execution-Grounded Automated AI Research <a href="https://arxiv.org/pdf/2601.14525" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] GPU-accelerated simulated annealing based on p-bits with real-world device-variability modeling <a href="https://arxiv.org/pdf/2601.14476" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Quality or Quantity? Error-Informed Selective Online Learning with Gaussian Processes in Multi-Agent Systems: Extended Version <a href="https://arxiv.org/pdf/2601.14275" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] End-to-End Transformer Acceleration Through Processing-in-Memory Architectures <a href="https://arxiv.org/pdf/2601.14260" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Variance-Adaptive Muon: Accelerating LLM Pretraining with NSR-Modulated and Variance-Scaled Momentum <a href="https://arxiv.org/pdf/2601.14603" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Measuring the State of Open Science in Transportation Using Large Language Models <a href="https://arxiv.org/pdf/2601.14429" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] SilentDrift: Exploiting Action Chunking for Stealthy Backdoor Attacks on Vision-Language-Action Models <a href="https://arxiv.org/pdf/2601.14323" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study <a href="https://arxiv.org/pdf/2601.14888" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control <a href="https://arxiv.org/pdf/2601.15015" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems <a href="https://arxiv.org/pdf/2601.15059" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] LoRAP: Low-Rank Aggregation Prompting for Quantized Graph Neural Networks Training <a href="https://arxiv.org/pdf/2601.15079" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] Rethinking Video Generation Model for the Embodied World <a href="https://arxiv.org/pdf/2601.15282" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260122] TRSVR: An Adaptive Stochastic Trust-Region Method with Variance Reduction <a href="https://arxiv.org/pdf/2601.14647" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2026-01-23">2026-01-23<a href="#2026-01-23" class="hash-link" aria-label="Direct link to 2026-01-23" title="Direct link to 2026-01-23" translate="no">​</a></h2>
<p><strong>cs.DC total: 4</strong></p>
<ul>
<li class="">
<p><strong>[arXiv260123] DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, knowledge distillation, medical image segmentation, foundation models, lightweight models, mutual distillation, learnability-guided sample selection]</li>
<li class=""><strong>authors:</strong> Hanwen Zhang, Qiaojin Shen, Yuxi Liu, Yuesheng Zhu, Guibo Luo</li>
<li class=""><strong>institution:</strong> Peking University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.16073" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.16073</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DSFedMed, a dual-scale federated learning framework for medical image segmentation that uses mutual knowledge distillation between a centralized foundation model and lightweight client models. It employs generated medical images and a learnability-guided sample selection strategy to facilitate efficient distillation. The method achieves higher accuracy while significantly reducing communication costs and inference time compared to existing federated foundation model baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260123] Scaling Sample-Based Quantum Diagonalization on GPU-Accelerated Systems using OpenMP Offload</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [OpenMP Offload, Davidson algorithm, quantum diagonalization, hybrid quantum-HPC, GPU acceleration]</li>
<li class=""><strong>authors:</strong> Robert Walkup, Juha Jäykkä, Igor Pasichnyk, Zachary Streeter, Kasia Świrydowicz, Mikko Tukiainen, Yasuko Eckert, Luke Bertels, Daniel Claudino, Peter Groszkowski, Travis S. Humble, Constantinos Evangelinos, Javier Robledo-Moreno, William Kirby, Antonio Mezzacapo, Antonio Córcoles, Seetharami Seelam</li>
<li class=""><strong>institution:</strong> IBM Research, Advanced Micro Devices, Inc., Oak Ridge National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.16169" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.16169</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a method for scaling sample-based quantum diagonalization (SQD) by offloading the computationally intensive classical diagonalization step to GPUs using OpenMP Offload. The approach leverages the Davidson algorithm on GPU-accelerated systems, including Frontier, to achieve a performance boost of approximately 100x per node compared to previous CPU-based implementations, significantly reducing processing time from hours to minutes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260123] Securing LLM-as-a-Service for Small Businesses: An Industry Case Study of a Distributed Chatbot Deployment Platform</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [retrieval-augmented generation (RAG), prompt injection defense, k3s clusters, multi-tenant platform, encrypted overlay network, container-based isolation]</li>
<li class=""><strong>authors:</strong> Jiazhu Xie, Bowen Li, Heyu Fu, Chong Gao, Ziqi Xu, Fengling Han</li>
<li class=""><strong>institution:</strong> RMIT University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.15528" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.15528</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a distributed, open-source platform built on lightweight k3s clusters to help small businesses deploy secure, multi-tenant LLM-based chatbots using a no-code workflow. The platform integrates defenses against prompt injection attacks and uses an encrypted overlay network for cost-efficient resource pooling and data isolation. The evaluation through a real-world e-commerce deployment demonstrates that secure and efficient LLM services can be achieved under the cost and operational constraints of small businesses.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv260123] Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [GPU computing], [RT cores, bounding volume hierarchy (BVH), fixed-radius nearest neighbor search (FRNN), periodic boundary conditions, Lennard-Jones model]</li>
<li class=""><strong>authors:</strong> Enzo Meneses, Hugo Bec, Cristóbal A. Navarroa, Benoît Crespin, Felipe A. Quezada, Nancy Hitschfeld, Heinich Porro, Maxime Maria</li>
<li class=""><strong>institution:</strong> Universidad Austral de Chile, University of Limoges, University of Chile</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2601.15633" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2601.15633</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces three techniques to improve particle physics simulations using RT cores for fixed-radius nearest neighbor search: a real-time BVH update/rebuild optimizer, a neighbor-list-free RT core method, and support for periodic boundary conditions. The methods significantly accelerate simulations, improve energy efficiency, and scale across GPU generations, while also identifying cases where traditional GPU computation remains preferable.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 16</strong></p>
<ul>
<li class="">[arXiv260123] Structured Hints for Sample-Efficient Lean Theorem Proving <a href="https://arxiv.org/pdf/2601.16172" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] Learning to Discover at Test Time <a href="https://arxiv.org/pdf/2601.16175" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] Q-Probe: Scaling Image Quality Assessment to High Resolution via Context-Aware Agentic Probing <a href="https://arxiv.org/pdf/2601.15356" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] Performance-guided Reinforced Active Learning for Object Detection <a href="https://arxiv.org/pdf/2601.15688" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning <a href="https://arxiv.org/pdf/2601.15761" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] Decoupling Return-to-Go for Efficient Decision Transformer <a href="https://arxiv.org/pdf/2601.15953" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] Non-Stationary Functional Bilevel Optimization <a href="https://arxiv.org/pdf/2601.15363" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour <a href="https://arxiv.org/pdf/2601.15995" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] Statistical Reinforcement Learning in the Real World: A Survey of Challenges and Future Directions <a href="https://arxiv.org/pdf/2601.15353" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation <a href="https://arxiv.org/pdf/2601.15330" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] PhysProver: Advancing Automatic Theorem Proving for Physics <a href="https://arxiv.org/pdf/2601.15737" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models <a href="https://arxiv.org/pdf/2601.15690" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] LLM-in-Sandbox Elicits General Agentic Intelligence <a href="https://arxiv.org/pdf/2601.16206" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind <a href="https://arxiv.org/pdf/2601.15715" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors <a href="https://arxiv.org/pdf/2601.15625" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards <a href="https://arxiv.org/pdf/2601.15609" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 7</strong></p>
<ul>
<li class="">[arXiv260123] Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling <a href="https://arxiv.org/pdf/2601.15664" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification <a href="https://arxiv.org/pdf/2601.15498" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] ToolCaching: Towards Efficient Caching for LLM Tool-calling <a href="https://arxiv.org/pdf/2601.15335" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] Low-Dimensional Adaptation of Rectified Flow: A New Perspective through the Lens of Diffusion and Stochastic Localization <a href="https://arxiv.org/pdf/2601.15500" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents <a href="https://arxiv.org/pdf/2601.15311" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design <a href="https://arxiv.org/pdf/2601.15710" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv260123] A Rolling-Space Branch-and-Price Algorithm for the Multi-Compartment Vehicle Routing Problem with Multiple Time Windows <a href="https://arxiv.org/pdf/2601.16194" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2026-02-03T05:09:39.000Z" itemprop="dateModified">Feb 3, 2026</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20260112-20260118"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20260112-20260118</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20260126-20260201"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20260126-20260201</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2026-01-19" class="table-of-contents__link toc-highlight">2026-01-19</a></li><li><a href="#2026-01-21" class="table-of-contents__link toc-highlight">2026-01-21</a></li><li><a href="#2026-01-22" class="table-of-contents__link toc-highlight">2026-01-22</a></li><li><a href="#2026-01-23" class="table-of-contents__link toc-highlight">2026-01-23</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>