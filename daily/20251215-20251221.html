<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251215-20251221" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251215-20251221 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20251215-20251221"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251215-20251221 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-12-15"><meta data-rh="true" property="og:description" content="2025-12-15"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20251215-20251221"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251215-20251221" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251215-20251221" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20251215-20251221","item":"https://darkknight996.github.io/daily/20251215-20251221"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.f46e64d1.js" defer="defer"></script>
<script src="/assets/js/main.df0eedc0.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251103-20251109"><span title="20251103-20251109" class="linkLabel_WmDU">20251103-20251109</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251110-20251116"><span title="20251110-20251116" class="linkLabel_WmDU">20251110-20251116</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251117-20251123"><span title="20251117-20251123" class="linkLabel_WmDU">20251117-20251123</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251124-20251130"><span title="20251124-20251130" class="linkLabel_WmDU">20251124-20251130</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251201-20251207"><span title="20251201-20251207" class="linkLabel_WmDU">20251201-20251207</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251208-20251214"><span title="20251208-20251214" class="linkLabel_WmDU">20251208-20251214</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20251215-20251221"><span title="20251215-20251221" class="linkLabel_WmDU">20251215-20251221</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251222-20251228"><span title="20251222-20251228" class="linkLabel_WmDU">20251222-20251228</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251229-20260104"><span title="20251229-20260104" class="linkLabel_WmDU">20251229-20260104</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260105-20260111"><span title="20260105-20260111" class="linkLabel_WmDU">20260105-20260111</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260112-20260118"><span title="20260112-20260118" class="linkLabel_WmDU">20260112-20260118</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260119-20260125"><span title="20260119-20260125" class="linkLabel_WmDU">20260119-20260125</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260126-20260201"><span title="20260126-20260201" class="linkLabel_WmDU">20260126-20260201</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20260202-20260208"><span title="20260202-20260208" class="linkLabel_WmDU">20260202-20260208</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251215-20251221</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251215-20251221</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-15">2025-12-15<a href="#2025-12-15" class="hash-link" aria-label="Direct link to 2025-12-15" title="Direct link to 2025-12-15" translate="no">​</a></h2>
<p><strong>cs.DC total: 15</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251215] Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed network algorithms], [multi-packet messaging, pruning, closeness centrality, decentralized computation, message efficiency]</li>
<li class=""><strong>authors:</strong> Patrick D. Manya, Eugene M. Mbuyi, Gothy T. Ngoie, Jordan F. Masakuna</li>
<li class=""><strong>institution:</strong> University of Kinshasa</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11512" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11512</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper enhances a distributed pruning method for closeness centrality by using multi-packet messaging to batch data into larger blocks. This reduces the number of exchanged messages and improves communication efficiency, particularly for large networks, with a manageable trade-off in local memory overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Seamless Transitions: A Comprehensive Review of Live Migration Technologies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [virtualization], [live migration, container migration, virtual machine migration, migration techniques, migration units, infrastructure characteristics]</li>
<li class=""><strong>authors:</strong> Sima Attar-Khorasani, Lincoln Sherpa, Matthias Lieber, Siavash Ghiasvand</li>
<li class=""><strong>institution:</strong> TUD Dresden University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10979" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10979</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper provides a comprehensive review of live migration technologies, focusing on container-based and virtual machine-based approaches. It analyzes migration techniques, units, and infrastructure, concluding that practical challenges and resource demands can sometimes outweigh the benefits, making implementation difficult to justify.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] An Efficient Approach for Energy Conservation in Cloud Computing Environment</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [cloud computing], [task scheduling, resource utilization, fitness value, multi-criteria energy-efficient task scheduling (MCEETS), MaxUtil]</li>
<li class=""><strong>authors:</strong> Sohan Kumar Pande, Sanjaya Kumar Panda, Preeti Ranjan Sahu</li>
<li class=""><strong>institution:</strong> Not specified</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10974" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10974</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a multi-criteria energy-efficient task scheduling (MCEETS) algorithm for cloud computing, which uses a fitness value based on CPU, disk, I/O utilization, and task processing time to improve resource utilization. Simulation results show that the proposed algorithm consumes less energy than the existing MaxUtil algorithm.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [dynamic scheduling, multi-objective scheduling, hybrid priority scheduler, predictive backfill, smart batch, fragmentation reduction, job starvation, GPU utilization]</li>
<li class=""><strong>authors:</strong> Akhmadillo Mamirov</li>
<li class=""><strong>institution:</strong> The College of Wooster</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10980" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10980</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces three dynamic multi-objective schedulers (Hybrid Priority, Predictive Backfill, and Smart Batch) designed to reduce fragmentation and starvation in GPU clusters. Through simulation, these schedulers significantly outperform static baselines in utilization, throughput, and fairness, demonstrating that adaptive scheduling can meaningfully improve GPU efficiency in heterogeneous AI clusters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] An LLVM-Based Optimization Pipeline for SPDZ</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [secure multiparty computation], [LLVM, SPDZ, secret sharing, batching, GPU kernels, non-blocking scheduler]</li>
<li class=""><strong>authors:</strong> Tianye Dai, Hammurabi Mendes, Heuichan Lim</li>
<li class=""><strong>institution:</strong> Davidson College</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11112" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11112</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an LLVM-based compiler pipeline for the SPDZ MPC protocol, which uses C with privacy annotations and LLVM IR to automatically batch operations and a runtime scheduler to overlap communication and computation, including GPU kernel mapping. The evaluation shows significant speedups over MP-SPDZ, indicating that leveraging LLVM with protocol-aware scheduling is effective for extracting parallelism without sacrificing usability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [hybrid parallelism, data parallelism, pipeline parallelism, model partitioning, network scheduling, runtime adaptation]</li>
<li class=""><strong>authors:</strong> Jianli Jin, Ziyang Lin, Qianli Dong, Yi Chen, Jayanth Srinivasa, Myungjin Lee, Zhaowei Tan, Fan Lai</li>
<li class=""><strong>institution:</strong> UIUC, Northwestern University, University of California, Riverside, Cisco Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10990" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10990</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dora is a framework that uses heterogeneity-aware model partitioning, contention-aware network scheduling, and a runtime adapter to achieve QoE-aware hybrid parallelism for distributed edge AI. It demonstrates significant improvements in execution speed and energy efficiency while maintaining quality of experience under dynamic runtime conditions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [GPU-native compilation, parallel traditional compilation, neural compilation, sequence-to-sequence translation, probabilistic verification, hybrid architecture, in-VRAM iteration]</li>
<li class=""><strong>authors:</strong> Adilet Metinov, Gulida M. Kudakeeva, Gulnara D. Kabaeva</li>
<li class=""><strong>institution:</strong> Institute of Information Technology, Kyrgyz State Technical University named after I. Razzakov</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11200" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11200</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper establishes theoretical foundations for three GPU-native compilation approaches—parallel traditional, neural, and hybrid—to eliminate CPU-GPU data transfers during code iteration cycles. It demonstrates that these methods can achieve 10-100x speedups by keeping compilation and execution entirely in GPU memory. The main conclusion is that GPU-native compilation offers a path to drastically reduce latency and energy consumption in AI code generation systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [cluster scheduling, disaggregated architecture, co-execution group, two-tier scheduling, round-robin, warm-start context switching]</li>
<li class=""><strong>authors:</strong> Tianyuan Wu, Lunxi Cao, Yining Wei, Wei Gao, Yuheng Zhao, Dakai An, Shaopan Xiong, Zhiqiang Lv, Ju Huang, Siran Yang, Yinghao Yu, Jiamang Wang, Lin Qu, Wei Wang</li>
<li class=""><strong>institution:</strong> Hong Kong University of Science and Technology, UIUC, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11306" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11306</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces RollMux, a cluster scheduling framework that improves efficiency in disaggregated RL post-training by multiplexing jobs to utilize idle phases. Its core method involves a two-tier scheduler and a co-execution group abstraction to orchestrate cross-cluster execution. The evaluation shows RollMux significantly improves cost efficiency over standard disaggregated and co-located baselines while maintaining service-level objectives.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [hierarchical federated learning, aggregated federated learning, continual federated learning, decentralized aggregation, gossip-based protocols]</li>
<li class=""><strong>authors:</strong> Sumit Chongder</li>
<li class=""><strong>institution:</strong> Maharashtra Institute of Technology - Art, Design and Technology University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10987" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10987</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an evaluation framework to compare centralized and decentralized aggregation algorithms in federated learning systems. It finds that decentralized methods (Aggregated and Continual Federated Learning) outperform centralized Hierarchical Federated Learning in metrics like precision and recall on standard datasets, highlighting the advantages of distributed computation and aggregation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Agentic Operator Generation for ML ASICs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [Triton, ATen kernels, PyTorch OpInfo, JIT compilation, large language models, agentic AI, kernel generation]</li>
<li class=""><strong>authors:</strong> Alec M. Hammond, Aram Markosyan, Aman Dontula, Simon Mahns, Zacharias Fisches, Dmitrii Pedchenko, Keyur Muzumdar, Natacha Supper, Mark Saroufim, Joe Isaacson, Laura Wang, Warren Hunt, Kaustubh Gondkar, Roman Levenstein, Gabriel Synnaeve, Richard Li, Jacob Kahn, Ajit Mathews</li>
<li class=""><strong>institution:</strong> Meta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.10977" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.10977</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents TritorX, an agentic AI system that uses large language models combined with a custom linter, JIT compilation, and a PyTorch OpInfo test harness to automatically generate functionally correct Triton ATen kernels for ML accelerators like MTIA. The system prioritizes broad operator coverage over performance for a limited set, successfully generating kernels for 481 unique operators that pass over 20,000 tests. This approach enables the rapid overnight generation of complete PyTorch ATen backends for new accelerator platforms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [runtime parallelization, branch-aware memory management, adaptive scheduling, DAG partitioning, buffer reuse, heterogeneous inference]</li>
<li class=""><strong>authors:</strong> Chong Tang, Hao Dai, Jagmohan Chauhan</li>
<li class=""><strong>institution:</strong> University of Southampton, University College London</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11532" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11532</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Parallax is a framework that accelerates mobile DNN inference by partitioning the computation graph to expose parallelism and using branch-aware memory management with adaptive scheduling to handle operator fallbacks. It reduces latency by up to 46% and energy consumption by up to 30% compared to state-of-the-art frameworks, while controlling memory overhead, without requiring model refactoring.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [continuous learning, camera grouping, GPU allocation, transmission control, cross-camera correlation]</li>
<li class=""><strong>authors:</strong> Yuze He, Ferdi Kossmann, Srinivasan Seshan, Peter Steenkiste</li>
<li class=""><strong>institution:</strong> Carnegie Mellon University, Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11727" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11727</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ECCO is a video analytics framework that improves resource efficiency by grouping cameras with similar data drift patterns to share retrained models. It uses a dynamic grouping algorithm, GPU allocator, and transmission controller to reduce compute and communication costs. The system increases retraining accuracy by 6.7%-18.1% or supports 3.3x more cameras at the same accuracy compared to baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [Snowflake algorithm, network-derived identity, private IPv4 address, bit-allocation scheme (1-41-16-6), stateless microservices, container orchestration]</li>
<li class=""><strong>authors:</strong> Manideep Reddy Chinthareddy</li>
<li class=""><strong>institution:</strong> Independent researcher (based on email domain)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11643" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11643</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a stateless, cloud-agnostic distributed ID generator that eliminates the need for explicit worker IDs by deriving node uniqueness from a container&#x27;s private IPv4 address. It introduces a modified bit-allocation scheme to incorporate this network-derived entropy while preserving monotonicity. The method demonstrates performance comparable to traditional stateful generators while offering improved scalability and operational simplicity in containerized environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [HPC resource access], [RESTful API, performance testing, proxy-based APIs, I/O bottlenecks, architectural redesign, security, authorization]</li>
<li class=""><strong>authors:</strong> Elia Palme, Juan Pablo Dorsch, Ali Khosravi, Giovanni Pizzi, Francesco Pagnamenta, Andrea Ceriani, Eirini Koutsaniti, Rafael Sarmiento, Ivano Bonesana, Alejandro Dabin</li>
<li class=""><strong>institution:</strong> CSCS – Swiss National Supercomputing Centre, PSI Center for Scientific Computing, Theory, and Data</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11634" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11634</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents FirecREST v2, a redesigned RESTful API for programmatic access to HPC resources, focusing on integrating enhanced security and high throughput. Through systematic performance testing, the authors identified and addressed bottlenecks in proxy-based APIs, achieving a ~100x performance improvement. The key conclusion is that a ground-up architectural redesign was necessary to meet growing user demands for scalable and secure HPC resource access.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251215] Hypergraph based Multi-Party Payment Channel</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain scalability], [hypergraph, payment channel networks, multi-party channels, off-chain scaling, DAG updates]</li>
<li class=""><strong>authors:</strong> Ayush Nainwal, Atharva Kamble, Nitin Awathare</li>
<li class=""><strong>institution:</strong> Indian Institute of Technology, Jodhpur; Indian Institute of Technology, Bombay</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.11775" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.11775</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces traditional bilateral channels with collectively funded hyperedges to enable leaderless, concurrent payments. This design addresses liquidity fragmentation and channel depletion in existing payment networks. An implementation demonstrates a high transaction success rate of approximately 94%, highlighting the robustness of the approach.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 16</strong></p>
<ul>
<li class="">[arXiv251215] Bandwidth-constrained Variational Message Encoding for Cooperative Multi-agent Reinforcement Learning <a href="https://arxiv.org/pdf/2512.11179" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Motif-2-12.7B-Reasoning: A Practitioner&#x27;s Guide to RL Training Recipes <a href="https://arxiv.org/pdf/2512.11463" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Rethinking Expert Trajectory Utilization in LLM Post-training <a href="https://arxiv.org/pdf/2512.11470" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] In-Context Multi-Objective Optimization <a href="https://arxiv.org/pdf/2512.11114" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents <a href="https://arxiv.org/pdf/2512.11277" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits <a href="https://arxiv.org/pdf/2512.11345" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance <a href="https://arxiv.org/pdf/2512.11421" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Multi-Objective Reinforcement Learning for Large-Scale Mixed Traffic Control <a href="https://arxiv.org/pdf/2512.11247" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Three methods, one problem: Classical and AI approaches to no-three-in-line <a href="https://arxiv.org/pdf/2512.11469" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound <a href="https://arxiv.org/pdf/2512.11169" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation <a href="https://arxiv.org/pdf/2512.11270" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning <a href="https://arxiv.org/pdf/2512.11342" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization <a href="https://arxiv.org/pdf/2512.11391" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry <a href="https://arxiv.org/pdf/2512.11558" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Agile Flight Emerges from Multi-Agent Competitive Racing <a href="https://arxiv.org/pdf/2512.11781" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Marti-5: A Mathematical Model of &quot;Self in the World&quot; as a First Step Toward Self-Awareness <a href="https://arxiv.org/pdf/2512.10985" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 6</strong></p>
<ul>
<li class="">[arXiv251215] A Scalable Multi-GPU Framework for Encrypted Large-Model Inference <a href="https://arxiv.org/pdf/2512.11269" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling <a href="https://arxiv.org/pdf/2512.11187" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Refining Graphical Neural Network Predictions Using Flow Matching for Optimal Power Flow with Constraint-Satisfaction Guarantee <a href="https://arxiv.org/pdf/2512.11127" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise <a href="https://arxiv.org/pdf/2512.11282" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning <a href="https://arxiv.org/pdf/2512.11342" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251215] Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration <a href="https://arxiv.org/pdf/2512.11587" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-16">2025-12-16<a href="#2025-12-16" class="hash-link" aria-label="Direct link to 2025-12-16" title="Direct link to 2025-12-16" translate="no">​</a></h2>
<p><strong>cs.DC total: 26</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251216] Strategic Server Deployment under Uncertainty in Mobile Edge Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [mobile edge computing], [stochastic bilevel optimization, submodular maximization, greedy algorithm]</li>
<li class=""><strong>authors:</strong> Duc A. Tran, Dung Truong, Duy Le</li>
<li class=""><strong>institution:</strong> University of Massachusetts Boston</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12532" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12532</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper formulates the strategic server deployment problem in mobile edge computing as a stochastic bilevel optimization and solves it by approximating the objective with submodular functions, enabling the use of greedy algorithms. The proposed method is evaluated with real-world data and shows significant performance improvements over alternative approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] A Conflict-Aware Resource Management Framework for the Computing Continuum</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [deep reinforcement learning, resource orchestration, conflict resolution, kubernetes, computing continuum]</li>
<li class=""><strong>authors:</strong> Vlad Popescu-Vifor, Ilir Murturi, Praveen Kumar Donta, Schahram Dustdar</li>
<li class=""><strong>institution:</strong> TU Wien, Stockholm University, University of Prishtina, ICREA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12299" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12299</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a framework for adaptive conflict resolution in resource orchestration for the computing continuum using Deep Reinforcement Learning (DRL). The framework was prototyped on a Kubernetes testbed and shows efficient resource reallocation and adaptive learning, providing a scalable and resilient solution for conflict-aware orchestration.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] Evaluating Asynchronous Semantics in Trace-Discovered Resilience Models: A Case Study on the OpenTelemetry Demo</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [fault-tolerance], [distributed tracing, Monte Carlo simulation, service dependency graph, chaos engineering, OpenTelemetry]</li>
<li class=""><strong>authors:</strong> Anatoly A. Krasnovsky</li>
<li class=""><strong>institution:</strong> Innopolis University, MB3R Lab</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12314" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12314</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a method to model microservice resilience by automatically deriving a service dependency graph from OpenTelemetry traces and using Monte Carlo simulation to estimate endpoint availability under failures. It concludes that for the studied system, adding explicit asynchronous semantics for Kafka message queues provides negligible benefit to availability predictions, suggesting a simpler connectivity-only model is sufficient.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] Beyond right or wrong : towards redefining adaptive learning indicators in virtual learning environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [educational technology], [Systematic Literature Review, adaptive learning, learning indicators, motivation, emotions, physiological responses, brain imaging, prior knowledge]</li>
<li class=""><strong>authors:</strong> Andreia dos Santos Sachete, Alba Valeria de SantAnna de Freitas Loiola, Fabio Diniz Rossi, Jose Valdeni de Lima, Raquel Salcedo Gomes</li>
<li class=""><strong>institution:</strong> Federal Institute Farroupilha, Federal University of Rio Grande do Sul</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12105" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12105</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper conducts a Systematic Literature Review to identify learning indicators beyond correctness for adaptive learning in Virtual Learning Environments. It concludes that indicators such as motivation, emotions, physiological responses, brain imaging, and prior knowledge are crucial for a more comprehensive assessment and adaptive training.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] Reputation-Based Leader Election under Partial Synchrony: Towards a Protocol-Independent Abstraction with Enhanced Guarantees</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed consensus], [Sliding Window Leader Election, reputation-based election, Byzantine fault tolerance, partial synchrony, protocol-independent abstraction]</li>
<li class=""><strong>authors:</strong> Xuyang Liu, Zijian Zhang, Zhen Li, Jiahang Sun, Jiamou Liu, Peng Jiang</li>
<li class=""><strong>institution:</strong> Beijing Institute of Technology, The University of Auckland</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12409" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12409</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a protocol-independent abstraction for leader election under partial synchrony and proposes the Sliding Window Leader Election (SWLE) mechanism, which uses consensus-behavior-based reputation scores to dynamically nominate leaders. The authors prove SWLE&#x27;s correctness and demonstrate through deployment that it significantly outperforms the state-of-the-art in throughput, latency, and Byzantine leader frequency under common faults.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] Accelerating Sparse Matrix-Matrix Multiplication on GPUs with Processing Near HBMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [processing near memory, hardware-software co-design, sparse matrix-matrix multiplication, graph neural networks, hash-based multi-phase SpGEMM, acceleration of indirect memory access]</li>
<li class=""><strong>authors:</strong> Shiju Li, Younghoon Min, Hane Yie, Hoshik Kim, Soohong Ahn, Joonseop Sim, Chul-Ho Lee, Jongryool Kim</li>
<li class=""><strong>institution:</strong> SK hynix, Texas State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12036" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12036</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a hardware-software co-designed framework for Sparse General Matrix-Matrix Multiplication (SpGEMM) on GPUs, featuring a novel near-memory processing technique called Acceleration of Indirect Memory Access (AIA). The method demonstrates significant performance improvements over state-of-the-art software libraries like cuSPARSE, particularly for graph analytics and Graph Neural Network training workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, heterogeneous environments, scheduling algorithm, multi-level search, successive halving, Proximal Policy Optimization (PPO)]</li>
<li class=""><strong>authors:</strong> Yongjun He, Shuai Zhang, Jiading Gai, Xiyuan Zhang, Boran Han, Bernie Wang, Huzefa Rangwala, George Karypis</li>
<li class=""><strong>institution:</strong> ETH Zurich, Amazon Web Services (AWS)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12476" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12476</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents HetRL, a distributed system for efficient reinforcement learning (RL) training of large language models (LLMs) in environments with heterogeneous GPUs and networks. It formulates the scheduling problem as a constrained joint optimization and introduces a novel algorithm using multi-level search and successive halving. The evaluation shows HetRL achieves up to 9.17x higher throughput than state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [bottleneck-aware tensor parallelism, low-rank bottleneck architectures, online-RMSNorm, linear layer grouping, low-rank activation checkpointing, 3D parallelism]</li>
<li class=""><strong>authors:</strong> Zhengyang Wang, Ziyue Liu, Ruijie Zhang, Avinash Maurya, Paul Hovland, Bogdan Nicolae, Franck Cappello, Zheng Zhang</li>
<li class=""><strong>institution:</strong> University of California, Santa Barbara, Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12131" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12131</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes BOOST, a training framework optimized for low-rank bottleneck architectures in large language models. It introduces bottleneck-aware tensor parallelism and other optimizations to reduce communication overhead and improve GPU utilization. The framework achieves significant speedup over both full-rank baselines and low-rank models with standard parallelism.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Low-Rank Adaptation (LoRA), dynamic rank adaptation, NUMA-aware scheduling, embedding tables (EMTs), delta-update, parameter synchronization]</li>
<li class=""><strong>authors:</strong> Wenjun Yu, Sitian Chen, Cheng Chen, Amelie Chi Zhou</li>
<li class=""><strong>institution:</strong> Hong Kong Baptist University, ByteDance</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12295" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12295</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces LiveUpdate, a system that co-locates Low-Rank Adaptation (LoRA) trainers within inference nodes to eliminate inter-cluster synchronization for Deep Learning Recommendation Models. It uses dynamic rank adaptation and NUMA-aware resource scheduling to minimize memory overhead and latency impact. LiveUpdate reduces update costs by 2x and improves accuracy compared to delta-update baselines, enabling near-zero-overhead model freshness.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] MVP-ORAM: a Wait-free Concurrent ORAM for Confidential BFT Storage</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [secure storage systems], [Oblivious RAM, wait-free concurrency, Byzantine fault tolerance, secret sharing, access pattern hiding]</li>
<li class=""><strong>authors:</strong> Robin Vassantlal, Hasan Heydari, Bernardo Ferreira, Alysson Bessani</li>
<li class=""><strong>institution:</strong> LASIGE, Faculdade de Ciências, Universidade de Lisboa</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12006" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12006</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents MVP-ORAM, a wait-free concurrent Oblivious RAM protocol that allows clients to perform operations independently without locks or trusted proxies, merging conflicting updates on the fly. It introduces a weaker, practical notion of obliviousness for skewed workloads and demonstrates that the protocol can be integrated into confidential Byzantine fault-tolerant data stores. The prototype implementation shows it can process hundreds of 4KB accesses per second in cloud environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] Ethical Risk Analysis of L2 Rollups</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain], [layer 2 rollups, ethical risk analysis, governance, upgrade timing, exit windows, proposer liveness, forced inclusion, data availability]</li>
<li class=""><strong>authors:</strong> Georgy Ishmaev, Emmanuelle Anceaume, Davide Frey, François Taïani</li>
<li class=""><strong>institution:</strong> Univ Rennes, Inria, CNRS, IRISA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12732" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12732</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper adapts Ethical Risk Analysis to evaluate Layer 2 rollup architectures, using a role-based taxonomy and empirical data from 129 projects and incident reports. It finds widespread ethical hazards, such as instant upgrades without exit windows and proposer controls that can freeze withdrawals, and provides ethically grounded mitigation suggestions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [sketched random matrix theory, marchenko-pastur law, frequent directions sketching, byzantine-robust aggregation, decentralized federated learning, blockchain]</li>
<li class=""><strong>authors:</strong> Animesh Mishra</li>
<li class=""><strong>institution:</strong> Department of Computer Science &amp; Engineering (implied from author email domain: snu.edu.in, likely Shiv Nadar University)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12617" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12617</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Spectral Sentinel, a Byzantine-robust decentralized federated learning framework that detects malicious clients by analyzing anomalies in the eigenspectra of gradient covariances using sketched random matrix theory. It achieves scalable detection for large models with provably optimal convergence and demonstrates practical deployment on blockchain networks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] Fine-Grained Energy Prediction For Parallellized LLM Inference With PIE-P</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [energy prediction, multi-GPU inference, tensor parallelism, pipeline parallelism, data parallelism, inter-GPU communication modeling]</li>
<li class=""><strong>authors:</strong> Anurag Dutt, Young Won Choi, Avirup Sil, Anshul Gandhi, Aruna Balasubramanian, Niranjan Balasubramanian</li>
<li class=""><strong>institution:</strong> Stony Brook University, IBM Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12801" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12801</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces PIE-P, a fine-grained energy prediction framework for multi-GPU LLM inference that addresses challenges like non-deterministic communication and parallelization overheads. It uses precise sampling and detailed modeling of inter-GPU communication to predict energy consumption across tensor, pipeline, and data parallelism strategies. The evaluation shows that PIE-P provides accurate predictions and significantly outperforms existing baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] SPARK: Igniting Communication-Efficient Decentralized Learning via Stage-wise Projected NTK and Accelerated Regularization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [decentralized federated learning, neural tangent kernel, random projection, stage-wise annealed distillation, Nesterov momentum, Jacobian compression]</li>
<li class=""><strong>authors:</strong> Li Xia</li>
<li class=""><strong>institution:</strong> Minzu University of China</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12737" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12737</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SPARK, a method for communication-efficient decentralized federated learning that combines random projection to compress Jacobian matrices, stage-wise annealed distillation to counteract compression noise, and Nesterov momentum for acceleration. It achieves a 98.7% reduction in communication overhead while maintaining convergence speed and superior accuracy, enabling practical deployment in bandwidth-limited edge environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] PROSERVE: Unified Multi-Priority Request Scheduling for LLM Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [request scheduling, service gain maximization, two-tier scheduling, SlideBatching, GoRouting, SLO attainment]</li>
<li class=""><strong>authors:</strong> Weizhe Huang, Tao Peng, Tongxuan Liu, Donghe Jin, Xianzhe Dong, Ke Zhang</li>
<li class=""><strong>institution:</strong> JD.com, USTC</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12928" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12928</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes PROSERVE, a two-tier scheduling framework for LLM serving that maximizes service gain by jointly optimizing for SLO attainment and client priorities. It uses SlideBatching for dynamic batch formation and GoRouting for gain-oriented request dispatching across distributed instances. Evaluations show it improves system gain by up to 35% and SLO attainment by up to 52% over baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] Towards Secure Decentralized Applications and Consensus Protocols in Blockchains (on Selfish Mining, Undercutting Attacks, DAG-Based Blockchains, E-Voting, Cryptocurrency Wallets, Secure-Logging, and CBDC)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain security], [selfish mining, undercutting attacks, DAG-based blockchains, e-voting, cryptocurrency wallets, secure-logging, CBDC]</li>
<li class=""><strong>authors:</strong> Ivan Homoliak</li>
<li class=""><strong>institution:</strong> Brno University of Technology</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.13213" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.13213</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This thesis proposes a holistic security reference architecture for analyzing vulnerabilities in blockchains and decentralized applications. It contributes methods for securing consensus protocols, wallets, e-voting, and logging, and introduces an interoperability protocol for central bank digital currencies. The main conclusion is that a comprehensive, standardized approach is needed to address the complex security challenges in full-stack decentralized systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [kernel fusion, distributed shared memory, inter-core connection, dataflow analysis, cost modeling]</li>
<li class=""><strong>authors:</strong> Ziyu Huang, Yangjie Zhou, Zihan Liu, Xinhao Luo, Yijia Diao, Minyi Guo, Jidong Zhai, Yu Feng, Chen Zhang, Anbang Wu, Jingwen Leng</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University, Shanghai Qi Zhi Institute, National University of Singapore, Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12949" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12949</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlashFuser is a compiler framework that expands kernel fusion for memory-bound deep learning workloads by utilizing the inter-core Distributed Shared Memory (DSM) on modern GPUs. It introduces a DSM communication abstraction, a dataflow analyzer, and a unified search engine to optimize execution plans. Evaluation on an NVIDIA H100 shows it reduces memory access by 58% and achieves significant kernel speedups over existing libraries and compilers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] Toward Self-Healing Networks-on-Chip: RL-Driven Routing in 2D Torus Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [reinforcement learning, adaptive routing, 2D torus, networks-on-chip, packet delivery ratio, fault-adaptive score]</li>
<li class=""><strong>authors:</strong> Mohammad Walid Charrwi, Zaid Hussain</li>
<li class=""><strong>institution:</strong> Kuwait University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.13096" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.13096</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a reinforcement learning (RL) based routing strategy for 2D torus Networks-on-Chip, where each router acts as an agent learning to forward packets based on network state. Compared to a traditional adaptive routing baseline, the RL method achieves significantly higher throughput and maintains better packet delivery under increasing node faults by exploiting path diversity. The results demonstrate that RL-driven routing offers superior throughput and fault resilience for torus NoC architectures.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] Temporal parallelisation of continuous-time maximum-a-posteriori trajectory estimation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [ai], [optimal control and state estimation], [parallel-in-time, maximum-a-posteriori, Onsager-Machlup functional, associative scan, Kalman-Bucy filter, Rauch-Tung-Striebel smoother]</li>
<li class=""><strong>authors:</strong> Hassan Razavi, Ángel F. García-Fernández, Simo Särkkä</li>
<li class=""><strong>institution:</strong> Aalto University, Universidad Polit´ecnica de Madrid</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.13319" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.13319</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a parallel-in-time method for computing continuous-time MAP trajectory estimates by reformulating the problem as an optimal control problem using the Onsager-Machlup functional. The solution leverages parallel associative scan algorithms, extending to parallel Kalman-Bucy filters and smoothers for linear Gaussian cases and nonlinear models via Taylor expansions. GPU experiments demonstrate the framework achieves significant computational speedup while maintaining the accuracy of sequential algorithms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] SPARS: A Reinforcement Learning-Enabled Simulator for Power Management in HPC Job Scheduling</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [reinforcement learning, discrete-event simulation, job scheduling, power management, energy efficiency, EASY Backfilling]</li>
<li class=""><strong>authors:</strong> Muhammad Alfian Amrizal, Raka Satya Prasasta, Santana Yuda Pradata, Kadek Gemilang Santiyuda, Reza Pulungan, Hiroyuki Takizawa</li>
<li class=""><strong>institution:</strong> Universitas Gadjah Mada, Universitas Ahmad Dahlan, Institut Bisnis dan Teknologi Indonesia, National Taiwan University of Science and Technology, Tohoku University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.13268" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.13268</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces SPARS, a simulator that integrates reinforcement learning with discrete-event simulation to manage node power states in HPC job scheduling. It enables the evaluation of power-aware scheduling policies, showing a trade-off between energy savings and job performance. The tool is designed to be lightweight, reproducible, and extensible for developing sustainable HPC operations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] SIGMA: An AI-Empowered Training Stack on Early-Life Hardware</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [early-life hardware, fault-tolerance, distributed training, MoE, model training stability, accelerator utilization, training stack]</li>
<li class=""><strong>authors:</strong> Lei Qu, Lianhai Ren, Peng Cheng, Rui Gao, Ruizhe Wang, Tianyu Chen, Xiao Liu, Xingjian Zhang, Yeyun Gong, Yifan Xiong, Yucheng Ding, Yuting Jiang, Zhenghao Lin, Zhongxin Guo, Ziyue Yang</li>
<li class=""><strong>institution:</strong> Microsoft Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.13488" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.13488</a></li>
<li class=""><strong>Simple LLM Summary:</strong> SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI accelerators. It introduces the LUCIA TRAINING PLATFORM (LTP) for system reliability and the LUCIA TRAINING FRAMEWORK (LTF) for stable and efficient model training, successfully demonstrating its capability by training a 200B MoE model with high utilization and minimal stability incidents. The work establishes a robust and cost-effective benchmark for AI infrastructure on emerging hardware.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] TreeVQA: A Tree-Structured Execution Framework for Shot Reduction in Variational Quantum Algorithms</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [variational quantum algorithms, tree-structured execution, shot reduction, VQA wrapper, joint execution, progressive branching]</li>
<li class=""><strong>authors:</strong> Yuewen Hou, Dhanvi Bharadwaj, Gokul Subramanian Ravi</li>
<li class=""><strong>institution:</strong> University of Michigan</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.12068" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.12068</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes TreeVQA, a tree-structured execution framework that reduces the shot count in Variational Quantum Algorithms by initially executing tasks jointly and branching only when their quantum executions diverge. Evaluations show the framework achieves significant shot count reductions, averaging 25.9x and over 100x for large-scale problems, while maintaining the same target accuracy.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [GPU kernels], [tile-based architecture, automated deployment, intermediate representation, network on chip, collective primitives, GEMM]</li>
<li class=""><strong>authors:</strong> Aofeng Shen, Chi Zhang, Yakup Budanaz, Alexandru Calotoiu, Torsten Hoefler, Luca Benini</li>
<li class=""><strong>institution:</strong> ETH Zurich</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.13638" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.13638</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes &quot;Design in Tiles (DiT)&quot;, an automated framework for deploying General Matrix Multiplication (GEMM) on tile-based many-PE accelerators by connecting a deployment toolchain with a configurable executable model. It demonstrates that this approach achieves higher PE utilization than an expert-tuned library on an NVIDIA GH200, resulting in a 1.2-2.0x speedup across diverse matrix shapes.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] SEDULity: A Proof-of-Learning Framework for Distributed and Secure Blockchains with Efficient Useful Work</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [proof-of-learning, consensus mechanism, incentive mechanism, distributed training, blockchain security]</li>
<li class=""><strong>authors:</strong> Weihang Cao, Mustafa Doger, Sennur Ulukus</li>
<li class=""><strong>institution:</strong> University of Maryland, College Park</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.13666" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.13666</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SEDULity, a Proof-of-Learning framework that replaces the traditional Proof-of-Work puzzle in blockchains with the useful work of training machine learning models. The framework is designed to be secure, efficient, and fully distributed, and it includes an incentive mechanism to ensure honest participation. Theoretical analysis and simulations demonstrate that the system can efficiently train models while maintaining blockchain security.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] Janus: Disaggregating Attention and Experts for Scalable MoE Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [mixture-of-experts, disaggregated inference, adaptive communication, lightweight scheduler, fine-grained resource management, GPU kernel]</li>
<li class=""><strong>authors:</strong> Zhexiang Zhang, Ye Wang, Xiangyu Wang, Yumiao Zhao, Jingzhe Jiang, Qizhen Weng, Shaohuai Shi, Yin Chen, Minchen Yu</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Shenzhen; Institute of Artificial Intelligence (TeleAI), China Telecom; Shenzhen Loop Area Institute; Harbin Institute of Technology, Shenzhen</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.13525" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.13525</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces Janus, a scalable inference system for Mixture-of-Experts (MoE) models that disaggregates attention and expert modules onto separate GPU sub-clusters for independent management and scaling. Its key innovations include an adaptive two-phase communication scheme, a GPU-kernel-based scheduler for expert load balancing, and fine-grained resource management. The evaluation shows Janus achieves up to 3.9x higher per-GPU throughput than state-of-the-art systems while meeting latency requirements.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251216] astroCAMP: A Community Benchmark and Co-Design Framework for Sustainable SKA-Scale Radio Imaging</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance computing], [co-design, benchmarking, energy efficiency, carbon-aware computing, radio interferometric imaging, hardware-software co-design, multi-objective optimization]</li>
<li class=""><strong>authors:</strong> Denisa-Andreea Constantinescu, Rubén Rodríguez Álvarez, Jacques Morin, Etienne Orliac, Mickaël Dardaillon, Sunrise Wang, Hugo Miomandre, Miguel Peón-Quirós, Jean-François Nezan, David Atienza</li>
<li class=""><strong>institution:</strong> EPFL, Univ Rennes, INSA Rennes, CNRS, Univ Côte d’Azur, OCA</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.13591" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.13591</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces astroCAMP, a framework for hardware-software co-design and benchmarking to improve the energy efficiency and computational performance of radio-interferometric imaging pipelines for the SKA telescope. It provides standardized metrics, datasets, and a multi-objective formulation to explore trade-offs between scientific fidelity, performance, and sustainability. The authors conclude that such a framework is essential for achieving sustainable, high-performance computing within the SKA&#x27;s strict power and operational constraints.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 38</strong></p>
<ul>
<li class="">[arXiv251216] Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning <a href="https://arxiv.org/pdf/2512.11902" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach <a href="https://arxiv.org/pdf/2512.11944" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving <a href="https://arxiv.org/pdf/2512.11872" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Policy Gradient Algorithms for Age-of-Information Cost Minimization <a href="https://arxiv.org/pdf/2512.11990" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL <a href="https://arxiv.org/pdf/2512.11862" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy <a href="https://arxiv.org/pdf/2512.12230" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] ElasticVR: Elastic Task Computing in Multi-User Multi-Connectivity Wireless Virtual Reality (VR) Systems <a href="https://arxiv.org/pdf/2512.12366" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning <a href="https://arxiv.org/pdf/2512.12046" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Reinforcement Learning for Latent-Space Thinking in LLMs <a href="https://arxiv.org/pdf/2512.11816" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Learning to Extract Context for Context-Aware LLM Inference <a href="https://arxiv.org/pdf/2512.11986" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction <a href="https://arxiv.org/pdf/2512.11930" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents <a href="https://arxiv.org/pdf/2512.12548" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Coupled Variational Reinforcement Learning for Language Model General Reasoning <a href="https://arxiv.org/pdf/2512.12576" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning <a href="https://arxiv.org/pdf/2512.12690" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning <a href="https://arxiv.org/pdf/2512.12706" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Self-Motivated Growing Neural Network for Adaptive Architecture via Local Structural Plasticity <a href="https://arxiv.org/pdf/2512.12713" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Information-Consistent Language Model Recommendations through Group Relative Policy Optimization <a href="https://arxiv.org/pdf/2512.12858" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization <a href="https://arxiv.org/pdf/2512.12922" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning <a href="https://arxiv.org/pdf/2512.12987" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training <a href="https://arxiv.org/pdf/2512.13043" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Deep Q-Learning-Based Intelligent Scheduling for ETL Optimization in Heterogeneous Data Environments <a href="https://arxiv.org/pdf/2512.13060" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization <a href="https://arxiv.org/pdf/2512.13070" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations <a href="https://arxiv.org/pdf/2512.13093" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning <a href="https://arxiv.org/pdf/2512.13095" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning <a href="https://arxiv.org/pdf/2512.13106" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning <a href="https://arxiv.org/pdf/2512.13159" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] SACn: Soft Actor-Critic with n-step Returns <a href="https://arxiv.org/pdf/2512.13165" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection <a href="https://arxiv.org/pdf/2512.13240" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning <a href="https://arxiv.org/pdf/2512.13278" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration <a href="https://arxiv.org/pdf/2512.13293" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3) <a href="https://arxiv.org/pdf/2512.13356" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles <a href="https://arxiv.org/pdf/2512.13359" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Differentiable Evolutionary Reinforcement Learning <a href="https://arxiv.org/pdf/2512.13399" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph <a href="https://arxiv.org/pdf/2512.13510" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Memory in the Age of AI Agents <a href="https://arxiv.org/pdf/2512.13564" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Image Diffusion Preview with Consistency Solver <a href="https://arxiv.org/pdf/2512.13592" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models <a href="https://arxiv.org/pdf/2512.13607" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] A Scientific Reasoning Model for Organic Synthesis Procedure Generation <a href="https://arxiv.org/pdf/2512.13668" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 20</strong></p>
<ul>
<li class="">[arXiv251216] AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org <a href="https://arxiv.org/pdf/2512.11935" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Neural CDEs as Correctors for Learned Time Series Models <a href="https://arxiv.org/pdf/2512.12116" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater <a href="https://arxiv.org/pdf/2512.12142" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI <a href="https://arxiv.org/pdf/2512.11893" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept <a href="https://arxiv.org/pdf/2512.12365" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT <a href="https://arxiv.org/pdf/2512.11870" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving <a href="https://arxiv.org/pdf/2512.11920" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs <a href="https://arxiv.org/pdf/2512.11851" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model <a href="https://arxiv.org/pdf/2512.12545" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Torch Geometric Pool: the Pytorch library for pooling in Graph Neural Networks <a href="https://arxiv.org/pdf/2512.12642" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Intelligent Scientific Literature Explorer using Machine Learning (ISLE) <a href="https://arxiv.org/pdf/2512.12760" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] HaShiFlex: A High-Throughput Hardened Shifter DNN Accelerator with Fine-Tuning Flexibility <a href="https://arxiv.org/pdf/2512.12847" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Distillation of Discrete Diffusion by Exact Conditional Distribution Matching <a href="https://arxiv.org/pdf/2512.12889" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation <a href="https://arxiv.org/pdf/2512.12929" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision <a href="https://arxiv.org/pdf/2512.12930" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models <a href="https://arxiv.org/pdf/2512.13194" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles <a href="https://arxiv.org/pdf/2512.13359" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Image Diffusion Preview with Consistency Solver <a href="https://arxiv.org/pdf/2512.13592" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval <a href="https://arxiv.org/pdf/2512.12284" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251216] Efficient Level-Crossing Probability Calculation for Gaussian Process Modeled Data <a href="https://arxiv.org/pdf/2512.12442" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-17">2025-12-17<a href="#2025-12-17" class="hash-link" aria-label="Direct link to 2025-12-17" title="Direct link to 2025-12-17" translate="no">​</a></h2>
<p><strong>cs.DC total: 7</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251217] Improving Slow Transfer Predictions: Generative Methods Compared</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [class imbalance, data augmentation, oversampling, SMOTE, CTGAN, stratified sampling]</li>
<li class=""><strong>authors:</strong> Jacob Taegon Kim, Alex Sim, Kesheng Wu, Jinoh Kim</li>
<li class=""><strong>institution:</strong> University of California, Berkeley, Lawrence Berkeley National Laboratory, Texas A&amp;M University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14522" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14522</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper compares various data augmentation strategies, including traditional oversampling and generative techniques like CTGAN, to address class imbalance in predicting slow data transfers in scientific networks. It finds that while augmentation can improve performance, increasing the imbalance ratio does not lead to significant gains. The main conclusion is that even advanced generative methods do not substantially outperform simple stratified sampling for this task.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251217] Cornserve: Efficiently Serving Any-to-Any Multimodal Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [multi-modal inference], [computation graph, disaggregation, distributed runtime, planner, Any-to-Any models]</li>
<li class=""><strong>authors:</strong> Jeff J. Ma, Jae-Won Chung, Jisang Ahn, Yizhuo Liang, Akshay Jajoo, Myungjin Lee, Mosharaf Chowdhury</li>
<li class=""><strong>institution:</strong> University of Michigan, University of Southern California, Cisco Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14098" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14098</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper presents Cornserve, a serving system for Any-to-Any multimodal models. Its core method involves a planner that automatically generates optimized deployment plans by potentially disaggregating the model graph, and a distributed runtime to execute it. The main conclusion is that Cornserve efficiently handles heterogeneous model serving, significantly improving throughput and reducing latency compared to existing solutions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251217] Performance and Stability of Barrier Mode Parallel Systems with Heterogeneous and Redundant Jobs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [barrier execution mode, parallel task scheduling, stability analysis, performance bounds, Apache Spark]</li>
<li class=""><strong>authors:</strong> Brenton Walker, Markus Fidler</li>
<li class=""><strong>institution:</strong> Institute of Communications Technology, Leibniz Universität Hannover</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14445" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14445</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper analyzes the performance and stability penalties in parallel systems that use barrier synchronization, as in Apache Spark&#x27;s Barrier Execution Mode. It develops models for systems with heterogeneous jobs and redundant tasks, deriving stability bounds and validating them against simulations and real Spark benchmarks. The main conclusion is that barriers introduce idle periods and scheduling overhead, which degrade system performance and stability compared to asynchronous execution.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251217] A Hybrid Reactive-Proactive Auto-scaling Algorithm for SLA-Constrained Edge Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [auto-scaling, Kubernetes, LSTM, hybrid reactive-proactive algorithm, SLA compliance]</li>
<li class=""><strong>authors:</strong> Suhrid Gupta, Muhammed Tawfiqul Islam, Rajkumar Buyya</li>
<li class=""><strong>institution:</strong> The University of Melbourne</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14290" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14290</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a hybrid auto-scaling algorithm for edge computing that combines a machine learning-based proactive component using LSTM for demand prediction with a reactive component for immediate adjustments based on current resource utilization. The algorithm is integrated into Kubernetes and evaluated in an edge environment. The results show that the proposed solution reduces the SLA violation rate to 6%, significantly outperforming existing solutions which had violation rates up to 23%.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251217] Real-Time Service Subscription and Adaptive Offloading Control in Vehicular Edge Computing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [deadline-constrained task offloading, resource allocation, linear program rounding, local-ratio techniques, approximation algorithm, vehicular edge computing, simulator]</li>
<li class=""><strong>authors:</strong> Chuanchao Gao, Arvind Easwaran</li>
<li class=""><strong>institution:</strong> Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14002" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14002</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes SARound, an approximation algorithm based on linear program rounding and local-ratio techniques, to solve a deadline-constrained task offloading and resource allocation problem in Vehicular Edge Computing. It also develops an online service subscription framework and a simulator called VecSim. Experimental results show that SARound outperforms existing baselines and improves the best-known approximation ratio for the problem.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251217] Q-IRIS: The Evolution of the IRIS Task-Based Runtime to Enable Classical-Quantum Workflows</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [quantum-classical hybrid runtime], [task-based runtime, quantum intermediate representation (QIR), quantum circuit cutting, heterogeneous scheduling, XACC, IRIS]</li>
<li class=""><strong>authors:</strong> Narasinga Rao Miniskar, Mohammad Alaul Haque Monil, Elaine Wong, Vicente Leyton-Ortega, Jeffrey S. Vetter, Seth R. Johnson, Travis S. Humble</li>
<li class=""><strong>institution:</strong> Oak Ridge National Laboratory</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.13931" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.13931</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents Q-IRIS, a proof-of-concept hybrid execution framework that integrates the IRIS task-based runtime with the XACC quantum programming framework via QIR-EE to orchestrate classical and quantum workloads. It demonstrates the framework by asynchronously scheduling quantum tasks and using quantum circuit cutting to improve simulator throughput. The work concludes by outlining key challenges for scaling such hybrid runtimes, including coordinated scheduling and managing classical-quantum interactions.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251217] PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [structured pruning, ADMM, gradient compression, hierarchical communication, leader-follower model, buffer compaction]</li>
<li class=""><strong>authors:</strong> Alireza Olama, Andreas Lundell, Izzat El Hajj, Johan Lilius, Jerker Björkqvist</li>
<li class=""><strong>institution:</strong> Åbo Akademi University, American University of Beirut, CSC - IT Center for Science</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14628" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14628</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PruneX is a distributed training system that co-designs a hierarchical structured pruning algorithm (H-SADMM) with cluster hierarchy to enforce node-level sparsity, enabling compacted gradient communication. This reduces inter-node bandwidth usage by ~60% and achieves better strong scaling speedup (6.75x) compared to dense baselines and Top-K compression on a 64-GPU supercomputer.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 18</strong></p>
<ul>
<li class="">[arXiv251217] Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis <a href="https://arxiv.org/pdf/2512.14157" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement Learning in Ride-Hailing <a href="https://arxiv.org/pdf/2512.13727" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] Understanding and Improving Hyperbolic Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2512.14202" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] Context-Picker: Dynamic context selection using multi-stage reinforcement learning <a href="https://arxiv.org/pdf/2512.14465" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce <a href="https://arxiv.org/pdf/2512.13726" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving <a href="https://arxiv.org/pdf/2512.14044" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN <a href="https://arxiv.org/pdf/2512.13715" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] Adaptive digital twins for predictive decision-making: Online Bayesian learning of transition dynamics <a href="https://arxiv.org/pdf/2512.13919" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data <a href="https://arxiv.org/pdf/2512.14329" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees <a href="https://arxiv.org/pdf/2512.14069" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] A Threshold-Triggered Deep Q-Network-Based Framework for Self-Healing in Autonomic Software-Defined IIoT-Edge Networks <a href="https://arxiv.org/pdf/2512.14297" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] A First-Order Logic-Based Alternative to Reward Models in RLHF <a href="https://arxiv.org/pdf/2512.14100" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] Explainable reinforcement learning from human feedback to improve alignment <a href="https://arxiv.org/pdf/2512.13837" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] Group-Theoretic Reinforcement Learning of Dynamical Decoupling Sequences <a href="https://arxiv.org/pdf/2512.13890" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs <a href="https://arxiv.org/pdf/2512.14698" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach <a href="https://arxiv.org/pdf/2512.13714" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes <a href="https://arxiv.org/pdf/2512.14617" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model <a href="https://arxiv.org/pdf/2512.14031" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 9</strong></p>
<ul>
<li class="">[arXiv251217] Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms <a href="https://arxiv.org/pdf/2512.13978" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts <a href="https://arxiv.org/pdf/2512.14461" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees <a href="https://arxiv.org/pdf/2512.14069" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery <a href="https://arxiv.org/pdf/2512.13930" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes <a href="https://arxiv.org/pdf/2512.14092" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction <a href="https://arxiv.org/pdf/2512.13886" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records <a href="https://arxiv.org/pdf/2512.13700" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] Informing Acquisition Functions via Foundation Models for Molecular Discovery <a href="https://arxiv.org/pdf/2512.13935" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251217] SuperWing: a comprehensive transonic wing dataset for data-driven aerodynamic design <a href="https://arxiv.org/pdf/2512.14397" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-18">2025-12-18<a href="#2025-12-18" class="hash-link" aria-label="Direct link to 2025-12-18" title="Direct link to 2025-12-18" translate="no">​</a></h2>
<p><strong>cs.DC total: 7</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251218] Optimizing Sensor Node Localization for Achieving Sustainable Smart Agriculture System Connectivity</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [wireless sensor networks], [Gradient-Based Iteration with Lagrange, grid-based allocation, Bluetooth, PSO, FAHP, clustering]</li>
<li class=""><strong>authors:</strong> Mohamed Naeem</li>
<li class=""><strong>institution:</strong> Arab Academy for Science, Technology and Maritime Transport</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14971" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14971</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Gradient-Based Iteration with Lagrange method to optimize sensor node deployment in smart agriculture, aiming to maximize coverage and minimize the number of sensors, cost, and power consumption. The proposed method outperforms classic deterministic deployment and particle swarm optimization, achieving 98.5% wireless sensor coverage.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] LeaseGuard: Raft Leases Done Right</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems, consensus algorithms], [Raft, leader leases, TLA+, quorum checks, failover optimization]</li>
<li class=""><strong>authors:</strong> A. Jesse Jiryu Davis, Murat Demirbas, Lingzhi Deng</li>
<li class=""><strong>institution:</strong> MongoDB</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15659" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15659</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces LeaseGuard, a novel lease algorithm for Raft-based distributed databases that ensures consistent reads without the overhead of quorum checks. It leverages Raft election guarantees and includes optimizations to maximize availability during leader failover. The evaluation shows it reduces read latency to zero network roundtrips and significantly improves write throughput compared to traditional methods.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [Shapley-CMI, Conditional Mutual Information, Private Set Intersection, Vertical Federated Learning, feature valuation]</li>
<li class=""><strong>authors:</strong> Unai Laskurain, Aitor Aguirre-Ortuzar, Urko Zurutuza</li>
<li class=""><strong>institution:</strong> Mondragon Unibertsitatea</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.14767" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.14767</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces a privacy-preserving system for evaluating feature contributions in Vertical Federated Learning using Shapley-CMI and a Private Set Intersection server to securely compute permutations and intersections without sharing raw data. The method enables fair data valuation before model training and maintains data confidentiality. Initial experiments confirm the system&#x27;s correctness and privacy, demonstrating its viability for secure feature contribution estimation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Optimizing Bloom Filters for Modern GPU Architectures</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [data structures], [Bloom filter, GPU, CUDA, vectorization, thread cooperation, compute latency, cache optimization]</li>
<li class=""><strong>authors:</strong> Daniel Jünger, Kevin Kristensen, Yunsong Wang, Xiangyao Yu, Bertil Schmidt</li>
<li class=""><strong>institution:</strong> NVIDIA Corporation, University of Wisconsin-Madison, Johannes Gutenberg University Mainz</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15595" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15595</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper explores optimizing Bloom filter implementations for modern GPU architectures by focusing on vectorization, thread cooperation, and compute latency. The optimized design overcomes the traditional trade-off between speed and precision, achieving high throughput while maintaining accuracy. It outperforms the state-of-the-art by up to 15.4x for construction and 11.35x for lookups on a B200 GPU.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Dynamic Rebatching for Efficient Early-Exit Inference with DREX</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [dynamic rebatching, early-exit, copy-free buffer, SLA-aware scheduler, KV cache, state-copying]</li>
<li class=""><strong>authors:</strong> Xuting Liu, Daniel Alexander, Siva Kesava Reddy Kakarla, Behnaz Arzani, Vincent Liu</li>
<li class=""><strong>institution:</strong> University of Pennsylvania, Microsoft Research</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15705" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15705</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes DREX, a system that uses Dynamic Rebatching to reorganize requests at each early-exit point in LLMs, allowing tokens ready to exit to be processed immediately while others continue. It introduces optimizations like a copy-free rebatching buffer and an analytical scheduler to predict rebatching profitability. The evaluation shows DREX improves throughput by 2-12% while maintaining output quality and eliminating involuntary exits.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] Reexamining Paradigms of End-to-End Data Movement</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [high-performance data transfer], [WAN performance prediction, latency-emulation testbed, hardware-software co-design, TCP congestion control, edge-to-core data movement]</li>
<li class=""><strong>authors:</strong> Chin Fang, Timothy Stitt, Michael J. McManus, Toshio Moriya</li>
<li class=""><strong>institution:</strong> Not explicitly provided in the given text.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15028" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15028</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper reexamines end-to-end data movement by analyzing six common paradigms, including network and host-side factors, using a latency-emulation testbed and production measurements. It concludes that bottlenecks often lie outside the network core and that a holistic hardware-software co-design is essential for consistent performance across different bandwidths.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251218] LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm training], [8-bit training, activation checkpointing, offloading, copy-engine collectives, dynamic tensor-level scaling, ZeRO-1]</li>
<li class=""><strong>authors:</strong> Erik Schultheis, Dan Alistarh</li>
<li class=""><strong>institution:</strong> IST Austria</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15306" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15306</a></li>
<li class=""><strong>Simple LLM Summary:</strong> LLMQ is an end-to-end CUDA/C++ framework that enables efficient 8-bit pretraining and fine-tuning of medium-sized language models (3B-32B parameters) on affordable consumer GPUs. It achieves this through optimizations like activation checkpointing, memory offloading, and copy-engine based collectives to overcome the memory and communication bottlenecks of these devices. The system can train a 7B model on a single 16GB GPU and rivals the efficiency of production-scale systems on more expensive hardware.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 17</strong></p>
<ul>
<li class="">[arXiv251218] Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models <a href="https://arxiv.org/pdf/2512.15521" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes <a href="https://arxiv.org/pdf/2512.14991" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning <a href="https://arxiv.org/pdf/2512.14726" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour <a href="https://arxiv.org/pdf/2512.14713" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning <a href="https://arxiv.org/pdf/2512.15405" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Double Horizon Model-Based Policy Optimization <a href="https://arxiv.org/pdf/2512.15439" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments <a href="https://arxiv.org/pdf/2512.15430" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction <a href="https://arxiv.org/pdf/2512.15605" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse <a href="https://arxiv.org/pdf/2512.14879" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models <a href="https://arxiv.org/pdf/2512.15089" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Spectral Representation-based Reinforcement Learning <a href="https://arxiv.org/pdf/2512.15036" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis <a href="https://arxiv.org/pdf/2512.15295" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Automatic Reward Shaping from Multi-Objective Human Heuristics <a href="https://arxiv.org/pdf/2512.15120" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning <a href="https://arxiv.org/pdf/2512.15274" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning <a href="https://arxiv.org/pdf/2512.15662" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning <a href="https://arxiv.org/pdf/2512.15687" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] QoS-Aware Hierarchical Reinforcement Learning for Joint Link Selection and Trajectory Optimization in SAGIN-Supported UAV Mobility Management <a href="https://arxiv.org/pdf/2512.15119" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 8</strong></p>
<ul>
<li class="">[arXiv251218] Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models <a href="https://arxiv.org/pdf/2512.15521" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Photonics-Enhanced Graph Convolutional Networks <a href="https://arxiv.org/pdf/2512.15549" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision <a href="https://arxiv.org/pdf/2512.15489" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation Layer Proposal <a href="https://arxiv.org/pdf/2512.14873" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] PIP<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> Net: Physics-informed Partition Penalty Deep Operator Network <a href="https://arxiv.org/pdf/2512.15086" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] A Roadmap for Applying Graph Neural Networks to Numerical Data: Insights from Cementitious Materials <a href="https://arxiv.org/pdf/2512.14855" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Accelerating High-Throughput Catalyst Screening by Direct Generation of Equilibrium Adsorption Structures <a href="https://arxiv.org/pdf/2512.15228" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251218] Restless Multi-Process Multi-Armed Bandits with Applications to Self-Driving Microscopies <a href="https://arxiv.org/pdf/2512.14930" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-12-19">2025-12-19<a href="#2025-12-19" class="hash-link" aria-label="Direct link to 2025-12-19" title="Direct link to 2025-12-19" translate="no">​</a></h2>
<p><strong>cs.DC total: 19</strong></p>
<ul>
<li class="">
<p><strong>[arXiv251219] Optimizing Agentic Language Model Inference via Speculative Tool Calls</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [speculative tool calls, tool cache, prefix-caching, vLLM]</li>
<li class=""><strong>authors:</strong> Daniel Nichols, Prajwal Singhania, Charles Jekel, Abhinav Bhatele, Harshitha Menon</li>
<li class=""><strong>institution:</strong> Lawrence Livermore National Laboratory, University of Maryland</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15834" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15834</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces systems optimizations for language model inference, specifically using speculative tool calls and keeping sequences resident to minimize overhead when agents use external tools. The method leads to significant throughput improvements, such as hundreds of tokens per second, in multi-agent settings. The authors also propose a &quot;tool cache&quot; API to facilitate adoption of these optimizations by LM providers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [serverless computing], [cold-start, anti-patterns, static analysis, SCABENCH, INITSCOPE]</li>
<li class=""><strong>authors:</strong> Syed Salauddin Mohammad Tariq, Foyzul Hassan, Amiangshu Bosu, Probir Roy</li>
<li class=""><strong>institution:</strong> University of Michigan–Dearborn, Wayne State University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16066" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16066</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper studies cold-start latency in serverless systems as a developer-visible design problem, deriving taxonomies of anti-patterns from issue reports and introducing the INITSCOPE analysis framework. The results show that INITSCOPE significantly improves localization accuracy and reduces diagnostic effort compared to prior tools, advancing evidence-driven practices for cold-start mitigation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [vector quantization, memory planning, cache optimization, spline networks, holographic topology]</li>
<li class=""><strong>authors:</strong> Jeff Smith</li>
<li class=""><strong>institution:</strong> 2nd Set AI</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15742" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15742</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes SHARe-KAN, a framework that uses Gain-Shape-Bias Vector Quantization to compress Kolmogorov-Arnold Networks (KANs) by exploiting functional redundancy while preserving their dense, holographic topology. Combined with a hardware-aware compiler (LUTHAM) for static memory planning, the method drastically reduces runtime memory by 88x without accuracy loss, effectively decoupling the workload from DRAM bandwidth constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [Multi-Instance GPU (MIG), online scheduling, fragmentation-aware, load balancing, dynamic partitioning, job migration]</li>
<li class=""><strong>authors:</strong> Hsu-Tzu Ting, Jerry Chou, Ming-Hung Chen, I-Hsin Chung</li>
<li class=""><strong>institution:</strong> National Tsing Hua University, IBM T. J. Watson Research Center</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16099" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16099</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes an online scheduling framework for Multi-Instance GPUs (MIG) that integrates conditional load balancing, dynamic partitioning, and job migration to minimize resource contention and combat GPU fragmentation. The method dynamically adapts job placement and reorganizes GPU allocations. Experimental results show that applying all these techniques improves system makespan by up to 35%.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [Multipath Memory Access (MMA), PCIe bottleneck, GPU-host memory bandwidth, dynamic library injection, prefix cache, model switching, vLLM]</li>
<li class=""><strong>authors:</strong> Lingfeng Tang, Daoping Zhang, Junjie Chen, Peihao Huang, Feng Jin, Chengguang Xu, Yuxin Chen, Feiqiang Sun, Guo Chen</li>
<li class=""><strong>institution:</strong> Hunan University, Tencent</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16056" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16056</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Multipath Memory Access (MMA), a scheme that enables efficient multipath data transfer between GPU and host memory to overcome the PCIe bandwidth bottleneck in LLM services. It achieves a 4.62x speedup in transfer bandwidth and significantly reduces time-to-first-token and model-switching latency for LLM serving, requiring no code modification for deployment.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [staggered batch scheduling, load-aware global allocation, DP+EP, P/D-separated, time-to-first-token]</li>
<li class=""><strong>authors:</strong> Jian Tian, Shuailong Li, Yang Cao, Wenbo Cui, Minghan Zhu, Wenkang Wu, Jianming Zhang, Yanpeng Wang, Zhiwen Xiao, Zhenyu Hou, Dou Shen</li>
<li class=""><strong>institution:</strong> Baidu Inc.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16134" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16134</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Staggered Batch Scheduling (SBS), a method that buffers incoming requests to form optimal batches before dispatching them to a DP+EP inference cluster, eliminating internal queuing. Combined with a Load-Aware Global Allocation strategy, this approach significantly reduces Time-to-First-Token (TTFT) by 30-40% and improves throughput by 15-20% compared to immediate scheduling baselines.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [retrieval-augmented generation, loop transformation, static control part, feedback-based iterative mechanism, equivalence checking]</li>
<li class=""><strong>authors:</strong> Yijie Zhi, Yayu Cao, Jianhua Dai, Xiaoyang Han, Jingwen Pu, Qingran Wu, Sheng Cheng, Ming Cai</li>
<li class=""><strong>institution:</strong> Zhejiang University, Zhejiang Institute of Administration, Beijing ShenZhou Aerospace Software Technology Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15766" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15766</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes LOOPRAG, a framework that uses retrieval-augmented generation to guide large language models in performing loop transformation optimizations. It employs a loop-aware retrieval algorithm and a feedback-based iterative mechanism with testing to generate correct and efficient code. The method demonstrates significant speedups over both traditional compilers and base LLMs on standard benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Private Virtual Tree Networks for Secure Multi-Tenant Environments Based on the VIRGO Overlay Network</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems security], [hierarchical overlay network, public key cryptography, delegation certificates, VIRGO, private virtual tree]</li>
<li class=""><strong>authors:</strong> Lican Huang</li>
<li class=""><strong>institution:</strong> Hangzhou Domain Zones Technology Co., Ltd.</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.15915" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.15915</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes Private Virtual Tree Networks (PVTNs), a cryptographic extension to the VIRGO overlay network that enforces secure, private hierarchical structures for multi-tenant environments. It uses public key encryption for confidential join requests and manager-signed delegation certificates for authorization, treating public keys as organizational secrets. The work concludes that PVTNs achieve scalable, dynamic management with strong security guarantees without requiring a global public key infrastructure.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [fault-tolerance], [log-based rollback recovery, asynchronous barrier snapshotting, data lineage capture, non-blocking recovery, data parallelization]</li>
<li class=""><strong>authors:</strong> Eric Simon, Renato B. Hoffmann, Lucas Alf, Dalvan Griebler</li>
<li class=""><strong>institution:</strong> SAP Labs, Pontifical Catholic University of Rio Grande do Sul</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16038" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16038</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces LOG.io, a log-based protocol for unified rollback recovery and fine-grained data lineage capture in distributed data pipelines. It supports non-deterministic operators and non-blocking recovery, enabling independent operator recovery and dynamic scaling. The evaluation shows that LOG.io matches or outperforms the Asynchronous Barrier Snapshotting protocol in scenarios with stragglers and moderate throughput, while data lineage capture adds minimal overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [sparse attention, dynamic programming, anchor layers, head-aware top-k, tile-level operations]</li>
<li class=""><strong>authors:</strong> Dhruv Deshmukh, Saurabh Goyal, Nipun Kwatra, Ramachandran Ramjee</li>
<li class=""><strong>institution:</strong> Microsoft Research India</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16391" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16391</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Kascade is a training-free sparse attention method that speeds up long-context LLM inference by computing exact Top-k indices in selected anchor layers and reusing them in nearby layers, selected via a dynamic-programming algorithm. It achieves significant speedups over dense attention baselines while maintaining high accuracy on long-context benchmarks.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [multi-agent reinforcement learning, QMIX, reactive jamming, channel hopping, power control, Upper Confidence Bound]</li>
<li class=""><strong>authors:</strong> Bahman Abolhassani, Tugba Erpek, Kemal Davaslioglu, Yalin E. Sagduyu, Sastry Kompella</li>
<li class=""><strong>institution:</strong> Nexcepta</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16813" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16813</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a multi-agent reinforcement learning framework based on the QMIX algorithm to coordinate anti-jamming strategies in swarm networks. The method enables agents to jointly select transmission channels and power levels to counter an adaptive reactive jammer. Simulation results show that QMIX achieves near-optimal performance with higher throughput and lower jamming incidence compared to baseline policies.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Lotus: Optimizing Disaggregated Transactions with Disaggregated Locks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed transaction systems], [disaggregated memory, lock disaggregation, RDMA, lock-first transaction protocol, application-aware lock management, lock-rebuild-free recovery]</li>
<li class=""><strong>authors:</strong> Zhisheng Hu, Pengfei Zuo, Junliang Hu, Yizou Chen, Yingjia Wang, Ming-Chang Yang</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Huawei</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16136" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16136</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents Lotus, a distributed transaction system for disaggregated memory that addresses RNIC bottlenecks by disaggregating locks from data and managing them on compute nodes. Its key techniques include an application-aware lock management mechanism, a lock-first transaction protocol, and a lock-rebuild-free recovery mechanism. Experiments show Lotus improves throughput by up to 2.1x and reduces latency by up to 49.4% compared to state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [distributed systems], [disaggregated memory, key-value store, index proxying, rank-aware hotness detection, two-level CN memory optimization, RPC-aggregated cache management]</li>
<li class=""><strong>authors:</strong> Zhisheng Hu, Jiacheng Shen, Ming-Chang Yang</li>
<li class=""><strong>institution:</strong> The Chinese University of Hong Kong, Duke Kunshan University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16148" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16148</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlexKV is a memory-disaggregated key-value store that dynamically offloads indexes to compute nodes to accelerate processing and improve cache efficiency. It addresses challenges like load imbalance and cache coherence with a hotness detection algorithm, memory optimization scheme, and RPC-aggregated cache mechanism. Experiments show it significantly improves throughput and reduces latency compared to state-of-the-art systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [llm inference], [CPU-GPU collaborative inference, expert caching, offloading, Mixture of Experts (MoE)]</li>
<li class=""><strong>authors:</strong> En-Ming Huang, Li-Shang Lin, Chun-Yi Lee</li>
<li class=""><strong>institution:</strong> National Taiwan University, National Tsing Hua University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16473" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16473</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a CPU-GPU collaborative inference framework for MoE-based LLMs that uses an expert caching mechanism on the GPU to reduce data transfer latency. It offloads computations for cache misses to the CPU, utilizing multithreading for efficiency. The evaluation shows performance improvements, demonstrating the framework&#x27;s potential for maximizing hardware utilization in memory-limited, consumer-grade systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Efficient Bitcoin Meta-Protocol Transaction and Data Discovery Through nLockTime Field Repurposing</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [blockchain protocol], [nLockTime, meta-protocol, transaction discovery, OP RETURN, witness data, indexing, Bitcoin]</li>
<li class=""><strong>authors:</strong> Nikodem Tomczak</li>
<li class=""><strong>institution:</strong> Thulge Labs</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16683" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16683</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper introduces the Lockchain Protocol, which repurposes Bitcoin&#x27;s nLockTime field as a compact metadata header to enable efficient transaction discovery for meta-protocols. This approach allows indexers to filter transactions by inspecting a fixed-size header first, reducing the need to parse large payloads like OP RETURN outputs or witness data. The main conclusion is that this method improves scalability and reduces centralization risks in indexing without requiring new on-chain storage mechanisms.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [federated learning, collagen VI-related dystrophies, immunofluorescence microscopy, classification, privacy-preserving]</li>
<li class=""><strong>authors:</strong> Astrid Brull, Sara Aguti, Véronique Bolduc, Ying Hu, Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del-Rio, Oleksii Sliusarenko, Haiyan Zhou, Francesco Muntoni, Carsten G. Bönnemann, Xabi Uribe-Etxebarria</li>
<li class=""><strong>institution:</strong> National Institutes of Health, University College London, Sherpa.ai</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16876" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16876</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper applies Federated Learning (FL) using the Sherpa.ai platform to train a machine learning model for diagnosing collagen VI-related dystrophies from decentralized immunofluorescence image datasets. The FL approach, which keeps patient data local, achieved a higher F1-score (0.82) than models trained on single-institution data, demonstrating improved diagnostic accuracy and generalizability for rare diseases.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Twinning for Space-Air-Ground-Sea Integrated Networks: Beyond Conventional Digital Twin Towards Goal-Oriented Semantic Twin</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [others], [goal-oriented semantic twin, digital twin, semantic technologies, knowledge-based semantics, data-driven semantics, perception-communication-computing-actuation]</li>
<li class=""><strong>authors:</strong> Yifei Qiu, Tianle Liao, Xin Jin, Shaohua Wu, Dusit Niyato, Qinyu Zhang</li>
<li class=""><strong>institution:</strong> Harbin Institute of Technology (Shenzhen), Nanyang Technological University</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16058" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16058</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Goal-Oriented Semantic Twin (GOST) framework for Space-Air-Ground-Sea Integrated Networks, prioritizing utility over high-fidelity modeling by leveraging semantic technologies and goal-oriented principles to create lightweight, task-specific representations. It concludes that GOST significantly outperforms conventional digital twins in tasks like collaborative tracking by improving timeliness and reducing computational overhead.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint</strong></p>
<ul>
<li class=""><strong>tags:</strong> [sys], [edge computing optimization], [Mixed Integer Linear Programming (MILP), heuristic algorithm (M-ESU/H), task offloading, server deployment, capacity upgrade, budget constraint, delay requirement]</li>
<li class=""><strong>authors:</strong> Endar Suprih Wihidayat, Sieteng Soh, Kwan-Wu Chin, Duc-son Pham</li>
<li class=""><strong>institution:</strong> Sebelas Maret University, Curtin University, University of Wollongong</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16792" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16792</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a Multi-stage Edge Server Upgrade (M-ESU) framework for network planning, which uses a Mixed Integer Linear Programming (MILP) model for optimal solutions in small networks and an efficient heuristic algorithm (M-ESU/H) for large-scale networks. The main conclusion is that M-ESU/H performs nearly optimally (within 1.25%) and significantly outperforms alternative heuristics by up to 21.57% in task satisfaction, demonstrating its scalability and practical value for long-term MEC system upgrades under budget constraints.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv251219] AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [federated cloud platform, service catalogue, machine learning lifecycle, interactive development environments, GPU resources, federated learning, traceability, reproducibility, cloud continuum]</li>
<li class=""><strong>authors:</strong> Ignacio Heredia, Álvaro López García, Germán Moltó, Amanda Calatrava, Valentin Kozlov, Alessandro Costantini, Viet Tran, Mario David, Daniel San Martín, Marcin Płóciennik, Marta Obregón Ruiz, Saúl Fernandez, Judith Sáinz-Pardo Díaz, Miguel Caballer, Caterina Alarcón Marín, Stefan Dlugolinsky, Martin Šeleng, Lisana Berberi, Khadijeh Alibabaei, Borja Esteban Sanchis, Pedro Castro, Giacinto Donvito, Diego Aguirre, Sergio Langarita, Vicente Rodriguez, Leonhard Duda, Andrés Heredia Canales, Susana Rebolledo Ruiz, João Machado, Giang Nguyen, Fernando Aguilar Gómez, Jaime Díez</li>
<li class=""><strong>institution:</strong> Instituto de Física de Cantabria (IFCA), Instituto de Instrumentación para Imagen Molecular (I3M), Institute of Informatics, Slovak Academy of Sciences (IISAS), Istituto Nazionale di Fisica Nucleare (INFN), Karlsruher Institut für Technologie, Poznańskie Centrum Superkomputerowo Sieciowe, Centro Nacional de Computação Avançada (CNCA)</li>
<li class=""><strong>link:</strong> <a href="https://arxiv.org/pdf/2512.16455" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/pdf/2512.16455</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents AI4EOSC, a federated cloud platform designed to support the full machine learning lifecycle for scientific research. The platform provides integrated access to distributed e-Infrastructures, offering services for model development, training, and deployment with a focus on reproducibility and customization. The main conclusion is that such a platform lowers the adoption barrier for AI in scientific communities by delivering a consistent, transparent, and comprehensive user experience.</li>
</ul>
</li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;reinforcement learning&quot; total: 23</strong></p>
<ul>
<li class="">[arXiv251219] Techno-economic optimization of a heat-pipe microreactor, part I: theory and cost optimization <a href="https://arxiv.org/pdf/2512.16032" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning <a href="https://arxiv.org/pdf/2512.15943" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models <a href="https://arxiv.org/pdf/2512.15973" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] DSO: Direct Steering Optimization for Bias Mitigation <a href="https://arxiv.org/pdf/2512.15926" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] INTELLECT-3: Technical Report <a href="https://arxiv.org/pdf/2512.16144" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation <a href="https://arxiv.org/pdf/2512.16145" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Hypernetworks That Evolve Themselves <a href="https://arxiv.org/pdf/2512.16406" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning <a href="https://arxiv.org/pdf/2512.16408" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm <a href="https://arxiv.org/pdf/2512.16444" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment <a href="https://arxiv.org/pdf/2512.16484" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] ParamExplorer: A framework for exploring parameters in generative art <a href="https://arxiv.org/pdf/2512.16529" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game <a href="https://arxiv.org/pdf/2512.16626" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Implementing a Sharia Chatbot as a Consultation Medium for Questions About Islam <a href="https://arxiv.org/pdf/2512.16644" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Olaf: Bringing an Animated Character to Life in the Physical World <a href="https://arxiv.org/pdf/2512.16705" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Meta-RL Induces Exploration in Language Agents <a href="https://arxiv.org/pdf/2512.16848" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning <a href="https://arxiv.org/pdf/2512.16861" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning <a href="https://arxiv.org/pdf/2512.16911" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward <a href="https://arxiv.org/pdf/2512.16912" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning <a href="https://arxiv.org/pdf/2512.16917" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification <a href="https://arxiv.org/pdf/2512.16921" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] The Red Queen&#x27;s Trap: Limits of Deep Evolution in High-Frequency Trading <a href="https://arxiv.org/pdf/2512.15732" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming <a href="https://arxiv.org/pdf/2512.15735" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Non-Asymptotic Global Convergence of PPO-Clip <a href="https://arxiv.org/pdf/2512.16565" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul>
<p><strong>cs.AI/cs.LG contains &quot;accelerate&quot; total: 13</strong></p>
<ul>
<li class="">[arXiv251219] TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration <a href="https://arxiv.org/pdf/2512.15773" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Surrogate Neural Architecture Codesign Package (SNAC-Pack) <a href="https://arxiv.org/pdf/2512.15998" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times <a href="https://arxiv.org/pdf/2512.16093" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Introduction to Symbolic Regression in the Physical Sciences <a href="https://arxiv.org/pdf/2512.15920" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Science Consultant Agent <a href="https://arxiv.org/pdf/2512.16171" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Pretrained Battery Transformer (PBT): A battery life prediction foundation model <a href="https://arxiv.org/pdf/2512.16334" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Multi-Fidelity Delayed Acceptance: hierarchical MCMC sampling for Bayesian inverse problems combining multiple solvers through deep neural networks <a href="https://arxiv.org/pdf/2512.16430" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] XTC, A Research Platform for Optimizing AI Workload Operators <a href="https://arxiv.org/pdf/2512.16512" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling <a href="https://arxiv.org/pdf/2512.16581" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference <a href="https://arxiv.org/pdf/2512.16843" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge <a href="https://arxiv.org/pdf/2512.15729" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Bayesian Modeling for Uncertainty Management in Financial Risk Forecasting and Compliance <a href="https://arxiv.org/pdf/2512.15739" target="_blank" rel="noopener noreferrer" class="">link</a></li>
<li class="">[arXiv251219] Riemannian Stochastic Interpolants for Amorphous Particle Systems <a href="https://arxiv.org/pdf/2512.16607" target="_blank" rel="noopener noreferrer" class="">link</a></li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2026-02-05T03:26:14.000Z" itemprop="dateModified">Feb 5, 2026</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20251208-20251214"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251208-20251214</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20251222-20251228"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251222-20251228</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-12-15" class="table-of-contents__link toc-highlight">2025-12-15</a></li><li><a href="#2025-12-16" class="table-of-contents__link toc-highlight">2025-12-16</a></li><li><a href="#2025-12-17" class="table-of-contents__link toc-highlight">2025-12-17</a></li><li><a href="#2025-12-18" class="table-of-contents__link toc-highlight">2025-12-18</a></li><li><a href="#2025-12-19" class="table-of-contents__link toc-highlight">2025-12-19</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>