<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-daily/20251013-20251019" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">20251013-20251019 | DarkKnight Note</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://darkknight996.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://darkknight996.github.io/daily/20251013-20251019"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="20251013-20251019 | DarkKnight Note"><meta data-rh="true" name="description" content="2025-10-13"><meta data-rh="true" property="og:description" content="2025-10-13"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://darkknight996.github.io/daily/20251013-20251019"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251013-20251019" hreflang="en"><link data-rh="true" rel="alternate" href="https://darkknight996.github.io/daily/20251013-20251019" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Daily","item":"https://darkknight996.github.io/category/daily"},{"@type":"ListItem","position":2,"name":"20251013-20251019","item":"https://darkknight996.github.io/daily/20251013-20251019"}]}</script><link rel="stylesheet" href="/assets/css/styles.2a9d613c.css">
<script src="/assets/js/runtime~main.df888d8c.js" defer="defer"></script>
<script src="/assets/js/main.928bd3f8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/favicon.ico"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/favicon.ico" alt="DarkKnight Note" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Dark Knight Note</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/DarkKnight996" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/intro"><span title="Introduction" class="linkLabel_WmDU">Introduction</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/category/daily"><span title="Daily" class="categoryLinkLabel_W154">Daily</span></a><button aria-label="Collapse sidebar category &#x27;Daily&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250901-20250907"><span title="20250901-20250907" class="linkLabel_WmDU">20250901-20250907</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250908-20250914"><span title="20250908-20250914" class="linkLabel_WmDU">20250908-20250914</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250915-20250921"><span title="20250915-20250921" class="linkLabel_WmDU">20250915-20250921</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250922-20250928"><span title="20250922-20250928" class="linkLabel_WmDU">20250922-20250928</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20250929-20251005"><span title="20250929-20251005" class="linkLabel_WmDU">20250929-20251005</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251006-20251012"><span title="20251006-20251012" class="linkLabel_WmDU">20251006-20251012</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/daily/20251013-20251019"><span title="20251013-20251019" class="linkLabel_WmDU">20251013-20251019</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251020-20251026"><span title="20251020-20251026" class="linkLabel_WmDU">20251020-20251026</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/daily/20251027-20251102"><span title="20251027-20251102" class="linkLabel_WmDU">20251027-20251102</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/category/paper"><span title="Paper" class="categoryLinkLabel_W154">Paper</span></a><button aria-label="Expand sidebar category &#x27;Paper&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/category/daily"><span>Daily</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">20251013-20251019</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>20251013-20251019</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-13">2025-10-13<a href="#2025-10-13" class="hash-link" aria-label="Direct link to 2025-10-13" title="Direct link to 2025-10-13" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] A Decentralized Microservice Scheduling Approach Using Service Mesh in
Cloud-Edge Systems</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [microservice scheduling, service mesh, decentralized systems, cloud-edge computing, sidecar proxy]</li>
<li class=""><strong>authors:</strong> Yangyang Wen, Paul Townend, Per-Olov Östberg, Abel Souza, Clément Courageux-Sudan</li>
<li class=""><strong>institution:</strong> Umeå University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.11189v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.11189v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes embedding lightweight scheduling logic into service mesh sidecar proxies to enable decentralized microservice scheduling in cloud-edge systems. The approach eliminates centralized control by making scheduling decisions locally at each sidecar. Initial results demonstrate improved scalability potential with reduced latency and response times under varying workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Improving AI Efficiency in Data Centres by Power Dynamic Response</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [AI data centres, power management, dynamic power response, energy efficiency, sustainability]</li>
<li class=""><strong>authors:</strong> Andrea Marinoni, Sai Shivareddy, Pietro Lio&#x27;, Weisi Lin, Erik Cambria, Clare Grey</li>
<li class=""><strong>institution:</strong> University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.11119v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.11119v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a dynamic power management approach for AI data centers by making input power as flexible as computing power. It evaluates passive and active devices using global power trend data, showing improvements in computational gain and energy efficiency. The method reduces capital expenditure and enhances environmental sustainability for AI hyperscalers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] An Explorative Study on Distributed Computing Techniques in Training and
Inference of Large Language Models</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference, LLM training], [distributed computing, model parallelism, metaheuristics, NSGA-II, attention mechanism]</li>
<li class=""><strong>authors:</strong> Sheikh Azizul Hakim, Saem Hasan</li>
<li class=""><strong>institution:</strong> Bangladesh University of Engineering and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.11211v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.11211v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper explores distributed computing techniques for large language models from two perspectives: implementing metaheuristics-based modifications to run LLMs on consumer hardware, and conducting comparative analysis of state-of-the-art LLM serving techniques. The study addresses challenges in both training and inference phases of large-scale language models. The research demonstrates how distributed approaches can democratize access to LLMs while optimizing serving performance.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline
Refactoring in Fragmented Serverless Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [dynamic pipeline reconfiguration, serverless computing, resource fragmentation, pipeline parallelism, GPU efficiency]</li>
<li class=""><strong>authors:</strong> Yanying Lin, Shijie Peng, Chengzhi Lu, Chengzhong Xu, Kejiang Ye</li>
<li class=""><strong>institution:</strong> Shenzhen Institutes of Advanced Technology, CAS</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.11938v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.11938v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FlexPipe introduces dynamic pipeline refactoring during runtime by decomposing LLMs into fine-grained stages and adjusting pipeline granularity based on real-time request patterns. It achieves significant improvements with up to 8.5× better resource efficiency and 38.3% lower latency compared to state-of-the-art systems, reducing GPU reservation requirements from 75% to 30% of peak capacity.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-14">2025-10-14<a href="#2025-10-14" class="hash-link" aria-label="Direct link to 2025-10-14" title="Direct link to 2025-10-14" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] GPU-Accelerated Algorithms for Process Mapping</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU-accelerated algorithms, process mapping, graph partitioning, hierarchical multisection, multilevel partitioning]</li>
<li class=""><strong>authors:</strong> Petr Samoldekin, Christian Schulz, Henning Woydt</li>
<li class=""><strong>institution:</strong> Heidelberg University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.12196v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.12196v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes two GPU-accelerated algorithms for process mapping: one using hierarchical multisection and another integrating mapping into multilevel graph partitioning. Both methods achieve significant speedups exceeding 300x compared to CPU-based algorithms, with the first maintaining competitive communication costs and the second offering even faster performance at reduced solution quality. These represent the first GPU-based approaches for process mapping in supercomputing environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Laminar: A Scalable Asynchronous RL Post-Training Framework</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [post-training], [reinforcement learning, large language models, asynchronous training, scalability, GPU utilization, trajectory generation, parameter synchronization]</li>
<li class=""><strong>authors:</strong> Guangming Sheng, Yuxuan Tong, Borui Wan, Wang Zhang, Chaobo Jia, Xibin Wu, Yuqi Wu, Xiang Li, Chi Zhang, Yanghua Peng, Haibin Lin, Xin Liu, Chuan Wu</li>
<li class=""><strong>institution:</strong> The University of Hong Kong, ByteDance</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.12633v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.12633v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Laminar introduces a fully decoupled architecture with relay workers for asynchronous weight synchronization and dynamic trajectory repacking to address GPU underutilization in RL post-training. The system achieves up to 5.48× throughput improvement and faster convergence compared to state-of-the-art frameworks. This demonstrates significant scalability enhancements for large-scale RL training on GPU clusters.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and
Framework: Migration Strategies, Performance Optimization and Analysis</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [model migration, performance optimization, hardware adaptation, memory optimization, parallelism]</li>
<li class=""><strong>authors:</strong> Yuze Sun, Wentao Luo, Yanfei Xiang, Jiancheng Pan, Jiahao Li, Quan Zhang, Xiaomeng Huang</li>
<li class=""><strong>institution:</strong> Tsinghua University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.17852v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.17852v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a framework for migrating atmospheric and oceanic AI models from PyTorch to MindSpore and optimizing them for Chinese chips. The migration preserves model accuracy while improving operational efficiency and reducing system dependencies. Chinese chips demonstrate viability as an alternative for scientific computing workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] nuGPR: GPU-Accelerated Gaussian Process Regression with Iterative
Algorithms and Low-Rank Approximations</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models training], [Gaussian Process Regression, GPU acceleration, iterative algorithms, low-rank approximations, numerical linear algebra, hyperparameter optimization]</li>
<li class=""><strong>authors:</strong> Ziqi Zhao, Vivek Sarin</li>
<li class=""><strong>institution:</strong> Texas A&amp;M University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.12128v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.12128v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes nuGPR, a GPU-accelerated Gaussian Process Regression framework that combines preconditioned conjugate gradient methods with low-rank approximations of covariance matrices. The approach uses numerical gradients for hyperparameter optimization and CUDA-based parallelization to significantly reduce computational costs. Experimental results show 2x faster training and 12x lower memory consumption compared to existing GPU implementations.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization
of Banded Matrices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU acceleration, bidiagonalization, banded matrices, SVD, Julia language, performance optimization]</li>
<li class=""><strong>authors:</strong> Evelyne Ringoot, Rabab Alomairy, Alan Edelman</li>
<li class=""><strong>institution:</strong> Massachusetts Institute of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.12705v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.12705v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a GPU-resident memory-aware algorithm for accelerating bidiagonalization of banded matrices, which is a key step in Singular Value Decomposition. The method adapts CPU-based bulge chasing algorithms for GPU throughput optimization using Julia&#x27;s hardware-agnostic abstractions. The GPU implementation outperforms CPU libraries by factors up to 100x for large matrices and breaks both memory and matrix bandwidth barriers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Metronome: Efficient Scheduling for Periodic Traffic Jobs with Network
and Priority Awareness</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [cloud native networks, periodic traffic patterns, distributed training, bandwidth allocation, multi-objective optimization]</li>
<li class=""><strong>authors:</strong> Hao Jiang, Meng Qin, Ruijie Kuai, Dandan Liang</li>
<li class=""><strong>institution:</strong> IEEE</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.12274v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.12274v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Metronome proposes a network-aware scheduling mechanism using time-division multiplexing and multi-objective optimization for periodic traffic jobs in cloud native networks. It dynamically allocates bandwidth while considering job priorities and latency requirements. Experiments show Metronome reduces job completion time by up to 19.50% and improves bandwidth utilization by 23.20% compared to Kubernetes schedulers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching
for Heterogeneous Tasks and Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [decentralized scheduling, load caching, heterogeneous clusters, balls-into-bins model, resource management]</li>
<li class=""><strong>authors:</strong> Wei Da, Evangelia Kalyvianaki</li>
<li class=""><strong>institution:</strong> The University of Cambridge</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.12889v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.12889v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Dodoor introduces a randomized decentralized scheduler that uses cached server information and a novel load scoring mechanism to schedule heterogeneous tasks. It reduces communication overhead by batch updates instead of real-time probing. Evaluations show significant reductions in scheduling messages (55-66%) and improvements in throughput, latency, and makespan across different workloads.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-15">2025-10-15<a href="#2025-10-15" class="hash-link" aria-label="Direct link to 2025-10-15" title="Direct link to 2025-10-15" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic
Serving</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [agentic workflows, resource pooling, stage isolation, KV cache optimization, workflow-aware serving]</li>
<li class=""><strong>authors:</strong> Nikos Pagonas, Yeounoh Chung, Kostis Kaffes, Arvind Krishnamurthy</li>
<li class=""><strong>institution:</strong> Columbia University, Google, University of Washington</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.14126v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.14126v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Cortex introduces stage isolation by provisioning dedicated resource pools for each workflow stage, reducing inter-stage interference. This approach improves KV cache utilization, throughput, and performance predictability. The system enables advanced agentic serving features like malleable resource management and speculative execution.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [quantization, accelerator, edge devices, block floating point, matrix multiplication]</li>
<li class=""><strong>authors:</strong> Jude Haris, José Cano</li>
<li class=""><strong>institution:</strong> University of Glasgow</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.13401v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.13401v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes a Flexible Block Floating-Point Quantization (F-BFQ) accelerator that dynamically switches between BFP variants for efficient LLM inference. Deployed on AMD Kria hardware, it achieves 1.4x faster inference than CPU execution while maintaining 5.2 tokens per second throughput. This enables optimized LLM deployment on resource-constrained edge devices through specialized quantization acceleration.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [adaptive scheduling, length prediction, workload balancing, prefill-decode disaggregation]</li>
<li class=""><strong>authors:</strong> Zhibin Wang, Zetao Hong, Xue Li, Zibo Wang, Shipeng Li, Qingkai Meng, Qing Wang, Chengying Huan, Rong Gu, Sheng Zhong, Chen Tian</li>
<li class=""><strong>institution:</strong> Nanjing University, Alibaba Group</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.13668v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.13668v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ARES introduces an adaptive decoding rescheduling system using lightweight LLM-native length prediction based on hidden states to anticipate future workloads. It achieves high prediction accuracy with reduced parameters and implements dynamic workload balancing in the decode phase. The method significantly reduces tail latency by 74.77% and improves goodput by up to 2.24× compared to static scheduling approaches.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI
Model Access</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [federated inference, resource scheduling, high-performance computing, inference-as-a-service, OpenAI-compatible API]</li>
<li class=""><strong>authors:</strong> Aditya Tanikanti, Benoit Côté, Yanfei Guo, Le Chen, Nickolaus Saint, Ryan Chard, Ken Raffenetti, Rajeev Thakur, Thomas Uram, Ian Foster, Michael E. Papka, Venkatram Vishwanath</li>
<li class=""><strong>institution:</strong> Argonne National Laboratory</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.13724v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.13724v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FIRST provides a federated inference framework that enables cloud-like AI model access across distributed HPC clusters using an OpenAI-compatible API. It supports multiple inference backends, auto-scaling, and both batch/interactive modes while maintaining private, secure on-premises infrastructure. The system allows researchers to efficiently run large-scale parallel inference workloads without relying on commercial cloud services.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Anonymized Network Sensing using C++26 std::execution on GPUs</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [GPU computing, network sensing, C++26 Senders model, asynchronous execution, GraphBLAS, performance optimization]</li>
<li class=""><strong>authors:</strong> Michael Mandulak, Sayan Ghosh, S M Ferdous, Mahantesh Halappanavar, George Slota</li>
<li class=""><strong>institution:</strong> Pacific Northwest National Laboratory, Rensselaer Polytechnic Institute</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.14050v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.14050v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper implements anonymized network sensing using C++26 std::execution Senders model for GPU acceleration. The method enables efficient task deployment on multi-GPU systems through standardized asynchronous semantics. Results show 55x performance improvement over serial GraphBLAS baseline while maintaining programming productivity.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Efficiently Executing High-throughput Lightweight LLM Inference
Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive
Context Management</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [pervasive context management, high-throughput applications, heterogeneous GPU clusters, opportunistic scaling, lightweight LLMs]</li>
<li class=""><strong>authors:</strong> Thanh Son Phung, Douglas Thain</li>
<li class=""><strong>institution:</strong> University of Notre Dame</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.14024v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.14024v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes Pervasive Context Management to decouple LLM initialization context from actual inferences, retaining context in GPUs to avoid repeated startup costs. This technique reduced execution time by 72.1% for a fact verification application and enabled opportunistic scaling across 32.8% of cluster GPUs, further cutting execution time to 13 minutes. The approach efficiently supports lightweight LLM inference on heterogeneous GPU clusters without traditional queue delays.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] On-Chain Decentralized Learning and Cost-Effective Inference for DeFi
Attack Mitigation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [on-chain learning, decentralized training, DeFi security, Proof-of-Improvement, model quantization, smart contract inference]</li>
<li class=""><strong>authors:</strong> Abdulrahman Alhaidari, Balaji Palanisamy, Prashant Krishnamurthy</li>
<li class=""><strong>institution:</strong> University of Pittsburgh</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16024v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16024v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents a decentralized on-chain learning framework that performs training on Layer-2 and enables gas-efficient inference on Layer-1 using quantization and loop-unrolling techniques. The system uses a Proof-of-Improvement protocol to verify model updates and prevent adversarial proposals. The approach successfully enables various ML models to run within Ethereum&#x27;s gas limits while maintaining bit-exact equivalence to off-chain counterparts, demonstrating effectiveness for DeFi attack mitigation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing
Disaggregated LLM Serving in AI Infrastructure</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [KV cache migration, dynamic resource allocation, load balancing, disaggregated serving, attention mechanism]</li>
<li class=""><strong>authors:</strong> Yiyuan He, Minxian Xu, Jingfeng Wu, Jianmin Hu, Chong Ma, Min Shen, Le Chen, Chengzhong Xu, Lin Qu, Kejiang Ye</li>
<li class=""><strong>institution:</strong> Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Alibaba Group Inc</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.13223v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.13223v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> BanaServe introduces unified KV cache sharing and dynamic module migration to balance computational and memory resources in disaggregated LLM serving systems. It enables both coarse-grained layer migration and fine-grained attention-level KV cache migration with overlapped transmission. The system achieves 1.2x-3.9x higher throughput than vLLM and 1.1x-2.8x better than DistServe while significantly reducing latency.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-16">2025-10-16<a href="#2025-10-16" class="hash-link" aria-label="Direct link to 2025-10-16" title="Direct link to 2025-10-16" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job
Atomization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [GPU scheduling, MIG, job-aware scheduling, decentralized negotiation, auction theory, online optimization]</li>
<li class=""><strong>authors:</strong> Michal Konopa, Jan Fesl, Ladislav Ber ánek</li>
<li class=""><strong>institution:</strong> University of South Bohemia</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.14599v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.14599v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> JASDA introduces a decentralized scheduling paradigm where jobs actively generate and score subjobs in response to scheduler-announced windows, while the scheduler performs policy-driven clearing. This bidirectional approach combines auction theory and online optimization to balance utilization, fairness, and responsiveness. The method provides scalable resource management for MIG-enabled GPU environments in AI and Agriculture 4.0 applications.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] xLLM Technical Report</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [inference framework, scheduling optimization, KV cache management, speculative decoding, multimodal processing]</li>
<li class=""><strong>authors:</strong> Tongxuan Liu, Tao Peng, Peijun Yang, Xiaoyang Zhao, Xiusheng Lu, Weizhe Huang, Zirui Liu, Xiaoyu Chen, Zhiwei Liang, Jun Xiong, Donghe Jin, Minchao Zhang, Jinrong Guo, Yingxu Deng, Xu Zhang, Xianzhe Dong, Siqi Wang, Siyu Wu, Yu Wu, Zihan Tang, Yuting Zeng, Yanshu Wang, Jinguang Liu, Meng Kang, Menxin Li, Yunlong Wang, Yiming Liu, Xiaolong Ma, Yifan Wang, Yichen Zhang, Jinrun Yin, Keyang Zheng, Jiawei Yin, Jun Zhang, Ziyue Wang, Xiaobo Lin, Liangyu Liu, Liwei Lan, Yang Liu, Chunhua Peng, Han Liu, Songcheng Ren, Xuezhu Wang, Yunheng Shen, Yi Wang, Guyue Liu, Hui Chen, Tong Yang, Hailong Yang, Jing Li, Guiguang Ding, Ke Zhang</li>
<li class=""><strong>institution:</strong> JD.com</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.14686v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.14686v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> xLLM introduces a decoupled service-engine architecture with intelligent scheduling and distributed KV cache management for efficient LLM serving. The framework employs workload-adaptive disaggregation policies and algorithmic optimizations like speculative decoding. Evaluations show xLLM achieves up to 2.2x higher throughput compared to existing frameworks while maintaining high resource efficiency.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon
Blockchain</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [reinforcement learning, MEV extraction, blockchain, Polygon Atlas, PPO, bidding strategy, high-frequency trading]</li>
<li class=""><strong>authors:</strong> Andrei Seoev, Leonid Gremyachikh, Anastasiia Smirnova, Yash Madhwal, Alisa Kalacheva, Dmitry Belousov, Ilia Zubov, Aleksei Smirnov, Denis Fedyanin, Vladimir Gorgadze, Yury Yanovich</li>
<li class=""><strong>institution:</strong> MEV-X, Moscow Institute of Physics and Technology, Skolkovo Institute of Science and Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.14642v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.14642v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper develops a reinforcement learning framework using PPO for MEV extraction on Polygon blockchain. The method creates a simulation environment for arbitrage opportunities and implements a real-time bidding agent. Results show the RL agent captures significantly higher profits (49-81%) compared to static strategies in high-frequency MEV environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FairBatching: Fairness-Aware Batch Formation for LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [scheduling, fairness, batch formation, prefill, decode, latency optimization]</li>
<li class=""><strong>authors:</strong> Hongtao Lyu, Boyue Liu, Mingyu Wu, Haibo Chen</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.14392v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.14392v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FairBatching introduces a fairness-aware scheduler that dynamically allocates resources between prefill and decode tasks through adaptive batch capacity determination and fair batch formation algorithms. It addresses computational unfairness in existing schedulers by reclaiming resources from bursting decode tasks to serve prefill surges. The method reduces TTFT tail latency by up to 2.29x while maintaining TPOT SLOs, improving both single-node and cluster-level capacity significantly.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Multi-modal video data-pipelines for machine learning with minimal human
supervision</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [multi-modal learning, semantic segmentation, depth estimation, real-time processing, autonomous data-pipeline]</li>
<li class=""><strong>authors:</strong> Mihai-Cristian Pîrvu, Marius Leordeanu</li>
<li class=""><strong>institution:</strong> University Politehnica of Bucharest</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.14862v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.14862v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper presents an autonomous multi-modal video data pipeline using pre-trained experts and PHG-MAE model for minimal human supervision. The distilled model achieves competitive performance with &lt;1M parameters compared to 300M parameter models. The framework enables real-time semantic segmentation and depth estimation on commodity hardware.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] ScalePool: Hybrid XLink-CXL Fabric for Composable Resource
Disaggregation in Unified Scale-up Domains</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [CXL, XLink, memory disaggregation, accelerator communication, memory pooling]</li>
<li class=""><strong>authors:</strong> Hyein Woo, Miryeong Kwon, Jiseon Kim, Eunjee Na, Hanjin Choi, Seonghyeon Jang, Myoungsoo Jung</li>
<li class=""><strong>institution:</strong> Panmnesia, Inc.</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.14580v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.14580v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> ScalePool proposes a hybrid XLink-CXL fabric architecture that enables composable resource disaggregation through unified hardware interconnects. It uses XLink for low-latency intra-cluster accelerator communication and CXL for scalable inter-cluster memory sharing with explicit memory tiering. Evaluation shows 1.22x average LLM training acceleration and up to 4.5x latency reduction for memory-intensive workloads compared to RDMA-based systems.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-17">2025-10-17<a href="#2025-10-17" class="hash-link" aria-label="Direct link to 2025-10-17" title="Direct link to 2025-10-17" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] PRISM: Probabilistic Runtime Insights and Scalable Performance Modeling
for Large-Scale Distributed Training</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [performance modeling, distributed training, GPU variability, parallelization strategies, communication kernels]</li>
<li class=""><strong>authors:</strong> Alicia Golden, Michael Kuchnik, Samuel Hsia, Zachary DeVito, Gu-Yeon Wei, David Brooks, Carole-Jean Wu</li>
<li class=""><strong>institution:</strong> Meta (FAIR), Harvard University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15596v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15596v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> PRISM introduces a probabilistic performance modeling framework for large-scale distributed training that accounts for runtime variability. It provides statistical guarantees on training time and identifies optimization opportunities in parallelization strategies. The framework demonstrates up to 1.26x performance improvement potential and identifies communication kernels as key targets for reducing performance variability.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Balancing Fairness and Performance in Multi-User Spark Workloads with
Dynamic Scheduling (extended version)</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [Apache Spark, fair scheduling, dynamic scheduling, task partitioning, workload management]</li>
<li class=""><strong>authors:</strong> Dāvis Kažemaks, Laurens Versluis, Burcu Kulahcioglu Ozkan, Jérémie Decouchant</li>
<li class=""><strong>institution:</strong> Delft University of Technology</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15485v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15485v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes User Weighted Fair Queuing (UWFQ) scheduler with runtime partitioning to improve fairness and reduce job response times in multi-user Spark environments. UWFQ uses virtual fair queuing and dynamic task granularity refinement based on expected runtime. Evaluation shows up to 74% reduction in average response time for small jobs compared to existing schedulers.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] BeLLMan: Controlling LLM Congestion</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [congestion control, latency optimization, energy efficiency, system load management]</li>
<li class=""><strong>authors:</strong> Tella Rajashekhar Reddy, Atharva Deshmukh, Karan Tandon, Rohan Gandhi, Anjaly Parayil, Debopam Bhattacherjee</li>
<li class=""><strong>institution:</strong> Microsoft</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15330v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15330v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> beLLMan is a controller that enables LLM infrastructure to signal applications to adjust output length based on system load. It reduces inference latency by up to 8x and energy consumption by 25% while serving 19% more requests during congestion periods. The system demonstrates effective load management for LLM applications through output length adaptation.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] GOGH: Correlation-Guided Orchestration of GPUs in Heterogeneous Clusters</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [heterogeneous clusters, resource allocation, neural networks, energy efficiency, GPU orchestration]</li>
<li class=""><strong>authors:</strong> Ahmad Raeisi, Mahdi Dolati, Sina Darabi, Sadegh Talebi, Patrick Eugster, Ahmad Khonsari</li>
<li class=""><strong>institution:</strong> University of Tehran (based on authors Ahmad Raeisi, Mahdi Dolati, Sina Darabi, Sadegh Talebi, Ahmad Khonsari) and Purdue University (based on author Patrick Eugster)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15652v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15652v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> The paper proposes GOGH, a learning-based system that uses two neural networks to allocate GPU resources in heterogeneous clusters. It provides initial performance estimates, optimizes resource allocation, and continuously refines predictions using real deployment data. The approach reduces energy consumption while meeting performance requirements in mixed-hardware environments.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Spatiotemporal Traffic Prediction in Distributed Backend Systems via
Graph Neural Networks</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [trace analysis], [graph neural networks, traffic prediction, distributed systems, spatiotemporal modeling]</li>
<li class=""><strong>authors:</strong> Zhimin Qiu, Feng Liu, Yuxiao Wang, Chenrui Hu, Ziyu Cheng, Di Wu</li>
<li class=""><strong>institution:</strong> Based on the technical focus on distributed backend systems and graph neural networks for traffic prediction, likely from institutions like Google Research, Microsoft Research, or academic labs such as MIT/Stanford (specific institution cannot be definitively determined from provided content)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.15215v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.15215v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a graph neural network approach that combines graph convolution for spatial dependencies and gated recurrent structures for temporal evolution to predict traffic in distributed backend systems. Experimental results demonstrate the model achieves stable performance with low error across different prediction horizons, significantly improving forecasting accuracy and robustness. The work validates graph neural networks&#x27; potential for complex system modeling in distributed environments.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-18">2025-10-18<a href="#2025-10-18" class="hash-link" aria-label="Direct link to 2025-10-18" title="Direct link to 2025-10-18" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Reimagining RDMA Through the Lens of ML</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [RDMA, distributed machine learning, collective communication, network transport, tail latency, congestion control, fault tolerance]</li>
<li class=""><strong>authors:</strong> Ertza Warraich, Ali Imran, Annus Zulfiqar, Shay Vargaftik, Sonia Fahmy, Muhammad Shahbaz</li>
<li class=""><strong>institution:</strong> Purdue University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16606v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16606v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Celeris introduces a domain-specific RDMA transport that removes retransmissions and in-order delivery, leveraging ML&#x27;s tolerance for partial data loss. It employs software-level mechanisms like adaptive timeouts and shifts loss recovery to the ML pipeline. Results show significant reductions in tail latency, BRAM usage, and improved NIC resilience for large-scale ML workloads.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Communication-Efficient and Memory-Aware Parallel Bootstrapping using
MPI</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [parallel computing, bootstrapping, MPI, communication efficiency, memory optimization]</li>
<li class=""><strong>authors:</strong> Di Zhang</li>
<li class=""><strong>institution:</strong> Xi&#x27;an Jiaotong-Liverpool University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16284v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16284v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes two parallel bootstrapping strategies using MPI - Local Statistic Aggregation and Synchronized Pseudo-Random Number Generation - to address communication overhead and memory constraints. The methods significantly reduce communication volume and memory usage compared to naive approaches. The analytical models demonstrate these approaches enable scalable parallel bootstrapping on large-scale distributed systems.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] FourierCompress: Layer-Aware Spectral Activation Compression for
Efficient and Accurate Collaborative LLM Inference</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [activation compression, frequency domain, edge computing, collaborative inference, Fast Fourier Transform]</li>
<li class=""><strong>authors:</strong> Jian Ma, Xinchen Lyu, Jun Jiang, Longhao Zou, Chenshan Ren, Qimei Cui, Xiaofeng Tao</li>
<li class=""><strong>institution:</strong> Beijing University of Posts and Telecommunications</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16418v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16418v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> FourierCompress proposes a layer-aware spectral compression method that transforms LLM activations to frequency domain using FFT and retains only low-frequency coefficients. This approach achieves 7.6x compression ratio with less than 0.3% accuracy loss while enabling hardware acceleration. The method significantly outperforms existing compression techniques like Top-k, QR and SVD in both efficiency and accuracy for collaborative LLM inference.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code
Generation</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [multi-agent systems, concurrent code generation, conflict-free replicated data types, coordination patterns, parallel speedup]</li>
<li class=""><strong>authors:</strong> Sergey Pugachev</li>
<li class=""><strong>institution:</strong> Independent researcher (no institutional affiliation indicated)</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.18893v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.18893v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> CodeCRDT introduces an observation-driven coordination pattern using CRDTs for multi-agent LLM code generation, enabling lock-free concurrent editing with strong eventual consistency. The approach achieves up to 21.1% speedup on some tasks but shows performance tradeoffs, with 100% convergence success but potential code quality degradation. Performance outcomes depend heavily on task structure characteristics and semantic conflict rates.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant
Optimization</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM training], [fault-tolerant optimization, distributed training, memory efficiency, computation efficiency, skip-connection, gradient approximation]</li>
<li class=""><strong>authors:</strong> Rizhen Hu, Yutong He, Ran Yan, Mou Sun, Binghang Yuan, Kun Yuan</li>
<li class=""><strong>institution:</strong> Peking University, HKUST, Zhejiang Lab</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16415v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16415v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> MeCeFO introduces a fault-tolerant optimization method for distributed LLM training that transfers tasks to neighboring nodes during failures using skip-connections, recomputation, and low-rank gradient approximations. It maintains convergence rates comparable to standard training while minimizing throughput drops. Empirical results show 5.0× to 6.7× greater resilience than previous approaches with only 4.18% performance degradation.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2025-10-19">2025-10-19<a href="#2025-10-19" class="hash-link" aria-label="Direct link to 2025-10-19" title="Direct link to 2025-10-19" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>[arXiv2510] Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [kernels], [MPI abstraction, distributed computing, C++ library, memory layout, GEMM kernel]</li>
<li class=""><strong>authors:</strong> Jiří Klepl, Martin Kruliš, Matyáš Brabec</li>
<li class=""><strong>institution:</strong> Charles University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16890v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16890v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes a novel MPI abstraction implemented as an extension of the C++ Noarr library, offering layout-agnostic design for distributed applications. The authors developed a distributed GEMM kernel as a case study to demonstrate the abstraction&#x27;s usability. Results show the abstraction achieves performance comparable to state-of-the-art MPI C++ bindings while providing more flexible application design.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Justitia: Fair and Efficient Scheduling for LLM Applications</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [scheduling], [LLM applications, fair scheduling, memory-centric modeling, demand prediction, virtual-time queuing]</li>
<li class=""><strong>authors:</strong> Mingyan Yang, Guanjie Wang, Manqi Luo, Yifei Liu, Chen Chen, Han Zhao, Yu Feng, Quan Chen, Minyi Guo</li>
<li class=""><strong>institution:</strong> Shanghai Jiao Tong University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.17015v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.17015v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> Justitia introduces a novel scheduler for LLM applications that uses memory-centric service cost modeling, neural network-based demand prediction, and virtual-time fair queuing. It achieves substantial improvements in scheduling efficiency while preserving fairness across diverse LLM applications. The system was implemented atop vLLM and demonstrated enhanced performance with guaranteed worst-case delay.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Tutoring LLM into a Better CUDA Optimizer</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [LLM inference], [code optimization, CUDA, parallel computing, tutoring, interactive debugging]</li>
<li class=""><strong>authors:</strong> Matyáš Brabec, Jiří Klepl, Michal Töpfer, Martin Kruliš</li>
<li class=""><strong>institution:</strong> Charles University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16933v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16933v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper explores using reasoning LLMs to generate optimized CUDA code through tutoring strategies including detailed prompts and interactive error correction. The study evaluates generated code through automated testing and manual code reviews. Results show LLMs are capable coders but require expert guidance to achieve performance comparable to human parallel computing experts.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Exact Nearest-Neighbor Search on Energy-Efficient FPGA Devices</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [Other models inference], [FPGA, energy efficiency, nearest neighbor search, high-dimensional data, parallel processing]</li>
<li class=""><strong>authors:</strong> Patrizio Dazzi, William Guglielmo, Franco Maria Nardini, Raffaele Perego, Salvatore Trani</li>
<li class=""><strong>institution:</strong> National Research Council of Italy (CNR) and University of Pisa</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16736v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16736v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper proposes two FPGA-based solutions for exact k-nearest neighbor search that optimize either throughput or latency while maintaining energy efficiency. The methods use parallel processing approaches for streaming and in-memory datasets. Experimental results show significant improvements over CPU-based competitors, achieving up to 16.6× better throughput/latency and 11.9× energy savings.</li>
</ul>
</li>
<li class="">
<p><strong>[arXiv2510] Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU
Infrastructure</strong></p>
<ul>
<li class=""><strong>tags:</strong> [mlsys], [cluster infrastructure], [GPU telemetry, performance diagnosis, host-side monitoring, eBPF, tail latency, root cause analysis, distributed learning, multi-tenant infrastructure]</li>
<li class=""><strong>authors:</strong> Erfan Darzi, Aldo Pareja, Shreeanant Bharadwaj</li>
<li class=""><strong>institution:</strong> Harvard University, MIT, MIT-IBM Watson AI Lab, Northeastern University</li>
<li class=""><strong>link:</strong> <a href="http://arxiv.org/pdf/2510.16946v1" target="_blank" rel="noopener noreferrer" class="">http://arxiv.org/pdf/2510.16946v1</a></li>
<li class=""><strong>Simple LLM Summary:</strong> This paper introduces an eBPF-based telemetry system for GPU performance diagnosis that correlates host metrics with GPU events. The system achieves 81-88% diagnostic accuracy and identifies root causes like NIC contention and PCIe pressure. It enables debugging in multi-tenant GPU infrastructure without cluster-wide instrumentation.</li>
</ul>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-10-29T04:16:08.000Z" itemprop="dateModified">Oct 29, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/daily/20251006-20251012"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">20251006-20251012</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/daily/20251020-20251026"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">20251020-20251026</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2025-10-13" class="table-of-contents__link toc-highlight">2025-10-13</a></li><li><a href="#2025-10-14" class="table-of-contents__link toc-highlight">2025-10-14</a></li><li><a href="#2025-10-15" class="table-of-contents__link toc-highlight">2025-10-15</a></li><li><a href="#2025-10-16" class="table-of-contents__link toc-highlight">2025-10-16</a></li><li><a href="#2025-10-17" class="table-of-contents__link toc-highlight">2025-10-17</a></li><li><a href="#2025-10-18" class="table-of-contents__link toc-highlight">2025-10-18</a></li><li><a href="#2025-10-19" class="table-of-contents__link toc-highlight">2025-10-19</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 DarkKnight996, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>