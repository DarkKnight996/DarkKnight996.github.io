# 20251027-20251102

## 2025-10-27

- **[arXiv2510] Learning to Schedule: A Supervised Learning Framework for Network-Aware
  Scheduling of Data-Intensive Workloads**
  - **tags:** [mlsys], [scheduling], [network-aware scheduling, supervised learning, data-intensive workloads, Kubernetes, job completion time prediction]
  - **authors:** Sankalpa Timilsina, Susmit Shannigrahi
  - **institution:** Tennessee Technological University
  - **link:** http://arxiv.org/pdf/2510.21419v1
  - **Simple LLM Summary:** This paper proposes a network-aware job scheduler using supervised learning to predict job completion times based on real-time cluster telemetry. The system employs a prediction-and-ranking mechanism that evaluates nodes and selects optimal placements for data-intensive workloads. Evaluation on a geo-distributed Kubernetes cluster showed 34-54% higher accuracy in node selection compared to the default Kubernetes scheduler.

- **[arXiv2510] From SLA to vendor-neutral metrics: An intelligent knowledge-based
  approach for multi-cloud SLA-based broker**
  - **tags:** [mlsys], [others], [cloud computing, multi-cloud, SLA management, vendor-neutral metrics, intelligent knowledge-based system, auto-scaling]
  - **authors:** Víctor Rampérez, Javier Soriano, David Lizcano, Shadi Aljawarneh, Juan A. Lara
  - **institution:** Universidad Politécnica de Madrid (UPM), Madrid Open University (UDIMA)
  - **link:** http://arxiv.org/pdf/2510.21173v1
  - **Simple LLM Summary:** This paper proposes an intelligent knowledge-based system that automatically translates high-level SLAs into vendor-neutral metrics for multi-cloud environments. The approach enables cross-provider metric measurement and provides consumer feedback through an intelligent tutoring system. Validation with IaaS and PaaS use cases demonstrates the system allows transparent multi-cloud exploitation across various application domains.

- **[arXiv2510] xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep
  Learning Training Workloads**
  - **tags:** [mlsys], [cluster infrastructure], [GPU memory estimation, dynamic analysis, resource management, scheduling]
  - **authors:** Jiabo Shi, Dimitrios Pezaros, Yehia Elkhatib
  - **institution:** University of Glasgow
  - **link:** http://arxiv.org/pdf/2510.21048v1
  - **Simple LLM Summary:** xMem proposes a CPU-based dynamic analysis framework to accurately estimate peak GPU memory requirements for deep learning training workloads without consuming GPU resources. The method achieves 91% reduction in median relative error and 75% reduction in OOM probability compared to existing solutions. This enables better GPU sharing and scheduling in cluster environments while significantly improving memory conservation potential.

- **[arXiv2510] Lincoln AI Computing Survey (LAICS) and Trends**
  - **tags:** [mlsys], [Other models training, Other models inference], [AI accelerators, performance analysis, power consumption, market segmentation, computing architectures]
  - **authors:** Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally, Jeremy Kepner
  - **institution:** MIT Lincoln Laboratory
  - **link:** http://arxiv.org/pdf/2510.20931v1
  - **Simple LLM Summary:** This paper updates the Lincoln AI Computing Survey by collecting performance and power consumption data of commercial AI accelerators, plotting them on scatter graphs, and analyzing market trends. It introduces a new categorization of computing architectures and examines how GenAI models have shifted computational demands toward matrix-vector operations and high memory bandwidth. The survey highlights ongoing innovations in AI hardware across various deployment scales from embedded systems to data centers.

- **[arXiv2510] ParaRNN: Unlocking Parallel Training of Nonlinear RNNs for Large
  Language Models**
  - **tags:** [mlsys], [LLM training], [parallel training, nonlinear RNNs, sequence modeling, Newton's iterations, parallel reductions]
  - **authors:** Federico Danieli, Pau Rodriguez, Miguel Sarabia, Xavier Suau, Luca Zappella
  - **institution:** Apple
  - **link:** http://arxiv.org/pdf/2510.21450v1
  - **Simple LLM Summary:** ParaRNN enables parallel training of nonlinear RNNs by formulating recurrence relationships as a system of equations and solving them using Newton's iterations with parallel reductions. This approach achieves up to 665x speedup over sequential methods and allows training 7B parameter RNNs with performance comparable to Transformers and Mamba2. The framework is released as open-source to facilitate scalable nonlinear RNN research.

- **[arXiv2510] REVE: A Foundation Model for EEG -- Adapting to Any Setup with
  Large-Scale Pretraining on 25,000 Subjects**
  - **tags:** [mlsys], [Other models training], [EEG foundation model, 4D positional encoding, masked autoencoding, brain-computer interfaces, clinical neuroscience]
  - **authors:** Yassine El Ouahidi, Jonathan Lys, Philipp Thölke, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon, Karim Jerbi, Giulia Lioi
  - **institution:** IMT Atlantique, Université de Montréal, Mila, UNIQUE
  - **link:** http://arxiv.org/pdf/2510.21585v1
  - **Simple LLM Summary:** REVE introduces a novel 4D positional encoding scheme and uses masked autoencoding pretraining on 60,000 hours of EEG data from 25,000 subjects. The model achieves state-of-the-art performance across 10 EEG tasks including motor imagery and seizure detection. It demonstrates strong generalization with minimal fine-tuning and enables standardized EEG research through released code and weights.
