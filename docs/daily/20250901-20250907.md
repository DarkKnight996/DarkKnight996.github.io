# 20250901-20250907

## 2025-09-01

- **[arXiv2509] STZ: A High Quality and High Speed Streaming Lossy Compression Framework
  for Scientific Data**
  - **tags:** [mlsys], [Other models inference], [streaming compression, lossy compression, scientific data, progressive decompression, random-access decompression]
  - **authors:** Daoce Wang, Pascal Grosset, Jesus Pulido, Jiannan Tian, Tushar M. Athawale, Jinda Jia, Baixi Sun, Boyuan Zhang, Sian Jin, Kai Zhao, James Ahrens, Fengguang Song
  - **institution:** Unknown (insufficient information provided)
  - **link:** http://arxiv.org/pdf/2509.01626v1
  - **Simple LLM Summary:** The paper proposes a novel streaming compression framework that supports both progressive and random-access decompression while maintaining high quality. It uses hierarchical partitioning and prediction strategies to achieve compression quality comparable to SZ3 with significantly faster speed. The method enables efficient on-demand data access and flexible analysis workflows for scientific data.

- **[arXiv2509] LiquidGEMM: Hardware-Efficient W4A8 GEMM Kernel for High-Performance LLM
  Serving**
  - **tags:** [mlsys], [kernels], [quantization, GEMM kernel, W4A8, LLM inference, hardware efficiency, Tensor Cores]
  - **authors:** Huanqi Hu, Bowen Xiao, Shixuan Sun, Jianian Yin, Zhexi Zhang, Xiang Luo, Chengquan Jiang, Weiqi Xu, Xiaoying Jia, Xin Liu, Minyi Guo
  - **institution:** NVIDIA
  - **link:** http://arxiv.org/pdf/2509.01229v1
  - **Simple LLM Summary:** LiquidGEMM introduces a hardware-efficient W4A8 GEMM kernel with LiquidQuant for fast dequantization and an implicit fine-grained pipeline to overlap operations. It achieves up to 2.90x speedup over existing W4A8 kernels and up to 4.94x system-level speedup, outperforming NVIDIA TensorRT-LLM kernels by 1.12-1.63x.

- **[arXiv2509] LobRA: Multi-tenant Fine-tuning over Heterogeneous Data**
  - **tags:** [mlsys], [finetuning], [multi-tenant fine-tuning, LoRA adapters, heterogeneous data, sequence length variation, workload balance]
  - **authors:** Sheng Lin, Fangcheng Fu, Haoyang Li, Hao Ge, Xuanyu Wang, Jiawen Niu, Yaofeng Tu, Bin Cui
  - **institution:** Based on the technical focus and terminology, likely from academic institutions or companies specializing in machine learning systems (e.g., universities with strong ML/AI departments or tech companies like Google, Microsoft, Meta)
  - **link:** http://arxiv.org/pdf/2509.01193v1
  - **Simple LLM Summary:** LobRA introduces a framework for efficient multi-tenant fine-tuning using LoRA adapters by addressing data heterogeneity through heterogeneous resource deployments and workload balancing. It reduces GPU time by 45.03%-60.67% for joint fine-tuning tasks. The method optimizes training efficiency in shared base model scenarios.

- **[arXiv2509] Optimal Parallel Scheduling under Concave Speedup Functions**
  - **tags:** [mlsys], [scheduling], [parallel scheduling, concave speedup functions, resource allocation, cloud computing, edge computing, AI applications]
  - **authors:** Chengzhang Li, Peizhong Ju, Atilla Eryilmaz, Ness Shroff
  - **institution:** Based on the technical content about cloud/edge computing and AI applications, likely from institutions like MIT, Stanford, UC Berkeley, or Microsoft Research (specific institution cannot be determined from provided content)
  - **link:** http://arxiv.org/pdf/2509.01811v1
  - **Simple LLM Summary:** The paper proposes the Consistent Derivative Ratio Rule and General Water-Filling method for optimal parallel job scheduling under arbitrary concave speedup functions. It introduces the SmartFill algorithm that selectively allocates resources rather than distributing to all active jobs. Numerical evaluations show SmartFill substantially outperforms prior heSRPT method across various concave speedup functions.

## 2025-09-02

- **[arXiv2509] An Efficient and Adaptive Watermark Detection System with Tile-based
  Error Correction**
  - **tags:** [mlsys], [kernels], [watermark detection, error correction, GPU optimization, tiling techniques, Reed-Solomon codes, resource-aware scheduling]
  - **authors:** Xinrui Zhong, Xinze Feng, Jingwei Zuo, Fanjiang Ye, Yi Mu, Junfeng Guo, Heng Huang, Myungjin Lee, Yuke Wang
  - **institution:** Unable to determine from provided information
  - **link:** http://arxiv.org/pdf/2509.02447v1
  - **Simple LLM Summary:** QRMark combines QR Code-inspired error correction with tiling techniques and Reed-Solomon codes to maintain detection accuracy while improving efficiency. The system implements resource-aware stream allocation and tile-based workload interleaving to optimize GPU utilization. Evaluations show QRMark achieves 2.43x inference speedup over sequential baselines while preserving robustness.

- **[arXiv2509] Efficient Pyramidal Analysis of Gigapixel Images on a Decentralized
  Modest Computer Cluster**
  - **tags:** [mlsys], [cluster infrastructure], [gigapixel image analysis, pyramid method, distributed computing, load balancing, biomedical imaging]
  - **authors:** Marie Reinbigler, Rishi Sharma, Rafael Pires, Elisabeth Brunet, Anne-Marie Kermarrec, Catalin Fetita
  - **institution:** Unknown (insufficient information to determine)
  - **link:** http://arxiv.org/pdf/2509.02440v1
  - **Simple LLM Summary:** The paper presents PyramidAI, a pyramidal approach that analyzes gigapixel images by starting with lower resolutions and progressively focusing on regions of interest at higher resolutions. It reduces processed data by up to 2.65x while maintaining accuracy and demonstrates efficient parallelization on modest computer clusters, cutting analysis time from over an hour to minutes with 12 workers.

- **[arXiv2509] HydroGAT: Distributed Heterogeneous Graph Attention Transformer for
  Spatiotemporal Flood Prediction**
  - **tags:** [mlsys], [Other models training], [graph neural networks, spatiotemporal prediction, flood forecasting, distributed training, heterogeneous graphs, attention mechanisms, hydrological modeling]
  - **authors:** Aishwarya Sarkar, Autrin Hakimi, Xiaoqiong Chen, Hai Huang, Chaoqun Lu, Ibrahim Demir, Ali Jannesari
  - **institution:** NERSC (National Energy Research Scientific Computing Center) / swapp-lab
  - **link:** http://arxiv.org/pdf/2509.02481v1
  - **Simple LLM Summary:** HydroGAT introduces a heterogeneous graph attention transformer that models both spatial river network topology and temporal dependencies for flood prediction. The method achieves superior discharge forecasting performance with NSE up to 0.97 while providing interpretable attention maps. A distributed training pipeline enables efficient high-resolution basin-scale modeling, demonstrating 15x speedup on supercomputing infrastructure.

- **[arXiv2509] KubeIntellect: A Modular LLM-Orchestrated Agent Framework for End-to-End
  Kubernetes Management**
  - **tags:** [mlsys], [cluster infrastructure], [Kubernetes management, LLM orchestration, modular agents, natural language interface, tool synthesis]
  - **authors:** Mohsen Seyedkazemi Ardebili, Andrea Bartolini
  - **institution:** Microsoft (based on Azure integration mentioned in abstract)
  - **link:** http://arxiv.org/pdf/2509.02449v1
  - **Simple LLM Summary:** KubeIntellect introduces an LLM-orchestrated agent framework that uses modular agents and a supervisor to interpret natural language queries for comprehensive Kubernetes operations. The system achieves 93% tool synthesis success rate and 100% reliability across 200 queries, demonstrating effective end-to-end Kubernetes management through natural language interaction. This represents a new class of interpretable, LLM-driven systems for complex infrastructure management.

- **[arXiv2509] A Continuous Energy Ising Machine Leveraging Difference-of-Convex
  Programming**
  - **tags:** [mlsys], [Other models inference], [Ising machine, combinatorial optimization, difference-of-convex programming, GPU acceleration, convergence guarantees]
  - **authors:** Debraj Banerjee, Santanu Mahapatra, Kunal Narayan Chaudhury
  - **institution:** Based on the technical content and GPU implementation across platforms, likely from academic institutions with strong computing/engineering programs (e.g., universities with HPC facilities) or industry research labs focusing on optimization algorithms
  - **link:** http://arxiv.org/pdf/2509.01928v1
  - **Simple LLM Summary:** This paper proposes a continuous energy Ising machine that relaxes binary spins to continuous variables and uses difference-of-convex programming for efficient optimization. The method requires only one matrix-vector multiplication per iteration and provides convergence guarantees. Experimental results show it consistently outperforms existing solvers across problem sizes from small to ultra-large scales on various GPU platforms.

- **[arXiv2509] MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to
  Break the GPU Memory Wall**
  - **tags:** [mlsys], [LLM training], [memory offloading, GPU memory optimization, multi-tier storage, I/O bottleneck mitigation]
  - **authors:** Avinash Maurya, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae
  - **institution:** Based on the technical focus and problem domain, likely from academic institutions or companies specializing in systems optimization for AI training (e.g., universities with strong systems research or AI infrastructure companies)
  - **link:** http://arxiv.org/pdf/2509.02480v1
  - **Simple LLM Summary:** MLP-Offload introduces a multi-level, multi-path offloading engine that optimizes LLM pre-training by efficiently distributing optimizer states across storage tiers. It reduces I/O bottlenecks during backward and update phases through cache-efficient strategies and concurrency control. The method achieves 2.5Ã— faster training iterations compared to state-of-the-art systems for models up to 280B parameters.

- **[arXiv2509] Batch Query Processing and Optimization for Agentic Workflows**
  - **tags:** [mlsys], [LLM inference], [batch query processing, agentic workflows, KV-cache sharing, GPU utilization, query optimization]
  - **authors:** Junyi Shen, Noppanat Wadlom, Yao Lu
  - **institution:** Based on the technical focus on LLM serving systems and optimization, likely from academic institutions with strong systems research groups (e.g., UC Berkeley, MIT, Stanford, CMU) or industry labs (e.g., Google Research, Microsoft Research)
  - **link:** http://arxiv.org/pdf/2509.02121v1
  - **Simple LLM Summary:** Halo introduces batch query processing and optimization for agentic LLM workflows by representing workflows as structured DAGs and performing plan-level optimization. It uses adaptive batching, KV-cache sharing, and compute-communication overlap to maximize hardware efficiency. Evaluation shows up to 18.6x speedup for batch inference and 4.7x throughput improvement without compromising output quality.

- **[arXiv2509] OASIS: Object-based Analytics Storage for Intelligent SQL Query
  Offloading in Scientific Tabular Workloads**
  - **tags:** [mlsys], [cluster infrastructure], [object storage, SQL query offloading, scientific workloads, storage hierarchy optimization]
  - **authors:** Soon Hwang, Junhyeok Park, Junghyun Ryu, Seonghoon Ahn, Jeoungahn Park, Jeongjin Lee, Soonyeal Yang, Jungki Noh, Woosuk Chung, Hoshik Kim, Youngjae Kim
  - **institution:** Based on the technical content and HPC focus, likely from a research institution or university with strong systems research (e.g., ETH Zurich, MIT, UC Berkeley, or national labs like Argonne/LBNL)
  - **link:** http://arxiv.org/pdf/2509.01966v1
  - **Simple LLM Summary:** OASIS introduces a Computation-Enabled Object Storage system that supports flexible output formats, complex operators, and dynamic execution path selection across storage layers. It integrates with Spark and demonstrates up to 32.7% performance improvement over existing COS systems in HPC scientific query workloads. The system optimizes data movement and operator execution close to storage for enhanced analytical efficiency.

## 2025-09-03

- **[arXiv2509] DPQuant: Efficient and Differentially-Private Model Training via Dynamic
  Quantization Scheduling**
  - **tags:** [mlsys], [Other models training], [differential privacy, quantization, training optimization, model efficiency, privacy-preserving ML]
  - **authors:** Yubo Gao, Renbo Tu, Gennady Pekhimenko, Nandita Vijaykumar
  - **institution:** University of Toronto
  - **link:** http://arxiv.org/pdf/2509.03472v1
  - **Simple LLM Summary:** DPQuant introduces a dynamic quantization framework that adaptively selects layers for quantization during differentially private training, combining probabilistic sampling and loss-aware prioritization. This approach reduces quantization variance amplified by DP-SGD noise injection while preserving privacy guarantees. The method achieves near Pareto-optimal accuracy-compute trade-offs with up to 2.21Ã— throughput improvements and minimal accuracy degradation across multiple models and datasets.

- **[arXiv2509] Combining Performance and Productivity: Accelerating the Network Sensing
  Graph Challenge with GPUs and Commodity Data Science Software**
  - **tags:** [mlsys], [kernels], [Graph Challenge, GPU acceleration, data science tools, RAPIDS, cuDF, cupy, GraphBLAS, network sensing, performance optimization]
  - **authors:** Siddharth Samsi, Dan Campbell, Emanuel Scoullos, Oded Green
  - **institution:** NVIDIA
  - **link:** http://arxiv.org/pdf/2509.03653v1
  - **Simple LLM Summary:** This paper presents an alternative implementation of the Network Sensing Graph Challenge using data science tools from NVIDIA's RAPIDS ecosystem. By leveraging cuDF and cupy instead of traditional HPC code, the authors achieve significant GPU acceleration. The method demonstrates speedups of 147x-2185x over CPU-based Pandas implementations across different NVIDIA GPU architectures.

- **[arXiv2509] CloudFormer: An Attention-based Performance Prediction for Public Clouds
  with Unknown Workload**
  - **tags:** [mlsys], [trace analysis], [performance prediction, cloud computing, virtual machines, transformer, attention mechanism, interference mitigation, system metrics]
  - **authors:** Amirhossein Shahbazinia, Darong Huang, Luis Costero, David Atienza
  - **institution:** Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne (EPFL)
  - **link:** http://arxiv.org/pdf/2509.03394v1
  - **Simple LLM Summary:** CloudFormer proposes a dual-branch Transformer model that jointly models temporal dynamics and system-level interactions to predict VM performance degradation in black-box cloud environments. The method leverages 206 system metrics at one-second resolution and demonstrates robust generalization across diverse workloads without scenario-specific tuning. Experimental results show CloudFormer achieves state-of-the-art performance with just 7.8% MAE, significantly outperforming existing methods by at least 28%.

- **[arXiv2509] FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale
  Training of LLMs**
  - **tags:** [mlsys], [LLM training], [fault tolerance, failure recovery, checkpoint-free, large-scale training, cluster management]
  - **authors:** Haijun Zhang, Jinxiang Wang, Zhenhua Yu, Yanyong Zhang, Xuejie Ji, Kaining Mao, Jun Zhang, Yaqing Zhang, Ting Wu, Fei Jie, Xiemin Huang, Zhifang Cai, Junhua Cheng, Shuwei Wang, Wei Li, Xiaoming Bao, Hua Xu, Shixiong Zhao, Jun Li, Hongwei Sun, Ziyang Zhang, Yi Xiong, Chunsheng Li
  - **institution:** iFLYTEK AI Engineering Institute, University of Science and Technology of China, Huawei Technologies Co., Ltd
  - **link:** http://arxiv.org/pdf/2509.03047v1
  - **Simple LLM Summary:** FlashRecovery introduces a three-module system for fast failure recovery in large-scale LLM training, featuring real-time failure detection, scale-independent task restart, and checkpoint-free single-step restoration. It achieves optimal recovery objectives with consistent recovery times across cluster scales. Experimental results show recovery within 150 seconds on a 4800-device cluster.

- **[arXiv2509] Mycroft: Tracing Dependencies in Collective Communication Towards
  Reliable LLM Training**
  - **tags:** [mlsys], [LLM training], [collective communication, distributed tracing, root cause analysis, fault detection, reliability]
  - **authors:** Yangtao Deng, Lei Zhang, Qinlong Wang, Xiaoyun Zhi, Xinlei Zhang, Zhuo Jiang, Haohan Xu, Lei Wang, Zuquan Song, Gaohong Liu, Yang Bai, Shuguang Wang, Wencong Xiao, Jianxi Ye, Minlan Yu, Hong Xu
  - **institution:** ByteDance
  - **link:** http://arxiv.org/pdf/2509.03018v1
  - **Simple LLM Summary:** Mycroft introduces a lightweight distributed tracing system that tracks collective communication states and leverages internal dependencies to resolve reliability issues in LLM training. It achieves rapid anomaly detection within 15 seconds in 90% of cases and root cause identification within 20 seconds in 60% of cases. The system has been successfully deployed at ByteDance for over six months, demonstrating effectiveness through extensive fault injection experiments.

## 2025-09-04

- **[arXiv2509] Counterfactual simulations for large scale systems with burnout
  variables**
  - **tags:** [mlsys], [Other models inference], [counterfactual simulation, burnout variables, uncertainty relaxation, parallel computation, online advertising]
  - **authors:** Benjamin Heymann
  - **institution:** ENSAE, Criteo AI LAB
  - **link:** http://arxiv.org/pdf/2509.04038v1
  - **Simple LLM Summary:** The paper introduces uncertainty relaxation algorithms that enable parallel computation for counterfactual simulations in systems with burnout variables. This approach significantly improves scalability compared to traditional sequential processing methods. The method is particularly applicable to online advertising systems where campaign budgets create complex dynamic constraints.

- **[arXiv2509] LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for
  High-Performance Distributed Training Systems**
  - **tags:** [mlsys], [checkpointing], [distributed training, gradient compression, differential checkpointing, fault tolerance, performance optimization]
  - **authors:** Chenxuan Yao, Yuchong Hu, Feifan Liu, Zhengyu Liu, Dan Feng
  - **institution:** Huazhong University of Science and Technology
  - **link:** http://arxiv.org/pdf/2509.04084v1
  - **Simple LLM Summary:** LowDiff proposes an efficient frequent checkpointing framework that reuses compressed gradients as differential checkpoints and employs batched gradient writes with dynamic tuning. It achieves high checkpointing frequency with minimal runtime overhead by leveraging layer-wise gradient reusing and asynchronous persistence strategies. Experimental results demonstrate the framework can perform checkpointing every iteration while maintaining less than 3.1% performance overhead.

- **[arXiv2509] Massively-Parallel Implementation of Inextensible Elastic Rods Using
  Inter-block GPU Synchronization**
  - **tags:** [mlsys], [kernels], [GPU computing, elastic rods, medical simulation, parallel algorithms, CUDA]
  - **authors:** Przemyslaw Korzeniowski, Niels Hald, Fernando Bello
  - **institution:** Imperial College London
  - **link:** http://arxiv.org/pdf/2509.04277v1
  - **Simple LLM Summary:** This paper presents a massively-parallel GPU implementation of inextensible elastic rods using inter-block synchronization, achieving significant speedups over CPU versions. The method enables multiple physics time-steps per kernel launch by utilizing all GPU streaming multiprocessors efficiently. The implementation achieves real-time haptic rates for medical simulations like catheter/guidewire pairs with performance boosts up to 40x faster than previous approaches.

- **[arXiv2509] Prob-GParareal: A Probabilistic Numerical Parallel-in-Time Solver for
  Differential Equations**
  - **tags:** [mlsys], [Other models inference], [probabilistic numerical solver, parallel-in-time methods, uncertainty quantification, Gaussian processes, differential equations]
  - **authors:** Guglielmo Gattiglio, Lyudmila Grigoryeva, Massimiliano Tamborrino
  - **institution:** University of Warwick, University of St. Gallen
  - **link:** http://arxiv.org/pdf/2509.03945v1
  - **Simple LLM Summary:** Prob-GParareal extends the GParareal algorithm using Gaussian processes to model correction functions, enabling uncertainty quantification for parallel-in-time solutions of differential equations. The method propagates numerical uncertainty across time domains and handles probabilistic initial conditions while maintaining compatibility with classical solvers. Theoretical analysis and numerical experiments demonstrate its accuracy and robustness across various ODE systems, with a nearest-neighbors variant showing improved scalability for PDE problems.

## 2025-09-05

- **[arXiv2509] Efficient Fault Localization in a Cloud Stack Using End-to-End
  Application Service Topology**
  - **tags:** [mlsys], [trace analysis], [fault localization, cloud computing, service topology, root cause detection, performance anomalies]
  - **authors:** Dhanya R Mathews, Mudit Verma, Pooja Aggarwal, J. Lakshmi
  - **institution:** Indian Institute of Science, Bangalore and IBM Research, India
  - **link:** http://arxiv.org/pdf/2509.05511v1
  - **Simple LLM Summary:** This paper proposes a topology-aware approach for efficient fault localization in cloud stacks by selecting informative metrics and incorporating end-to-end service topology into root cause detection algorithms. The method improves the accuracy of identifying performance anomaly sources by considering the complete service component relationships. Evaluation shows the proposed TA-RCD algorithm performs at least 2x better than state-of-the-art methods in Top-3 and Top-5 recall metrics.

- **[arXiv2509] veScale: Consistent and Efficient Tensor Programming with Eager-Mode
  SPMD**
  - **tags:** [mlsys], [LLM training], [SPMD, eager execution, distributed training, tensor programming, random number generation, performance optimization]
  - **authors:** Youjie Li, Cheng Wan, Zhiqi Lin, Hongyu Zhu, Jiacheng Yang, Ziang Song, Xinyi Di, Jiawei Wu, Huiyao Shu, Wenlei Bao, Yanghua Peng, Haibin Lin, Li-Wen Chang
  - **institution:** ByteDance
  - **link:** http://arxiv.org/pdf/2509.07003v1
  - **Simple LLM Summary:** veScale introduces an eager-mode training system that fully embraces the SPMD paradigm to simplify distributed tensor programming. It addresses consistency issues through a novel distributed RNG algorithm and improves performance by reducing PyTorch overhead and enhancing communication efficiency. Evaluations show veScale achieves up to 2.2x speedup over state-of-the-art systems while reducing code complexity by 78.4% and maintaining single-device-equivalent results.

- **[arXiv2509] Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface
  Visualization**
  - **tags:** [mlsys], [Other models training], [distributed computing, multi-GPU optimization, scientific visualization, 3D Gaussian splatting, high-performance computing]
  - **authors:** Mengjiao Han, Andres Sewell, Joseph Insley, Janet Knowles, Victor A. Mateevitsi, Michael E. Papka, Steve Petruzza, Silvio Rizzi
  - **institution:** Argonne National Laboratory
  - **link:** http://arxiv.org/pdf/2509.05216v1
  - **Simple LLM Summary:** This paper presents a distributed multi-GPU extension of the 3D Gaussian Splatting pipeline for scientific visualization. The method adapts a multi-GPU training backend to enable scalable processing of large datasets, achieving 5.6x speedup on benchmark datasets. This work enables high-resolution isosurface reconstructions that exceed single-GPU capacity and integrates 3D-GS into HPC workflows.

## 2025-09-06

- **[arXiv2509] Distributed Deep Learning using Stochastic Gradient Staleness**
  - **tags:** [mlsys], [Other models training], [distributed deep learning, stochastic gradient staleness, parallel backpropagation, data parallelism, training efficiency]
  - **authors:** Viet Hoang Pham, Hyo-Sung Ahn
  - **institution:** Posts and Telecommunications Institute of Technology (PTIT), Gwangju Institute of Science and Technology (GIST)
  - **link:** http://arxiv.org/pdf/2509.05679v1
  - **Simple LLM Summary:** This paper proposes a distributed training method combining data parallelism and fully decoupled parallel backpropagation to accelerate deep learning. The approach uses multiple computational units to process more training data per iteration while reducing locking issues. Experimental results on CIFAR-10 demonstrate significant improvements in training efficiency, with theoretical convergence guarantees to critical points.

- **[arXiv2509] The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU
  Libraries**
  - **tags:** [mlsys], [kernels], [kernel fusion, GPU optimization, C++ API, SRAM utilization, compile-time code generation]
  - **authors:** Oscar Amoros, Albert Andaluz, Johnny Nunez, Antonio J. Pena
  - **institution:** Barcelona Supercomputing Center
  - **link:** http://arxiv.org/pdf/2508.07071v2
  - **Simple LLM Summary:** The paper introduces a C++ API methodology using C++17 metaprogramming to automatically generate fused GPU kernels that combine multiple operations at compile time. This approach eliminates manual kernel development and pre-compilation while maximizing GPU resource utilization and keeping intermediate data in on-chip memory. Experimental results show performance improvements from 2x to over 1000x compared to traditional GPU libraries while maintaining high-level programmability.

- **[arXiv2509] Real-Time Analysis of Unstructured Data with Machine Learning on
  Heterogeneous Architectures**
  - **tags:** [mlsys], [Other models inference], [real-time processing, graph neural networks, heterogeneous architectures, FPGA acceleration, GPU implementation, particle physics, LHCb experiment]
  - **authors:** Fotis I. Giasemis
  - **institution:** CERN
  - **link:** http://arxiv.org/pdf/2508.07423v3
  - **Simple LLM Summary:** This paper presents a graph neural network-based pipeline for charged particle track reconstruction implemented on GPUs and FPGAs at the LHCb experiment. The system was deployed end-to-end in the first-level trigger and compared against classical tracking algorithms. The research demonstrates efficient machine learning deployment in high-frequency real-time environments while optimizing throughput and energy consumption.

## 2025-09-07

- **[arXiv2509] DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across
  Thousands of Computers**
  - **tags:** [mlsys], [Other models inference], [distributed systems, vector search, approximate nearest neighbor, graph index, scalability]
  - **authors:** Philip Adams, Menghao Li, Shi Zhang, Li Tan, Qi Chen, Mingqin Li, Zengzhong Li, Knut Risvik, Harsha Vardhan Simhadri
  - **institution:** Microsoft
  - **link:** http://arxiv.org/pdf/2509.06046v1
  - **Simple LLM Summary:** DISTRIBUTEDANN presents a distributed vector search service using a distributed key-value store and in-memory ANN index to scale a single graph across thousands of machines. It achieves 26ms median query latency and over 100,000 QPS on a 50 billion vector index, making it 6x more efficient than existing approaches. The system has been successfully deployed in the Bing search engine, replacing conventional scale-out architectures.
