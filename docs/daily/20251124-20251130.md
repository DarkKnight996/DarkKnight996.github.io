# 20251124-20251130

## 2025-11-24

**cs.DC total: 4**

- **[arXiv251124] Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption**
  - **tags:** [sys], [cloud computing], [Stochastic Reward Nets, anomaly detection, precision, recall, inspection frequency]
  - **authors:** Gabriel Job Antunes Grabher, Fumio Machida, Thomas Ropars
  - **institution:** Université Grenoble-Alpes, University of Tsukuba
  - **link:** https://arxiv.org/pdf/2511.17119
  - **Simple LLM Summary:** This paper uses Stochastic Reward Nets to model cloud services with performance anomaly detection. The study analyzes how detector characteristics like precision, recall, and inspection frequency affect latency and resource consumption. The main finding is that high precision alone suffices for good performance-cost trade-off with frequent detection, while recall becomes more important with infrequent detection.

- **[arXiv251124] Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems**
  - **tags:** [mlsys], [GPU kernels], [multi-agent systems, LLM-based optimization, PyTorch inference, explore-exploit tradeoff, error-fixing agents]
  - **authors:** Kirill Nagaitsev, Luka Grbcic, Samuel Williams, Costin Iancu
  - **institution:** Northwestern University, Lawrence Berkeley National Laboratory
  - **link:** https://arxiv.org/pdf/2511.16964
  - **Simple LLM Summary:** This paper proposes using LLM-based multi-agent systems to optimize PyTorch inference performance on GPUs. The research shows that exploit-heavy strategies combined with error-fixing agents achieve the best results, with the optimal implementation delivering an average 2.88x speedup on H100 GPUs across diverse machine learning tasks.

- **[arXiv251124] MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling**
  - **tags:** [mlsys], [llm training], [token scheduling, expert parallelism, micro-batch optimization]
  - **authors:** Chenqi Zhao, Wenfei Wu, Linhai Song, Yuchen Xu
  - **institution:** Peking University, Institute of Computing Technology, Chinese Academy of Sciences
  - **link:** https://arxiv.org/pdf/2511.16947
  - **Simple LLM Summary:** The paper proposes MicroEP, a parallelization strategy that achieves fine-grained load balancing in Mixture-of-Experts systems through token scheduling across GPUs. Experimental results show that their MicroMoE system improves training throughput by up to 47.6% compared to state-of-the-art systems while maintaining optimal load balance among GPUs.

- **[arXiv251124] Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design**
  - **tags:** [mlsys], [llm training], [mixture-of-experts, transformer, context parallelism, compressed convolutional attention, fault-tolerance, checkpoint-reshaping, microbenchmarks]
  - **authors:** Quentin Anthony, Yury Tokpanov, Skyler Szot, Srivatsan Rajagopal, Praneeth Medepalli, Rishi Iyer, Vasu Shyam, Anna Golubeva, Ansh Chaurasia, Xiao Yang, Tomas Figliolia, Robert Washbourne, Drew Thorstensen, Amartey Pearson, Zack Grossbart, Jason van Patten, Emad Barsoum, Zhenyu Gu, Yao Fu, Beren Millidge
  - **institution:** Zyphra, IBM, AMD
  - **link:** https://arxiv.org/pdf/2511.17127
  - **Simple LLM Summary:** This paper presents the first large-scale mixture-of-experts pretraining study on pure AMD hardware using MI300X GPUs with Pollara interconnect. The authors developed ZAYA1-base model with optimized transformer sizing rules and system design including fault-tolerance mechanisms and detailed training recipes. The results demonstrate that AMD's hardware, networking, and software stack are mature enough for competitive large-scale pretraining, achieving performance comparable to leading base models.


**cs.AI/cs.LG contains "reinforcement learning" total: 11**
- [arXiv251124] Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems [link](https://arxiv.org/pdf/2511.17435)
- [arXiv251124] CroTad: A Contrastive Reinforcement Learning Framework for Online Trajectory Anomaly Detection [link](https://arxiv.org/pdf/2511.16929)
- [arXiv251124] Dissecting Quantum Reinforcement Learning: A Systematic Evaluation of Key Components [link](https://arxiv.org/pdf/2511.17112)
- [arXiv251124] Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving [link](https://arxiv.org/pdf/2511.16916)
- [arXiv251124] Convergence and stability of Q-learning in Hierarchical Reinforcement Learning [link](https://arxiv.org/pdf/2511.17351)
- [arXiv251124] Predicting Talent Breakout Rate using Twitter and TV data [link](https://arxiv.org/pdf/2511.16905)
- [arXiv251124] MIR: Efficient Exploration in Episodic Multi-Agent Reinforcement Learning via Mutual Intrinsic Reward [link](https://arxiv.org/pdf/2511.17165)
- [arXiv251124] FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle [link](https://arxiv.org/pdf/2511.17171)
- [arXiv251124] R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability [link](https://arxiv.org/pdf/2511.17367)
- [arXiv251124] Intrinsic preservation of plasticity in continual quantum learning [link](https://arxiv.org/pdf/2511.17228)
- [arXiv251124] Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization [link](https://arxiv.org/pdf/2511.17489)

**cs.AI/cs.LG contains "accelerate" total: 8**
- [arXiv251124] Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration [link](https://arxiv.org/pdf/2511.17123)
- [arXiv251124] BITS for GAPS: Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates [link](https://arxiv.org/pdf/2511.16815)
- [arXiv251124] Generating transition states of chemical reactions via distance-geometry-based flow matching [link](https://arxiv.org/pdf/2511.17229)
- [arXiv251124] Stable Coresets via Posterior Sampling: Aligning Induced and Full Loss Landscapes [link](https://arxiv.org/pdf/2511.17399)
- [arXiv251124] Mesh RAG: Retrieval Augmentation for Autoregressive Mesh Generation [link](https://arxiv.org/pdf/2511.16807)
- [arXiv251124] Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation [link](https://arxiv.org/pdf/2511.16757)
- [arXiv251124] Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search [link](https://arxiv.org/pdf/2511.16681)
- [arXiv251124] Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions in Image Generation [link](https://arxiv.org/pdf/2511.17031)

## 2025-11-25

**cs.DC total: 18**

- **[arXiv251125] A novel strategy for multi-resource load balancing in agent-based systems**
  - **tags:** [ai], [multi-agent systems], [load balancing, multi-resource management, agent self-assessment, social behavior modeling, adaptation abilities]
  - **authors:** Leszek Sliwko, Aleksander Zgrzywa
  - **institution:** Wroclaw University of Technology
  - **link:** https://arxiv.org/pdf/2511.17580
  - **Simple LLM Summary:** This paper presents a multi-resource load balancing strategy that utilizes agent social behavior and adaptation capabilities to optimize enterprise system architectures. The approach enables agents to perform self-assessment for determining optimal configurations. Experimental results demonstrate the effectiveness of the implemented agent system for load balancing tasks.

- **[arXiv251125] Simulating Dynamic Cloud Marketspaces: Modeling Spot Instance Behavior and Scheduling with CloudSim Plus**
  - **tags:** [sys], [cloud resource management], [CloudSim Plus, spot instances, HLEM-VMP, Google Cluster Trace, virtual machine allocation]
  - **authors:** Christoph Goldgruber, Benedikt Pittl, Erich Schikuta
  - **institution:** University of Vienna
  - **link:** https://arxiv.org/pdf/2511.18137
  - **Simple LLM Summary:** This paper extends the CloudSim Plus simulation framework to model spot instance lifecycle management and adapts the HLEM-VMP allocation algorithm for dynamic cloud markets. The enhanced framework demonstrates reduced spot instance interruptions and shorter maximum interruption durations compared to baseline strategies. This contributes to more robust and cost-effective resource management in volatile cloud computing environments.

- **[arXiv251125] Pier: Efficient Large Language Model pretraining with Relaxed Global Communication**
  - **tags:** [mlsys], [llm training], [momentum warmup, momentum decay, data parallel, tensor parallel, DiLoCo, global communication optimization]
  - **authors:** Shuyuan Fan, Zhao Zhang
  - **institution:** Rutgers University
  - **link:** https://arxiv.org/pdf/2511.17849
  - **Simple LLM Summary:** Pier introduces an efficient LLM pretraining optimizer that reduces global communication bottlenecks through momentum warmup and momentum decay techniques. The system achieves significant speedups (up to 3.7x on 256 A100s) while maintaining model performance across GPT model variants. Experimental results demonstrate Pier's effectiveness with various parallelization strategies including data parallel and tensor parallel configurations.

- **[arXiv251125] Monotone Decontamination of Arbitrary Dynamic Graphs with Mobile Agents**
  - **tags:** [sys], [network algorithms], [mobile agents, monotone decontamination, dynamic graphs, edge-search, node-search, mixed-search]
  - **authors:** Rajashree Bar, Daibik Barik, Adri Bhattacharya, Partha Sarathi Mandal
  - **institution:** Indian Institute of Technology Guwahati, Indian Statistical Institute
  - **link:** https://arxiv.org/pdf/2511.18315
  - **Simple LLM Summary:** This paper studies monotone decontamination in arbitrary dynamic graphs using mobile agents, proposing two models based on edge reappearance timing. The authors establish both lower and upper bounds on the number of agents required for complete decontamination while maintaining monotonicity. The results demonstrate the challenges posed by dynamic edge changes and optimize agent requirements for network decontamination.

- **[arXiv251125] Root Cause Analysis for Microservice Systems via Cascaded Conditional Learning with Hypergraphs**
  - **tags:** [mlsys], [fault-tolerance], [cascaded conditional learning, hypergraph modeling, root cause localization, failure type identification]
  - **authors:** Shuaiyu Xie, Hanbin He, Jian Wang, Bing Li
  - **institution:** Wuhan University, Zhongguancun Laboratory
  - **link:** https://arxiv.org/pdf/2511.17566
  - **Simple LLM Summary:** This paper proposes CCLH, a root cause analysis framework that uses cascaded conditional learning and heterogeneous hypergraphs to model group-level relationships between microservice instances. The method addresses limitations in existing approaches by capturing causal dependencies between diagnostic tasks and simulating failure propagation. Experimental results show CCLH outperforms state-of-the-art methods in both root cause localization and failure type identification.

- **[arXiv251125] ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning**
  - **tags:** [mlsys], [llm training], [federated learning, decentralized federated learning, low-rank adaptation, alternating optimization, parameter-efficient fine-tuning]
  - **authors:** Xiaoyu Wang, Xiaotian Li, Zhixiang Zhou, Chen Li, Yong Liu
  - **institution:** New York University
  - **link:** https://arxiv.org/pdf/2511.18291
  - **Simple LLM Summary:** This paper introduces ADF-LoRA, a decentralized federated learning method that alternates updates between low-rank matrices and mixes both matrices to maintain parameter consistency. The approach addresses challenges of phase-state mismatch and block-wise divergence in peer-to-peer communication settings. Experimental results show ADF-LoRA achieves faster convergence and higher accuracy than existing LoRA variants in decentralized federated learning.

- **[arXiv251125] Comparative Analysis of Large Language Model Inference Serving Systems: A Performance Study of vLLM and HuggingFace TGI**
  - **tags:** [mlsys], [llm inference], [PagedAttention, throughput optimization, latency optimization, GPU memory utilization, batch processing]
  - **authors:** Saicharan Kolluru
  - **institution:** Independent researcher
  - **link:** https://arxiv.org/pdf/2511.17593
  - **Simple LLM Summary:** This paper compares two LLM serving frameworks - vLLM and HuggingFace TGI - through empirical evaluation of throughput, latency, and resource utilization. The study found that vLLM achieves significantly higher throughput using its novel PagedAttention mechanism, while TGI performs better for latency-sensitive interactive applications. The choice between frameworks should depend on specific workload requirements, with vLLM excelling in high-throughput scenarios and TGI better for low-latency use cases.

- **[arXiv251125] MIDAS: Adaptive Proxy Middleware for Mitigating Metadata Hotspots in HPC I/O at Scale**
  - **tags:** [sys], [HPC I/O optimization], [adaptive middleware, consistent hashing, power-of-d sampling, cooperative caching, self-stabilizing control loop, namespace-aware load balancing]
  - **authors:** Sangam Ghimire, Nigam Niraula, Nirjal Bhurtel, Paribartan Timalsina, Bishal Neupane, James Bhattarai, Sudan Jha
  - **institution:** Kathmandu University
  - **link:** https://arxiv.org/pdf/2511.18124
  - **Simple LLM Summary:** MIDAS introduces an adaptive proxy middleware that uses namespace-aware load balancing, cooperative caching, and self-stabilizing control loops to mitigate metadata hotspots in HPC systems. The system operates transparently between clients and metadata servers without requiring kernel or backend modifications. Experimental results show it reduces average queue lengths by 23% and worst-case hotspots by up to 80% compared to round-robin scheduling.

- **[arXiv251125] AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems**
  - **tags:** [mlsys], [multi-modal inference], [split computing, dual-stream architecture, adaptive compression, embodied self-awareness, edge-cloud computing]
  - **authors:** Rajat Bhattacharjya, Sing-Yao Wu, Hyunwoo Oh, Chaewon Nam, Suyeon Koo, Mohsen Imani, Elaheh Bozorgzadeh, Nikil Dutt
  - **institution:** University of California, Irvine, Kookmin University
  - **link:** https://arxiv.org/pdf/2511.18151
  - **Simple LLM Summary:** AVERY introduces an adaptive split computing framework for Vision-Language Models that uses a cognitive-inspired dual-stream approach separating real-time context processing from deep analysis. The system dynamically adjusts compression based on network conditions and operator intent through a lightweight onboard controller. This approach achieves 11.2% higher accuracy than raw compression and 93.98% lower energy consumption than full-edge execution, enabling efficient VLM deployment on resource-constrained UAVs in disaster scenarios.

- **[arXiv251125] SAGkit: A Python SAG Toolkit for Response Time Analysis of Hybrid-Triggered Jobs**
  - **tags:** [sys], [real-time systems], [schedule-abstraction graph, response-time analysis, hybrid-triggered jobs, non-preemptive systems]
  - **authors:** Ruide Cao, Zhuyun Qi, Qinyang He, Chenxi Ling, Yi Wang, Guoming Tang
  - **institution:** SUSTech, Tsinghua University, Nankai University, Peng Cheng Laboratory, The Hong Kong University of Science and Technology (Guangzhou)
  - **link:** https://arxiv.org/pdf/2511.17882
  - **Simple LLM Summary:** This paper introduces SAGkit, a Python toolkit implementing the schedule-abstraction graph framework for exact response-time analysis of hybrid-triggered jobs in distributed control systems. The toolkit addresses limitations of conventional methods by allowing job absence modeling and handling release jitter and execution time variations. Experimental results show SAGkit achieves exact timing analysis with acceptable runtime and memory overhead.

- **[arXiv251125] Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration**
  - **tags:** [mlsys], [GPU kernels], [low-rank approximation, FP8 precision, SVD, randomized SVD, memory bandwidth optimization]
  - **authors:** Alfredo Metere
  - **institution:** Metere Consulting, LLC
  - **link:** https://arxiv.org/pdf/2511.18674
  - **Simple LLM Summary:** This paper introduces Low-Rank GEMM, which uses low-rank matrix approximations with FP8 precision to achieve sub-quadratic complexity in matrix multiplication. The system automatically selects optimal decomposition methods and precision levels based on hardware capabilities. On NVIDIA RTX 4090, it achieves up to 378 TFLOPS with 75% memory savings and 7.8× speedup over traditional approaches for large matrices.

- **[arXiv251125] CycleSL: Server-Client Cyclical Update Driven Scalable Split Learning**
  - **tags:** [mlsys], [others], [split learning, cyclical updates, feature resampling, alternating block coordinate descent, aggregation-free training]
  - **authors:** Mengdi Wang, Efe Bozkir, Enkelejda Kasneci
  - **institution:** Technical University of Munich, Munich Center for Machine Learning (MCML)
  - **link:** https://arxiv.org/pdf/2511.18611
  - **Simple LLM Summary:** CycleSL introduces a cyclical update framework for split learning where the server model is optimized first using resampled client features, followed by client updates using the updated server. This aggregation-free approach addresses scalability issues and performance degradation in existing split learning methods. Empirical results demonstrate improved model performance across multiple datasets with non-IID data distribution and partial client participation.

- **[arXiv251125] Federated style aware transformer aggregation of representations**
  - **tags:** [mlsys], [others], [personalized federated learning, disentangled representation, transformer attention, prototype aggregation, style-content separation]
  - **authors:** Mincheol Jeon, Euinam Huh
  - **institution:** Kyunghee University
  - **link:** https://arxiv.org/pdf/2511.18841
  - **Simple LLM Summary:** The paper proposes FedSTAR, a federated learning framework that disentangles client-specific style factors from shared content representations and uses Transformer-based attention for prototype aggregation. This approach reduces communication overhead by exchanging compact prototypes and style vectors instead of full model parameters. Experimental results show improved personalization and robustness in heterogeneous environments without increasing communication costs.

- **[arXiv251125] An Online Fragmentation-Aware GPU Scheduler for Multi-Tenant MIG-based Clouds**
  - **tags:** [mlsys], [cluster infrastructure], [multi-instance GPU, fragmentation-aware scheduling, greedy algorithm, resource allocation, GPU partitioning]
  - **authors:** Marco Zambianco, Lorenzo Fasol, Roberto Doriguzzi-Corin
  - **institution:** Fondazione Bruno Kessler (FBK)
  - **link:** https://arxiv.org/pdf/2511.18906
  - **Simple LLM Summary:** The paper proposes a fragmentation-aware GPU scheduling framework for MIG-based cloud environments that uses a fragmentation metric and greedy algorithm to minimize resource inefficiency. The method achieves higher workload acceptance rates compared to baseline strategies, increasing scheduled workloads by an average of 10% under heavy load conditions while using similar numbers of GPUs.

- **[arXiv251125] AME: An Efficient Heterogeneous Agentic Memory Engine for Smartphones**
  - **tags:** [mlsys], [others], [vector database, hardware-aware matrix pipeline, workload-aware scheduling, on-chip memory optimization, concurrent query processing]
  - **authors:** Xinkui Zhao, Qingyu Ma, Yifan Zhang, Hengxuan Lou, Guanjie Cheng, Shuiguang Deng, Jianwei Yin
  - **institution:** Zhejiang University
  - **link:** https://arxiv.org/pdf/2511.19192
  - **Simple LLM Summary:** AME is an on-device agentic memory engine that introduces a hardware-aware matrix pipeline and workload-aware scheduling scheme to optimize vector database operations on smartphones. The system addresses mobile SoC constraints and continuous learning workloads through efficient compute utilization and coordinated task management. Experimental results show significant improvements in query throughput, index construction speed, and insertion performance compared to existing approaches.

- **[arXiv251125] N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory**
  - **tags:** [mlsys], [cluster infrastructure], [branch-and-bound, distributed computing, sliding-window algorithm, MPI, deterministic solving]
  - **authors:** Longfei Wang, Junyan Liu, Fan Zhang, Jiangwen Wei, Yuanhua Tang, Jie Sun, Xiaodong Luo
  - **institution:** Shenzhen Research Institute of Big Data, Huawei Technologies Co., Ltd., Chinese University of Hong Kong, Shenzhen
  - **link:** https://arxiv.org/pdf/2511.18723
  - **Simple LLM Summary:** The paper proposes N2N, a parallel framework that maps branch-and-bound nodes to distributed computing nodes for solving large-scale MILP problems. It supports both deterministic and nondeterministic modes and integrates with existing solvers like SCIP and HiGHS. Experimental results show N2N-SCIP achieves significant speedups over ParaSCIP, demonstrating superior performance in distributed memory environments.

- **[arXiv251125] Constant-Size Certificates for Leader Election in Chordal Graphs and Related Classes**
  - **tags:** [sys], [distributed computing], [local certification, proof labelling schemes, self-stabilizing algorithms, chordal graphs, dismantlable graphs, leader election, spanning tree construction]
  - **authors:** Jérémie Chalopin, Maria Kokkou
  - **institution:** Aix Marseille University, CNRS, LIS
  - **link:** https://arxiv.org/pdf/2511.19208
  - **Simple LLM Summary:** This paper presents constant-size local certification schemes for leader election in chordal and K4-free dismantlable graphs, and for spanning tree construction in dismantlable graphs. The certification enables each node to verify solution correctness using only local neighborhood information. Additionally, the authors propose a method to transform any certification scheme into a silent self-stabilizing algorithm by adding just one extra state.

- **[arXiv251125] IOMMU Support for Virtual-Address Remote DMA in an ARMv8 environment**
  - **tags:** [sys], [computer architecture], [IOMMU, SMMU, DMA, virtual address translation, kernel modules, ARMv8]
  - **authors:** Antonis Psistakis
  - **institution:** University of Crete, Foundation for Research and Technology - Hellas
  - **link:** https://arxiv.org/pdf/2511.19258
  - **Simple LLM Summary:** This thesis implemented custom kernel modules to test ARM's System Memory Management Unit (SMMU) for virtual-to-physical address translation in DMA operations. The research successfully demonstrated SMMU functionality by mapping virtual addresses and configuring page tables for dynamic translation. The work confirmed that the IOMMU approach can support virtual-address remote DMA in ARMv8 environments.


**cs.AI/cs.LG contains "reinforcement learning" total: 47**
- [arXiv251125] Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement Learning [link](https://arxiv.org/pdf/2511.17598)
- [arXiv251125] PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning [link](https://arxiv.org/pdf/2511.17927)
- [arXiv251125] LLM Reasoning for Cold-Start Item Recommendation [link](https://arxiv.org/pdf/2511.18261)
- [arXiv251125] Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction [link](https://arxiv.org/pdf/2511.17879)
- [arXiv251125] MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning [link](https://arxiv.org/pdf/2511.18181)
- [arXiv251125] Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning Platform for Individual Behavioral Learning [link](https://arxiv.org/pdf/2511.18000)
- [arXiv251125] Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change [link](https://arxiv.org/pdf/2511.17630)
- [arXiv251125] IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment [link](https://arxiv.org/pdf/2511.18055)
- [arXiv251125] Enhancing Robustness of Offline Reinforcement Learning Under Data Corruption via Sharpness-Aware Minimization [link](https://arxiv.org/pdf/2511.17568)
- [arXiv251125] Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch [link](https://arxiv.org/pdf/2511.17826)
- [arXiv251125] A New Error Temporal Difference Algorithm for Deep Reinforcement Learning in Microgrid Optimization [link](https://arxiv.org/pdf/2511.18093)
- [arXiv251125] Multi-Value Alignment for LLMs via Value Decorrelation and Extrapolation [link](https://arxiv.org/pdf/2511.17579)
- [arXiv251125] Dialogue Diplomats: An End-to-End Multi-Agent Reinforcement Learning System for Automated Conflict Resolution and Consensus Building [link](https://arxiv.org/pdf/2511.17654)
- [arXiv251125] Deep Gaussian Process Proximal Policy Optimization [link](https://arxiv.org/pdf/2511.18214)
- [arXiv251125] AURA: Adaptive Unified Reasoning and Automation with LLM-Guided MARL for NextG Cellular Networks [link](https://arxiv.org/pdf/2511.17506)
- [arXiv251125] A Novel and Practical Universal Adversarial Perturbations against Deep Reinforcement Learning based Intrusion Detection Systems [link](https://arxiv.org/pdf/2511.18223)
- [arXiv251125] Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization [link](https://arxiv.org/pdf/2511.17963)
- [arXiv251125] Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently [link](https://arxiv.org/pdf/2511.17852)
- [arXiv251125] Tail Distribution of Regret in Optimistic Reinforcement Learning [link](https://arxiv.org/pdf/2511.18247)
- [arXiv251125] SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization [link](https://arxiv.org/pdf/2511.17938)
- [arXiv251125] LEARN: Learning End-to-End Aerial Resource-Constrained Multi-Robot Navigation [link](https://arxiv.org/pdf/2511.17765)
- [arXiv251125] Smart Manufacturing: MLOps-Enabled Event-Driven Architecture for Enhanced Control in Steel Production [link](https://arxiv.org/pdf/2511.17632)
- [arXiv251125] Training Emergent Joint Associations: A Reinforcement Learning Approach to Creative Thinking in Language Models [link](https://arxiv.org/pdf/2511.17876)
- [arXiv251125] Boosting Reinforcement Learning in 3D Visuospatial Tasks Through Human-Informed Curriculum Design [link](https://arxiv.org/pdf/2511.17595)
- [arXiv251125] A Reinforcement Learning Framework for Resource Allocation in Uplink Carrier Aggregation in the Presence of Self Interference [link](https://arxiv.org/pdf/2511.17931)
- [arXiv251125] General Agentic Memory Via Deep Research [link](https://arxiv.org/pdf/2511.18423)
- [arXiv251125] ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints [link](https://arxiv.org/pdf/2511.18450)
- [arXiv251125] How to Train Your Latent Control Barrier Function: Smooth Safety Filtering Under Hard-to-Model Constraints [link](https://arxiv.org/pdf/2511.18606)
- [arXiv251125] Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition [link](https://arxiv.org/pdf/2511.18671)
- [arXiv251125] Reinforcement Learning for Self-Healing Material Systems [link](https://arxiv.org/pdf/2511.18728)
- [arXiv251125] ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion [link](https://arxiv.org/pdf/2511.18742)
- [arXiv251125] Periodic Asynchrony: An Effective Method for Accelerating On-Policy Reinforcement Learning [link](https://arxiv.org/pdf/2511.18871)
- [arXiv251125] Accelerating Reinforcement Learning via Error-Related Human Brain Signals [link](https://arxiv.org/pdf/2511.18878)
- [arXiv251125] Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation [link](https://arxiv.org/pdf/2511.18958)
- [arXiv251125] FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning [link](https://arxiv.org/pdf/2511.18977)
- [arXiv251125] Dynamic Mixture of Experts Against Severe Distribution Shifts [link](https://arxiv.org/pdf/2511.18987)
- [arXiv251125] RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning [link](https://arxiv.org/pdf/2511.19168)
- [arXiv251125] Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization [link](https://arxiv.org/pdf/2511.19218)
- [arXiv251125] MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization [link](https://arxiv.org/pdf/2511.19253)
- [arXiv251125] Leveraging LLMs for reward function design in reinforcement learning control tasks [link](https://arxiv.org/pdf/2511.19355)
- [arXiv251125] LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems [link](https://arxiv.org/pdf/2511.19368)
- [arXiv251125] DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research [link](https://arxiv.org/pdf/2511.19399)
- [arXiv251125] Learning Robust Social Strategies with Large Language Models [link](https://arxiv.org/pdf/2511.19405)
- [arXiv251125] SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning [link](https://arxiv.org/pdf/2511.19422)
- [arXiv251125] Prequential posteriors [link](https://arxiv.org/pdf/2511.17721)
- [arXiv251125] On a Reinforcement Learning Methodology for Epidemic Control, with application to COVID-19 [link](https://arxiv.org/pdf/2511.18035)
- [arXiv251125] Reinforcement Learning for Portfolio Optimization with a Financial Goal and Defined Time Horizons [link](https://arxiv.org/pdf/2511.18076)

**cs.AI/cs.LG contains "accelerate" total: 27**
- [arXiv251125] Learning Rate Scheduling with Matrix Factorization for Private Training [link](https://arxiv.org/pdf/2511.17994)
- [arXiv251125] Periodicity-Enforced Neural Network for Designing Deterministic Lateral Displacement Devices [link](https://arxiv.org/pdf/2511.17754)
- [arXiv251125] Gate-level boolean evolutionary geometric attention neural networks [link](https://arxiv.org/pdf/2511.17550)
- [arXiv251125] FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning [link](https://arxiv.org/pdf/2511.17885)
- [arXiv251125] scipy.spatial.transform: Differentiable Framework-Agnostic 3D Transformations in Python [link](https://arxiv.org/pdf/2511.18157)
- [arXiv251125] Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture for Scientific Discovery [link](https://arxiv.org/pdf/2511.18298)
- [arXiv251125] Energy-based Autoregressive Generation for Neural Population Dynamics [link](https://arxiv.org/pdf/2511.17606)
- [arXiv251125] Accelerating Time Series Foundation Models with Speculative Decoding [link](https://arxiv.org/pdf/2511.18191)
- [arXiv251125] ProHD: Projection-Based Hausdorff Distance Approximation [link](https://arxiv.org/pdf/2511.18207)
- [arXiv251125] A Multidisciplinary Design and Optimization (MDO) Agent Driven by Large Language Models [link](https://arxiv.org/pdf/2511.17511)
- [arXiv251125] Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction [link](https://arxiv.org/pdf/2511.18150)
- [arXiv251125] Reduced-Basis Deep Operator Learning for Parametric PDEs with Independently Varying Boundary and Source Data [link](https://arxiv.org/pdf/2511.18260)
- [arXiv251125] NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations [link](https://arxiv.org/pdf/2511.18793)
- [arXiv251125] FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories [link](https://arxiv.org/pdf/2511.18834)
- [arXiv251125] Accelerating Reinforcement Learning via Error-Related Human Brain Signals [link](https://arxiv.org/pdf/2511.18878)
- [arXiv251125] Skeletons Matter: Dynamic Data Augmentation for Text-to-Query [link](https://arxiv.org/pdf/2511.18934)
- [arXiv251125] Understanding, Accelerating, and Improving MeanFlow Training [link](https://arxiv.org/pdf/2511.19065)
- [arXiv251125] Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks [link](https://arxiv.org/pdf/2511.19265)
- [arXiv251125] CDLM: Consistency Diffusion Language Models For Faster Sampling [link](https://arxiv.org/pdf/2511.19269)
- [arXiv251125] Tiny-TSM: Efficiently Training a Lightweight SOTA Time Series Foundation Model [link](https://arxiv.org/pdf/2511.19272)
- [arXiv251125] LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems [link](https://arxiv.org/pdf/2511.19368)
- [arXiv251125] Flow Map Distillation Without Data [link](https://arxiv.org/pdf/2511.19428)
- [arXiv251125] Physics-informed Neural Operator Learning for Nonlinear Grad-Shafranov Equation [link](https://arxiv.org/pdf/2511.19114)
- [arXiv251125] TorchQuantumDistributed [link](https://arxiv.org/pdf/2511.19291)
- [arXiv251125] High-throughput validation of phase formability and simulation accuracy of Cantor alloys [link](https://arxiv.org/pdf/2511.19335)
- [arXiv251125] Artificial Intelligence Driven Workflow for Accelerating Design of Novel Photosensitizers [link](https://arxiv.org/pdf/2511.19347)
- [arXiv251125] Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design [link](https://arxiv.org/pdf/2511.19423)
