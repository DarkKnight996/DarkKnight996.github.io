# 20250929-20251005

## 2025-09-29

- **[arXiv2509] Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs**
  - **tags:** [mlsys], [Other models inference], [FPGA acceleration, Vision GNNs, Dynamic Image Graph Construction, hardware optimization, parallel sorting]
  - **authors:** Anvitha Ramachandran, Dhruv Parikh, Viktor Prasanna
  - **institution:** University of Southern California
  - **link:** http://arxiv.org/pdf/2509.25121v1
  - **Simple LLM Summary:** The paper proposes a streaming FPGA accelerator for Dynamic Image Graph Construction in Vision GNNs, using on-chip buffers and parallel sorting to process features efficiently. This design minimizes memory traffic and achieves significant speedups over CPU and GPU baselines. The modular architecture supports various image resolutions and ViG model variants while maintaining high performance.

- **[arXiv2509] Context-Driven Performance Modeling for Causal Inference Operators on
  Neural Processing Units**
  - **tags:** [mlsys], [LLM inference], [NPU performance analysis, causal inference operators, attention mechanisms, edge computing, hardware-aware optimization]
  - **authors:** Neelesh Gupta, Rakshith Jayanth, Dhruv Parikh, Viktor Prasanna
  - **institution:** University of Southern California
  - **link:** http://arxiv.org/pdf/2509.25155v1
  - **Simple LLM Summary:** This paper conducts comprehensive performance analysis of causal inference operators including quadratic attention and sub-quadratic alternatives on Neural Processing Units. The study benchmarks various attention mechanisms and identifies that quadratic attention becomes memory-bound with cache inefficiency, while sub-quadratic models can become compute-bound on vector cores. These findings provide insights for co-designing hardware-aware models and optimization strategies for on-device long-context inference.

- **[arXiv2509] SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in
  Long-Context LLM Serving**
  - **tags:** [mlsys], [LLM inference], [dynamic sparse attention, KV cache management, hierarchical storage, HBM-DRAM optimization, long-context LLM serving]
  - **authors:** Qihui Zhou, Peiqi Yin, Pengfei Zuo, James Cheng
  - **institution:** The Chinese University of Hong Kong, Huawei Cloud
  - **link:** http://arxiv.org/pdf/2509.24626v1
  - **Simple LLM Summary:** SparseServe introduces an efficient hierarchical HBM-DRAM management system for dynamic sparse attention in long-context LLM serving. It addresses KV cache fragmentation, HBM contention, and prefill demands through fragmentation-aware transfers, working-set-aware batching, and layer-segmented prefill. The system achieves significantly lower latency and higher throughput compared to state-of-the-art serving systems.

- **[arXiv2509] Intent-Driven Storage Systems: From Low-Level Tuning to High-Level
  Understanding**
  - **tags:** [mlsys], [LLM inference], [storage systems, intent-driven optimization, parameter configuration, workload adaptation]
  - **authors:** Shai Bergman, Won Wook Song, Lukas Cavigelli, Konstantin Berestizshevsky, Ke Zhou, Ji Zhang
  - **institution:** Huawei Zurich Research Center, Huazhong University of Science and Technology
  - **link:** http://arxiv.org/pdf/2510.15917v1
  - **Simple LLM Summary:** The paper proposes Intent-Driven Storage Systems (IDSS) that use large language models to infer workload intent from unstructured signals and generate adaptive storage configurations. Experimental results show IDSS can improve IOPS by up to 2.45x through optimized caching and prefetching configurations. This demonstrates LLMs can effectively bridge application semantics with low-level system control when properly constrained within policy guardrails.

- **[arXiv2509] HAPT: Heterogeneity-Aware Automated Parallel Training on Heterogeneous
  Clusters**
  - **tags:** [mlsys], [cluster infrastructure], [heterogeneous clusters, automated parallel training, inter-operator parallelism, 1F1B scheduler, computation-communication overlap]
  - **authors:** Antian Liang, Zhigang Zhao, Kai Zhang, Xuri Shi, Chuantao Li, Chunxiao Wang, Zhenying He, Yinan Jing, X. Sean Wang
  - **institution:** Fudan University, Shandong Computer Science Center (National Supercomputer Center in Jinan)
  - **link:** http://arxiv.org/pdf/2509.24859v1
  - **Simple LLM Summary:** Hapt introduces a fine-grained planner for inter-operator parallel strategy search and a heterogeneity-aware 1F1B scheduler to optimize distributed training on heterogeneous clusters. The framework achieves balanced workload distribution and maximizes computation-communication overlap with minimal memory overhead. Evaluation shows Hapt delivers 1.3x-1.6x higher performance than state-of-the-art training frameworks on heterogeneous clusters.

- **[arXiv2509] From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts
  Routing**
  - **tags:** [mlsys], [LLM inference], [mixture-of-experts, load balancing, routing algorithm, inference optimization, plug-and-play]
  - **authors:** Rana Shahout, Colin Cai, Yilun Du, Minlan Yu, Michael Mitzenmacher
  - **institution:** Harvard University, University of California, Berkeley
  - **link:** http://arxiv.org/pdf/2510.03293v1
  - **Simple LLM Summary:** LASER is a plug-and-play inference-time routing algorithm that balances expert load in Mixture-of-Experts models by adapting to gate score distributions. It routes tokens to strongest experts when scores show clear preference, and expands to less-loaded experts when scores are uniform. The method improves load balancing, reduces latency, and increases throughput while maintaining accuracy, without requiring model retraining or fine-tuning.

- **[arXiv2509] Asynchronous Policy Gradient Aggregation for Efficient Distributed
  Reinforcement Learning**
  - **tags:** [mlsys], [Other models training], [distributed reinforcement learning, policy gradient methods, asynchronous computation, communication efficiency, heterogeneous environments]
  - **authors:** Alexander Tyurin, Andrei Spiridonov, Varvara Rudenko
  - **institution:** Opta
  - **link:** http://arxiv.org/pdf/2509.24305v1
  - **Simple LLM Summary:** This paper introduces two new algorithms, Rennala NIGT and Malenia NIGT, for asynchronous policy gradient aggregation in distributed reinforcement learning. Rennala NIGT improves computational and communication complexity in homogeneous settings, while Malenia NIGT handles heterogeneous environments with better theoretical guarantees. Experimental results show both methods significantly outperform prior approaches.

- **[arXiv2509] RServe: Overlapping Encoding and Prefill for Efficient LMM Inference**
  - **tags:** [mlsys], [LLM inference], [multimodal models, inference serving, scheduling, parallelism, latency optimization]
  - **authors:** Tianyu Guo, Tianming Xu, Xianjie Chen, Junru Chen, Nong Xiao, Xianwei Zhang
  - **institution:** Sun Yat-sen University, Rednote
  - **link:** http://arxiv.org/pdf/2509.24381v1
  - **Simple LLM Summary:** REDServe proposes a novel scheduling strategy that overlaps multimodal encoding with language model computation within requests and balances computational loads across requests using schedulable tokens and token budgets. The system achieves up to 66% latency reduction and 109% throughput improvement compared to existing approaches. This demonstrates significant performance gains in large multimodal model inference serving through improved intra- and inter-request parallelism.

- **[arXiv2509] Experience Deploying Containerized GenAI Services at an HPC Center**
  - **tags:** [mlsys], [LLM inference], [containerization, HPC, Kubernetes, vLLM, converged computing, reproducibility]
  - **authors:** Angel M. Beltre, Jeff Ogden, Kevin Pedretti
  - **institution:** Sandia National Laboratories
  - **link:** http://arxiv.org/pdf/2509.20603v2
  - **Simple LLM Summary:** The paper presents a converged computing architecture integrating HPC and Kubernetes platforms to deploy containerized GenAI services, specifically demonstrating Llama LLM deployment using vLLM inference server. This approach enables reproducible GenAI workload deployment across different computing environments using multiple container runtimes. The experience provides practical insights for HPC container community and guides future tool development for containerized AI service deployment.

- **[arXiv2509] Zeppelin: Balancing Variable-length Workloads in Data Parallel Large
  Model Training**
  - **tags:** [mlsys], [LLM training], [distributed training, attention optimization, communication efficiency, load balancing, sequence partitioning]
  - **authors:** Chang Chen, Tiancheng Chen, Jiangfei Duan, Qianchao Zhu, Zerui Wang, Qinghao Hu, Peng Sun, Xiuhong Li, Chao Yang, Torsten Hoefler
  - **institution:** Peking University, ETH Zurich, The Chinese University of Hong Kong, Shanghai AI Laboratory, Nanyang Technological University
  - **link:** http://arxiv.org/pdf/2509.21841v2
  - **Simple LLM Summary:** Zeppelin introduces a hierarchical sequence partitioning method for attention modules, a routing layer for inter-node transfers, and a remapping layer for layout transformations between modules. The system addresses load imbalance in data-parallel LLM training with variable sequence lengths. Evaluations show Zeppelin achieves an average 2.80× speedup over state-of-the-art methods.

- **[arXiv2509] A Scalable Distributed Framework for Multimodal GigaVoxel Image
  Registration**
  - **tags:** [mlsys], [kernels], [image registration, distributed computing, GPU acceleration, medical imaging, memory optimization]
  - **authors:** Rohit Jena, Vedant Zope, Pratik Chaudhari, James C. Gee
  - **institution:** University of Pennsylvania
  - **link:** http://arxiv.org/pdf/2509.25044v1
  - **Simple LLM Summary:** The paper proposes FFDP, a distributed framework with optimized non-GEMM kernels for large-scale multimodal image registration. It enables convolution-aware tensor sharding and achieves 6-7x speedup over existing methods while reducing memory consumption by 20-59%. The system successfully registered a 100-micron human brain MRI volume 570x larger than standard clinical data in about a minute using 8 GPUs.

- **[arXiv2509] Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in
  Cloud Computing**
  - **tags:** [mlsys], [scheduling], [reinforcement learning, ant colony optimization, IoT data scheduling, cloud computing, resource optimization]
  - **authors:** Noor Islam S. Mohammad
  - **institution:** New York University Tandon School of Engineering
  - **link:** http://arxiv.org/pdf/2508.04334v2
  - **Simple LLM Summary:** This paper proposes a hybrid scheduling algorithm combining deep reinforcement learning and ant colony optimization for IoT data scheduling in cloud environments. The method achieves significant improvements in response time (18.4% reduction), resource utilization (12.7% improvement), and energy consumption (9.3% decrease) compared to existing approaches. The integration of model-free RL with swarm intelligence proves effective for scalable and energy-efficient IoT-cloud data scheduling.

## 2025-09-30

- **[arXiv2509] I Like To Move It -- Computation Instead of Data in the Brain**
  - **tags:** [mlsys], [Other models inference], [brain simulation, structural plasticity, connectome, spike exchange, computational efficiency, Barnes-Hut approximation]
  - **authors:** Fabian Czappa, Marvin Kaster, Felix Wolf
  - **institution:** RWTH Aachen University
  - **link:** http://arxiv.org/pdf/2509.26193v1
  - **Simple LLM Summary:** The paper introduces a new algorithm that reduces communication overhead in brain simulations by moving computation instead of data. This approach decreases connectivity update time by a factor of six and spike exchange time by over two orders of magnitude. The method enhances scalability for simulating large-scale neural networks with structural plasticity.

- **[arXiv2509] Parallax: Efficient LLM Inference Service over Decentralized Environment**
  - **tags:** [mlsys], [scheduling], [decentralized inference, LLM serving, heterogeneous GPUs, two-phase scheduler, model allocation, pipeline selection]
  - **authors:** Chris Tong, Youhe Jiang, Gufeng Chen, Tianyi Zhao, Sibian Lu, Wenjie Qu, Eric Yang, Lynn Ai, Binhang Yuan
  - **institution:** Gradient, HKUST, National University of Singapore
  - **link:** http://arxiv.org/pdf/2509.26182v1
  - **Simple LLM Summary:** Parallax introduces a decentralized LLM serving system with a two-phase scheduler that allocates model layers across heterogeneous GPUs and dynamically selects execution pipelines. It optimizes latency and throughput under memory and network constraints. Evaluation shows consistent improvements over baselines, making volunteer computing practical for LLM inference.

- **[arXiv2509] Artificial Intelligence for Cost-Aware Resource Prediction in Big Data
  Pipelines**
  - **tags:** [mlsys], [trace analysis], [resource prediction, random forest, cloud computing, cost-aware autoscaling, big data pipelines]
  - **authors:** Harshit Goyal
  - **institution:** BITS Pilani
  - **link:** http://arxiv.org/pdf/2510.05127v1
  - **Simple LLM Summary:** This paper uses Random Forest regression on Google Borg cluster traces to predict resource utilization in big data pipelines. The model achieves high accuracy (R²≈0.99) in capturing non-linear workload-resource relationships. Results demonstrate AI-driven prediction enables cost-aware autoscaling while maintaining service quality in cloud environments.

- **[arXiv2509] Tuning the Tuner: Introducing Hyperparameter Optimization for
  Auto-Tuning**
  - **tags:** [mlsys], [others], [hyperparameter optimization, auto-tuning, performance optimization, search space exploration]
  - **authors:** Floris-Jan Willemsen, Rob V. van Nieuwpoort, Ben van Werkhoven
  - **institution:** Leiden University, Netherlands eScience Center
  - **link:** http://arxiv.org/pdf/2509.26300v1
  - **Simple LLM Summary:** This paper introduces a method for hyperparameter tuning of optimization algorithms in auto-tuning systems, featuring a statistical evaluation approach and simulation mode that reduces tuning costs by two orders of magnitude. The research demonstrates that hyperparameter tuning improves auto-tuner performance by 94.8% on average and can be further optimized with meta-strategies for 204.7% improvement. The work establishes hyperparameter tuning as a powerful technique for advancing auto-tuning research and practice.

- **[arXiv2509] Efficient Construction of Large Search Spaces for Auto-Tuning**
  - **tags:** [mlsys], [others], [auto-tuning, constraint satisfaction problem, search space construction, performance optimization, HPC]
  - **authors:** Floris-Jan Willemsen, Rob V. van Nieuwpoort, Ben van Werkhoven
  - **institution:** Leiden University, Netherlands eScience Center
  - **link:** http://arxiv.org/pdf/2509.26253v1
  - **Simple LLM Summary:** This paper reformulates search space construction for auto-tuning as a Constraint Satisfaction Problem (CSP) and develops an optimized CSP solver with runtime constraint translation. The approach achieves 4 orders of magnitude speedup over brute-force methods and 1-2 orders over chain-of-trees frameworks. This eliminates scalability barriers in auto-tuning, enabling exploration of previously unattainable problem scales.

- **[arXiv2509] LoRAFusion: Efficient LoRA Fine-Tuning for LLMs**
  - **tags:** [mlsys], [finetuning], [LoRA, kernel fusion, adaptive batching, distributed training, parameter-efficient fine-tuning]
  - **authors:** Zhanda Zhu, Qidong Su, Yaoyao Ding, Kevin Song, Shang Wang, Gennady Pekhimenko
  - **institution:** University of Toronto, Vector Institute, NVIDIA
  - **link:** http://arxiv.org/pdf/2510.00206v1
  - **Simple LLM Summary:** LoRAFusion introduces kernel-level graph-splitting to fuse memory-bound operations and scheduling-level adaptive batching for multi-job fine-tuning. It achieves up to 1.96× speedup over Megatron-LM and 1.46× improvement over mLoRA while maintaining model quality. The system reduces redundant memory accesses and enables concurrent fine-tuning of multiple LoRA adapters on shared GPUs.

- **[arXiv2509] Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework**
  - **tags:** [mlsys], [LLM inference], [datacenter lifecycle, TCO optimization, hardware refresh, power provisioning, cooling systems, networking, operational software]
  - **authors:** Jovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Ricardo Bianchini
  - **institution:** University of Illinois Urbana-Champaign, Microsoft Azure Research
  - **link:** http://arxiv.org/pdf/2509.26534v1
  - **Simple LLM Summary:** This paper proposes a holistic framework for AI datacenter lifecycle management across building, hardware refresh, and operation stages. The framework co-optimizes decisions across power, cooling, networking, and refresh strategies while accounting for workload dynamics and hardware evolution. The proposed system reduces total cost of ownership by up to 40% compared to traditional approaches.

- **[arXiv2509] Accelerating LLM Inference with Precomputed Query Storage**
  - **tags:** [mlsys], [LLM inference], [query caching, storage optimization, semantic matching, vector database, latency reduction]
  - **authors:** Jay H. Park, Youngju Cho, Choungsol Lee, Moonwook Oh, Euiseong Seo
  - **institution:** Samsung Electronics, Sungkyunkwan University
  - **link:** http://arxiv.org/pdf/2509.25919v1
  - **Simple LLM Summary:** StorInfer precomputes and stores query-response pairs offline, then uses semantic matching to retrieve stored responses during inference, bypassing GPU computation when possible. The system employs adaptive query generation techniques to maximize coverage while avoiding duplicates. Evaluation shows up to 17.3% latency reduction with no quality loss, demonstrating effective storage-assisted inference for predictable query distributions.

- **[arXiv2509] SysMoBench: Evaluating AI on Formally Modeling Complex Real-World
  Systems**
  - **tags:** [mlsys], [LLM inference], [formal modeling, system verification, TLA+, benchmark evaluation, distributed systems, concurrent systems]
  - **authors:** Qian Cheng, Ruize Tang, Emilie Ma, Finn Hackett, Peiyang He, Yiming Su, Ivan Beschastnikh, Yu Huang, Xiaoxing Ma, Tianyin Xu
  - **institution:** Nanjing University, University of British Columbia, University of Illinois Urbana-Champaign
  - **link:** http://arxiv.org/pdf/2509.23130v2
  - **Simple LLM Summary:** This paper introduces SysMoBench, a benchmark for evaluating AI's ability to generate formal models of complex real-world systems using TLA+. The research demonstrates current limitations in LLMs' capacity to accurately model complete systems while providing automated evaluation metrics. The benchmark enables systematic assessment of AI capabilities in formal system modeling and opens new research directions.

- **[arXiv2509] Efficient Distributed Training via Dual Batch Sizes and Cyclic
  Progressive Learning**
  - **tags:** [mlsys], [Other models training], [distributed training, batch size optimization, progressive learning, parameter server, image resolution]
  - **authors:** Kuan-Wei Lu, Ding-Yong Hong, Pangfeng Liu, Jan-Jan Wu
  - **institution:** Academia Sinica, National Taiwan University
  - **link:** http://arxiv.org/pdf/2509.26092v1
  - **Simple LLM Summary:** This paper proposes a hybrid distributed training method combining dual batch sizes (using large batches for efficiency and small batches for generalization) with cyclic progressive learning (gradually increasing image resolution). Experimental results show the approach improves both accuracy and training efficiency on CIFAR-100 and ImageNet datasets compared to conventional methods.

## 2025-10-05

- **[arXiv2510] Speculative Actions: A Lossless Framework for Faster Agentic Systems**
  - **tags:** [mlsys], [LLM inference], [speculative execution, agent systems, parallel processing, latency optimization]
  - **authors:** Naimeng Ye, Arnav Ahuja, Georgios Liargkovas, Yunan Lu, Kostis Kaffes, Tianyi Peng
  - **institution:** Columbia University
  - **link:** http://arxiv.org/pdf/2510.04371v1
  - **Simple LLM Summary:** This paper proposes speculative actions, a lossless framework inspired by speculative execution in microprocessors and speculative decoding in LLM inference. The method predicts likely agent actions using faster models to enable parallel execution of multiple steps. Evaluation across gaming, e-commerce, and web search environments shows up to 55% next-action prediction accuracy and significant latency reductions.

- **[arXiv2510] From Patchwork to Network: A Comprehensive Framework for Demand Analysis
  and Fleet Optimization of Urban Air Mobility**
  - **tags:** [mlsys], [scheduling], [urban air mobility, fleet optimization, parallel simulation, demand forecasting, ground transportation integration]
  - **authors:** Xuan Jiang, Xuanyu Zhou, Yibo Zhao, Shangqing Cao, Jinhua Zhao, Mark Hansen, Raja Sengupta
  - **institution:** University of California, Berkeley
  - **link:** http://arxiv.org/pdf/2510.04186v1
  - **Simple LLM Summary:** This paper introduces LPSim, a large-scale parallel simulation framework using multi-GPU computing to co-optimize Urban Air Mobility demand, fleet operations, and ground transportation interactions. The extended equilibrium search algorithm forecasts demand and determines optimal fleet composition. Results from the San Francisco Bay Area case study show over 20 minutes' travel time savings for 230,000 trips, but highlight critical dependence on ground access integration and dynamic scheduling.
