# 20260209-20260215

## 2026-02-09

**cs.DC total: 21**

- **[arXiv260209] Computationally Efficient Laplacian CL-colME**
  - **tags:** [mlsys], [cluster infrastructure], [decentralized learning, collaborative mean estimation, consensus algorithms, Laplacian-based consensus, graph-based methods]
  - **authors:** Nikola Stankovic
  - **institution:** IEEE
  - **link:** https://arxiv.org/pdf/2602.06070
  - **Simple LLM Summary:** This paper proposes CL-colME, a Laplacian-based consensus variant of collaborative mean estimation that avoids computationally expensive normalization processes. The method maintains the convergence and accuracy of the previous C-colME framework while improving computational efficiency in decentralized, heterogeneous networks.

- **[arXiv260209] Experimental Analysis of Server-Side Caching for Web Performance**
  - **tags:** [sys], [web performance optimization], [server-side caching, in-memory cache, response time, time-to-live, performance evaluation]
  - **authors:** Mohammad Umar, Bharat Tripathi
  - **institution:** Allenhouse Business School
  - **link:** https://arxiv.org/pdf/2602.06074
  - **Simple LLM Summary:** This paper experimentally compares the performance of a web server with and without simple in-memory caching using a fixed time-to-live. The results show that server-side caching significantly reduces response times, demonstrating its effectiveness for small-scale applications and educational use cases.

- **[arXiv260209] PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference**
  - **tags:** [mlsys], [llm inference], [kernel-level optimization, load-balanced execution groups, I/O-aware grouping, KV cache reorganization, FlashAttention]
  - **authors:** Rui Ning, Wei Zhang, Fan Lai
  - **institution:** Nanjing University, University of Illinois Urbana-Champaign
  - **link:** https://arxiv.org/pdf/2602.06072
  - **Simple LLM Summary:** PackInfer is a kernel-level attention framework that improves batched LLM inference by packing heterogeneous requests into load-balanced execution groups and reorganizing KV caches into group-contiguous layouts. This approach reduces computation and I/O imbalance, leading to reduced latency and increased throughput compared to FlashAttention.

- **[arXiv260209] iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems**
  - **tags:** [mlsys], [cluster infrastructure], [reinforcement learning, iterative scheduling, Markov decision process, resource investment problem, decomposition, reconfiguration]
  - **authors:** Yi-Xiang Hu, Yuke Wang, Feng Wu, Zirui Huang, Shuli Zeng, Xiang-Yang Li
  - **institution:** University of Science and Technology of China
  - **link:** https://arxiv.org/pdf/2602.06064
  - **Simple LLM Summary:** The paper introduces iScheduler, a framework that uses reinforcement learning to iteratively schedule tasks by decomposing large resource investment problems into subproblems modeled as a Markov decision process. It achieves fast schedule generation and supports efficient updates by reusing unaffected schedules. Experiments show it reduces time to feasibility by up to 43x while maintaining competitive resource costs compared to commercial solvers.

- **[arXiv260209] FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs**
  - **tags:** [mlsys], [GPU kernels], [BlockPerm-SJLT, sketch-kernel co-design, sparse sketching, oblivious subspace embedding, CUDA kernel, memory bandwidth]
  - **authors:** Rajat Vadiraj Dwaraknath, Sungyoon Kim, Mert Pilanci
  - **institution:** Stanford University
  - **link:** https://arxiv.org/pdf/2602.06071
  - **Simple LLM Summary:** The paper introduces FlashSketch, a method that co-designs a new sparse sketch family (BlockPerm-SJLT) with an optimized CUDA kernel to improve GPU efficiency. This approach explicitly trades off sketching robustness with hardware performance. The result is a significant speedup (1.7x geomean) over prior state-of-the-art GPU sketches across various benchmarks.

- **[arXiv260209] Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing**
  - **tags:** [mlsys], [llm inference], [scaling laws, heterogeneous orchestration, hardware-aware routing, progressive sample multiplexing, cost models, Intelligence Per Watt, Energy-Coverage Efficiency, Price-Power-Performance]
  - **authors:** Satyam Kumar, Saurabh Jha
  - **institution:** Not explicitly provided in the given text.
  - **link:** https://arxiv.org/pdf/2602.06057
  - **Simple LLM Summary:** This paper introduces QEIL, a framework that uses inference-time scaling laws and heterogeneous hardware orchestration (across CPU, GPU, NPU) to optimize LLM inference on edge devices. It demonstrates that this approach achieves significant improvements in coverage, energy efficiency, latency, and cost compared to homogeneous methods, establishing it as an optimal strategy for energy-constrained edge AI systems.

- **[arXiv260209] HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference**
  - **tags:** [mlsys], [post-training], [hybrid quantization, structural pruning, Fisher Information Matrix, sensitivity-aware pruning, post-training quantization, model compression]
  - **authors:** Dinesh Gopalan, Ratul Ali
  - **institution:** AMD, Jahangirnagar University
  - **link:** https://arxiv.org/pdf/2602.06069
  - **Simple LLM Summary:** The paper introduces HQP, a sensitivity-aware hybrid framework that combines structural pruning using a dynamic weight sensitivity metric from an efficient Fisher Information Matrix approximation, followed by conditional 8-bit post-training quantization. It achieves up to 3.12× inference speedup and 55% model size reduction while limiting accuracy drop to under 1.5% on edge platforms like NVIDIA Jetson. The method outperforms single-objective compression techniques, providing a hardware-agnostic solution for ultra-low-latency edge AI inference.

- **[arXiv260209] Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers**
  - **tags:** [mlsys], [llm training], [matrix-based optimizers, data parallelism, tensor parallelism, asynchronous compute, load balancing, Shampoo, Muon, SOAP, ZeRO]
  - **authors:** Liangyu Wang, Siqi Zhang, Junjie Wang, Yiming Dong, Bo Zheng, Zihan Qiu, Shengkun Tang, Di Wang, Rui Men, Dayiheng Liu
  - **institution:** KAUST, Alibaba Group, Peking University, MBZUAI
  - **link:** https://arxiv.org/pdf/2602.06079
  - **Simple LLM Summary:** The paper proposes Canzona, a framework that decouples logical optimizer assignment from physical parameter distribution to efficiently run matrix-based optimizers (like Shampoo) in distributed LLM training. It introduces load-balanced partitioning for Data Parallelism and an asynchronous compute pipeline for Tensor Parallelism. Evaluations show it preserves parallel architecture efficiency, achieving a 1.57x end-to-end speedup and reducing optimizer step latency by 5.8x.

- **[arXiv260209] Mapping Gemma3 onto an Edge Dataflow Architecture**
  - **tags:** [mlsys], [llm inference], [dequantization engine, tiled matrix multiplication, FlowQKV, FusedDQP, FlowKV, Q4NX quantization, dataflow architecture]
  - **authors:** Shouyu Du, Miaoxiang Yu, Zhiheng Ni, Jillian Cai, Qing Yang, Tao Wei, Zhenyu Xu
  - **institution:** Clemson University, University of Rhode Island
  - **link:** https://arxiv.org/pdf/2602.06063
  - **Simple LLM Summary:** This paper presents the first end-to-end deployment of the Gemma3 family of models on an AMD Ryzen AI NPU, introducing hardware-aware techniques like FlowQKV for prefill and FusedDQP for decoding, along with a custom 4-bit quantization format. The methods achieve significant speed and power efficiency improvements over iGPU and CPU baselines. The work demonstrates that modern NPUs can enable practical, low-power LLM and VLM inference at the edge and provides a blueprint for mapping transformers onto tiled dataflow accelerators.

- **[arXiv260209] MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments**
  - **tags:** [mlsys], [multi-modal inference], [memory-centric benchmark, mobile GUI agents, LLM-as-judge, pass@k, cross-session learning, cross-temporal retention, cross-spatial retention, Progressive Scrutiny]
  - **authors:** Guangyi Liu, Pengxiang Zhao, Yaozhen Liang, Qinyi Luo, Shunye Tang, Yuxiang Chai, Weifeng Lin, Han Xiao, WenHao Wang, Siheng Chen, Zhengxi Lu, Gao Wu, Hao Wang, Liang Liu, Yong Liu
  - **institution:** Zhejiang University, Nankai University, The Chinese University of Hong Kong, Shanghai Jiao Tong University, vivo AI Lab
  - **link:** https://arxiv.org/pdf/2602.06075
  - **Simple LLM Summary:** This paper introduces MemGUI-Bench, a comprehensive benchmark and evaluation pipeline designed to assess the memory capabilities of mobile GUI agents. The method includes a memory taxonomy, 128 memory-challenging tasks, and an automated evaluator (MemGUI-Eval) using staged LLM-as-judge and Progressive Scrutiny. The main conclusion is that current state-of-the-art agents exhibit significant memory deficits, with the benchmark identifying five distinct failure modes and synthesizing actionable design implications.

- **[arXiv260209] LAAFD: LLM-based Agents for Accelerated FPGA Design**
  - **tags:** [mlsys], [llm inference], [high-level synthesis, FPGA, agentic workflow, pipelining, vectorization, dataflow partitioning, co-simulation]
  - **authors:** Maxim Moraru, Kamalavasan Kamalakkannan, Jered Dominguez-Trujillo, Patrick Diehl, Atanu Barai, Julien Loiseau, Zachary Kent Baker, Howard Pritchard, Galen M Shipman
  - **institution:** Los Alamos National Laboratory
  - **link:** https://arxiv.org/pdf/2602.06085
  - **Simple LLM Summary:** This paper introduces LAAFD, an agentic workflow that uses large language models to automatically translate general-purpose C++ code into optimized FPGA kernels for Vitis HLS. The system leverages HLS co-simulation and synthesis feedback to iteratively apply hardware optimizations like pipelining and dataflow. The results show that LAAFD achieves performance comparable to hand-tuned baselines, significantly lowering the expertise barrier for FPGA acceleration.

- **[arXiv260209] BouquetFL: Emulating diverse participant hardware in Federated Learning**
  - **tags:** [mlsys], [others], [federated learning, hardware emulation, resource restriction, GPU constraints, CPU throttling, memory constraints, heterogeneous clients]
  - **authors:** Arno Geimer
  - **institution:** Not specified (author email domain not provided)
  - **link:** https://arxiv.org/pdf/2602.06498
  - **Simple LLM Summary:** BouquetFL is a framework that simulates heterogeneous client hardware in federated learning by programmatically restricting CPU, memory, and GPU resources on a single machine. It enables controlled experimentation under realistic hardware diversity without requiring multiple physical devices. The tool addresses a methodological gap in FL research by providing an accessible way to study system heterogeneity, bringing experimental practice closer to practical deployment conditions.

- **[arXiv260209] AdFL: In-Browser Federated Learning for Online Advertisement**
  - **tags:** [mlsys], [others], [federated learning, differential privacy, in-browser training, ad viewability prediction]
  - **authors:** Ahmad Alemari, Pritam Sen, Cristian Borcea
  - **institution:** New Jersey Institute of Technology, Jazan University
  - **link:** https://arxiv.org/pdf/2602.06336
  - **Simple LLM Summary:** This paper proposes AdFL, a federated learning framework that runs directly in users' web browsers to learn ad preferences without sharing raw data. It demonstrates the framework's feasibility for real-time in-browser training and shows that an ad viewability prediction model built on it achieves high performance, which is maintained even when differential privacy is applied for enhanced security.

- **[arXiv260209] FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training**
  - **tags:** [mlsys], [llm training], [ZeRO-3, host memory caching, parameter-efficient fine-tuning, all-gather, data parallel]
  - **authors:** Gyeongseo Park, Eungyeong Lee, Song-woo Sok, Myung-Hoon Cha, Kwangwon Koh, Baik-Song An, Hongyeon Kim, Ki-Dong Kang
  - **institution:** Electronics and Telecommunications Research Institute (ETRI)
  - **link:** https://arxiv.org/pdf/2602.06499
  - **Simple LLM Summary:** The paper proposes FCDP, a method that caches forward-pass parameters in host memory to reuse them during the backward pass, eliminating redundant inter-node communication while preserving a minimal GPU memory footprint. It achieves up to 100x higher throughput than ZeRO-3 on commodity hardware by leveraging host memory as a fast cache layer instead of an overflow tier.

- **[arXiv260209] Degradation of Feature Space in Continual Learning**
  - **tags:** [ai], [continual learning], [isotropic regularization, contrastive learning, rehearsal strategies, catastrophic forgetting, feature space geometry]
  - **authors:** Chiara Lanza, Roberto Pereira, Marco Miozzo, Eduard Angelats, Paolo Dini
  - **institution:** CTTC (Centre Tecnològic de Telecomunicacions de Catalunya)
  - **link:** https://arxiv.org/pdf/2602.06586
  - **Simple LLM Summary:** This paper investigates whether enforcing isotropy in the feature space can improve continual learning by mitigating catastrophic forgetting. Using contrastive continual learning techniques with rehearsal on CIFAR datasets, the authors find that isotropic regularization actually degrades model performance. The conclusion is that isotropy, beneficial in centralized training, is not a suitable inductive bias for non-stationary continual learning scenarios.

- **[arXiv260209] Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms**
  - **tags:** [mlsys], [cluster infrastructure], [reinforcement learning, serverless computing, autoscaling, OpenFaaS, farm skeleton, Gymnasium]
  - **authors:** Lanpei Li, Massimo Coppola, Malio Li, Valerio Besozzi, Jack Bell, Vincenzo Lomonaco
  - **institution:** National Research Council of Italy (CNR), University of Pisa, LUISS University
  - **link:** https://arxiv.org/pdf/2602.06555
  - **Simple LLM Summary:** The paper proposes a framework using Reinforcement Learning (RL) to dynamically manage the autoscaling of a parallel task farm skeleton on a serverless platform (OpenFaaS). It evaluates RL-based policies against a reactive baseline, finding that AI-driven management better handles platform-specific limitations, improving Quality of Service (QoS) while maintaining efficient resource usage.

- **[arXiv260209] DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving**
  - **tags:** [mlsys], [llm inference], [dual-mapping scheduling, cache-affinity scheduling, load-balancing scheduling, KV cache reuse, SLO-aware routing, hotspot-aware rebalancing, dual-hash-ring scaling]
  - **authors:** Ying Yuan, Pengfei Zuo, Bo Wang, Zhangyu Chen, Zhipeng Tan, Zhou Yu
  - **institution:** Huazhong University of Science and Technology, Huawei
  - **link:** https://arxiv.org/pdf/2602.06502
  - **Simple LLM Summary:** The paper proposes DualMap, a dual-mapping scheduling strategy for distributed LLM serving that maps each request to two candidate instances using independent hash functions to intelligently balance KV cache reuse and load distribution. It incorporates SLO-aware routing, hotspot-aware rebalancing, and lightweight scaling to handle dynamic workloads. Experiments show DualMap improves effective request capacity by up to 2.25x under the same TTFT SLO constraints compared to state-of-the-art methods.

- **[arXiv260209] Wonderboom -- Efficient, and Censorship-Resilient Signature Aggregation for Million Scale Consensus**
  - **tags:** [sys], [blockchain consensus], [signature aggregation, censorship-resilient, Byzantine Fault Tolerant, quorum attestation, validator set, simulation]
  - **authors:** Zeta Avarikioti, Ray Neiheiser, Krzysztof Pietrzak, Michelle X. Yeo
  - **institution:** TU Wien, Institute of Science and Technology Austria, Nanyang Technological University, Aarhus University, Common Prefix
  - **link:** https://arxiv.org/pdf/2602.06655
  - **Simple LLM Summary:** The paper introduces Wonderboom, a new protocol designed to efficiently aggregate signatures from millions of validators in Ethereum's consensus mechanism. It achieves this up to 32 times faster than the current state-of-the-art while providing stronger security guarantees against censorship and stake-shifting attacks. The authors implement a simulation tool to demonstrate that Wonderboom can handle over 2 million signatures within a single Ethereum slot.

- **[arXiv260209] Same Engine, Multiple Gears: Parallelizing Fixpoint Iteration at Different Granularities (Extended Version)**
  - **tags:** [sys], [static analysis], [fixpoint iteration, parallelization, top-down solver, task granularity, immediate approach, independent approach, publish/subscribe]
  - **authors:** Ali Rasim Kocal, Michael Schwarz, Simmo Saan, Helmut Seidl
  - **institution:** Technical University of Munich, National University of Singapore, University of Tartu
  - **link:** https://arxiv.org/pdf/2602.06680
  - **Simple LLM Summary:** This paper proposes a parallel fixpoint engine for static analysis that is parametric in task granularity, enabling it to operate at different levels of parallelism. It implements two parallelization philosophies—immediate and independent—within the Goblint framework. The results demonstrate the approach's effectiveness in reducing analysis times for large real-world programs.

- **[arXiv260209] Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI**
  - **tags:** [sys], [concurrent logic programming], [Grassroots Logic Programs, multiagent transition systems, operational semantics, peer-to-peer, deterministic semantics]
  - **authors:** Ehud Shapiro
  - **institution:** London School of Economics, Weizmann Institute of Science
  - **link:** https://arxiv.org/pdf/2602.06934
  - **Simple LLM Summary:** This paper presents deterministic operational semantics (dGLP and madGLP) to facilitate the implementation of Grassroots Logic Programs, a concurrent logic programming language for peer-to-peer systems. The semantics were used as formal specifications for an AI to generate implementations in Dart for workstations and smartphones. The main conclusion is that the developed mathematical specifications enabled correct, implementation-ready semantics, though the development process required iterative refinement across mathematical, informal, and code layers.

- **[arXiv260209] Distributed Knowledge in Simplicial Models**
  - **tags:** [sys], [distributed computing], [simplicial complexes, epistemic logic, Kripke models, distributed knowledge, majority consensus]
  - **authors:** Éric Goubault, Jérémy Ledent, Sergio Rajsbaum
  - **institution:** LIX, CNRS, École Polytechnique, Institut Polytechnique de Paris; Université Paris Cité, CNRS, IRIF; Instituto de Matemáticas, UNAM
  - **link:** https://arxiv.org/pdf/2602.06945
  - **Simple LLM Summary:** This paper introduces simplicial complexes as models for multi-agent epistemic logic, shifting the focus from possible worlds to agents' local views. It connects this topological approach to distributed computing, showing how distributed knowledge relates to solving the majority consensus task. The work describes the specific distributed knowledge used when the task is solvable and presents a logical obstruction when it is not.


**cs.AI/cs.LG contains "reinforcement learning" total: 30**
- [arXiv260209] Transformer-Based Reinforcement Learning for Autonomous Orbital Collision Avoidance in Partially Observable Environments [link](https://arxiv.org/pdf/2602.06088)
- [arXiv260209] Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning [link](https://arxiv.org/pdf/2602.06107)
- [arXiv260209] Self-Improving World Modelling with Latent Actions [link](https://arxiv.org/pdf/2602.06130)
- [arXiv260209] Flow Matching for Offline Reinforcement Learning with Discrete Actions [link](https://arxiv.org/pdf/2602.06138)
- [arXiv260209] Learning Rate Scaling across LoRA Ranks and Transfer to Full Finetuning [link](https://arxiv.org/pdf/2602.06204)
- [arXiv260209] Online Adaptive Reinforcement Learning with Echo State Networks for Non-Stationary Dynamics [link](https://arxiv.org/pdf/2602.06326)
- [arXiv260209] Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation [link](https://arxiv.org/pdf/2602.06359)
- [arXiv260209] Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization [link](https://arxiv.org/pdf/2602.06394)
- [arXiv260209] TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking [link](https://arxiv.org/pdf/2602.06440)
- [arXiv260209] Prism: Spectral Parameter Sharing for Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2602.06476)
- [arXiv260209] AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents [link](https://arxiv.org/pdf/2602.06485)
- [arXiv260209] Adaptive Uncertainty-Aware Tree Search for Robust Reasoning [link](https://arxiv.org/pdf/2602.06493)
- [arXiv260209] Progress Constraints for Reinforcement Learning in Behavior Trees [link](https://arxiv.org/pdf/2602.06525)
- [arXiv260209] Dynamics-Aligned Shared Hypernetworks for Zero-Shot Actuator Inversion [link](https://arxiv.org/pdf/2602.06550)
- [arXiv260209] SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees [link](https://arxiv.org/pdf/2602.06554)
- [arXiv260209] SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs [link](https://arxiv.org/pdf/2602.06566)
- [arXiv260209] Sample-Efficient Policy Space Response Oracles with Joint Experience Best Response [link](https://arxiv.org/pdf/2602.06599)
- [arXiv260209] The hidden risks of temporal resampling in clinical reinforcement learning [link](https://arxiv.org/pdf/2602.06603)
- [arXiv260209] Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations [link](https://arxiv.org/pdf/2602.06643)
- [arXiv260209] compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data [link](https://arxiv.org/pdf/2602.06669)
- [arXiv260209] F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare [link](https://arxiv.org/pdf/2602.06717)
- [arXiv260209] Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions [link](https://arxiv.org/pdf/2602.06746)
- [arXiv260209] Soft Forward-Backward Representations for Zero-shot Reinforcement Learning with General Utilities [link](https://arxiv.org/pdf/2602.06769)
- [arXiv260209] Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling [link](https://arxiv.org/pdf/2602.06795)
- [arXiv260209] AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models [link](https://arxiv.org/pdf/2602.06825)
- [arXiv260209] A first realization of reinforcement learning-based closed-loop EEG-TMS [link](https://arxiv.org/pdf/2602.06907)
- [arXiv260209] Continuous-time reinforcement learning: ellipticity enables model-free value function approximation [link](https://arxiv.org/pdf/2602.06930)
- [arXiv260209] Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics [link](https://arxiv.org/pdf/2602.06939)
- [arXiv260209] Optimal Derivative Feedback Control for an Active Magnetic Levitation System: An Experimental Study on Data-Driven Approaches [link](https://arxiv.org/pdf/2602.06944)
- [arXiv260209] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning [link](https://arxiv.org/pdf/2602.06960)

**cs.AI/cs.LG contains "accelerate" total: 16**
- [arXiv260209] Analyzing Diffusion and Autoregressive Vision Language Models in Multimodal Embedding Space [link](https://arxiv.org/pdf/2602.06056)
- [arXiv260209] Compressing LLMs with MoP: Mixture of Pruners [link](https://arxiv.org/pdf/2602.06127)
- [arXiv260209] Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding [link](https://arxiv.org/pdf/2602.06161)
- [arXiv260209] To 2:4 Sparsity and Beyond: Neuron-level Activation Function to Accelerate LLM Pre-Training [link](https://arxiv.org/pdf/2602.06183)
- [arXiv260209] Accelerating Vision Transformers on Brain Processing Unit [link](https://arxiv.org/pdf/2602.06300)
- [arXiv260209] Beyond Code Contributions: How Network Position, Temporal Bursts, and Code Review Activities Shape Contributor Influence in Large-Scale Open Source Ecosystems [link](https://arxiv.org/pdf/2602.06426)
- [arXiv260209] Principle-Evolvable Scientific Discovery via Uncertainty Minimization [link](https://arxiv.org/pdf/2602.06448)
- [arXiv260209] SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers [link](https://arxiv.org/pdf/2602.06706)
- [arXiv260209] GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models [link](https://arxiv.org/pdf/2602.06718)
- [arXiv260209] AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models [link](https://arxiv.org/pdf/2602.06825)
- [arXiv260209] Are Deep Learning Based Hybrid PDE Solvers Reliable? Why Training Paradigms and Update Strategies Matter [link](https://arxiv.org/pdf/2602.06842)
- [arXiv260209] Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping [link](https://arxiv.org/pdf/2602.06850)
- [arXiv260209] AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents [link](https://arxiv.org/pdf/2602.06855)
- [arXiv260209] When RL Meets Adaptive Speculative Training: A Unified Training-Serving System [link](https://arxiv.org/pdf/2602.06932)
- [arXiv260209] DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos [link](https://arxiv.org/pdf/2602.06949)
- [arXiv260209] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning [link](https://arxiv.org/pdf/2602.06960)

## 2026-02-10

**cs.DC total: 18**

- **[arXiv260210] Privacy-Preserving Coding Schemes for Multi-Access Distributed Computing Models**
  - **tags:** [sys], [distributed computing], [placement delivery arrays, coded MapReduce, privacy-preserving coding, multi-access distributed computing]
  - **authors:** Shanuja Sasi
  - **institution:** Indian Institute of Technology Kanpur
  - **link:** https://arxiv.org/pdf/2602.07850
  - **Simple LLM Summary:** This paper introduces privacy constraints into the multi-access distributed computing (MADC) model and develops private coded schemes using extended placement delivery arrays. The proposed coding schemes reduce communication bottlenecks without file replication while ensuring the privacy of each reducer's assigned function.

- **[arXiv260210] Parallel Track Transformers: Enabling Fast GPU Inference with Reduced Synchronization**
  - **tags:** [mlsys], [llm inference], [tensor parallelism, Parallel Track Transformer, multi-GPU synchronization, model architecture, serving efficiency]
  - **authors:** Chong Wang, Nan Du, Tom Gunter, Tao Lei, Kulin Seth, Senyu Tong, Jianyu Wang, Guoli Yin, Xiyou Zhou, Kelvin Zou, Ruoming Pang
  - **institution:** Apple
  - **link:** https://arxiv.org/pdf/2602.07306
  - **Simple LLM Summary:** The paper proposes the Parallel Track (PT) Transformer, a novel model architecture that uses multiple independent transformer "tracks" to minimize inter-GPU synchronization during inference. It achieves significant reductions in synchronization operations and improves serving efficiency, including faster token generation and higher throughput, when integrated into existing LLM serving frameworks.

- **[arXiv260210] Knowledge Graphs-Driven Intelligence for Distributed Decision Systems**
  - **tags:** [mlsys], [others], [Knowledge Graphs, Graph Embeddings, GraphSAGE, Knowledge Sharing, Distributed Decision Systems]
  - **authors:** Rosario Napoli, Gabriele Morabito, Antonio Celesti, Massimo Villari, Maria Fazio
  - **institution:** University of Messina
  - **link:** https://arxiv.org/pdf/2602.07614
  - **Simple LLM Summary:** This paper proposes a distributed knowledge-sharing architecture that uses Knowledge Graphs and Graph Embeddings, aggregated locally via GraphSAGE, to create a global semantic abstraction called a Knowledge Map for decentralized coordination. The method was validated in a resource orchestration use case, showing it effectively maintains semantic coherence and adaptability in dynamic environments like Edge Computing and IoT.

- **[arXiv260210] Wireless Streamlet: A Spectrum-Aware and Cognitive Consensus Protocol for Edge IoT**
  - **tags:** [sys], [blockchain consensus], [Channel-Aware Leader Election (CALE), coded dual-chain architecture, TDMA voting schedule, erasure coding, spectrum-aware consensus]
  - **authors:** Taotao Wang, Long Shi, Fang Liu, Qing Yang, Shengli Zhang
  - **institution:** Shenzhen University, Nanjing University of Science and Technology
  - **link:** https://arxiv.org/pdf/2602.07630
  - **Simple LLM Summary:** This paper proposes Wireless Streamlet, a blockchain consensus protocol for edge IoT that uses a Channel-Aware Leader Election mechanism and a coded dual-chain architecture to improve reliability and efficiency in wireless networks. It demonstrates higher throughput and lower latency in lossy environments while reducing storage requirements.

- **[arXiv260210] Multi-Agentic AI for Fairness-Aware and Accelerated Multi-modal Large Model Inference in Real-world Mobile Edge Networks**
  - **tags:** [mlsys], [llm inference], [multi-agentic AI, natural language reasoning, containerized deployment, runtime telemetry, prompt scheduling, resource management]
  - **authors:** Haiyuan Li, Hari Madhukumar, Shuangyi Yan, Yulei Wu, Dimitra Simeonidou
  - **institution:** University of Bristol
  - **link:** https://arxiv.org/pdf/2602.07215
  - **Simple LLM Summary:** The paper proposes a multi-agentic AI framework, powered by foundation language models, to optimize latency and fairness for multi-modal large model inference in mobile edge networks. The agents cooperatively manage prompt routing and model deployment using natural language reasoning over system telemetry. Experiments on a city-wide testbed show the solution reduces average latency by over 80% and improves fairness without requiring fine-tuning.

- **[arXiv260210] ZipFlow: a Compiler-based Framework to Unleash Compressed Data Movement for Modern GPUs**
  - **tags:** [mlsys], [GPU kernels], [data compression, GPU decompression, compiler-based framework, parallel patterns, scheduling strategies, PCIe bandwidth optimization]
  - **authors:** Gwangoo Yeo, Zhiyang Shen, Wei Cui, Matteo Interlandi, Rathijit Sen, Bailu Ding, Qi Chen, Minsoo Rhu
  - **institution:** KAIST, Tsinghua University, Microsoft Research Asia, Microsoft Gray Systems Lab
  - **link:** https://arxiv.org/pdf/2602.08190
  - **Simple LLM Summary:** ZipFlow is a compiler-based framework that optimizes compressed data transfer for GPU-accelerated data analytics by classifying compression algorithms into parallel patterns and applying tailored scheduling strategies. It achieves significant speedups over state-of-the-art GPU compression libraries and CPU-based query engines, enhancing end-to-end performance in data-intensive workloads.

- **[arXiv260210] Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection**
  - **tags:** [mlsys], [llm inference], [ensemble selection, mutual information, greedy algorithm, Gaussian-copula, correlated errors, budgeted selection]
  - **authors:** Yigit Turkmen, Baturalp Buyukates, Melih Bastopcu
  - **institution:** Bilkent University, University of Birmingham
  - **link:** https://arxiv.org/pdf/2602.08003
  - **Simple LLM Summary:** The paper proposes a greedy mutual-information selection algorithm for forming LLM ensembles under a query budget, aiming to maximize the information about the true label rather than just picking the highest-performing models. It shows that due to correlated errors among models, performance can saturate, and its method consistently outperforms baselines like selecting top-k models by accuracy on question answering and sentiment classification datasets.

- **[arXiv260210] Accuracy-Delay Trade-Off in LLM Offloading via Token-Level Uncertainty**
  - **tags:** [mlsys], [llm inference], [mobile edge computing, offloading, token-level uncertainty, greedy offloading algorithm, accuracy-delay trade-off]
  - **authors:** Yumin Kim, Hyeonsu Lyu, Minjae Lee, Hyun Jong Yang
  - **institution:** Seoul National University, POSTECH
  - **link:** https://arxiv.org/pdf/2602.07958
  - **Simple LLM Summary:** The paper proposes an uncertainty-aware offloading framework for LLM inference in mobile edge computing, using a token-level uncertainty metric to dynamically decide between local and server-side processing. It introduces a greedy offloading algorithm (GOA) that prioritizes offloading high-uncertainty queries to minimize delay while maintaining accuracy. The experiments show GOA achieves a favorable accuracy-delay trade-off, outperforming baseline strategies across varying user densities.

- **[arXiv260210] Fork, Explore, Commit: OS Primitives for Agentic Exploration**
  - **tags:** [mlsys], [others], [branch context, copy-on-write, FUSE, atomic commit, process isolation, first-commit-wins]
  - **authors:** Cong Wang, Yusheng Zheng
  - **institution:** Multikernel Technologies, Inc., University of California, Santa Cruz
  - **link:** https://arxiv.org/pdf/2602.08199
  - **Simple LLM Summary:** The paper introduces a new OS abstraction called a "branch context" to support AI agentic exploration, providing isolated copy-on-write workspaces and process groups with atomic commit/rollback. It is realized through BranchFS, a FUSE-based filesystem, and a proposed branch() syscall for Linux. Preliminary evaluation shows efficient branch creation and commit overhead, addressing the need for reliable state isolation in parallel AI agent workflows.

- **[arXiv260210] The CAPSARII Approach to Cyber-Secure Wearable, Ultra-Low-Power Networked Sensors for Soldier Health Monitoring**
  - **tags:** [mlsys], [others], [wearable sensors, Internet of Battlefield Things (IoBT), edge AI, cloud-based analytics, smart textile integration, ultra-low-power optimization, encryption, authentication]
  - **authors:** Luciano Bozzi, Christian Celidonio, Umberto Nuzzi, Massimo Biagini, Stefano Cherubin, Asbjørn Djupdal, Tor Andre Haugdahl, Andrea Aliverti, Alessandra Angelucci, Giovanni Agosta, Gerardo Pelosi, Paolo Belluco, Samuele Polistina, Riccardo Volpi, Luigi Malagò, Michael Schneider, Florian Wieczorek, Xabier Eguiluz
  - **institution:** Sea Sky Technologies, NTNU, Politecnico di Milano, LWT3, QUAESTA AI, BORN GmbH, IKERLAN
  - **link:** https://arxiv.org/pdf/2602.08080
  - **Simple LLM Summary:** The CAPSARII project proposes a wearable sensor system and IoBT framework to monitor soldiers' health using edge AI for real-time decision support and cloud analytics. It focuses on ultra-low-power optimization, smart textile integration, and cybersecurity. The approach aims to enhance soldier protection and operational effectiveness through data-driven insights.

- **[arXiv260210] HEAL: Online Incremental Recovery for Leaderless Distributed Systems Across Persistency Models**
  - **tags:** [sys], [distributed systems], [leaderless systems, incremental recovery, linearizable consistency, memory persistency models, fault tolerance]
  - **authors:** Antonis Psistakis, Burak Ocalan, Fabien Chaix, Ramnatthan Alagappan, Josep Torrellas
  - **institution:** University of Illinois Urbana-Champaign, Foundation for Research and Technology-Hellas
  - **link:** https://arxiv.org/pdf/2602.08257
  - **Simple LLM Summary:** This paper proposes HEAL, a low-overhead online incremental recovery scheme for non-transactional leaderless distributed systems. It presents algorithms for linearizable consistency across different memory persistency models. Experimental results show HEAL significantly reduces recovery latency and throughput degradation compared to conventional and leader-based recovery schemes.

- **[arXiv260210] PARD: Enhancing Goodput for Inference Pipeline via Proactive Request Dropping**
  - **tags:** [mlsys], [others], [proactive request dropping, adaptive request priority, inference pipeline optimization]
  - **authors:** Zhixin Zhao, Yitao Hu, Simin Chen, Mingfang Ji, Wei Yang, Yuhao Zhang, Laiping Zhao, Wenxin Li, Xiulong Liu, Wenyu Qu, Hao Wang
  - **institution:** Tianjin University, University of Texas at Dallas, Stevens Institute of Technology
  - **link:** https://arxiv.org/pdf/2602.08747
  - **Simple LLM Summary:** PARD introduces a proactive request dropping system for DNN inference pipelines, combining timely dropping decisions based on runtime information and an adaptive priority mechanism to select which requests to drop. It achieves higher goodput and reduces wasted resources compared to reactive dropping methods.

- **[arXiv260210] Equilibria: Fair Multi-Tenant CXL Memory Tiering At Scale**
  - **tags:** [sys], [memory tiering], [CXL, multi-tenancy, fairness policies, promotion and demotion, thrashing suppression, TPP, service level objectives]
  - **authors:** Kaiyang Zhao, Neha Gholkar, Hasan Maruf, Abhishek Dhanotia, Johannes Weiner, Gregory Price, Ning Sun, Bhavya Dwivedi, Stuart Clark, Dimitrios Skarlatos
  - **institution:** Carnegie Mellon University, Meta
  - **link:** https://arxiv.org/pdf/2602.08800
  - **Simple LLM Summary:** Equilibria is an OS framework for fair, multi-tenant memory tiering using CXL memory. It provides per-container controls, enforces fairness policies, and suppresses thrashing to mitigate interference. The system improves performance over the state-of-the-art Linux solution by up to 52% for production workloads.

- **[arXiv260210] Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research**
  - **tags:** [mlsys], [llm training], [PyTorch-native, distributed pretraining, modular design, declarative configuration, parallelization strategies, ablation studies]
  - **authors:** Max Lübbering, Timm Ruland, Richard Rutmann, Felix Stollenwerk, David Fitzek, Michael Fromm, Alexander Weber, Rafet Sifa, Nicolas Flores-Herr, Joachim Köhler, Mehdi Ali
  - **institution:** Fraunhofer IAIS, AI Sweden, University of Bonn, Lamarr Institute
  - **link:** https://arxiv.org/pdf/2602.08387
  - **Simple LLM Summary:** The paper introduces Modalities, a PyTorch-native framework designed for large-scale LLM training and research. It integrates advanced parallelization strategies to enable efficient pretraining and systematic ablation studies at scale, and uses a modular, declarative configuration to improve reproducibility and extensibility compared to existing frameworks.

- **[arXiv260210] Recursive QAOA for Interference-Aware Resource Allocation in Wireless Networks**
  - **tags:** [ai], [quantum optimization], [RQAOA, QAOA, QUBO, Ising model, variable elimination, channel assignment]
  - **authors:** Kuan-Cheng Chen, Hiromichi Matsuyama, Wei-hao Huang, Yu Yamashiro
  - **institution:** J-ij Europe Ltd, Jij Inc.
  - **link:** https://arxiv.org/pdf/2602.07483
  - **Simple LLM Summary:** This paper proposes using the Recursive Quantum Approximate Optimization Algorithm (RQAOA) for interference-aware wireless resource allocation, formulating the problem as a QUBO/Ising model. The method recursively eliminates variables to reduce problem size and stabilize training. The results show that RQAOA can find feasible, optimal solutions and mitigate scalability issues present in standard QAOA.

- **[arXiv260210] RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks**
  - **tags:** [mlsys], [others], [knowledge distillation, federated learning, Kullback-Leibler divergence, logit-based knowledge transfer, robust aggregation]
  - **authors:** Pouria Arefijamal, Mahdi Ahmadlou, Bardia Safaei, Jörg Henkel
  - **institution:** Sharif University of Technology, Karlsruhe Institute of Technology (KIT)
  - **link:** https://arxiv.org/pdf/2602.08446
  - **Simple LLM Summary:** This paper introduces RIFLE, a robust federated learning framework that replaces gradient sharing with logit-based knowledge distillation to train deep models on resource-constrained IoT devices. It uses a KL divergence-based mechanism to validate client updates for security and privacy. The method significantly reduces training time and improves accuracy and robustness against attacks compared to conventional FL baselines.

- **[arXiv260210] DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce**
  - **tags:** [mlsys], [llm training], [gradient compression, multi-hop all-reduce, quantization, fused kernel, NCCL, PyTorch DDP]
  - **authors:** Wenchen Han, Shay Vargaftik, Michael Mitzenmacher, Ran Ben Basat
  - **institution:** University College London, Broadcom, Harvard University
  - **link:** https://arxiv.org/pdf/2602.08923
  - **Simple LLM Summary:** DynamiQ is a quantization framework designed to accelerate gradient synchronization in multi-hop all-reduce for large model training by better representing partial sums and using a fused decompress-accumulate-recompress kernel. It demonstrates consistent performance improvements over state-of-the-art methods while maintaining near-baseline model accuracy.

- **[arXiv260210] Towards CXL Resilience to CPU Failures**
  - **tags:** [sys], [fault-tolerance], [CXL, cache coherence, hardware logging, data replication, recovery protocol]
  - **authors:** Antonis Psistakis, Burak Ocalan, Chloe Alverti, Fabien Chaix, Ramnatthan Alagappan, Josep Torrellas
  - **institution:** University of Illinois Urbana-Champaign, National Technical University of Athens, Foundation for Research and Technology-Hellas
  - **link:** https://arxiv.org/pdf/2602.08271
  - **Simple LLM Summary:** This paper proposes ReCXL, a system that extends the CXL specification to handle compute node failures by replicating cache line updates to other nodes via a hardware Logging Unit. The method ensures fault tolerance by logging writes and using these logs to recover directory and memory state after a failure. The evaluation shows that ReCXL achieves fault-tolerant execution with only a 30% performance slowdown compared to a non-fault-tolerant platform.


**cs.AI/cs.LG contains "reinforcement learning" total: 68**
- [arXiv260210] Regret Analysis of Unichain Average Reward Constrained MDPs with General Parameterization [link](https://arxiv.org/pdf/2602.08000)
- [arXiv260210] Direct Soft-Policy Sampling via Langevin Dynamics [link](https://arxiv.org/pdf/2602.07873)
- [arXiv260210] Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities [link](https://arxiv.org/pdf/2602.08092)
- [arXiv260210] ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation [link](https://arxiv.org/pdf/2602.07883)
- [arXiv260210] Joint Reward Modeling: Internalizing Chain-of-Thought for Efficient Visual Reward Models [link](https://arxiv.org/pdf/2602.07533)
- [arXiv260210] Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs [link](https://arxiv.org/pdf/2602.07729)
- [arXiv260210] Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems [link](https://arxiv.org/pdf/2602.08104)
- [arXiv260210] TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation [link](https://arxiv.org/pdf/2602.07595)
- [arXiv260210] Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions [link](https://arxiv.org/pdf/2602.07341)
- [arXiv260210] Horizon Imagination: Efficient On-Policy Training in Diffusion World Models [link](https://arxiv.org/pdf/2602.08032)
- [arXiv260210] Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning [link](https://arxiv.org/pdf/2602.07830)
- [arXiv260210] The Laplacian Keyboard: Beyond the Linear Span [link](https://arxiv.org/pdf/2602.07730)
- [arXiv260210] High Fidelity Textual User Representation over Heterogeneous Sources via Reinforcement Learning [link](https://arxiv.org/pdf/2602.07333)
- [arXiv260210] VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos [link](https://arxiv.org/pdf/2602.07801)
- [arXiv260210] Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge [link](https://arxiv.org/pdf/2602.07588)
- [arXiv260210] The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL [link](https://arxiv.org/pdf/2602.07078)
- [arXiv260210] Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning [link](https://arxiv.org/pdf/2602.07889)
- [arXiv260210] Learning to Self-Verify Makes Language Models Better Reasoners [link](https://arxiv.org/pdf/2602.07594)
- [arXiv260210] Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System [link](https://arxiv.org/pdf/2602.07308)
- [arXiv260210] FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability-Plasticity Tradeoff [link](https://arxiv.org/pdf/2602.08040)
- [arXiv260210] rePIRL: Learn PRM with Inverse RL for LLM Reasoning [link](https://arxiv.org/pdf/2602.07832)
- [arXiv260210] AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering [link](https://arxiv.org/pdf/2602.07906)
- [arXiv260210] Generative Reasoning Re-ranker [link](https://arxiv.org/pdf/2602.07774)
- [arXiv260210] Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling [link](https://arxiv.org/pdf/2602.08052)
- [arXiv260210] Preference Conditioned Multi-Objective Reinforcement Learning: Decomposed, Diversity-Driven Policy Optimization [link](https://arxiv.org/pdf/2602.07764)
- [arXiv260210] TodoEvolve: Learning to Architect Agent Planning Systems [link](https://arxiv.org/pdf/2602.07839)
- [arXiv260210] Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning [link](https://arxiv.org/pdf/2602.08054)
- [arXiv260210] CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning [link](https://arxiv.org/pdf/2602.07496)
- [arXiv260210] Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model [link](https://arxiv.org/pdf/2602.07422)
- [arXiv260210] Risk-Sensitive Exponential Actor Critic [link](https://arxiv.org/pdf/2602.07202)
- [arXiv260210] Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning [link](https://arxiv.org/pdf/2602.07441)
- [arXiv260210] Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation [link](https://arxiv.org/pdf/2602.07227)
- [arXiv260210] When Is Compositional Reasoning Learnable from Verifiable Rewards? [link](https://arxiv.org/pdf/2602.07992)
- [arXiv260210] Efficient Planning in Reinforcement Learning via Model Introspection [link](https://arxiv.org/pdf/2602.07719)
- [arXiv260210] MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation [link](https://arxiv.org/pdf/2602.07848)
- [arXiv260210] CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization [link](https://arxiv.org/pdf/2602.08210)
- [arXiv260210] DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning [link](https://arxiv.org/pdf/2602.08213)
- [arXiv260210] SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning [link](https://arxiv.org/pdf/2602.08234)
- [arXiv260210] Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs [link](https://arxiv.org/pdf/2602.08241)
- [arXiv260210] Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers [link](https://arxiv.org/pdf/2602.08244)
- [arXiv260210] When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems [link](https://arxiv.org/pdf/2602.08272)
- [arXiv260210] Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression [link](https://arxiv.org/pdf/2602.08324)
- [arXiv260210] Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System [link](https://arxiv.org/pdf/2602.08335)
- [arXiv260210] OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration [link](https://arxiv.org/pdf/2602.08344)
- [arXiv260210] Does Your Reasoning Model Implicitly Know When to Stop Thinking? [link](https://arxiv.org/pdf/2602.08354)
- [arXiv260210] Learning Human-Like Badminton Skills for Humanoid Robots [link](https://arxiv.org/pdf/2602.08370)
- [arXiv260210] Reinforcement Learning with Backtracking Feedback [link](https://arxiv.org/pdf/2602.08377)
- [arXiv260210] Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning [link](https://arxiv.org/pdf/2602.08382)
- [arXiv260210] Intelligent support for Human Oversight: Integrating Reinforcement Learning with Gaze Simulation to Personalize Highlighting [link](https://arxiv.org/pdf/2602.08403)
- [arXiv260210] Beyond Correctness: Learning Robust Reasoning via Transfer [link](https://arxiv.org/pdf/2602.08489)
- [arXiv260210] Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards [link](https://arxiv.org/pdf/2602.08499)
- [arXiv260210] Learning Self-Correction in Vision-Language Models via Rollout Augmentation [link](https://arxiv.org/pdf/2602.08503)
- [arXiv260210] Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO [link](https://arxiv.org/pdf/2602.08533)
- [arXiv260210] Conditional Sequence Modeling for Safe Reinforcement Learning [link](https://arxiv.org/pdf/2602.08584)
- [arXiv260210] Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces [link](https://arxiv.org/pdf/2602.08616)
- [arXiv260210] From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism [link](https://arxiv.org/pdf/2602.08655)
- [arXiv260210] LLaDA2.1: Speeding Up Text Diffusion via Token Editing [link](https://arxiv.org/pdf/2602.08676)
- [arXiv260210] Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning [link](https://arxiv.org/pdf/2602.08689)
- [arXiv260210] SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity [link](https://arxiv.org/pdf/2602.08690)
- [arXiv260210] Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning [link](https://arxiv.org/pdf/2602.08734)
- [arXiv260210] Bayesian Preference Learning for Test-Time Steerable Reward Models [link](https://arxiv.org/pdf/2602.08819)
- [arXiv260210] Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning [link](https://arxiv.org/pdf/2602.08835)
- [arXiv260210] Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems [link](https://arxiv.org/pdf/2602.08847)
- [arXiv260210] AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection [link](https://arxiv.org/pdf/2602.08868)
- [arXiv260210] Efficient and Stable Reinforcement Learning for Diffusion Language Models [link](https://arxiv.org/pdf/2602.08905)
- [arXiv260210] StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors [link](https://arxiv.org/pdf/2602.08934)
- [arXiv260210] Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2602.08965)
- [arXiv260210] iGRPO: Self-Feedback-Driven LLM Reasoning [link](https://arxiv.org/pdf/2602.09000)

**cs.AI/cs.LG contains "accelerate" total: 30**
- [arXiv260210] When Excellence Stops Producing Knowledge: A Practitioner's Observation on Research Funding [link](https://arxiv.org/pdf/2602.07039)
- [arXiv260210] DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents [link](https://arxiv.org/pdf/2602.07035)
- [arXiv260210] FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging [link](https://arxiv.org/pdf/2602.08024)
- [arXiv260210] "Death" of a Chatbot: Investigating and Designing Toward Psychologically Safe Endings for Human-AI Relationships [link](https://arxiv.org/pdf/2602.07193)
- [arXiv260210] MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning [link](https://arxiv.org/pdf/2602.07543)
- [arXiv260210] Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions [link](https://arxiv.org/pdf/2602.07341)
- [arXiv260210] Systematic Performance Assessment of Deep Material Networks for Multiscale Material Modeling [link](https://arxiv.org/pdf/2602.07192)
- [arXiv260210] GRAFT: Decoupling Ranking and Calibration for Survival Analysis [link](https://arxiv.org/pdf/2602.07884)
- [arXiv260210] MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training [link](https://arxiv.org/pdf/2602.07790)
- [arXiv260210] Optimizing Few-Step Generation with Adaptive Matching Distillation [link](https://arxiv.org/pdf/2602.07345)
- [arXiv260210] Interpreting Physics in Video World Models [link](https://arxiv.org/pdf/2602.07050)
- [arXiv260210] PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging [link](https://arxiv.org/pdf/2602.07044)
- [arXiv260210] Accelerating Social Science Research via Agentic Hypothesization and Experimentation [link](https://arxiv.org/pdf/2602.07983)
- [arXiv260210] aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones [link](https://arxiv.org/pdf/2602.07264)
- [arXiv260210] GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design [link](https://arxiv.org/pdf/2602.07491)
- [arXiv260210] STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction [link](https://arxiv.org/pdf/2602.08245)
- [arXiv260210] Noise Stability of Transformer Models [link](https://arxiv.org/pdf/2602.08287)
- [arXiv260210] Fast Flow Matching based Conditional Independence Tests for Causal Discovery [link](https://arxiv.org/pdf/2602.08315)
- [arXiv260210] Prism: Spectral-Aware Block-Sparse Attention [link](https://arxiv.org/pdf/2602.08426)
- [arXiv260210] GISA: A Benchmark for General Information-Seeking Assistant [link](https://arxiv.org/pdf/2602.08543)
- [arXiv260210] Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction [link](https://arxiv.org/pdf/2602.08585)
- [arXiv260210] QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill [link](https://arxiv.org/pdf/2602.08722)
- [arXiv260210] Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization [link](https://arxiv.org/pdf/2602.08774)
- [arXiv260210] Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems [link](https://arxiv.org/pdf/2602.08792)
- [arXiv260210] FlattenGPT: Depth Compression for Transformer with Layer Flattening [link](https://arxiv.org/pdf/2602.08858)
- [arXiv260210] ARO: A New Lens On Matrix Optimization For Large Models [link](https://arxiv.org/pdf/2602.09006)
- [arXiv260210] ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling [link](https://arxiv.org/pdf/2602.09009)
- [arXiv260210] Electron-Informed Coarse-Graining Molecular Representation Learning for Real-World Molecular Physics [link](https://arxiv.org/pdf/2602.07087)
- [arXiv260210] Fast and Robust Likelihood-Guided Diffusion Posterior Sampling with Amortized Variational Inference [link](https://arxiv.org/pdf/2602.07102)
- [arXiv260210] Optimizing Spectral Prediction in MXene-Based Metasurfaces Through Multi-Channel Spectral Refinement and Savitzky-Golay Smoothing [link](https://arxiv.org/pdf/2602.08406)

## 2026-02-11

**cs.DC total: 11**

- **[arXiv260211] Distributed Hybrid Parallelism for Large Language Models: Comparative Study and System Design Guide**
  - **tags:** [mlsys], [llm training], [hybrid parallelization, collective operations, communication-computation overlap, automated strategy search, cost models]
  - **authors:** Hossam Amer, Rezaul Karim, Ali Pourranjbar, Weiwei Zhang, Walid Ahmed, Boxing Chen
  - **institution:** Huawei Canada
  - **link:** https://arxiv.org/pdf/2602.09109
  - **Simple LLM Summary:** This paper provides a comprehensive review and comparative study of distributed hybrid parallelization strategies for efficient large language model training and inference. It examines methods like communication-computation overlap and automated strategy search using cost models, concluding with empirical case studies and design guidelines to help practitioners select optimal parallelism strategies.

- **[arXiv260211] The Coordination Criterion**
  - **tags:** [sys], [distributed computing theory], [Lamport histories, happens-before, monotonicity, coordination-free implementation, asynchronous message-passing]
  - **authors:** Joseph M. Hellerstein
  - **institution:** UC Berkeley, Amazon Web Services
  - **link:** https://arxiv.org/pdf/2602.09435
  - **Simple LLM Summary:** The paper introduces a general Coordination Criterion for distributed systems, showing that a specification can be implemented without coordination if and only if its observable outcomes are monotone with respect to extending Lamport histories. This provides a unified theoretical boundary explaining when coordination is intrinsically necessary across various classical results like CAP and CALM.

- **[arXiv260211] Harvest: Adaptive Photonic Switching Schedules for Collective Communication in Scale-up Domains**
  - **tags:** [mlsys], [cluster infrastructure], [silicon photonics, circuit-switched networks, topology reconfiguration, dynamic programming, collective communication, AllReduce, Recursive Doubling]
  - **authors:** Mahir Rahman, Samuel Joseph, Nihar Kodkani, Behnaz Arzani, Vamsi Addanki
  - **institution:** Purdue University, Microsoft Research
  - **link:** https://arxiv.org/pdf/2602.09188
  - **Simple LLM Summary:** Harvest is a systematic method for synthesizing adaptive photonic switching schedules that balance reconfiguration delay against communication congestion and propagation delay to minimize collective completion time. It formulates the problem as a dynamic program and demonstrates significant performance improvements over static and naive reconfiguration baselines across various collective algorithms.

- **[arXiv260211] Rashomon Sets and Model Multiplicity in Federated Learning**
  - **tags:** [mlsys], [others], [Rashomon set, model multiplicity, federated learning, fairness, privacy]
  - **authors:** Xenia Heilmann, Luca Corbucci, Mattia Cerrato
  - **institution:** Johannes Gutenberg University, University of Pisa
  - **link:** https://arxiv.org/pdf/2602.09520
  - **Simple LLM Summary:** This paper formalizes Rashomon sets for Federated Learning, proposing three definitions: a global set, a t-agreement set, and individual client sets. It shows how to estimate model multiplicity metrics under FL's privacy constraints. The results demonstrate that these federated Rashomon sets provide valuable insights, allowing clients to deploy models better suited to their local data and fairness requirements.

- **[arXiv260211] ALPHA-PIM: Analysis of Linear Algebraic Processing for High-Performance Graph Applications on a Real Processing-In-Memory System**
  - **tags:** [sys], [graph processing], [processing-in-memory, sparse matrix-vector multiplication, data partitioning, DMA engines, interconnection networks]
  - **authors:** Marzieh Barkhordar, Alireza Tabatabaeian, Mohammad Sadrosadati, Christina Giannoula, Juan Gomez Luna, Izzat El Hajj, Onur Mutlu, Alaa R. Alameldeen
  - **institution:** Simon Fraser University, ETH Zürich, University of Toronto, NVIDIA, American University of Beirut
  - **link:** https://arxiv.org/pdf/2602.09174
  - **Simple LLM Summary:** This paper implements and analyzes common graph algorithms on UPMEM's real-world Processing-In-Memory (PIM) system, comparing performance against CPU and GPU baselines. The study concludes that optimal data partitioning across PIM cores is crucial for performance and identifies key hardware limitations, recommending future enhancements like improved DMA engines and direct core interconnects.

- **[arXiv260211] It's not a lie if you don't get caught: simplifying reconfiguration in SMR through dirty logs**
  - **tags:** [sys], [distributed systems, consensus protocols], [state-machine replication, reconfiguration, modularity, dirty logs, inner log, outer log, consensus protocol interchangeability]
  - **authors:** Allen Clement, Natacha Crooks, Neil Giridharan, Alex Shamis
  - **institution:** Subzero Labs, UC Berkeley
  - **link:** https://arxiv.org/pdf/2602.09441
  - **Simple LLM Summary:** The paper presents Gauss, a reconfiguration engine that introduces a separation between a consensus protocol's "inner log" and a sanitized "outer log" to enable modular upgrades. This approach allows independent reconfiguration of membership, failure thresholds, and the consensus protocol itself with minimal downtime. The initial evaluation on the Rialo blockchain demonstrates that this separation enables seamless evolution of the SMR stack across diverse protocol implementations.

- **[arXiv260211] LLM-CoOpt: A Co-Design and Optimization Framework for Efficient LLM Inference on Heterogeneous Platforms**
  - **tags:** [mlsys], [llm inference], [key-value cache optimization, FP8 quantization, grouped-query attention, paged attention, algorithm-hardware co-design]
  - **authors:** Jie Kong, Wei Wang, Jiehan Zhou, Chen Yu
  - **institution:** Shandong University of Science and Technology, Huazhong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2602.09323
  - **Simple LLM Summary:** The paper proposes LLM-CoOpt, a co-design framework that integrates KV cache optimization with FP8 quantization, grouped-query attention, and paged attention to improve LLM inference efficiency. Experiments on LLaMa-13B show it increases throughput by up to 13.43% and reduces latency by up to 16.79% while maintaining accuracy. This demonstrates a practical optimization path for efficient large-scale language model inference.

- **[arXiv260211] Efficient Remote Prefix Fetching with GPU-native Media ASICs**
  - **tags:** [mlsys], [llm inference], [KV cache reuse, video codec compression, GPU-native media ASICs, pipelined fetching, codec-friendly tensor layout]
  - **authors:** Liang Mi, Weijun Wang, Jinghan Chen, Ting Cao, Haipeng Dai, Yunxin Liu
  - **institution:** Nanjing University, Institute for AI Industry Research (AIR), Tsinghua University
  - **link:** https://arxiv.org/pdf/2602.09725
  - **Simple LLM Summary:** The paper proposes KVFetcher, a system that accelerates remote KV cache reuse for LLM inference by compressing the cache into a video format and using GPU-native hardware codecs for efficient transmission and decompression. This approach minimizes time-to-first-token by eliminating resource contention and masking network latency. Experiments show it reduces TTFT by up to 3.51x compared to state-of-the-art methods while maintaining accuracy.

- **[arXiv260211] High-performance Vector-length Agnostic Quantum Circuit Simulations on ARM Processors**
  - **tags:** [sys], [quantum computing simulation], [vector-length agnostic (VLA), ARM SVE, quantum state-vector simulation, memory layout adjustment, load buffering, loop control, gate fusion]
  - **authors:** Ruimin Shi, Gabin Schieffer, Pei-Hung Lin, Maya Gokhale, Andreas Herten, Ivy Peng
  - **institution:** KTH Royal Institute of Technology, Lawrence Livermore National Laboratory, Jülich Supercomputing Centre
  - **link:** https://arxiv.org/pdf/2602.09604
  - **Simple LLM Summary:** The paper proposes a vector-length agnostic (VLA) design for quantum circuit simulations, using optimization techniques like VLEN-adaptive memory layout and gate fusion to achieve high performance portability across ARM processors. The implementation in Google's Qsim demonstrates significant speedups on multiple ARM platforms, showing that high-performance portability is achievable with VLA designs for quantum simulation workloads.

- **[arXiv260211] Revealing the Challenges of Attention-FFN Disaggregation for Modern MoE Models and Hardware Systems**
  - **tags:** [mlsys], [llm inference], [Attention-FFN Disaggregation (AFD), Expert Parallelism (EP), roofline model, Hardware FLOPS Utilization (HFU), MoE models]
  - **authors:** Guowei Liu, Hongming Li, Yaning Guo, Yongxi Lyu, Mo Zhou, Yi Liu, Zhaogeng Li, Yanpeng Wang
  - **institution:** Baidu Inc.
  - **link:** https://arxiv.org/pdf/2602.09721
  - **Simple LLM Summary:** This paper systematically analyzes the Attention-FFN Disaggregation (AFD) architecture for deploying large-scale Mixture-of-Experts (MoE) models by extending the roofline model to the communication level. It finds that AFD suffers from a "dead zone" on standard clusters where scaling is limited by interconnect bandwidth, but can be beneficial on high-bandwidth Superpod-class hardware with specific model characteristics. The conclusion is that AFD is a promising but non-universal solution, effective only for particular hardware-model combinations.

- **[arXiv260211] Architectural Foundations for Checkpointing and Restoration in Quantum HPC Systems**
  - **tags:** [sys], [quantum computing], [checkpointing, dynamic circuits, mid-circuit measurements, classical feed forward, conditional execution, variational eigensolvers, quantum approximate optimization]
  - **authors:** Qiang Guan, Qinglei Cao, Xiaoyi Lu, Siyuan Niu
  - **institution:** Kent State University, Saint Louis University, University of Florida, University of Central Florida
  - **link:** https://arxiv.org/pdf/2602.09325
  - **Simple LLM Summary:** This paper proposes a checkpointing and restoration framework for quantum HPC systems that redefines the problem as capturing control flow and algorithmic state, rather than quantum state. It leverages dynamic circuit features like mid-circuit measurements and classical feedback to enable resilient execution. The main conclusion is that this approach allows for restartable quantum workflows, making it suitable for iterative algorithms common in scientific computing.


**cs.AI/cs.LG contains "reinforcement learning" total: 31**
- [arXiv260211] UI-Venus-1.5 Technical Report [link](https://arxiv.org/pdf/2602.09082)
- [arXiv260211] P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads [link](https://arxiv.org/pdf/2602.09443)
- [arXiv260211] Rollout-Training Co-Design for Efficient LLM-Based Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2602.09578)
- [arXiv260211] On the Optimal Reasoning Length for RL-Trained Language Models [link](https://arxiv.org/pdf/2602.09591)
- [arXiv260211] Risk-sensitive reinforcement learning using expectiles, shortfall risk and optimized certainty equivalent risk [link](https://arxiv.org/pdf/2602.09300)
- [arXiv260211] Reward Modeling for Reinforcement Learning-Based LLM Reasoning: Design, Challenges, and Evaluation [link](https://arxiv.org/pdf/2602.09305)
- [arXiv260211] Boltzmann Reinforcement Learning for Noise resilience in Analog Ising Machines [link](https://arxiv.org/pdf/2602.09162)
- [arXiv260211] Bridging Efficiency and Transparency: Explainable CoT Compression in Multimodal Large Reasoning Models [link](https://arxiv.org/pdf/2602.09485)
- [arXiv260211] Squeezing More from the Stream : Learning Representation Online for Streaming Reinforcement Learning [link](https://arxiv.org/pdf/2602.09396)
- [arXiv260211] EExApp: GNN-Based Reinforcement Learning for Radio Unit Energy Optimization in 5G O-RAN [link](https://arxiv.org/pdf/2602.09206)
- [arXiv260211] CausalGDP: Causality-Guided Diffusion Policies for Reinforcement Learning [link](https://arxiv.org/pdf/2602.09207)
- [arXiv260211] $n$-Musketeers: Reinforcement Learning Shapes Collaboration Among Language Models [link](https://arxiv.org/pdf/2602.09173)
- [arXiv260211] Online Learning in MDPs with Partially Adversarial Transitions and Losses [link](https://arxiv.org/pdf/2602.09474)
- [arXiv260211] SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning [link](https://arxiv.org/pdf/2602.09463)
- [arXiv260211] Training deep physical neural networks with local physical information bottleneck [link](https://arxiv.org/pdf/2602.09569)
- [arXiv260211] ExO-PPO: an Extended Off-policy Proximal Policy Optimization Algorithm [link](https://arxiv.org/pdf/2602.09726)
- [arXiv260211] Grounding LTL Tasks in Sub-Symbolic RL Environments for Zero-Shot Generalization [link](https://arxiv.org/pdf/2602.09761)
- [arXiv260211] Flexible Entropy Control in RLVR with Gradient-Preserving Perspective [link](https://arxiv.org/pdf/2602.09782)
- [arXiv260211] A Controlled Study of Double DQN and Dueling DQN Under Cross-Environment Transfer [link](https://arxiv.org/pdf/2602.09810)
- [arXiv260211] Code2World: A GUI World Model via Renderable Code Generation [link](https://arxiv.org/pdf/2602.09856)
- [arXiv260211] ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference [link](https://arxiv.org/pdf/2602.10004)
- [arXiv260211] Answer First, Reason Later: Aligning Search Relevance via Mode-Balanced Reinforcement Learning [link](https://arxiv.org/pdf/2602.10006)
- [arXiv260211] A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging [link](https://arxiv.org/pdf/2602.10007)
- [arXiv260211] ADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning [link](https://arxiv.org/pdf/2602.10019)
- [arXiv260211] Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection [link](https://arxiv.org/pdf/2602.10042)
- [arXiv260211] Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning [link](https://arxiv.org/pdf/2602.10044)
- [arXiv260211] Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization [link](https://arxiv.org/pdf/2602.10048)
- [arXiv260211] Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability [link](https://arxiv.org/pdf/2602.10067)
- [arXiv260211] Anagent For Enhancing Scientific Table & Figure Analysis [link](https://arxiv.org/pdf/2602.10081)
- [arXiv260211] CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs [link](https://arxiv.org/pdf/2602.10085)
- [arXiv260211] Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning [link](https://arxiv.org/pdf/2602.10090)

**cs.AI/cs.LG contains "accelerate" total: 12**
- [arXiv260211] Efficient Distance Pruning for Process Suffix Comparison in Prescriptive Process Monitoring [link](https://arxiv.org/pdf/2602.09039)
- [arXiv260211] UniComp: A Unified Evaluation of Large Language Model Compression via Pruning, Quantization and Distillation [link](https://arxiv.org/pdf/2602.09130)
- [arXiv260211] SpinCastML an Open Decision-Making Application for Inverse Design of Electrospinning Manufacturing: A Machine Learning, Optimal Sampling and Inverse Monte Carlo Approach [link](https://arxiv.org/pdf/2602.09120)
- [arXiv260211] Genocide by Algorithm in Gaza: Artificial Intelligence, Countervailing Responsibility, and the Corruption of Public Discourse [link](https://arxiv.org/pdf/2602.09202)
- [arXiv260211] Accelerating Post-Quantum Cryptography via LLM-Driven Hardware-Software Co-Design [link](https://arxiv.org/pdf/2602.09410)
- [arXiv260211] From Adam to Adam-Like Lagrangians: Second-Order Nonlocal Dynamics [link](https://arxiv.org/pdf/2602.09101)
- [arXiv260211] Learning to Discover Iterative Spectral Algorithms [link](https://arxiv.org/pdf/2602.09530)
- [arXiv260211] FlyAOC: Evaluating Agentic Ontology Curation of Drosophila Scientific Knowledge Bases [link](https://arxiv.org/pdf/2602.09163)
- [arXiv260211] Stemphonic: All-at-once Flexible Multi-stem Music Generation [link](https://arxiv.org/pdf/2602.09891)
- [arXiv260211] Drug Release Modeling using Physics-Informed Neural Networks [link](https://arxiv.org/pdf/2602.09963)
- [arXiv260211] Position: Message-passing and spectral GNNs are two sides of the same coin [link](https://arxiv.org/pdf/2602.10031)
- [arXiv260211] E2CAR: An Efficient 2D-CNN Framework for Real-Time EEG Artifact Removal on Edge Devices [link](https://arxiv.org/pdf/2602.09035)
