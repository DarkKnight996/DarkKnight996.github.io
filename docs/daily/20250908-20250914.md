# 20250908-20250914

## 2025-09-08

- **[arXiv2509] Research on fault diagnosis and root cause analysis based on full stack
  observability**
  - **tags:** [mlsys], [trace analysis], [root cause analysis, causal discovery, multi-modal fusion, fault diagnosis, observability data]
  - **authors:** Jian Hou
  - **institution:** Huazhong University of Science and Technology
  - **link:** http://arxiv.org/pdf/2509.12231v1
  - **Simple LLM Summary:** The paper proposes KylinRCA framework that integrates temporal causal discovery and cross-modal graph learning for fault diagnosis. It achieves global root cause localization and generates auditable evidence chains using mask-based explanation methods. The framework provides an effective solution for fault diagnosis under full-stack observability by combining dynamic causal analysis with multi-modal data fusion.

- **[arXiv2509] DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World
  Serving**
  - **tags:** [mlsys], [LLM inference], [speculative decoding, dynamic adaptation, KLD stability, large-batch serving]
  - **authors:** Mingyu Yang, Jae-Young Choi, Kihyo Moon, Minsung Jang, Eunjoo Jeon
  - **institution:** Samsung SDS
  - **link:** http://arxiv.org/pdf/2509.01083v2
  - **Simple LLM Summary:** The paper proposes DSDE, a training-free framework that uses Kullback-Leibler divergence variance as a stability signal to dynamically adjust speculation length in speculative decoding. This approach achieves competitive latency with existing baselines and demonstrates superior robustness across diverse workloads, particularly in challenging low-acceptance-rate scenarios. The results validate post-hoc signals as valuable components for building more robust LLM inference systems.

- **[arXiv2509] MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS**
  - **tags:** [mlsys], [LLM inference], [MaaS, SLO-aware orchestration, heterogeneous model instances, parallelism strategies, continuous batching]
  - **authors:** Mo Xuan, Zhang yue, Wu Weigang
  - **institution:** Sun Yat-sen University
  - **link:** http://arxiv.org/pdf/2509.06362v1
  - **Simple LLM Summary:** MaaSO proposes a three-module orchestrator for Model-as-a-Service platforms, including a profiler for instance performance characterization, a placer for configuration optimization, and a distributor for SLO-aware request routing. The system improves SLO satisfaction ratio by 15-30% and reduces response latency by 40-60% compared to existing approaches while lowering orchestration overhead. It effectively manages heterogeneous LLM instances with different parallelism strategies to meet diverse service requirements.

## 2025-09-09

- **[arXiv2509] Towards Scalable Proteomics: Opportunistic SMC Samplers on HTCondor**
  - **tags:** [mlsys], [cluster infrastructure], [Sequential Monte Carlo, Bayesian inference, proteomics, high-performance computing, opportunistic computing]
  - **authors:** Matthew Carter, Lee Devlin, Alexander Philips, Edward Pyzer-Knapp, Paul Spirakis, Simon Maskell
  - **institution:** University of Liverpool, Xyme
  - **link:** http://arxiv.org/pdf/2509.08020v1
  - **Simple LLM Summary:** This paper introduces an opportunistic computing framework for Sequential Monte Carlo samplers using HTCondor to leverage idle compute resources at the University of Liverpool. The proposed Coordinator-Manager-Follower architecture reduces synchronization overhead and enables scalable Bayesian inference for proteomics without dedicated HPC infrastructure. Results show the framework achieves accurate inference with weak scaling, generating more samples under fixed time constraints as resources increase.

- **[arXiv2509] AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with
  FaaS-hosted MCP Services**
  - **tags:** [mlsys], [LLM inference], [Agentic AI, Workflow Patterns, FaaS, MCP, Multi-step Tasks]
  - **authors:** Shiva Sai Krishna Anand Tokal, Vaibhav Jha, Anand Eswaran, Praveen Jayachandran, Yogesh Simmhan
  - **institution:** Indian Institute of Science, IBM India Research Lab
  - **link:** http://arxiv.org/pdf/2509.07595v1
  - **Simple LLM Summary:** The paper proposes AgentX, a novel agentic workflow pattern composed of stage designer, planner, and executor agents that outperforms state-of-the-art patterns. It leverages Model Context Protocol tools and deploys MCP servers as cloud Functions as a Service. Empirical evaluation shows competitive success rates with insights into latency and cost trade-offs for practical applications.

- **[arXiv2509] DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for
  Efficient MoE LLM Inference**
  - **tags:** [mlsys], [LLM inference], [Mixture of Experts, expert scheduling, memory optimization, prefetching, cache management]
  - **authors:** Yuning Zhang, Grant Pinkert, Nan Yang, Yanli Li, Dong Yuan
  - **institution:** The University of Sydney
  - **link:** http://arxiv.org/pdf/2509.07379v1
  - **Simple LLM Summary:** DuoServe-MoE proposes a dual-phase inference system that separates prefill and decode stages with specialized expert scheduling strategies. It uses CUDA pipeline overlapping for prefill and lightweight prediction for decode stage expert prefetching. The system achieves 1.42-7.54× latency improvement while maintaining only 15% peak memory usage of full model size.

- **[arXiv2509] DREAMS: Decentralized Resource Allocation and Service Management across
  the Compute Continuum Using Service Affinity**
  - **tags:** [mlsys], [scheduling], [decentralized resource allocation, microservice placement, compute continuum, Raft consensus, fault tolerance]
  - **authors:** Hai Dinh-Tuan, Tien Hung Nguyen, Sanjeet Raj Pandey
  - **institution:** Technische Universität Berlin
  - **link:** http://arxiv.org/pdf/2509.07497v1
  - **Simple LLM Summary:** DREAMS proposes a decentralized framework using autonomous agents with Raft-based consensus and cost-benefit voting for microservice placement across compute continuum domains. The system achieves globally optimized service placements while maintaining high fault tolerance and privacy. Evaluations show key coordination operations scale sub-linearly with domain numbers, confirming efficiency and scalability.

- **[arXiv2509] Astra: A Multi-Agent System for GPU Kernel Performance Optimization**
  - **tags:** [mlsys], [kernels], [GPU kernel optimization, multi-agent LLM system, CUDA code generation, performance optimization]
  - **authors:** Anjiang Wei, Tianran Sun, Yogesh Seenichamy, Hang Song, Anne Ouyang, Azalia Mirhoseini, Ke Wang, Alex Aiken
  - **institution:** Stanford University, Shanghai Jiao Tong University, Nanjing University
  - **link:** http://arxiv.org/pdf/2509.07506v1
  - **Simple LLM Summary:** Astra introduces the first LLM-based multi-agent system for GPU kernel optimization that starts from existing CUDA implementations and uses specialized agents for iterative code generation, testing, and profiling. The system achieves an average 1.32x speedup on SGLang kernels through autonomous optimization techniques like loop transformations and memory access improvements. This work demonstrates multi-agent LLM systems as a promising paradigm for automating GPU kernel optimization.

- **[arXiv2509] MoE-Compression: How the Compression Error of Experts Affects the
  Inference Accuracy of MoE Model?**
  - **tags:** [mlsys], [LLM inference], [model compression, Mixture of Experts, error analysis, lossy compression]
  - **authors:** Songkai Ma, Zhaorui Zhang, Sheng Di, Benben Liu, Xiaodong Yu, Xiaoyi Lu, Dan Wang
  - **institution:** Hong Kong Polytechnic University, Argonne National Laboratory, The University of Hong Kong, Stevens Institute of Technology, University of California Merced
  - **link:** http://arxiv.org/pdf/2509.07727v1
  - **Simple LLM Summary:** This paper proposes using error-bounded lossy compression algorithms to compress non-activated experts in MoE models for efficient inference. Experiments show shallow layer experts tolerate compression errors well, middle layers are highly sensitive to errors, while deep layer errors can sometimes improve accuracy. The study provides a comprehensive error sensitivity analysis across different expert layers in MoE models.

- **[arXiv2509] Accelerating Frontier MoE Training with 3D Integrated Optics**
  - **tags:** [mlsys], [LLM training], [3D integrated photonics, mixture of experts, scale-up networks, high-bandwidth interconnects, GPU clusters]
  - **authors:** Mikhail Bernadskiy, Peter Carson, Thomas Graham, Taylor Groves, Ho John Lee, Eric Yeh
  - **institution:** Lightmatter
  - **link:** http://arxiv.org/pdf/2510.15893v1
  - **Simple LLM Summary:** This paper proposes using 3D integrated photonics to overcome copper interconnect limitations in large-scale AI training. The method enables 8X scale-up capability and 2.7X faster training for trillion-parameter MoE models by creating high-bandwidth, low-latency optical connections across multiple data center racks. The photonic approach significantly reduces communication bottlenecks and unlocks unprecedented model scaling potential.

## 2025-09-10

- **[arXiv2509] Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and
  Dynamic Parallelism**
  - **tags:** [mlsys], [LLM inference], [heterogeneous GPU clusters, fine-grained parallelism, dynamic parallelism, attention computation, load dispatching]
  - **authors:** Zizhao Mo, Jianxiong Liao, Huanle Xu, Zhi Zhou, Chengzhong Xu
  - **institution:** University of Macau, Sun Yat-sen University
  - **link:** http://arxiv.org/pdf/2509.08309v1
  - **Simple LLM Summary:** Hetis introduces a fine-grained and dynamic parallelism approach for LLM serving in heterogeneous GPU clusters, selectively parallelizing compute-intensive operations and distributing attention computations at head granularity. It employs an online load dispatching policy to balance network latency, computational load, and memory intensity. Evaluation shows Hetis improves throughput by up to 2.25× and reduces latency by 1.49× compared to existing systems.

- **[arXiv2509] Optimizing the Variant Calling Pipeline Execution on Human Genomes Using
  GPU-Enabled Machines**
  - **tags:** [mlsys], [scheduling], [variant calling, GPU optimization, machine learning, job shop scheduling, genomic data processing]
  - **authors:** Ajay Kumar, Praveen Rao, Peter Sanders
  - **institution:** The University of Missouri, Karlsruhe Institute of Technology
  - **link:** http://arxiv.org/pdf/2509.09058v1
  - **Simple LLM Summary:** This paper proposes a machine learning-based approach to optimize variant calling pipeline execution on GPU-enabled machines. The method uses ML to predict execution times of pipeline stages and generates optimal execution plans inspired by flexible job shop scheduling. Evaluation showed 2X speedup over greedy approaches and 1.6X speedup over dynamic resource-based approaches.

- **[arXiv2509] Design and Implementation of Code Completion System Based on LLM and
  CodeBERT Hybrid Subsystem**
  - **tags:** [mlsys], [LLM inference], [code completion, hybrid model, CodeBERT, GPT-3.5, code generation, deep learning]
  - **authors:** Bingbing Zhang, Ziyu Lin, Yingxin Su
  - **institution:** Xiamen Institute of Technology, Google LLC, University of California Davis
  - **link:** http://arxiv.org/pdf/2509.08215v1
  - **Simple LLM Summary:** This paper proposes a hybrid model combining CodeBERT and GPT-3.5 for code completion tasks, leveraging CodeBERT's context understanding and GPT-3.5's generation capabilities. The hybrid approach demonstrated superior performance in accuracy, code quality, and efficiency compared to benchmarks. Robustness testing confirmed the model's reliability and stability for practical software development applications.

## 2025-09-11

- **[arXiv2509] TrEnv: Transparently Share Serverless Execution Environments Across
  Different Functions and Nodes**
  - **tags:** [mlsys], [LLM inference], [serverless computing, LLM agents, execution environment sharing, memory optimization, cold start reduction]
  - **authors:** Jialiang Huang, Teng Ma, Zheng Liu, Sixing Lin, Kang Chen, Jinlei Jiang, Xia Liao, Yingdi Shan, Yongwei Wu, Ning Zhang, Mengting Lu, Tao Ma, Haifeng Gong, Mingxing Zhang
  - **institution:** Tsinghua University, Alibaba Group, Zhejiang University, Intel
  - **link:** http://arxiv.org/pdf/2509.09525v1
  - **Simple LLM Summary:** TrEnv introduces a serverless platform that transparently shares execution environments across functions and nodes using repurposable sandboxes and memory templates. It reduces startup latency by up to 7x and memory usage by 48% for container-based settings, and achieves 58% lower P99 latency with 61% memory savings for VM-based agents compared to state-of-the-art systems. The platform is specifically optimized for LLM agent workloads with unpredictable invocation patterns.

## 2025-09-12

- **[arXiv2509] The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards
  Autonomous Science**
  - **tags:** [mlsys], [others], [autonomous science, scientific workflows, AI agents, swarm intelligence, workflow evolution]
  - **authors:** Woong Shin, Renan Souza, Daniel Rosendo, Frédéric Suter, Feiyi Wang, Prasanna Balaprakash, Rafael Ferreira da Silva
  - **institution:** Oak Ridge National Laboratory
  - **link:** http://arxiv.org/pdf/2509.09915v1
  - **Simple LLM Summary:** The paper proposes a conceptual framework where scientific workflows evolve along intelligence and composition dimensions, transitioning from static systems to intelligent swarm-based architectures. It presents an architectural blueprint for autonomous science laboratories that integrate AI agents as ecosystem components. The approach aims to achieve 100x acceleration in scientific discovery by enabling fully autonomous, distributed scientific workflows.

- **[arXiv2509] SynergAI: Edge-to-Cloud Synergy for Architecture-Driven High-Performance
  Orchestration for AI Inference**
  - **tags:** [mlsys], [scheduling], [edge-to-cloud computing, AI inference, performance-aware scheduling, Kubernetes, QoS optimization]
  - **authors:** Foteini Stathopoulou, Aggelos Ferikoglou, Manolis Katsaragakis, Dimosthenis Masouros, Sotirios Xydis, Dimitrios Soudris
  - **institution:** National Technical University of Athens
  - **link:** http://arxiv.org/pdf/2509.12252v1
  - **Simple LLM Summary:** SynergAI introduces an architecture-aware framework for AI inference serving across heterogeneous edge-to-cloud infrastructures, combining offline and online decision-making policies for intelligent workload scheduling. The system dynamically allocates workloads across diverse hardware architectures to minimize QoS violations. Evaluation shows it achieves 2.4x fewer QoS violations compared to state-of-the-art solutions through optimized architecture-aware deployments.

- **[arXiv2509] MinatoLoader: Accelerating Machine Learning Training Through Efficient
  Data Preprocessing**
  - **tags:** [mlsys], [Other models training], [data loader, data preprocessing, GPU utilization, batch construction, PyTorch, training acceleration]
  - **authors:** Rahma Nouaji, Stella Bitchebe, Ricardo Macedo, Oana Balmau
  - **institution:** McGill University, INESC TEC, University of Minho
  - **link:** http://arxiv.org/pdf/2509.10712v1
  - **Simple LLM Summary:** MinatoLoader introduces a novel data loading approach that prioritizes fast-to-preprocess samples and processes slower samples in parallel to address GPU idleness caused by preprocessing variability. The system achieves up to 7.5× training speedup and increases GPU utilization from 46.4% to 90.45% while maintaining model accuracy across various workloads. This demonstrates significant improvements over existing data loaders like PyTorch DataLoader and DALI in single-server multi-GPU setups.

- **[arXiv2509] Coordinated Reinforcement Learning Prefetching Architecture for
  Multicore Systems**
  - **tags:** [mlsys], [Other models inference], [reinforcement learning, hardware prefetching, multicore systems, coordinated learning, memory bandwidth optimization]
  - **authors:** Mohammed Humaid Siddiqui, Fernando Guzman, Yufei Wu, Ruishu Ann
  - **institution:** Fairleigh Dickinson University
  - **link:** http://arxiv.org/pdf/2509.10719v1
  - **Simple LLM Summary:** The paper proposes CRL-Pythia, a coordinated reinforcement learning prefetcher that enables cross-core information sharing and cooperative prefetching decisions in multicore systems. This approach significantly reduces redundant prefetch requests and improves learning convergence across cores. Experimental results show CRL-Pythia achieves approximately 12% IPC improvement for bandwidth-constrained workloads while maintaining moderate hardware overhead.

## 2025-09-13

- **[arXiv2509] FastTrack: GPU-Accelerated Tracking for Visual SLAM**
  - **tags:** [mlsys], [Other models inference], [GPU acceleration, visual SLAM, stereo feature matching, local map tracking, CUDA implementation, real-time performance]
  - **authors:** Kimia Khabiri, Parsa Hosseininejad, Shishir Gopinath, Karthik Dantu, Steven Y. Ko
  - **institution:** University at Buffalo (based on author affiliations: 1 - University at Buffalo, 2 - University of Texas at Austin)
  - **link:** http://arxiv.org/pdf/2509.10757v1
  - **Simple LLM Summary:** This paper presents FastTrack, a GPU-accelerated tracking system for visual-inertial SLAM that uses CUDA to speed up stereo feature matching and local map tracking. The implementation within ORB-SLAM3 demonstrates up to 2.8× performance improvement on both desktop and embedded platforms. Evaluation using EuRoC and TUM-VI datasets confirms the effectiveness of GPU acceleration for real-time SLAM applications.

## 2025-09-14

- **[arXiv2509] Parallel/Distributed Tabu Search for Scheduling Microprocessor Tasks in
  Hybrid Flowshop**
  - **tags:** [mlsys], [scheduling], [tabu search, hybrid flow shop, distributed computing, multiprocessor tasks, makespan minimization]
  - **authors:** Adam Janiak, Damian Kowalczyk, Maciej Lichtenstein
  - **institution:** Wrocław University of Technology
  - **link:** http://arxiv.org/pdf/2509.11396v1
  - **Simple LLM Summary:** This paper presents a parallel/distributed tabu search algorithm for scheduling multiprocessor tasks in hybrid flow shop environments to minimize makespan. The proposed method effectively balances computational load across heterogeneous networks while evaluating neighborhood solutions. The algorithm demonstrates efficient performance for solving this NP-hard scheduling problem through distributed computing mechanisms.

- **[arXiv2509] Chameleon: Taming Dynamic Operator Sequences for Memory-Intensive LLM
  Training**
  - **tags:** [mlsys], [LLM training], [memory optimization, swap mechanism, dynamic operator sequences, eager mode, online profiling]
  - **authors:** Zibo Wang, Yuhang Zhou, Zhibin Wang, Shipeng Li, Xinjing Huang, Chendong Cai, Bingxu Mu, Yuqing Sun, Zhiheng Hu, Bin She, Shu You, Guanghuan Fang, Rong Gu, Wanchun Dou, Guihai Chen, Chen Tian
  - **institution:** Nanjing University, Huawei Technologies Co., Ltd
  - **link:** http://arxiv.org/pdf/2509.11076v1
  - **Simple LLM Summary:** Chameleon introduces a lightweight online profiler to handle dynamic operator sequences in Eager Mode and generates effective swap policies for memory optimization during LLM training. It reduces profiling overhead by 84.25% and enables training models up to 4x larger than hardware memory. The system improves performance by up to 38.94% compared to traditional methods like recomputation or high-degree parallelism.

- **[arXiv2509] GFS: A Preemption-aware Scheduling Framework for GPU Clusters with
  Predictive Spot Instance Management**
  - **tags:** [mlsys], [scheduling], [GPU scheduling, preemption-aware, spot instances, resource management, predictive allocation]
  - **authors:** Jiaang Duan, Shenglin Xu, Shiyou Qian, Dingyu Yang, Kangjin Wang, Chenzhi Liao, Yinghao Yu, Qin Hua, Hanwen Hu, Qi Wang, Wenchao Wu, Dongqing Bao, Tianyu Lu, Jian Cao, Guangtao Xue, Guodong Yang, Liping Zhang, Gang Chen
  - **institution:** Shanghai Jiao Tong University, Zhejiang University, Alibaba Group
  - **link:** http://arxiv.org/pdf/2509.11134v1
  - **Simple LLM Summary:** GFS introduces a preemption-aware scheduling framework with predictive spot instance management for GPU clusters. It uses demand forecasting, dynamic quota allocation, and preemptive scheduling to prioritize high-priority tasks while minimizing impact on low-priority ones. Results show 33.0% lower eviction rates, 44.1% reduced queuing delays, and significant cost savings in production clusters.

- **[arXiv2509] VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing
  for Energy-Efficient LLM Serving**
  - **tags:** [mlsys], [LLM inference], [energy efficiency, frequency scaling, request routing, SLO-aware serving, prefill-decode disaggregation]
  - **authors:** Jiahuan Yu, Aryan Taneja, Junfeng Lin, Minjia Zhang
  - **institution:** University of Illinois Urbana-Champaign, Tsinghua University
  - **link:** http://arxiv.org/pdf/2509.04827v2
  - **Simple LLM Summary:** VoltanaLLM introduces a control-theoretic approach combining feedback-driven frequency control and state-space routing in prefill/decode disaggregated architectures. It dynamically adjusts GPU frequencies and routes requests to minimize energy consumption while maintaining latency SLOs. The system achieves up to 36.3% energy savings with near-perfect SLO attainment across multiple LLMs and real-world datasets.
