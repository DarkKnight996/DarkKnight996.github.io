## 2025-10-21

- **[arXiv2510] MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long
  Context Training**
  - **tags:** [mlsys], [LLM training], [distributed training, dynamic sparse attention, ultra-long context, computational efficiency]
  - **authors:** Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu
  - **institution:** Microsoft Research
  - **link:** http://arxiv.org/pdf/2510.18830v1
  - **Simple LLM Summary:** MTraining introduces a distributed methodology using dynamic sparse attention with three key components to enable efficient LLM training with ultra-long contexts. It successfully expanded Qwen2.5-3B's context window from 32K to 512K tokens on 32 A100 GPUs. The approach achieves up to 6x higher training throughput while maintaining model accuracy across various benchmarks.

- **[arXiv2510] SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices**
  - **tags:** [mlsys], [scheduling], [LLM inference, edge computing, SLO-driven scheduling, dynamic rate control, utility maximization]
  - **authors:** Pan Zhou, Yiming Lei, Ling Liu, Xiaoqiong Xu, Ying Cai, Daji Ergu, Hongfang Yu, Yueyue Dai
  - **institution:** Southwest Minzu University
  - **link:** http://arxiv.org/pdf/2510.18544v1
  - **Simple LLM Summary:** SLICE proposes an SLO-driven scheduling framework combining utility-maximizing request scheduling with dynamic iterative generation rate control for LLM inference on edge devices. It significantly improves SLO attainment compared to state-of-the-art systems like Orca and FastServe. Experimental results show up to 35x higher SLO attainment and 3.4x faster task completion times.

- **[arXiv2510] Comparative analysis of large data processing in Apache Spark using
  Java, Python and Scala**
  - **tags:** [mlsys], [Other models inference], [Apache Spark, programming languages, performance comparison, data processing, ETL workflows, Apache Iceberg]
  - **authors:** Ivan Borodii, Illia Fedorovych, Halyna Osukhivska, Diana Velychko, Roman Butsii
  - **institution:** Ternopil Ivan Puluj National Technical University
  - **link:** http://arxiv.org/pdf/2510.19012v1
  - **Simple LLM Summary:** This study compares Apache Spark's performance across Java, Python and Scala by executing ETL operations including data loading from CSV files, transformation and loading into Apache Iceberg tables. Results show Python performs best with small datasets (5MB), while Scala excels in complex operations combining multiple files. The programming language choice significantly impacts Spark processing efficiency, with optimal selection depending on data size and operation complexity.

- **[arXiv2510] A Distributed Framework for Causal Modeling of Performance Variability
  in GPU Traces**
  - **tags:** [mlsys], [trace analysis], [GPU traces, performance variability, causal modeling, distributed framework, HPC]
  - **authors:** Ankur Lahiry, Ayush Pokharel, Banooqa Banday, Seth Ockerman, Amal Gueroudji, Mohammad Zaeed, Tanzima Z. Islam, Line Pouchard
  - **institution:** Texas State University
  - **link:** http://arxiv.org/pdf/2510.18300v1
  - **Simple LLM Summary:** This paper presents a distributed framework that partitions and processes GPU trace data concurrently using causal graph methods and parallel coordinating charts. The approach exposes performance variability and dependencies across execution flows in HPC systems. Experimental results show a 67% improvement in scalability for analyzing multiple traces independently.

- **[arXiv2510] Tokencake: A KV-Cache-centric Serving Framework for LLM-based
  Multi-Agent Applications**
  - **tags:** [mlsys], [LLM inference], [KV-Cache optimization, multi-agent systems, GPU memory management, scheduling algorithms]
  - **authors:** Zhuohang Bian, Feiyang Wu, Teng Ma, Youwei Zhuo
  - **institution:** Beihang University, Peking University, Alibaba
  - **link:** http://arxiv.org/pdf/2510.18586v1
  - **Simple LLM Summary:** Tokencake introduces a KV-Cache-centric serving framework that co-optimizes scheduling and memory management for LLM-based multi-agent applications. It uses dynamic memory partitioning and proactive offload mechanisms to handle space contention and time underutilization. Evaluation shows it reduces latency by over 47.06% and improves GPU memory utilization by up to 16.9% compared to vLLM.


## 2025-10-22

- **[arXiv2510] Serverless GPU Architecture for Enterprise HR Analytics: A
  Production-Scale BDaaS Implementation**
  - **tags:** [mlsys], [Other models inference], [serverless GPU, TabNet, enterprise analytics, compliance, cost efficiency, interpretability]
  - **authors:** Guilin Zhang, Wulan Guo, Ziqi Tan, Srinivas Vippagunta, Suchitra Raman, Shreeshankar Chatterjee, Ju Lin, Shang Liu, Mary Schladenhauffen, Jeffrey Luo, Hailong Jiang
  - **institution:** George Washington University and Workday, Inc.
  - **link:** http://arxiv.org/pdf/2510.19689v1
  - **Simple LLM Summary:** This paper presents a serverless GPU architecture integrating TabNet for enterprise HR analytics, achieving significant improvements in throughput and latency while maintaining compliance. The design combines GPU acceleration with serverless elasticity to reduce costs and ensure interpretability. Results show 4.5x higher throughput and 90% lower cost per inference compared to Spark baselines, with minimal compliance overhead.

- **[arXiv2510] Enabling Reconfiguration-Communication Overlap for Collective
  Communication in Optical Networks**
  - **tags:** [mlsys], [cluster infrastructure], [optical networks, collective communication, network reconfiguration, distributed machine learning]
  - **authors:** Changbo Wu, Zhuolong Yu, Gongming Zhao, Hongli Xu
  - **institution:** University of Science and Technology of China
  - **link:** http://arxiv.org/pdf/2510.19322v1
  - **Simple LLM Summary:** SWOT introduces a demand-aware optical network framework that enables intra-collective reconfiguration and overlaps optical switch reconfigurations with ongoing transmissions. This approach dynamically aligns network resources with collective communication traffic patterns in distributed machine learning. Simulation results demonstrate significant performance improvements over traditional one-shot reconfiguration strategies.

- **[arXiv2510] HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via
  Hybrid Expert/Data Transmission**
  - **tags:** [mlsys], [LLM training], [mixture-of-experts, expert parallelism, cross-datacenter training, communication optimization, hybrid transmission]
  - **authors:** Weihao Yang, Hao Huang, Donglei Wu, Ningke Li, Yanqi Pan, Qiyang Zheng, Wen Xia, Shiyi Li, Qiang Wang
  - **institution:** Harbin Institute of Technology, Shenzhen
  - **link:** http://arxiv.org/pdf/2510.19470v1
  - **Simple LLM Summary:** HybridEP proposes a framework that dynamically transforms expert placement and uses hybrid expert/data transmission to reduce communication overhead in cross-datacenter MoE training. It employs domain-based partitioning and parameter-efficient migration guided by a stream-based model. Experimental results show HybridEP achieves up to 5.6x speedup over existing systems under constrained bandwidth.

- **[arXiv2510] RLBoost: Harvesting Preemptible Resources for Cost-Efficient
  Reinforcement Learning on LLMs**
  - **tags:** [mlsys], [LLM training], [reinforcement learning, preemptible resources, cost efficiency, rollout optimization, cluster scheduling, resource utilization]
  - **authors:** Yongji Wu, Xueshen Liu, Haizhong Zheng, Juncheng Gu, Beidi Chen, Z. Morley Mao, Arvind Krishnamurthy, Ion Stoica
  - **institution:** UC Berkeley, Google, CMU, University of Michigan
  - **link:** http://arxiv.org/pdf/2510.19225v1
  - **Simple LLM Summary:** RLBoost introduces a hybrid architecture that efficiently utilizes preemptible GPU resources for RL training on LLMs through adaptive rollout offload, pull-based weight transfer, and token-level response migration. This approach significantly improves training throughput by 1.51x-1.97x while reducing costs by 28%-49% compared to using only on-demand resources. The system effectively addresses resource under-utilization in RL workflows by leveraging the stateless nature of rollout stages.

- **[arXiv2510] LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework
  For Controllable Lyric Translation**
  - **tags:** [mlsys], [Other models training], [lyric translation, curriculum learning, reinforcement learning, unsupervised learning, controllable translation]
  - **authors:** Le Ren, Xiangjian Zeng, Qingqiang Wu, Ruoxuan Liang
  - **institution:** Xiamen University
  - **link:** http://arxiv.org/pdf/2510.19967v1
  - **Simple LLM Summary:** LyriCAR proposes a difficulty-aware curriculum reinforcement learning framework for controllable lyric translation, using adaptive curriculum strategies to guide training with progressively complex challenges. The method achieves state-of-the-art performance in EN-ZH lyric translation while reducing training steps by nearly 40%. Experimental results demonstrate superior performance across both standard translation metrics and multi-dimensional reward scores.

- **[arXiv2510] Next-Generation Event-Driven Architectures: Performance, Scalability,
  and Intelligent Orchestration Across Messaging Frameworks**
  - **tags:** [mlsys], [scheduling], [event-driven architectures, messaging frameworks, AI-enhanced orchestration, performance benchmarking, reinforcement learning, predictive scaling]
  - **authors:** Jahidul Arafat, Fariha Tasmin, Sanjaya Poudel
  - **institution:** Auburn University
  - **link:** http://arxiv.org/pdf/2510.04404v2
  - **Simple LLM Summary:** This paper introduces AIEO, an AI-enhanced event orchestration system using machine learning for predictive scaling and reinforcement learning for dynamic resource allocation. It comprehensively benchmarks 12 messaging frameworks across different workloads and demonstrates significant improvements in latency, resource utilization, and cost optimization. The research provides standardized benchmarking methodologies and intelligent orchestration solutions for next-generation distributed systems.

- **[arXiv2510] AdaSPEC: Selective Knowledge Distillation for Efficient Speculative
  Decoders**
  - **tags:** [mlsys], [LLM inference], [speculative decoding, knowledge distillation, token filtering, model alignment, efficiency optimization]
  - **authors:** Yuezhou Hu, Jiaxin Guo, Xinyu Feng, Tuo Zhao
  - **institution:** University of California, Berkeley, Tsinghua University, Georgia Institute of Technology
  - **link:** http://arxiv.org/pdf/2510.19779v1
  - **Simple LLM Summary:** AdaSPEC introduces selective token filtering during knowledge distillation to improve speculative decoding efficiency. By filtering out difficult-to-fit tokens using a reference model, it enhances draft-target model alignment on simpler tokens. The method achieves up to 15% higher token acceptance rates than DistillSpec while maintaining generation quality across various tasks.

- **[arXiv2510] Serverless GPU Architecture for Enterprise HR Analytics: A
  Production-Scale BDaaS Implementation**
  - **tags:** [mlsys], [Other models inference], [serverless GPU, TabNet, enterprise analytics, compliance, HR analytics, BDaaS, interpretability]
  - **authors:** Guilin Zhang, Wulan Guo, Ziqi Tan, Srinivas Vippagunta, Suchitra Raman, Shreeshankar Chatterjee, Ju Lin, Shang Liu, Mary Schladenhauffen, Jeffrey Luo, Hailong Jiang
  - **institution:** George Washington University and Workday, Inc.
  - **link:** http://arxiv.org/pdf/2510.19689v1
  - **Simple LLM Summary:** This paper presents a serverless GPU architecture integrated with TabNet for enterprise HR analytics, achieving significant improvements in throughput, latency, and cost compared to Spark baselines. The design ensures compliance through feature-mask interpretability while maintaining performance under load. The implementation provides a practical blueprint for secure and efficient analytics in regulated environments.

## 2025-10-23

- **[arXiv2510] Collective Communication for 100k+ GPUs**
  - **tags:** [mlsys], [LLM training], [collective communication, NCCLX, large-scale GPU clusters, distributed training, communication efficiency]
  - **authors:** Min Si, Pavan Balaji, Yongzhou Chen, Ching-Hsiang Chu, Adi Gangidi, Saif Hasan, Subodh Iyengar, Dan Johnson, Bingzhe Liu, Jingliang Ren, Ashmitha Jeevaraj Shetty, Greg Steinbrecher, Xinfeng Xie, Yulun Wang, Bruce Wu, Jingyi Yang, Mingran Yang, Minlan Yu, Cen Zhao, Wes Bland, Denis Boyda, Suman Gumudavelli, Cristian Lumezanu, Rui Miao, Zhe Qu, Venkat Ramesh, Maxim Samoylov, Jan Seidel, Feng Tian, Qiye Tan, Shuqiang Zhang, Yimeng Zhao, Shengbao Zheng, Art Zhu, Hongyi Zeng
  - **institution:** Meta
  - **link:** http://arxiv.org/pdf/2510.20171v1
  - **Simple LLM Summary:** Meta developed NCCLX, a collective communication framework optimized for large-scale GPU clusters exceeding 100,000 GPUs. The framework addresses throughput and latency limitations in traditional methods for LLM workloads. Empirical evaluation on Llama4 demonstrates substantial improvements in communication efficiency for training and inference.

- **[arXiv2510] Morpheus: Lightweight RTT Prediction for Performance-Aware Load
  Balancing**
  - **tags:** [mlsys], [cluster infrastructure], [RTT prediction, load balancing, Kubernetes, performance monitoring, edge computing]
  - **authors:** Panagiotis Giannakopoulos, Bart van Knippenberg, Kishor Chandra Joshi, Nicola Calabretta, George Exarchakos
  - **institution:** Eindhoven University of Technology, Thermo Fisher Scientific
  - **link:** http://arxiv.org/pdf/2510.20506v1
  - **Simple LLM Summary:** This paper develops lightweight RTT predictors using time-series monitoring data from Kubernetes GPU clusters to enable performance-aware load balancing. The approach achieves 95% accuracy with low overhead by selecting highly correlated metrics. Results show significant RTT reduction and resource efficiency improvements, demonstrating feasibility for production systems.

- **[arXiv2510] GPU-Accelerated Primal Heuristics for Mixed Integer Programming**
  - **tags:** [mlsys], [Other models inference], [GPU acceleration, mixed integer programming, primal heuristics, feasibility pump, feasibility jump, fix-and-propagate]
  - **authors:** Akif Çördük, Piotr Sielski, Alice Boucher, Kumar Aatish
  - **institution:** Nvidia
  - **link:** http://arxiv.org/pdf/2510.20499v1
  - **Simple LLM Summary:** This paper introduces GPU-accelerated primal heuristics for Mixed Integer Programming, combining several state-of-the-art methods like Feasibility Pump with GPU parallelization. The approach uses a GPU-accelerated PDLP as an approximate LP solver and a new probing cache for faster operations. Results show significant improvements in finding feasible solutions and objective quality on the MIPLIB2017 benchmark.

- **[arXiv2510] AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for
  Scalable LLM Training**
  - **tags:** [mlsys], [LLM training], [distributed training, ZeRO optimization, communication efficiency, asynchronous scheduling, memory optimization]
  - **authors:** Huawei Bai, Yifan Huang, Wenqi Shi, Ansheng You, Feifan Shao, Tengfei Han, Minghui Yu
  - **institution:** ByteDance
  - **link:** http://arxiv.org/pdf/2510.20111v1
  - **Simple LLM Summary:** AsyncHZP introduces an asynchronous hierarchical variant of ZeRO that adaptively reshards parameters across replica groups and uses multi-stream scheduling to overlap communication with computation. The method significantly reduces communication overhead while maintaining memory efficiency. Empirical results show it outperforms traditional ND parallelism and achieves state-of-the-art performance for both dense and MoE models at scale.

- **[arXiv2510] WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables**
  - **tags:** [mlsys], [kernels], [GPU hash tables, concurrent data structures, performance optimization, benchmarking framework]
  - **authors:** Hunter McCoy, Prashant Pandey
  - **institution:** Northeastern University
  - **link:** http://arxiv.org/pdf/2509.16407v2
  - **Simple LLM Summary:** WarpSpeed implements eight concurrent GPU hash table designs with optimization techniques like fingerprint-based metadata and specialized GPU instructions. The library provides a unified benchmarking framework and rich API for modern GPU applications. Evaluation shows improved performance and scalability across diverse workloads and real-world applications.

- **[arXiv2510] FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data
  Systems**
  - **tags:** [mlsys], [Other models inference], [Viterbi decoding, resource efficiency, FPGA acceleration, edge computing, adaptive algorithms]
  - **authors:** Ziheng Deng, Xue Liu, Jiantong Jiang, Yankai Li, Qingxu Deng, Xiaochun Yang
  - **institution:** Northeastern University, The University of Melbourne
  - **link:** http://arxiv.org/pdf/2510.19301v2
  - **Simple LLM Summary:** FLASH Viterbi introduces a fast and adaptive Viterbi decoding operator using non-recursive divide-and-conquer with pruning and parallelization techniques. It also presents FLASH-BS Viterbi with dynamic beam search for memory efficiency and includes FPGA-based hardware accelerators. The algorithms demonstrate superior performance in decoding time and memory efficiency while maintaining adaptability for modern data systems.

- **[arXiv2510] FlashMP: Fast Discrete Transform-Based Solver for Preconditioning
  Maxwell's Equations on GPUs**
  - **tags:** [mlsys], [kernels], [GPU computing, Maxwell's equations, preconditioning, discrete transforms, domain decomposition, parallel efficiency]
  - **authors:** Haoyuan Zhang, Yaqian Gao, Xinxin Zhang, Jialin Li, Runfeng Jin, Yidong Chen, Feng Zhang, Wu Yuan, Wenpeng Ma, Shan Liang, Jian Zhang, Zhonghua Lu
  - **institution:** Chinese Academy of Sciences, University of Chinese Academy of Sciences, Tsinghua University, Xinyang Normal University
  - **link:** http://arxiv.org/pdf/2508.07193v2
  - **Simple LLM Summary:** FlashMP introduces a novel GPU-based preconditioning system using discrete transforms to solve Maxwell's equations efficiently. It employs domain decomposition for multi-GPU scalability and achieves significant speedups over existing libraries. Evaluations show up to 16x reduction in iteration counts and 2.5x-4.9x speedups with high parallel efficiency on large GPU clusters.

- **[arXiv2510] RailS: Load Balancing for All-to-All Communication in Distributed
  Mixture-of-Experts Training**
  - **tags:** [mlsys], [LLM training], [load balancing, all-to-all communication, mixture-of-experts, distributed training, network optimization]
  - **authors:** Heng Xu, Zhiwei Yu, Chengze Du, Ying Zhou, Letian Li, Haojie Wang, Weiqiang Cheng, Jialong Li
  - **institution:** Shenzhen University of Advanced Technology, Tsinghua University, Chinese University of Hong Kong, China Mobile
  - **link:** http://arxiv.org/pdf/2510.19262v2
  - **Simple LLM Summary:** RailS introduces a distributed load-balancing framework that leverages Rail topology symmetry and local LPT scheduling to optimize all-to-all communication in MoE training. It activates parallel rails for multipath transmission and achieves near-optimal load balance. The system improves bandwidth by 20%-78% and reduces iteration time by 18%-40% for Mixtral workloads.

- **[arXiv2510] Symbiosis: Multi-Adapter Inference and Fine-Tuning**
  - **tags:** [mlsys], [finetuning], [parameter-efficient fine-tuning, multi-adapter inference, resource optimization, model sharing]
  - **authors:** Saransh Gupta, Umesh Deshpande, Travis Janssen, Swami Sundararaman
  - **institution:** IBM Research
  - **link:** http://arxiv.org/pdf/2507.03220v3
  - **Simple LLM Summary:** Symbiosis introduces a base-model-as-a-service approach that enables sharing frozen base model layers across multiple fine-tuning or inference processes. It uses split-execution to decouple client-specific adapters from shared base layers, allowing independent resource management and mixing of PEFT methods. The system demonstrates efficient simultaneous fine-tuning of 20 Gemma2-27B adapters on 8 GPUs while improving GPU utilization and preserving user privacy.

- **[arXiv2510] Transferable Graph Learning for Transmission Congestion Management via
  Busbar Splitting**
  - **tags:** [mlsys], [Other models inference], [graph neural networks, network topology optimization, congestion management, power systems, transfer learning]
  - **authors:** Ali Rajaei, Peter Palensky, Jochen L. Cremer
  - **institution:** Delft University of Technology
  - **link:** http://arxiv.org/pdf/2510.20591v1
  - **Simple LLM Summary:** This paper proposes a graph neural network approach for network topology optimization via busbar splitting to manage transmission congestion. The method uses heterogeneous edge-aware message passing to predict effective splitting actions, achieving significant speed-up while maintaining solution quality. Results demonstrate AC-feasible solutions within one minute with only 2.3% optimality gap on large-scale systems.

- **[arXiv2510] Symbolic Regression and Differentiable Fits in Beyond the Standard Model
  Physics**
  - **tags:** [mlsys], [Other models inference], [symbolic regression, particle physics, supersymmetry, differentiable methods, neural networks]
  - **authors:** Shehu AbdusSalam, Steven Abel, Deaglan Bartlett, Miguel Crispim Romão
  - **institution:** Shahid Beheshti University, Durham University, CNRS, Sorbonne Université, University of Oxford
  - **link:** http://arxiv.org/pdf/2510.20453v1
  - **Simple LLM Summary:** This paper applies symbolic regression to derive analytical expressions for physical observables in Beyond Standard Model physics. The method produces accurate differentiable fits that enable efficient parameter estimation compared to conventional sampling approaches. Symbolic regression demonstrates superior global robustness over neural networks while achieving comparable performance.

- **[arXiv2510] Competition is the key: A Game Theoretic Causal Discovery Approach**
  - **tags:** [mlsys], [Other models training], [causal discovery, reinforcement learning, game theory, finite-sample guarantees, DDQN, GES, GraN-DAG]
  - **authors:** Amartya Roy, Souvik Chakraborty
  - **institution:** Indian Institute of Technology Delhi, Robert Bosch GmbH
  - **link:** http://arxiv.org/pdf/2510.20106v1
  - **Simple LLM Summary:** This paper introduces a game-theoretic reinforcement learning framework for causal discovery where a DDQN agent competes against baseline algorithms like GES or GraN-DAG. The method provides provable finite-sample guarantees and consistently outperforms existing approaches on both synthetic and real-world benchmarks. It achieves scalability to large graphs while unifying strong empirical performance with rigorous theoretical foundations.

- **[arXiv2510] A Survey on Cache Methods in Diffusion Models: Toward Efficient
  Multi-Modal Generation**
  - **tags:** [mlsys], [Other models inference], [diffusion models, cache-based acceleration, inference optimization, feature reuse, computational redundancy]
  - **authors:** Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang
  - **institution:** Shanghai Jiao Tong University, Tsinghua University, The Hong Kong University of Science and Technology
  - **link:** http://arxiv.org/pdf/2510.19755v2
  - **Simple LLM Summary:** This survey introduces Diffusion Caching as a training-free method that identifies and reuses computational redundancies in diffusion models through feature-level cross-step reuse and inter-layer scheduling. The approach reduces computational overhead without modifying model parameters while maintaining generation quality. The paper shows caching methods have evolved from static reuse to dynamic prediction and can integrate with other acceleration techniques for efficient multimodal generation.
