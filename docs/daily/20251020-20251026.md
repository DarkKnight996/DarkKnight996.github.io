## 2025-10-21

- **[arXiv2510] MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long
  Context Training**
  - **tags:** [mlsys], [LLM training], [distributed training, dynamic sparse attention, ultra-long context, computational efficiency]
  - **authors:** Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu
  - **institution:** Microsoft Research
  - **link:** http://arxiv.org/pdf/2510.18830v1
  - **Simple LLM Summary:** MTraining introduces a distributed methodology using dynamic sparse attention with three key components to enable efficient LLM training with ultra-long contexts. It successfully expanded Qwen2.5-3B's context window from 32K to 512K tokens on 32 A100 GPUs. The approach achieves up to 6x higher training throughput while maintaining model accuracy across various benchmarks.

- **[arXiv2510] SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices**
  - **tags:** [mlsys], [scheduling], [LLM inference, edge computing, SLO-driven scheduling, dynamic rate control, utility maximization]
  - **authors:** Pan Zhou, Yiming Lei, Ling Liu, Xiaoqiong Xu, Ying Cai, Daji Ergu, Hongfang Yu, Yueyue Dai
  - **institution:** Southwest Minzu University
  - **link:** http://arxiv.org/pdf/2510.18544v1
  - **Simple LLM Summary:** SLICE proposes an SLO-driven scheduling framework combining utility-maximizing request scheduling with dynamic iterative generation rate control for LLM inference on edge devices. It significantly improves SLO attainment compared to state-of-the-art systems like Orca and FastServe. Experimental results show up to 35x higher SLO attainment and 3.4x faster task completion times.

- **[arXiv2510] Comparative analysis of large data processing in Apache Spark using
  Java, Python and Scala**
  - **tags:** [mlsys], [Other models inference], [Apache Spark, programming languages, performance comparison, data processing, ETL workflows, Apache Iceberg]
  - **authors:** Ivan Borodii, Illia Fedorovych, Halyna Osukhivska, Diana Velychko, Roman Butsii
  - **institution:** Ternopil Ivan Puluj National Technical University
  - **link:** http://arxiv.org/pdf/2510.19012v1
  - **Simple LLM Summary:** This study compares Apache Spark's performance across Java, Python and Scala by executing ETL operations including data loading from CSV files, transformation and loading into Apache Iceberg tables. Results show Python performs best with small datasets (5MB), while Scala excels in complex operations combining multiple files. The programming language choice significantly impacts Spark processing efficiency, with optimal selection depending on data size and operation complexity.

- **[arXiv2510] A Distributed Framework for Causal Modeling of Performance Variability
  in GPU Traces**
  - **tags:** [mlsys], [trace analysis], [GPU traces, performance variability, causal modeling, distributed framework, HPC]
  - **authors:** Ankur Lahiry, Ayush Pokharel, Banooqa Banday, Seth Ockerman, Amal Gueroudji, Mohammad Zaeed, Tanzima Z. Islam, Line Pouchard
  - **institution:** Texas State University
  - **link:** http://arxiv.org/pdf/2510.18300v1
  - **Simple LLM Summary:** This paper presents a distributed framework that partitions and processes GPU trace data concurrently using causal graph methods and parallel coordinating charts. The approach exposes performance variability and dependencies across execution flows in HPC systems. Experimental results show a 67% improvement in scalability for analyzing multiple traces independently.

- **[arXiv2510] Tokencake: A KV-Cache-centric Serving Framework for LLM-based
  Multi-Agent Applications**
  - **tags:** [mlsys], [LLM inference], [KV-Cache optimization, multi-agent systems, GPU memory management, scheduling algorithms]
  - **authors:** Zhuohang Bian, Feiyang Wu, Teng Ma, Youwei Zhuo
  - **institution:** Beihang University, Peking University, Alibaba
  - **link:** http://arxiv.org/pdf/2510.18586v1
  - **Simple LLM Summary:** Tokencake introduces a KV-Cache-centric serving framework that co-optimizes scheduling and memory management for LLM-based multi-agent applications. It uses dynamic memory partitioning and proactive offload mechanisms to handle space contention and time underutilization. Evaluation shows it reduces latency by over 47.06% and improves GPU memory utilization by up to 16.9% compared to vLLM.


## 2025-10-22

- **[arXiv2510] Serverless GPU Architecture for Enterprise HR Analytics: A
  Production-Scale BDaaS Implementation**
  - **tags:** [mlsys], [Other models inference], [serverless GPU, TabNet, enterprise analytics, compliance, cost efficiency, interpretability]
  - **authors:** Guilin Zhang, Wulan Guo, Ziqi Tan, Srinivas Vippagunta, Suchitra Raman, Shreeshankar Chatterjee, Ju Lin, Shang Liu, Mary Schladenhauffen, Jeffrey Luo, Hailong Jiang
  - **institution:** George Washington University and Workday, Inc.
  - **link:** http://arxiv.org/pdf/2510.19689v1
  - **Simple LLM Summary:** This paper presents a serverless GPU architecture integrating TabNet for enterprise HR analytics, achieving significant improvements in throughput and latency while maintaining compliance. The design combines GPU acceleration with serverless elasticity to reduce costs and ensure interpretability. Results show 4.5x higher throughput and 90% lower cost per inference compared to Spark baselines, with minimal compliance overhead.

- **[arXiv2510] Enabling Reconfiguration-Communication Overlap for Collective
  Communication in Optical Networks**
  - **tags:** [mlsys], [cluster infrastructure], [optical networks, collective communication, network reconfiguration, distributed machine learning]
  - **authors:** Changbo Wu, Zhuolong Yu, Gongming Zhao, Hongli Xu
  - **institution:** University of Science and Technology of China
  - **link:** http://arxiv.org/pdf/2510.19322v1
  - **Simple LLM Summary:** SWOT introduces a demand-aware optical network framework that enables intra-collective reconfiguration and overlaps optical switch reconfigurations with ongoing transmissions. This approach dynamically aligns network resources with collective communication traffic patterns in distributed machine learning. Simulation results demonstrate significant performance improvements over traditional one-shot reconfiguration strategies.

- **[arXiv2510] HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via
  Hybrid Expert/Data Transmission**
  - **tags:** [mlsys], [LLM training], [mixture-of-experts, expert parallelism, cross-datacenter training, communication optimization, hybrid transmission]
  - **authors:** Weihao Yang, Hao Huang, Donglei Wu, Ningke Li, Yanqi Pan, Qiyang Zheng, Wen Xia, Shiyi Li, Qiang Wang
  - **institution:** Harbin Institute of Technology, Shenzhen
  - **link:** http://arxiv.org/pdf/2510.19470v1
  - **Simple LLM Summary:** HybridEP proposes a framework that dynamically transforms expert placement and uses hybrid expert/data transmission to reduce communication overhead in cross-datacenter MoE training. It employs domain-based partitioning and parameter-efficient migration guided by a stream-based model. Experimental results show HybridEP achieves up to 5.6x speedup over existing systems under constrained bandwidth.

- **[arXiv2510] RLBoost: Harvesting Preemptible Resources for Cost-Efficient
  Reinforcement Learning on LLMs**
  - **tags:** [mlsys], [LLM training], [reinforcement learning, preemptible resources, cost efficiency, rollout optimization, cluster scheduling, resource utilization]
  - **authors:** Yongji Wu, Xueshen Liu, Haizhong Zheng, Juncheng Gu, Beidi Chen, Z. Morley Mao, Arvind Krishnamurthy, Ion Stoica
  - **institution:** UC Berkeley, Google, CMU, University of Michigan
  - **link:** http://arxiv.org/pdf/2510.19225v1
  - **Simple LLM Summary:** RLBoost introduces a hybrid architecture that efficiently utilizes preemptible GPU resources for RL training on LLMs through adaptive rollout offload, pull-based weight transfer, and token-level response migration. This approach significantly improves training throughput by 1.51x-1.97x while reducing costs by 28%-49% compared to using only on-demand resources. The system effectively addresses resource under-utilization in RL workflows by leveraging the stateless nature of rollout stages.

- **[arXiv2510] LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework
  For Controllable Lyric Translation**
  - **tags:** [mlsys], [Other models training], [lyric translation, curriculum learning, reinforcement learning, unsupervised learning, controllable translation]
  - **authors:** Le Ren, Xiangjian Zeng, Qingqiang Wu, Ruoxuan Liang
  - **institution:** Xiamen University
  - **link:** http://arxiv.org/pdf/2510.19967v1
  - **Simple LLM Summary:** LyriCAR proposes a difficulty-aware curriculum reinforcement learning framework for controllable lyric translation, using adaptive curriculum strategies to guide training with progressively complex challenges. The method achieves state-of-the-art performance in EN-ZH lyric translation while reducing training steps by nearly 40%. Experimental results demonstrate superior performance across both standard translation metrics and multi-dimensional reward scores.

- **[arXiv2510] Next-Generation Event-Driven Architectures: Performance, Scalability,
  and Intelligent Orchestration Across Messaging Frameworks**
  - **tags:** [mlsys], [scheduling], [event-driven architectures, messaging frameworks, AI-enhanced orchestration, performance benchmarking, reinforcement learning, predictive scaling]
  - **authors:** Jahidul Arafat, Fariha Tasmin, Sanjaya Poudel
  - **institution:** Auburn University
  - **link:** http://arxiv.org/pdf/2510.04404v2
  - **Simple LLM Summary:** This paper introduces AIEO, an AI-enhanced event orchestration system using machine learning for predictive scaling and reinforcement learning for dynamic resource allocation. It comprehensively benchmarks 12 messaging frameworks across different workloads and demonstrates significant improvements in latency, resource utilization, and cost optimization. The research provides standardized benchmarking methodologies and intelligent orchestration solutions for next-generation distributed systems.

- **[arXiv2510] AdaSPEC: Selective Knowledge Distillation for Efficient Speculative
  Decoders**
  - **tags:** [mlsys], [LLM inference], [speculative decoding, knowledge distillation, token filtering, model alignment, efficiency optimization]
  - **authors:** Yuezhou Hu, Jiaxin Guo, Xinyu Feng, Tuo Zhao
  - **institution:** University of California, Berkeley, Tsinghua University, Georgia Institute of Technology
  - **link:** http://arxiv.org/pdf/2510.19779v1
  - **Simple LLM Summary:** AdaSPEC introduces selective token filtering during knowledge distillation to improve speculative decoding efficiency. By filtering out difficult-to-fit tokens using a reference model, it enhances draft-target model alignment on simpler tokens. The method achieves up to 15% higher token acceptance rates than DistillSpec while maintaining generation quality across various tasks.

- **[arXiv2510] Serverless GPU Architecture for Enterprise HR Analytics: A
  Production-Scale BDaaS Implementation**
  - **tags:** [mlsys], [Other models inference], [serverless GPU, TabNet, enterprise analytics, compliance, HR analytics, BDaaS, interpretability]
  - **authors:** Guilin Zhang, Wulan Guo, Ziqi Tan, Srinivas Vippagunta, Suchitra Raman, Shreeshankar Chatterjee, Ju Lin, Shang Liu, Mary Schladenhauffen, Jeffrey Luo, Hailong Jiang
  - **institution:** George Washington University and Workday, Inc.
  - **link:** http://arxiv.org/pdf/2510.19689v1
  - **Simple LLM Summary:** This paper presents a serverless GPU architecture integrated with TabNet for enterprise HR analytics, achieving significant improvements in throughput, latency, and cost compared to Spark baselines. The design ensures compliance through feature-mask interpretability while maintaining performance under load. The implementation provides a practical blueprint for secure and efficient analytics in regulated environments.