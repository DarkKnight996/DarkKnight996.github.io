# 20251006-20251012

## 2025-10-06

- **[arXiv2510] OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM
  Training**
  - **tags:** [mlsys], [LLM training], [pipeline parallelism, memory optimization, scheduling optimization, activation offloading]
  - **authors:** Hongpei Li, Han Zhang, Huikang Liu, Dongdong Ge, Yinyu Ye
  - **institution:** Shanghai University of Finance and Economics, National University of Singapore, Shanghai Jiao Tong University, Stanford University
  - **link:** http://arxiv.org/pdf/2510.05186v1
  - **Simple LLM Summary:** This paper proposes OptPipe, a pipeline parallelism optimization method that formulates scheduling as a constrained optimization problem to balance memory usage and computation efficiency. It dynamically optimizes activation offloading and pipeline scheduling based on model structure and hardware configuration. Experimental results show up to 50% reduction in pipeline idle time and improved throughput under memory constraints.

## 2025-10-07

- **[arXiv2510] Adaptive Protein Design Protocols and Middleware**
  - **tags:** [mlsys], [cluster infrastructure], [protein design, high-performance computing, machine learning, adaptive protocols, computational biology]
  - **authors:** Aymen Alsaadi, Jonathan Ash, Mikhail Titov, Matteo Turilli, Andre Merzky, Shantenu Jha, Sagar Khare
  - **institution:** Rutgers University
  - **link:** http://arxiv.org/pdf/2510.06396v1
  - **Simple LLM Summary:** IMPRESS integrates machine learning with high-performance computing for computational protein design. It develops adaptive protocols and middleware that enable dynamic resource allocation and asynchronous workload execution. This approach improves protein design quality consistency and increases throughput efficiency.

- **[arXiv2510] Toward Systems Foundations for Agentic Exploration**
  - **tags:** [mlsys], [checkpointing], [agentic exploration, snapshot/restore mechanisms, fork semantics, external side-effects, native forking]
  - **authors:** Jiakai Xu, Tianle Zhou, Eugene Wu, Kostis Kaffes
  - **institution:** Columbia University
  - **link:** http://arxiv.org/pdf/2510.05556v1
  - **Simple LLM Summary:** This paper benchmarks six snapshot/restore mechanisms and finds current tools like CRIU are too slow for agentic exploration. The authors identify three fundamental challenges: fork semantics, external side-effects, and native forking. They argue that existing systems cannot efficiently support LLM-powered agents that need to branch, backtrack and search across execution paths.

- **[arXiv2510] MuFASA -- Asynchronous Checkpoint for Weakly Consistent Fully Replicated
  Databases**
  - **tags:** [mlsys], [checkpointing], [distributed databases, eventual consistency, asynchronous checkpointing, snapshot algorithms, replicated systems]
  - **authors:** Raaghav Ravishankar, Sandeep Kulkarni, Nitin H Vaidya
  - **institution:** Michigan State University, Georgetown University
  - **link:** http://arxiv.org/pdf/2510.06404v1
  - **Simple LLM Summary:** MuFASA introduces an asynchronous checkpointing algorithm for weakly consistent fully replicated databases that creates strongly consistent snapshots with minimal overhead. The method requires only O(n) new messages and adds a single counter to existing messages. This approach helps identify anomalies in eventually consistent systems by focusing on snapshots around the time of occurrence.

- **[arXiv2510] EARL: Efficient Agentic Reinforcement Learning Systems for Large
  Language Models**
  - **tags:** [mlsys], [LLM training], [agentic reinforcement learning, distributed training, dynamic parallelism, memory optimization]
  - **authors:** Zheyue Tan, Mustapha Abdullahi, Tuo Shi, Huining Yuan, Zelai Xu, Chao Yu, Boxun Li, Bo Zhao
  - **institution:** Aalto University, Tsinghua University, Infinigence-AI
  - **link:** http://arxiv.org/pdf/2510.05943v1
  - **Simple LLM Summary:** EARL introduces a parallelism selector that dynamically adjusts model and training parallelism based on sequence length and system load, along with a data dispatcher for decentralized data exchange. These components enhance throughput, reduce out-of-memory failures, and enable stable large-scale training of agentic LLMs without context length penalties. The system effectively addresses bottlenecks in memory usage and cross-device data movement during agentic RL training.

- **[arXiv2510] Safe and Compliant Cross-Market Trade Execution via Constrained RL and
  Zero-Knowledge Audits**
  - **tags:** [mlsys], [Other models training], [reinforcement learning, algorithmic trading, constrained Markov decision process, zero-knowledge proofs, compliance enforcement, safe AI]
  - **authors:** Ailiya Borjigin, Cong He
  - **institution:** Probe Group Pte. Ltd.
  - **link:** http://arxiv.org/pdf/2510.04952v2
  - **Simple LLM Summary:** This paper presents a cross-market algorithmic trading system using constrained reinforcement learning with proximal policy optimization and runtime action shielding. The system incorporates zero-knowledge compliance audits to verify constraint satisfaction without revealing proprietary information. Experimental results show the approach reduces implementation shortfall and variance while maintaining strict compliance across various stress scenarios.

- **[arXiv2510] When Does Global Attention Help? A Unified Empirical Study on Atomistic
  Graph Learning**
  - **tags:** [mlsys], [Other models inference], [graph neural networks, global attention, atomistic graph learning, benchmarking framework, message passing neural networks, graph transformers]
  - **authors:** Arindam Chowdhury, Massimiliano Lupo Pasini
  - **institution:** Oak Ridge National Laboratory
  - **link:** http://arxiv.org/pdf/2510.05583v1
  - **Simple LLM Summary:** This paper introduces a unified benchmarking framework to systematically evaluate global attention mechanisms in atomistic graph learning. The study compares four model classes including MPNNs and graph transformers across seven datasets. Results show encoder-augmented MPNNs provide robust baselines, while fused local-global models benefit properties with long-range interactions, with attention mechanisms showing accuracy-compute trade-offs.

- **[arXiv2510] Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data
  Movement Forecasting**
  - **tags:** [mlsys], [LLM inference], [Mixture of Experts, data movement forecasting, expert selection, performance optimization, wafer-scale GPUs, trace analysis]
  - **authors:** Zhongkai Yu, Yue Guan, Zihao Yu, Chenyang Zhou, Shuyi Pei, Yangwook Kang, Yufei Ding, Po-An Tsai
  - **institution:** UCSD
  - **link:** http://arxiv.org/pdf/2510.05497v1
  - **Simple LLM Summary:** This paper conducts comprehensive data movement profiling of large-scale MoE LLMs and identifies key patterns through temporal and spatial analysis. The authors propose architectural modifications based on their insights to optimize expert selection and data movement. Their approach achieves significant speedups (6.3× on DeepSeek V3 and 4.0× on Qwen3) in multi-unit serving systems.

- **[arXiv2510] On-Package Memory with Universal Chiplet Interconnect Express (UCIe): A
  Low Power, High Bandwidth, Low Latency and Low Cost Approach**
  - **tags:** [mlsys], [memory], [chiplet interconnect, on-package memory, UCIe, bandwidth density, low latency, power efficiency]
  - **authors:** Debendra Das Sharma, Swadesh Choudhary, Peter Onufryk, Rob Pelt
  - **institution:** Intel Corporation, AMD Corporation
  - **link:** http://arxiv.org/pdf/2510.06513v1
  - **Simple LLM Summary:** This paper proposes enhancing UCIe with memory semantics to create power-efficient on-package memory solutions by reusing existing LPDDR6 and HBM memory through a logic die or enabling DRAM dies to natively support UCIe. The approaches achieve significantly higher bandwidth density (up to 10x), lower latency (up to 3x), lower power consumption (up to 3x), and reduced cost compared to existing HBM4 and LPDDR solutions, addressing memory bottlenecks in AI applications.

## 2025-10-08

- **[arXiv2510] Vectorized FlashAttention with Low-cost Exponential Computation in
  RISC-V Vector Processors**
  - **tags:** [mlsys], [kernels], [FlashAttention, RISC-V vector processors, exponential approximation, tiling strategies, performance optimization]
  - **authors:** Vasileios Titopoulos, Kosmas Alexandridis, Giorgos Dimitrakopoulos
  - **institution:** Democritus University of Thrace
  - **link:** http://arxiv.org/pdf/2510.06834v1
  - **Simple LLM Summary:** This paper presents a vectorized implementation of FlashAttention for RISC-V processors using low-cost exponential approximations to simplify softmax computations. It employs tiling strategies to improve memory locality while avoiding ISA extensions. Experimental results demonstrate significant performance gains in attention layer processing with scalable vectorized implementations.

- **[arXiv2510] REACH: Reinforcement Learning for Adaptive Microservice Rescheduling in
  the Cloud-Edge Continuum**
  - **tags:** [mlsys], [scheduling], [cloud-edge continuum, microservice architecture, reinforcement learning, resource management, latency optimization]
  - **authors:** Xu Bai, Muhammed Tawfiqul Islam, Rajkumar Buyya, Adel N. Toosi
  - **institution:** University of Melbourne
  - **link:** http://arxiv.org/pdf/2510.06675v1
  - **Simple LLM Summary:** REACH uses reinforcement learning to dynamically reschedule microservices across cloud-edge infrastructure in real-time. The algorithm adapts to fluctuating resource availability and performance variations. Experimental results show REACH reduces end-to-end latency by 7.9-10% across benchmark applications while mitigating latency fluctuations.

## 2025-10-09

- **[arXiv2510] Maple: A Multi-agent System for Portable Deep Learning across Clusters**
  - **tags:** [mlsys], [cluster infrastructure], [multi-agent system, deep learning, GPU clusters, command line generation, natural language processing, distributed training]
  - **authors:** Molang Wu, Zhao Zhang
  - **institution:** Rutgers University
  - **link:** http://arxiv.org/pdf/2510.08842v1
  - **Simple LLM Summary:** Maple introduces a multi-agent system that generates correct deep learning command lines from natural language input, addressing challenges in heterogeneous GPU cluster environments. The system achieves 92.0% accuracy across 567 test cases using multiple language models totaling 10B parameters. Results demonstrate Maple's effectiveness in enabling portable and scalable distributed deep learning across diverse HPC systems.

- **[arXiv2510] Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D
  Reconstruction**
  - **tags:** [mlsys], [edge computing], [reinforcement learning, multi-view 3D reconstruction, edge resource management, Q-learning, reliability]
  - **authors:** Motahare Mounesan, Sourya Saha, Houchao Gan, Md. Nurul Absur, Saptarshi Debroy
  - **institution:** City University of New York
  - **link:** http://arxiv.org/pdf/2510.08839v1
  - **Simple LLM Summary:** This paper proposes a reinforcement learning framework with two cooperative Q-learning agents for camera and server selection to manage edge resources in multi-view 3D reconstruction. The system operates online and learns policies through environmental interactions. Results demonstrate improved application reliability by effectively balancing latency and reconstruction quality in dynamic edge environments.

- **[arXiv2510] Man-Made Heuristics Are Dead. Long Live Code Generators!**
  - **tags:** [mlsys], [LLM inference], [automated policy generation, code synthesis, web caching, congestion control, Linux kernel integration]
  - **authors:** Rohit Dwivedula, Divyanshu Saxena, Aditya Akella, Swarat Chaudhuri, Daehyeok Kim
  - **institution:** The University of Texas at Austin
  - **link:** http://arxiv.org/pdf/2510.08803v1
  - **Simple LLM Summary:** This paper introduces PolicySmith, a framework that uses LLM-driven code generation to automatically synthesize optimal heuristics for system policies. The method replaces manual policy design with automated search techniques using generative models. Results show the generated policies outperform established baselines in web caching and can be directly integrated into the Linux kernel for congestion control.

- **[arXiv2510] Distributed Resource Selection for Self-Organising Cloud-Edge Systems**
  - **tags:** [mlsys], [scheduling], [distributed resource selection, cloud-edge systems, consensus-based mechanism, dynamic allocation, self-organising orchestration]
  - **authors:** Quentin Renau, Amjad Ullah, Emma Hart
  - **institution:** Edinburgh Napier University
  - **link:** http://arxiv.org/pdf/2510.08228v1
  - **Simple LLM Summary:** This paper proposes a distributed consensus-based mechanism for resource selection in cloud-edge environments that uses local knowledge and inter-agent collaboration instead of central control. The approach achieves efficient resource allocation decisions while maintaining optimality and scalability. Results show the method provides rapid allocations up to 30 times faster than centralized heuristics, with computation time being the key factor influencing allocation decisions.

- **[arXiv2510] From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving
  with Layered Prefill**
  - **tags:** [mlsys], [scheduling], [LLM inference, stall-free scheduling, layered prefill, Mixture-of-Experts, energy efficiency, memory optimization]
  - **authors:** Gunjun Lee, Jiwon Kim, Jaiyoung Park, Younjoo Lee, Jung Ho Ahn
  - **institution:** Seoul National University
  - **link:** http://arxiv.org/pdf/2510.08055v1
  - **Simple LLM Summary:** The paper proposes layered prefill, a new scheduling paradigm that partitions transformer models into layer groups and interleaves prefill and decode operations across these groups. This approach eliminates redundant weight reloads in MoE models while maintaining stall-free decoding. Results show significant improvements in TTFT (up to 70%), end-to-end latency (41%), and energy efficiency (up to 22%) compared to chunked prefill methods.

- **[arXiv2510] DYNAMIX: RL-based Adaptive Batch Size Optimization in Distributed
  Machine Learning Systems**
  - **tags:** [mlsys], [Other models training], [reinforcement learning, batch size optimization, distributed machine learning, Proximal Policy Optimization, adaptive systems]
  - **authors:** Yuanjun Dai, Keqiang He, An Wang
  - **institution:** Case Western Reserve University, Shanghai Jiao Tong University
  - **link:** http://arxiv.org/pdf/2510.08522v1
  - **Simple LLM Summary:** DYNAMIX uses reinforcement learning with Proximal Policy Optimization to dynamically adjust batch sizes in distributed machine learning systems. It employs multi-dimensional state representations including network metrics and resource utilization for decision-making. The method achieves up to 6.3% accuracy improvement and 46% training time reduction while scaling effectively to 32 nodes.

- **[arXiv2510] pyGinkgo: A Sparse Linear Algebra Operator Framework for Python**
  - **tags:** [mlsys], [kernels], [sparse linear algebra, high-performance computing, GPU acceleration, Python interface, Pybind11, NumPy compatibility, PyTorch compatibility]
  - **authors:** Keshvi Tuteja, Gregor Olenik, Roman Mishchuk, Yu-Hsiang Tsai, Markus Götz, Achim Streit, Hartwig Anzt, Charlotte Debus
  - **institution:** Karlsruhe Institute of Technology, Technical University of Munich
  - **link:** http://arxiv.org/pdf/2510.08230v1
  - **Simple LLM Summary:** pyGinkgo provides a Python interface to the Ginkgo library using Pybind11, offering high-performance sparse linear algebra operations across multiple hardware backends. It demonstrates superior performance in sparse matrix-vector products and iterative solvers compared to existing Python libraries while maintaining compatibility with NumPy and PyTorch. The framework achieves performance parity with native C++ implementations while providing Python usability.

- **[arXiv2510] Adaptive Execution Scheduler for DataDios SmartDiff**
  - **tags:** [mlsys], [scheduling], [adaptive scheduler, differencing engine, latency optimization, memory management, Dask parallelism]
  - **authors:** Aryan Poduri
  - **institution:** DataDios
  - **link:** http://arxiv.org/pdf/2510.07811v1
  - **Simple LLM Summary:** This paper presents an adaptive scheduler for a data differencing engine that dynamically tunes batch size and worker count within resource constraints to minimize tail latency. It uses a preflight profiler, online memory model, and guarded hill-climbing policy to safely select between in-memory and Dask-based execution backends. The scheduler achieves 23-28% lower p95 latency and 16-22% reduced peak memory compared to baseline methods while eliminating out-of-memory errors.

- **[arXiv2510] SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM
  Inference**
  - **tags:** [mlsys], [LLM inference], [hardware specialization, prefill-decode disaggregation, cost optimization, energy efficiency, systolic arrays, GDDR memory]
  - **authors:** Hengrui Zhang, Pratyush Patel, August Ning, David Wentzlaff
  - **institution:** Princeton University
  - **link:** http://arxiv.org/pdf/2510.08544v1
  - **Simple LLM Summary:** This paper proposes SPAD, specialized hardware designs with separate Prefill Chips (larger systolic arrays, GDDR memory) and Decode Chips (reduced compute, high bandwidth) tailored for LLM inference phases. Compared to H100 baselines, SPAD achieves similar performance with 19-41% lower hardware costs and 2-17% lower TDP. The design maintains efficiency even under changing workloads through flexible chip reallocation.

## 2025-10-10

- **[arXiv2510] Hierarchical Scheduling for Multi-Vector Image Retrieval**
  - **tags:** [mlsys], [scheduling], [image retrieval, hierarchical scheduling, multi-vector retrieval, efficiency optimization, similarity consistency]
  - **authors:** Maoliang Li, Ke Li, Yaoyang Liu, Jiayu Chen, Zihao Zheng, Yinjun Wu, Xiang Chen
  - **institution:** Peking University
  - **link:** http://arxiv.org/pdf/2510.08976v1
  - **Simple LLM Summary:** The paper proposes HiMIR, a hierarchical scheduling framework for multi-vector image retrieval that uses multiple granularities for better query-image alignment and reduces redundancy through cross-hierarchy similarity consistency. Experimental results show HiMIR achieves significant accuracy improvements while reducing computation by up to 3.5× compared to existing MVR systems. The method also includes automatic parameter configuration for practical deployment across diverse datasets.

- **[arXiv2510] Slicing Is All You Need: Towards A Universal One-Sided Algorithm for
  Distributed Matrix Multiplication**
  - **tags:** [mlsys], [kernels], [distributed matrix multiplication, one-sided communication, GPU-to-GPU communication, partitioning algorithms, PGAS framework]
  - **authors:** Benjamin Brock, Renato Golin
  - **institution:** Intel Corporation
  - **link:** http://arxiv.org/pdf/2510.08874v1
  - **Simple LLM Summary:** This paper introduces a universal one-sided algorithm for distributed matrix multiplication that supports all partitioning combinations using slicing-based index arithmetic. The method computes overlapping tile sets for local matrix multiplies and implements direct GPU-to-GPU communication via a C++ PGAS framework. Results show competitive performance with optimized libraries like PyTorch DTensor across various partitionings and replication factors.

## 2025-10-11

- **[arXiv2510] Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude
  Economy Networks via LLM-Enhanced Optimization**
  - **tags:** [mlsys], [LLM inference], [UAV trajectory optimization, resource allocation, reinforcement learning, vision-language models, low-altitude economy networks]
  - **authors:** Yang Li, Ruichen Zhang, Yinqiu Liu, Guangyuan Liu, Dusit Niyato, Abbas Jamalipour, Xianbin Wang, Dong In Kim
  - **institution:** Nanyang Technological University, University of Sydney, University of Electronic Science and Technology of China, Sungkyunkwan University
  - **link:** http://arxiv.org/pdf/2510.10028v1
  - **Simple LLM Summary:** This paper proposes a hierarchical optimization framework combining ARPO algorithm for resource allocation and LLM-enhanced reinforcement learning for UAV trajectory optimization. The LLM improves reward design offline without real-time latency. Numerical results show improved inference performance and communication efficiency in dynamic LAENet environments.

- **[arXiv2510] SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based
  Model Inference**
  - **tags:** [mlsys], [LLM inference], [speculative decoding, mixture-of-experts, expert prefetching, inference acceleration, computational optimization]
  - **authors:** Liangkun Chen, Zijian Wen, Tian Wu, Xiaoxi Zhang, Chuan Wu
  - **institution:** Sun Yat-sen University, The University of Hong Kong
  - **link:** http://arxiv.org/pdf/2510.10302v1
  - **Simple LLM Summary:** SP-MoE introduces speculative expert prefetching and compute-communication pipelining to accelerate MoE-based model inference. It uses structural correspondence between draft and target models to prefetch experts ahead of verification. The method achieves 1.07-3.5× speedup over state-of-the-art approaches across various datasets and environments.

## 2025-10-12

- **[arXiv2510] DCP: Addressing Input Dynamism In Long-Context Training via Dynamic
  Context Parallelism**
  - **tags:** [mlsys], [LLM training], [dynamic context parallelism, long-context training, attention optimization, distributed training, computation balance]
  - **authors:** Chenyu Jiang, Zhenkun Cai, Ye Tian, Zhen Jia, Yida Wang, Chuan Wu
  - **institution:** The University of Hong Kong, Amazon Web Services
  - **link:** http://arxiv.org/pdf/2510.10620v1
  - **Simple LLM Summary:** DCP introduces a dynamic context parallel training framework with fine-grained blockwise partitioning to handle variable sequence lengths and attention patterns. It enables flexible mapping of data and computation blocks to devices, reducing communication overhead and improving load balance. Experimental results show significant speedup in attention computation (1.19x-3.77x) and end-to-end training performance (up to 1.46x) under different attention patterns.
