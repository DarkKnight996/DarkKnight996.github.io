# 20251229-20260104

## 2025-12-29

**cs.DC total: 16**

- **[arXiv251229] LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices**
  - **tags:** [mlsys], [llm inference], [pipeline parallelism, model offloading, fine-grained offline allocation scheduler, online memory adaptation, collaborative inference]
  - **authors:** Mingyu Sun, Xiao Zhang, Shen Qu, Yan Li, Mengbai Xiao, Yuan Yuan, Dongxiao Yu
  - **institution:** Shandong University
  - **link:** https://arxiv.org/pdf/2512.21835
  - **Simple LLM Summary:** The paper proposes LIME, a collaborative system that uses interleaved pipeline parallelism and model offloading to enable lossless LLM inference across multiple memory-constrained edge devices. It introduces a fine-grained offline scheduler and online memory adaptation to optimize resource usage and minimize latency. Experiments show LIME achieves significant speedups over baselines on heterogeneous edge devices without compromising model accuracy.

- **[arXiv251229] DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction**
  - **tags:** [mlsys], [others], [deep-surrogate, two-stage design, mixture-of-experts, error-bounded lossy compression, quality prediction]
  - **authors:** Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Lukić, Franck Cappello
  - **institution:** University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory
  - **link:** https://arxiv.org/pdf/2512.21433
  - **Simple LLM Summary:** DeepCQ is a deep-surrogate framework that predicts lossy compression quality using a two-stage design and mixture-of-experts approach for time-evolving data. It generalizes across compressors, metrics, and datasets, achieving prediction errors under 10% and reducing computational overhead in scientific data analysis.

- **[arXiv251229] Demystifying ARM SME to Optimize General Matrix Multiplications**
  - **tags:** [mlsys], [GPU kernels], [cache-aware partitioning, data packing, micro-kernels, multi-vector loads, tile registers, ARM SME]
  - **authors:** Chencheng Deng, Weiling Yang, Jianbin Fang, Dezun Dong
  - **institution:** National University of Defense Technology
  - **link:** https://arxiv.org/pdf/2512.21473
  - **Simple LLM Summary:** This paper presents MpGEMM, an open-source library that optimizes General Matrix Multiplication (GEMM) by leveraging ARM's Scalable Matrix Extension (SME) through techniques like cache-aware partitioning and specialized micro-kernels. It achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library on real-world workloads from DeepSeek and LLaMA.

- **[arXiv251229] Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum**
  - **tags:** [mlsys], [others], [data spaces, cloud-edge continuum, edge computing, containerized microservices, AI/ML services, IoT testbed]
  - **authors:** Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda
  - **institution:** Spark Works Ltd, IONOS SE, Iquadrat Informática S.L.
  - **link:** https://arxiv.org/pdf/2512.21340
  - **Simple LLM Summary:** This paper presents a real-world use case of intelligent infrastructure monitoring implemented within a data space-enabled cloud-edge framework. It leverages edge computing, containerized microservices, and interoperable data sharing to address challenges like sensor integration and data privacy. The implementation demonstrates the transformative potential of combining AI, edge computing, and data spaces for building scalable and resilient smart city ecosystems.

- **[arXiv251229] Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers**
  - **tags:** [mlsys], [fault-tolerance], [LSTM, Random Forest, MQTT, InfluxDB, Streamlit, IoT monitoring, synthetic data, ASHRAE 2021]
  - **authors:** Krishna Chaitanya Sunkara, Rambabu Konakanchi
  - **institution:** Oracle, Charles Schwab
  - **link:** https://arxiv.org/pdf/2512.21801
  - **Simple LLM Summary:** The paper presents a smart IoT system that uses LSTM networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. It demonstrates high accuracy on synthetic data and shows that proactive leak management can prevent significant energy waste. The work establishes a proof-of-concept for sustainable data center operations, though it requires empirical validation.

- **[arXiv251229] nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures**
  - **tags:** [mlsys], [llm inference], [e-graph, term rewriting, equality saturation, auto vectorize, auto distribution, auto schedule, non-uniform memory access (NUMA) abstraction, roofline model]
  - **authors:** Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang
  - **institution:** Canaan Inc.
  - **link:** https://arxiv.org/pdf/2512.21571
  - **Simple LLM Summary:** nncase is an end-to-end compiler framework that uses an e-graph-based term rewriting engine to optimize LLM deployment across heterogeneous memory architectures. It unifies optimization through modules for auto vectorization, distribution, and scheduling. The evaluation shows it outperforms other frameworks and achieves performance comparable to hand-optimized solutions, demonstrating the viability of automated compilation for high-performance LLM inference.

- **[arXiv251229] Embedding Samples Dispatching for Recommendation Model Training in Edge Environments**
  - **tags:** [mlsys], [others], [embedding cache, parameter server, edge computing, sample dispatching, HybridDis]
  - **authors:** Guopeng Li, Haisheng Tan, Chi Zhang, Hongqiu Ni, Zilong Wang, Xinyue Zhang, Yang Xu, Han Tian
  - **institution:** University of Science and Technology of China, Hefei University of Technology
  - **link:** https://arxiv.org/pdf/2512.21615
  - **Simple LLM Summary:** The paper proposes ESD, a mechanism that optimizes the dispatch of input embedding samples to edge workers to minimize transmission costs in distributed DLRM training. It introduces HybridDis, a dispatch method combining an optimal and a heuristic algorithm to balance decision quality and resource use. Experiments show ESD reduces embedding transmission cost by up to 36.76% and achieves up to 1.74x training speedup.

- **[arXiv251229] LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol for Multicore Real-Time Systems**
  - **tags:** [sys], [real-time systems], [lock-free protocol, fault tolerance, resource sharing, multicore, worst-case response time analysis]
  - **authors:** Nan Chen, Xiaotian Dai, Tong Cheng, Alan Burns, Iain Bate, Shuai Zhao
  - **institution:** University of York, Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.21701
  - **Simple LLM Summary:** The paper proposes LEFT-RS, a lock-free fault-tolerant resource sharing protocol for multicore real-time systems that allows concurrent read access to global resources and parallel entry into critical sections. It aims to improve efficiency and fault resilience by enabling tasks to complete earlier if others fail, while limiting overhead. The evaluation shows the method significantly outperforms existing approaches, achieving up to an 84.5% average improvement in schedulability.

- **[arXiv251229] Proceedings First Workshop on Adaptable Cloud Architectures**
  - **tags:** TBD
  - **authors:** Giuseppe De Palma, Saverio Giallorenzo
  - **institution:** TBD
  - **link:** https://arxiv.org/pdf/2512.22054

- **[arXiv251229] Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference**
  - **tags:** [mlsys], [multi-modal inference], [vision transformer, cloud-device collaboration, patch-level importance scoring, dynamic scheduling, weighted ensembling]
  - **authors:** Linyi Jiang, Yifei Zhu, Hao Yin, Bo Li
  - **institution:** Shanghai Jiao Tong University, Tsinghua University, Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.21730
  - **Simple LLM Summary:** Hyperion is a cloud-device collaborative framework that enables low-latency inference on Ultra-HD video using vision transformers by identifying and transmitting critical image patches, dynamically adjusting transmission quality, and fusing edge and cloud results. It improves frame processing rate by up to 1.61× and accuracy by up to 20.2% compared to state-of-the-art baselines under dynamic network conditions.

- **[arXiv251229] Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism**
  - **tags:** [mlsys], [llm inference], [mixture-of-experts (MoE), disaggregated expert parallelism (DEP), fine-grained task scheduling, FinDEP, task pipelining, optimization problem]
  - **authors:** Xinglin Pan, Shaohuai Shi, Wenxiang Lin, Yuxin Wang, Zhenheng Tang, Wei Wang, Xiaowen Chu
  - **institution:** The Hong Kong University of Science and Technology (Guangzhou), Harbin Institute of Technology, Shenzhen, Hong Kong Baptist University, The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.21487
  - **Simple LLM Summary:** This paper proposes FinDEP, a fine-grained task scheduling algorithm for disaggregated expert parallelism (DEP) to improve the inference throughput of MoE-based large language models. The method partitions computation and communication tasks and formulates an optimization problem to maximize task overlap. Experiments show FinDEP achieves up to 1.61x throughput improvement over prior methods.

- **[arXiv251229] Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View**
  - **tags:** [mlsys], [fault-tolerance], [federated fine-tuning, adaptive aggregation, connection failures, data heterogeneity, convergence guarantee, LoRA]
  - **authors:** Yanmeng Wang, Zhiwen Dai, Shuai Wang, Jian Zhou, Fu Xiao, Tony Q. S. Quek, Tsung-Hui Chang
  - **institution:** Nanjing University of Posts and Telecommunications, The Hong Kong University of Science and Technology
  - **link:** https://arxiv.org/pdf/2512.22035
  - **Simple LLM Summary:** The paper proposes FedAuto, a federated fine-tuning framework that uses adaptive aggregation to handle unreliable network connections and heterogeneous client data without prior knowledge of network conditions. It provides a strong per-round convergence guarantee. Experiments show it outperforms existing methods in various failure scenarios for both full and parameter-efficient fine-tuning like LoRA.

- **[arXiv251229] Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models**
  - **tags:** [mlsys], [llm inference], [model parallelism, block placement, request routing, mixed integer linear programming, performance modeling, distributed inference]
  - **authors:** Tingyang Sun, Ting He, Bo Ji, Parimal Parag
  - **institution:** Pennsylvania State University, Virginia Tech, Indian Institute of Science
  - **link:** https://arxiv.org/pdf/2512.21884
  - **Simple LLM Summary:** This paper presents the first systematic study of resource allocation for distributed large language model inference, focusing on optimizing block placement and request routing. It develops performance models, formulates the problem as a mixed integer linear program, and provides efficient offline and online algorithms with performance guarantees. The proposed solution is shown to substantially reduce inference time compared to the state-of-the-art in geographically-distributed settings.

- **[arXiv251229] Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications**
  - **tags:** [mlsys], [fault-tolerance], [agentic workflow, LLM-driven graph traversal, service dependency graph (SDG), hammock-block program dependence graph (PDG), root-cause analysis (RCA)]
  - **authors:** Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer
  - **institution:** University of Illinois at Urbana-Champaign, IBM Research
  - **link:** https://arxiv.org/pdf/2512.22113
  - **Simple LLM Summary:** This paper introduces PRAXIS, an orchestrator that uses an LLM-driven structured traversal over service dependency and program dependence graphs to diagnose code-related cloud incidents. It significantly improves root-cause analysis accuracy and reduces computational cost compared to prior methods.

- **[arXiv251229] BLEST: Blazingly Efficient BFS using Tensor Cores**
  - **tags:** [mlsys], [GPU kernels], [Binarised Virtual Slice Sets (BVSS), graph reordering, batched SpMSpV multiplication, kernel fusion, lazy vertex update]
  - **authors:** Deniz Elbek, Kamer Kaya
  - **institution:** Sabanci University
  - **link:** https://arxiv.org/pdf/2512.21967
  - **Simple LLM Summary:** BLEST is a GPU-accelerated BFS framework that leverages Tensor Cores by reformulating the pull-based BFS pipeline using a bitmap-oriented structure and a batched sparse matrix-sparse vector multiplication pattern. It introduces techniques like Binarised Virtual Slice Sets for load balancing and employs graph reordering for memory efficiency. The experiments show that BLEST achieves significant speedups over state-of-the-art frameworks like BerryBees, Gunrock, and GSWITCH.

- **[arXiv251229] FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion**
  - **tags:** [mlsys], [llm training], [expert parallelism, data shuffling, transformation-communication fusion, pipelined communication engine, load-balancing]
  - **authors:** Zhuoran Zhu, Chunyang Zhu, Hao Lin, Xu Fu, Yiming Zhou, Quanlu Zhang, Zhenhua Li, Feng Qian, Chao Yu, Boxun Li, Guohao Dai, Yu Wang
  - **institution:** Tsinghua University, Infinigence AI, University of Southern California, Zhongguancun Academy, Shanghai Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.22036
  - **Simple LLM Summary:** The paper introduces FUSCO, a communication library that fuses data transformation and communication to optimize distributed data shuffling for Mixture-of-Experts (MoE) model training. It addresses the layout mismatch between expert-major data and device-major communication by using a pipelined engine with lightweight planning. The results show that FUSCO significantly outperforms existing libraries like NCCL and DeepEP in both training and inference latency for MoE models.


**cs.AI/cs.LG contains "reinforcement learning" total: 12**
- [arXiv251229] A Reinforcement Learning Approach to Synthetic Data Generation [link](https://arxiv.org/pdf/2512.21395)
- [arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search [link](https://arxiv.org/pdf/2512.21648)
- [arXiv251229] Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model [link](https://arxiv.org/pdf/2512.21540)
- [arXiv251229] CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation [link](https://arxiv.org/pdf/2512.21351)
- [arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning [link](https://arxiv.org/pdf/2512.21446)
- [arXiv251229] Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations [link](https://arxiv.org/pdf/2512.21586)
- [arXiv251229] Generative Actor Critic [link](https://arxiv.org/pdf/2512.21527)
- [arXiv251229] A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning [link](https://arxiv.org/pdf/2512.21412)
- [arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO [link](https://arxiv.org/pdf/2512.21514)
- [arXiv251229] Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities [link](https://arxiv.org/pdf/2512.21717)
- [arXiv251229] A Comedy of Estimators: On KL Regularization in RL Training of LLMs [link](https://arxiv.org/pdf/2512.21852)
- [arXiv251229] Meta-Learning-Based Handover Management in NextG O-RAN [link](https://arxiv.org/pdf/2512.22022)

**cs.AI/cs.LG contains "accelerate" total: 12**
- [arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search [link](https://arxiv.org/pdf/2512.21648)
- [arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning [link](https://arxiv.org/pdf/2512.21446)
- [arXiv251229] AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification [link](https://arxiv.org/pdf/2512.21544)
- [arXiv251229] Discovering Sparse Recovery Algorithms Using Neural Architecture Search [link](https://arxiv.org/pdf/2512.21563)
- [arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization [link](https://arxiv.org/pdf/2512.21769)
- [arXiv251229] How Do Agents Perform Code Optimization? An Empirical Study [link](https://arxiv.org/pdf/2512.21757)
- [arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism [link](https://arxiv.org/pdf/2512.21452)
- [arXiv251229] Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms [link](https://arxiv.org/pdf/2512.21925)
- [arXiv251229] DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction [link](https://arxiv.org/pdf/2512.22007)
- [arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars [link](https://arxiv.org/pdf/2512.22065)
- [arXiv251229] Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling [link](https://arxiv.org/pdf/2512.22066)
- [arXiv251229] Enabling Ultra-Fast Cardiovascular Imaging Across Heterogeneous Clinical Environments with a Generalist Foundation Model and Multimodal Database [link](https://arxiv.org/pdf/2512.21652)

## 2025-12-30

**cs.DC total: 42**

- **[arXiv251230] Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments**
  - **tags:** [mlsys], [llm inference], [adaptive GPU resource allocation, serverless computing, multi-agent systems, workload scheduling, O(N) algorithm]
  - **authors:** Guilin Zhang, Wulan Guo, Ziqi Tan
  - **institution:** George Washington University
  - **link:** https://arxiv.org/pdf/2512.22149
  - **Simple LLM Summary:** This paper proposes an adaptive GPU resource allocation framework for multi-agent LLM systems in serverless environments. It uses an O(N) complexity algorithm to dynamically allocate resources based on workload, reducing latency by 85% compared to round-robin scheduling while maintaining throughput.

- **[arXiv251230] HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration**
  - **tags:** [mlsys], [llm inference], [edge-cloud collaboration, task decomposition, parallel execution, resource-aware routing, adaptive scheduling]
  - **authors:** Jiangwen Dong, Jiayu Li, Wanyu Lin
  - **institution:** The Hong Kong Polytechnic University
  - **link:** https://arxiv.org/pdf/2512.22137
  - **Simple LLM Summary:** The paper proposes HybridFlow, a framework that dynamically decomposes complex queries into interdependent subtasks and uses a learned router to adaptively assign them to edge or cloud LLMs based on utility and resource budgets. It demonstrates that this approach reduces inference time and token usage while maintaining competitive accuracy on several reasoning benchmarks.

- **[arXiv251230] TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures**
  - **tags:** [mlsys], [GPU kernels], [spatial dataflow, tile-based compilation, MLIR, Triton, hardware representation, on-chip network, data reuse]
  - **authors:** Wei Li, Zhenyu Bai, Heru Wang, Pranav Dangi, Zhiqiang Zhang, Cheng Tan, Huiying Lan, Weng-Fai Wong, Tulika Mitra
  - **institution:** National University of Singapore, Arizona State University, Google, Lumai Ltd.
  - **link:** https://arxiv.org/pdf/2512.22168
  - **Simple LLM Summary:** This paper presents TL, an end-to-end compiler framework that compiles tile-based programs for spatial dataflow accelerators by distributing tile instances across cores and optimizing data movement over the on-chip network. It achieves better performance than vendor libraries on kernels like GEMM and FlashAttention by increasing data reuse and reducing communications.

- **[arXiv251230] On Harnessing Idle Compute at the Edge for Foundation Model Training**
  - **tags:** [mlsys], [llm training], [selective hybrid tensor parallelism, parameter server, cost optimization model, decentralized training, edge computing]
  - **authors:** Leyang Xue, Meghana Madhyastha, Myungjin Lee, Amos Storkey, Randal Burns, Mahesh K. Marina
  - **institution:** The University of Edinburgh, Johns Hopkins University, Cisco Research
  - **link:** https://arxiv.org/pdf/2512.22142
  - **Simple LLM Summary:** The paper introduces Cleave, a new paradigm for decentralized foundation model training that uses selective hybrid tensor parallelism and a parameter server framework to partition training operations across edge devices. It matches cloud-based training performance by efficiently scaling to thousands of devices, handling memory limits and device heterogeneity, and outperforms existing edge-training methods in speed and failure recovery.

- **[arXiv251230] GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems**
  - **tags:** [mlsys], [llm inference], [GPU virtualization, benchmarking, container isolation, multi-tenancy, CUDA, performance evaluation, MIG, HAMi-core, BUD-FCSP]
  - **authors:** Jithin VG, Ditto PS
  - **institution:** Bud Ecosystem Inc
  - **link:** https://arxiv.org/pdf/2512.22125
  - **Simple LLM Summary:** The paper introduces GPU-Virt-Bench, a comprehensive benchmarking framework with 56 metrics across 10 categories to evaluate software-based GPU virtualization systems. It demonstrates the framework by comparing solutions like HAMi-core and BUD-FCSP against ideal MIG behavior, providing critical performance insights for deploying GPU resources in multi-tenant environments.

- **[arXiv251230] HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA**
  - **tags:** [mlsys], [others], [FPGA acceleration, High Level Synthesis (HLS), parameter quantization, layer fusion, input-points pruning, Uniform Random Sampling (URS), fixed-point implementation]
  - **authors:** Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn
  - **institution:** National University of Sciences and Technology, RPTU Kaiserslautern-Landau
  - **link:** https://arxiv.org/pdf/2512.22139
  - **Simple LLM Summary:** This paper introduces HLS4PC, a parameterizable HLS framework for accelerating point-based 3D point cloud models on FPGAs. It applies hardware-aware compression techniques like quantization and pruning to create a lighter model variant, PointMLP-Lite, and demonstrates that the FPGA implementation achieves significantly higher throughput compared to prior works, GPUs, and CPUs.

- **[arXiv251230] GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs**
  - **tags:** [mlsys], [GPU kernels], [Minimal Executable Program (MEP), Automatic Error Repair, Performance Pattern Inheritance, iterative optimization, cross-platform portability]
  - **authors:** Ruifan Chu, Anbang Wang, Xiuxiu Bai, Shuai Liu, Xiaoshe Dong
  - **institution:** School of Software Engineering, Xi’an Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.22147
  - **Simple LLM Summary:** This paper presents an LLM-based framework for GPU kernel optimization that avoids expensive full application builds by using automatically generated Minimal Executable Programs (MEPs) for iterative optimization and evaluation. The framework integrates Automatic Error Repair and Performance Pattern Inheritance to fix faults and reuse effective strategies. The method achieves significant speedups on NVIDIA and DCU platforms and demonstrates practical, low-cost optimization without full-source dependencies.

- **[arXiv251230] SoDA: An Efficient Interaction Paradigm for the Agentic Web**
  - **tags:** [mlsys], [others], [Sovereign Digital Avatar (SoDA), orthogonal decoupling, Intent-Permission Handshake Mechanism, A2A protocols, dual-factor adaptive routing, Retrieval-Augmented Generation (RAG)]
  - **authors:** Zicai Cui, Zhouyuan Jian, Weiwen Liu, Weinan Zhang
  - **institution:** Shanghai Jiao Tong University, Shanghai Innovation Institute
  - **link:** https://arxiv.org/pdf/2512.22135
  - **Simple LLM Summary:** The paper proposes the Sovereign Digital Avatar (SoDA), a user-centric interaction paradigm that decouples storage, computation, and interaction to combat data lock-in and cognitive overload in the Agentic Web. It employs an Intent-Permission Handshake Mechanism for secure operation. Empirical results show SoDA significantly reduces token consumption and user cognitive load while improving information efficiency.

- **[arXiv251230] SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware**
  - **tags:** [mlsys], [others], [structured pruning, multi-objective optimization, view-adaptive compression, distributed inference]
  - **authors:** Mahadev Sunil Kumar, Arnab Raha, Debayan Das, Gopakumar G, Amitava Mukherjee
  - **institution:** Accenture PLC, Intel Corporation, Indian Institute of Science, Amrita Vishwa Vidyapeetham, Birla Institute of Technology and Science
  - **link:** https://arxiv.org/pdf/2512.22136
  - **Simple LLM Summary:** The paper presents SlimEdge, a method for deploying distributed deep neural networks on constrained edge devices by integrating structured model pruning with multi-objective optimization to tailor network capacity to hardware constraints. It demonstrates the framework using a Multi-View CNN for 3D object recognition, adaptively compressing the model based on the contribution of individual views. The resulting models meet specified accuracy and memory bounds while achieving up to 5x faster inference across diverse hardware platforms.

- **[arXiv251230] AiiDAlab: on the route to accelerate science**
  - **tags:** [sys], [scientific workflow management], [AiiDAlab, AiiDA, web-based interface, computational workflows, provenance tracking, FAIR principles, electronic laboratory notebooks]
  - **authors:** Aliaksandr V.Yakutovich, Jusong Yu, Daniel Hollas, Edan Bainglass, Corsin Battaglia, Miki Bonacci, Lucas Fernandez Vilanova, Stephan Henne, Anders Kaestner, Michel Kenzelmann, Graham Kimbell, Jakob Lass, Fabio Lopes, Daniel G. Mazzone, Andres Ortega-Guerrero, Xing Wang, Nicola Marzari, Carlo A. Pignedoli, Giovanni Pizzi
  - **institution:** Empa-Swiss Federal Laboratories for Materials Science and Technology, Paul Scherrer Institute, École Polytechnique Fédérale de Lausanne, University of Bristol, ETH Zurich
  - **link:** https://arxiv.org/pdf/2512.22173
  - **Simple LLM Summary:** The paper presents AiiDAlab, a web-based platform designed to simplify the execution and management of complex computational workflows on supercomputers. Its core method is providing an intuitive browser interface that abstracts technical complexities and automatically tracks full simulation provenance via the AiiDA engine. The main conclusion is that AiiDAlab has matured into a cross-disciplinary platform that accelerates scientific discovery by allowing researchers to focus on science rather than computational details, ensuring reproducibility and adherence to FAIR data principles.

- **[arXiv251230] iOS as Acceleration**
  - **tags:** [mlsys], [others], [distributed pipeline parallelism, model partitioning, mobile computing, iOS, thermal throttling, memory constraints]
  - **authors:** Alexander K. Chen
  - **institution:** Independent High School Researcher
  - **link:** https://arxiv.org/pdf/2512.22180
  - **Simple LLM Summary:** This paper proposes a proof-of-concept system that uses distributed pipeline parallelism to harness the compute power of iOS devices, partitioning model weights to work around their memory limitations. It demonstrates acceleration for tasks like modest model training and batch inference, concluding that ubiquitous mobile devices have significant potential to contribute to machine learning in resource-constrained environments.

- **[arXiv251230] BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs**
  - **tags:** [mlsys], [fault-tolerance], [bit-flip fault localization, differential analysis, residual-path perturbation, loss-sensitivity profiling, transformer architectures]
  - **authors:** Muhammad Zeeshan Karamat, Sadman Saif, Christiana Chamon Garcia
  - **institution:** Virginia Tech
  - **link:** https://arxiv.org/pdf/2512.22174
  - **Simple LLM Summary:** This paper introduces BitFlipScope, a software framework for localizing and recovering from bit-flip corruptions in Large Language Models. It uses differential analysis with a reference model or, when unavailable, residual-path perturbation and loss-sensitivity profiling to identify fault-affected regions. The framework enables lightweight performance recovery without fine-tuning, enhancing fault resilience for LLM deployment.

- **[arXiv251230] MatKV: Trading Compute for Flash Storage in LLM Inference**
  - **tags:** [mlsys], [llm inference], [retrieval augmented generation, key-value vectors, prefill phase, materialization, flash storage]
  - **authors:** Kun-Woo Shin, Jay H. Park, Moonwook Oh, Yohan Jo, Jaeyoung Do, Sang-Won Lee
  - **institution:** Seoul National University, Samsung Electronics
  - **link:** https://arxiv.org/pdf/2512.22195
  - **Simple LLM Summary:** The paper proposes MatKV, a method that precomputes and stores key-value vectors for RAG documents in flash storage to avoid recomputing them during inference. This approach reduces inference time and power consumption by half while maintaining accuracy. It also enables optimizations like overlapping KV loading with decoding and using lower-end GPUs for decoding.

- **[arXiv251230] Valori: A Deterministic Memory Substrate for AI Systems**
  - **tags:** [mlsys], [others], [fixed-point arithmetic, Q16.16, state machine, vector embeddings, deterministic memory]
  - **authors:** Varshith Gudur
  - **institution:** Valori Kernel Project, Independent Researcher
  - **link:** https://arxiv.org/pdf/2512.22280
  - **Simple LLM Summary:** This paper introduces Valori, a deterministic memory substrate for AI systems that replaces floating-point operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. It guarantees bit-identical memory states and search results across different hardware platforms. The authors conclude that deterministic memory is a necessary primitive for building trustworthy and auditable AI systems.

- **[arXiv251230] SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM**
  - **tags:** [sys], [HPC/CFD], [GPU porting, unified memory, memory pool manager, portable programming model, strong scalability, weak scalability]
  - **authors:** Simone Bnà, Giuseppe Giaquinto, Ettore Fadiga, Tommaso Zanelli, Francesco Bottau
  - **institution:** Cineca Supercomputing Centre, Università degli Studi di Napoli Federico II
  - **link:** https://arxiv.org/pdf/2512.22215
  - **Simple LLM Summary:** This paper presents SPUMA, a minimally invasive approach for porting the OPENFOAM CFD software to NVIDIA and AMD GPUs using a portable programming model and a memory pool manager leveraging unified memory. Performance tests on pre-exascale clusters show good scalability and up to an 82% reduction in energy consumption compared to CPU-based simulations.

- **[arXiv251230] Scalable Cloud-Native Architectures for Intelligent PMU Data Processing**
  - **tags:** [mlsys], [cluster infrastructure], [distributed stream processing, containerized microservices, elastic resource orchestration, edge-cloud hybrid architecture, machine learning for time-series analysis]
  - **authors:** Nachiappan Chockalingam, Akshay Deshpande, Lokesh Butra, Ram Sekhar Bodala, Nitin Saksena, Adithya Parthasarathy, Balakrishna Pothineni, Akash Kumar Agarwal
  - **institution:** IEEE, NTT Data, Amtrak, Albertsons Companies
  - **link:** https://arxiv.org/pdf/2512.22231
  - **Simple LLM Summary:** This paper proposes a scalable cloud-native architecture integrating AI, edge, and cloud computing for processing PMU data. It uses distributed stream processing, microservices, and elastic orchestration to enable low-latency analytics and anomaly detection. The approach achieves sub-second response times and provides a robust foundation for next-generation smart grid monitoring.

- **[arXiv251230] Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs**
  - **tags:** [mlsys], [llm inference], [mega-kernel, SM-level graph, cross-operator software pipelining, kernel fusion, CUDA, decentralized scheduling]
  - **authors:** Xinhao Cheng, Zhihao Zhang, Yu Zhou, Jianan Ji, Jinchen Jiang, Zepeng Zhao, Ziruo Xiao, Zihao Ye, Yingyi Huang, Ruihang Lai, Hongyi Jin, Bohan Hou, Mengdi Wu, Yixin Dong, Anthony Yip, Zihao Ye, Songting Wang, Wenqin Yang, Xupeng Miao, Tianqi Chen, Zhihao Jia
  - **institution:** Carnegie Mellon University, Tsinghua University, NVIDIA, University of Michigan, Purdue University
  - **link:** https://arxiv.org/pdf/2512.22219
  - **Simple LLM Summary:** This paper introduces Mirage Persistent Kernel (MPK), a compiler and runtime system that automatically transforms multi-GPU model inference into a single, high-performance mega-kernel using an SM-level graph representation for fine-grained task scheduling. The system enables cross-operator software pipelining and kernel fusion, significantly reducing inference latency. The evaluation shows MPK outperforms existing kernel-per-operator systems by up to 1.7x, pushing LLM inference performance close to hardware limits.

- **[arXiv251230] Efficient Multi-Model Orchestration for Self-Hosted Large Language Models**
  - **tags:** [mlsys], [llm inference], [Kubernetes, Helm, scale-to-zero, DistilBERT classifier, hybrid routing, keyword heuristics]
  - **authors:** Bhanu Prakash Vangala, Tanu Malik
  - **institution:** University of Missouri
  - **link:** https://arxiv.org/pdf/2512.22402
  - **Simple LLM Summary:** The paper introduces "Pick and Spin," a framework built on Kubernetes for orchestrating self-hosted LLMs. It uses a hybrid routing module with keyword heuristics and a DistilBERT classifier, along with adaptive scale-to-zero automation, to efficiently manage multiple models. The system achieves higher success rates, lower latency, and reduced GPU costs compared to static deployments, making enterprise-grade LLM performance more affordable on private infrastructure.

- **[arXiv251230] Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries**
  - **tags:** [mlsys], [llm inference], [text-to-sql, cloud compute cost, google bigquery, reasoning models, bytes processed, slot utilization, valid efficiency score (ves)]
  - **authors:** Saurabh Deochake, Debajyoti Mukhopadhyay
  - **institution:** SentinelOne, WIDiCoReL Research Lab
  - **link:** https://arxiv.org/pdf/2512.22364
  - **Simple LLM Summary:** This paper presents an empirical study evaluating the cloud compute costs of SQL queries generated by six state-of-the-art LLMs on Google BigQuery. The core method involves measuring bytes processed, slot utilization, and estimated cost for 180 query executions on a 230GB dataset. The main conclusion is that reasoning models process significantly fewer bytes (44.5% less) and incur lower costs than standard models while maintaining high correctness, and that execution time is a poor proxy for cloud query cost.

- **[arXiv251230] Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving**
  - **tags:** [mlsys], [llm inference], [speculative decoding, adaptive algorithm, multi-armed bandit, dynamic batch size]
  - **authors:** Rui Li, Zhaoning Zhang, Libo Zhang, Huaimin Wang, Xiang Fu, Zhiquan Lai
  - **institution:** National University of Defense Technology
  - **link:** https://arxiv.org/pdf/2512.22420
  - **Simple LLM Summary:** The paper proposes Nightjar, a learning-based adaptive speculative decoding algorithm that dynamically adjusts the speculative length or disables speculation based on the real-time request load (batch size). This overcomes the limitation of fixed-length speculative decoding, which suffers from verification overhead under high loads. Experiments show Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding.

- **[arXiv251230] Object Abstraction To Streamline Edge-Cloud-Native Application Development**
  - **tags:** [sys], [cloud-native computing], [Object-as-a-Service, edge-cloud continuum, serverless, FaaS, state management, orchestration, SLA-driven management]
  - **authors:** Pawissanutt Lertpongrujikorn
  - **institution:** University of North Texas
  - **link:** https://arxiv.org/pdf/2512.22534
  - **Simple LLM Summary:** This dissertation introduces the Object-as-a-Service (OaaS) paradigm, a unified approach to cloud-native development that abstracts infrastructure complexity by consolidating resource, state, and workflow management. Through empirical studies and prototypes like Oparaca and EdgeWeaver, it demonstrates that OaaS reduces development effort and improves performance compared to traditional serverless models, establishing a foundation for platforms that empower developers to focus on application logic.

- **[arXiv251230] Role-Based Fault Tolerance System for LLM RL Post-Training**
  - **tags:** [mlsys], [fault-tolerance], [role-based fault isolation, role-aware monitoring, non-disruptive recovery, dynamic point-to-point communication, UCX, warm standbys]
  - **authors:** Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin
  - **institution:** Zhejiang University, State Key Laboratory of Mathematical Engineering and Advanced Computing
  - **link:** https://arxiv.org/pdf/2512.22492
  - **Simple LLM Summary:** The paper introduces RobustRL, a fault tolerance system for RL post-training of LLMs that uses a role-based approach to isolate and recover from GPU failures. Its method involves detecting failures with role-aware monitoring, restarting only the failed component (trainer or rollout) without disrupting the entire task, and reconnecting it using dynamic communication. The system significantly improves the Effective Training Time Ratio and reduces end-to-end training time compared to prior methods.

- **[arXiv251230] RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure**
  - **tags:** [mlsys], [llm training], [disaggregated infrastructure, hardware-affinity workload mapping, fine-grained asynchrony, statefulness-aware computation, agentic RL]
  - **authors:** Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang
  - **institution:** HKUST, Alibaba Group, Tongyi Lab
  - **link:** https://arxiv.org/pdf/2512.22560
  - **Simple LLM Summary:** The paper presents RollArt, a distributed system designed to scale agentic reinforcement learning training by disaggregating the heterogeneous workload across specialized hardware. Its core methods include hardware-affinity workload mapping, fine-grained asynchrony, and statefulness-aware computation to mitigate synchronization overhead. The results show that RollArt improves training throughput and reduces end-to-end training time by 1.35-2.05× compared to monolithic baselines.

- **[arXiv251230] OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads**
  - **tags:** [mlsys], [cluster infrastructure], [RDMA, best-effort transport, adaptive timeouts, Hadamard Transform, Erasure Coding, tail latency, collective communication]
  - **authors:** Ertza Warraich, Ali Imran, Annus Zulfiqar, Shay Vargaftik, Sonia Fahmy, Muhammad Shahbaz
  - **institution:** Purdue University, Broadcom, University of Michigan
  - **link:** https://arxiv.org/pdf/2512.22743
  - **Simple LLM Summary:** This paper presents OptiNIC, a domain-specific RDMA transport that eliminates retransmissions and in-order delivery, shifting loss recovery to the ML pipeline to reduce tail latency. It uses adaptive timeouts to trigger forward progress and retains standard congestion control. The evaluation shows OptiNIC significantly improves time-to-accuracy, throughput, and resilience for distributed ML workloads.

- **[arXiv251230] Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference**
  - **tags:** [mlsys], [multi-modal inference], [energy characterization, dynamic voltage and frequency scaling (DVFS), GPU underutilization, stage-level analysis]
  - **authors:** Mona Moghadampanah, Adib Rezaei Shahmirzadi, Farhana Amin, Dimitrios S. Nikolopoulos
  - **institution:** Virginia Tech
  - **link:** https://arxiv.org/pdf/2512.22695
  - **Simple LLM Summary:** This paper provides a stage-level energy characterization of multimodal large language model (MLLM) inference, identifying "modality inflation" as a key inefficiency. It demonstrates that dynamic voltage and frequency scaling (DVFS) is an effective optimization, offering energy savings with minimal performance impact for more efficient MLLM serving systems.

- **[arXiv251230] Two-Robot Computational Landscape: A Complete Characterization of Model Power in Minimal Mobile Robot Systems**
  - **tags:** [sys], [distributed computing], [Look-Compute-Move (LCM), OBLOT, FSTA, FCOM, LUMI, FSYNCH, SSYNCH, ASYNCH, robot models, scheduler, simulation-free method]
  - **authors:** Naoki Kitamura, Yuichi Sudo, Koichi Wada
  - **institution:** The University of Osaka, Hosei University
  - **link:** https://arxiv.org/pdf/2512.22770
  - **Simple LLM Summary:** This paper presents a complete characterization of the computational power of two autonomous mobile robots across major models and schedulers using a novel simulation-free method. It concludes that the computational landscape for two robots fundamentally differs from the general case, with key findings including the equivalence of FSTA and LUMI under full synchrony and the orthogonality of FSTA and FCOM.

- **[arXiv251230] Argus: Token Aware Distributed LLM Inference Optimization**
  - **tags:** [mlsys], [llm inference], [token-aware offloading, length-aware semantics, lyapunov optimization, integer nonlinear programming, edge-cloud system]
  - **authors:** Panlong Wu, Yifei Zhong, Danyang Chen, Ting Wang, Fangxin Wang
  - **institution:** CUHK(SZ)
  - **link:** https://arxiv.org/pdf/2512.22925
  - **Simple LLM Summary:** This paper presents Argus, a token-aware distributed LLM inference framework for edge-cloud systems. Its core method includes a module to predict output token lengths and a Lyapunov-guided optimization module for task offloading, solved by a novel iterative algorithm. The conclusion is that Argus achieves robust and efficient performance in dynamic, heterogeneous environments.

- **[arXiv251230] A Domain Decomposition-based Solver for Acoustic Wave propagation in Two-Dimensional Random Media**
  - **tags:** [sys], [computational mathematics], [domain decomposition, stochastic Galerkin method, polynomial chaos expansion, Neumann-Neumann preconditioner, conjugate gradient]
  - **authors:** Sudhi Sharma Padillath Vasudevan
  - **institution:** Carleton University
  - **link:** https://arxiv.org/pdf/2512.23027
  - **Simple LLM Summary:** This paper presents a domain decomposition-based solver for acoustic wave propagation in random media, using an intrusive stochastic Galerkin method with polynomial chaos expansion. The method transforms the stochastic PDE into a deterministic system and employs a conjugate gradient solver with a two-level Neumann-Neumann preconditioner to handle high computational costs. The results demonstrate the solver's efficient scalability with increasing problem size and number of random parameters.

- **[arXiv251230] Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware**
  - **tags:** [mlsys], [llm inference], [quantization, mixture-of-experts (MoE), on-premise deployment, benchmarking, consumer-grade hardware]
  - **authors:** Alex Khalil, Guillaume Heilles, Maria Parraga, Simon Heilles
  - **institution:** UCLouvain, Universidad Espíritu Santo, DENEM Labs
  - **link:** https://arxiv.org/pdf/2512.23029
  - **Simple LLM Summary:** This paper benchmarks the performance of a quantized, open-source Qwen3-30B MoE model deployed on a private server with consumer-grade hardware (NVIDIA RTX 5090). It finds that this on-premise setup can achieve performance comparable to commercial cloud services, offering SMBs a cost-effective and privacy-preserving alternative for LLM inference.

- **[arXiv251230] Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation**
  - **tags:** [mlsys], [cluster infrastructure], [osmotic learning, self-supervised learning, distributed representation learning, decentralized clustering, information diffusion, dynamic equilibrium]
  - **authors:** Mario Colosi, Reza Farahani, Maria Fazio, Radu Prodan, Massimo Villari
  - **institution:** University of Messina, University of Klagenfurt, University of Innsbruck
  - **link:** https://arxiv.org/pdf/2512.23096
  - **Simple LLM Summary:** This paper introduces Osmotic Learning (OSM-L), a self-supervised distributed learning paradigm that iteratively aligns local data representations to enable information diffusion and convergence into a shared latent knowledge without raw data exchange. The method functions as a decentralized clustering mechanism to uncover hidden structures in distributed data. Experimental results confirm its convergence and high accuracy in aligning local information while preserving contextual integrity.

- **[arXiv251230] FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs**
  - **tags:** [mlsys], [others], [federated learning, graph neural networks, fairness, overlapping subgraphs, weighted aggregation, privacy-preserving estimation]
  - **authors:** Zihao Zhou, Shusen Yang, Fangyuan Zhao, Xuebin Ren
  - **institution:** Xi'an Jiaotong University
  - **link:** https://arxiv.org/pdf/2512.23235
  - **Simple LLM Summary:** The paper proposes FairGFL, a fairness-aware federated learning algorithm for graph data that addresses unfairness from imbalanced overlapping subgraphs. It uses a privacy-preserving weighted aggregation method and a composite loss regularizer to improve fairness while maintaining model utility. Experiments on benchmark datasets show it outperforms baselines in both utility and fairness.

- **[arXiv251230] Optimal Configuration of API Resources in Cloud Native Computing**
  - **tags:** [sys], [cloud computing], [black-box optimization, bayesian optimization, factor screening, Kubernetes, microservices, resource allocation]
  - **authors:** Eddy Truyen, Wouter Joosen
  - **institution:** KU Leuven
  - **link:** https://arxiv.org/pdf/2512.23494
  - **Simple LLM Summary:** This paper applies a black-box optimization framework to find optimal CPU and memory configurations for microservices in Kubernetes during the DevOps Release phase. It concludes that factor screening is useful for finding the optimal configuration with a limited budget, but Bayesian optimization without screening is better for finding a near-optimal solution.

- **[arXiv251230] An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes**
  - **tags:** [mlsys], [cluster infrastructure], [Kubernetes autoscaling, AIOps, SLO-aware control, cost optimization, demand forecasting, multi-signal framework]
  - **authors:** Vinoth Punniyamoorthy, Bikesh Kumar, Sumit Saha, Lokesh Butra, Mayilsamy Palanigounder, Akash Kumar Agarwal, Kabilan Kannan
  - **institution:** IEEE, East West Bank, NTT Data, Albertsons
  - **link:** https://arxiv.org/pdf/2512.23415
  - **Simple LLM Summary:** This paper proposes an AIOps-driven autoscaling framework for Kubernetes that integrates SLO-aware and cost-conscious control with lightweight demand forecasting. The method uses a multi-signal approach to improve responsiveness and stability. The results show it reduces SLO violations by up to 31%, improves scaling response time by 24%, and lowers infrastructure cost by 18% compared to baseline Kubernetes autoscalers.

- **[arXiv251230] Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL**
  - **tags:** [mlsys], [llm inference], [lyapunov optimization, deep reinforcement learning, edge-cloud partitioning, transformer layer decomposition, queue stability]
  - **authors:** Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer
  - **institution:** University of Innsbruck, Sharif University of Technology
  - **link:** https://arxiv.org/pdf/2512.23310
  - **Simple LLM Summary:** The paper proposes Splitwise, a framework that uses Lyapunov-assisted deep reinforcement learning to dynamically partition LLM inference tasks between edge devices and the cloud at a fine-grained sub-layer level. It aims to jointly minimize latency and energy consumption while maintaining accuracy under variable network conditions. Experiments show it significantly reduces latency and energy use compared to existing methods.

- **[arXiv251230] Decoupling Adaptive Control in TeaStore**
  - **tags:** [sys], [self-adaptive software systems, cloud-native computing], [MAPE-K control loop, microservices, Operator pattern, software architectural methods, modularity, system-wide consistency, planning]
  - **authors:** Eddy Truyen
  - **institution:** DistriNet, KU Leuven
  - **link:** https://arxiv.org/pdf/2512.23495
  - **Simple LLM Summary:** This paper examines how software architectural methods, the cloud-native Operator pattern, and legacy programming techniques can be used to decouple self-adaptive control logic from the TeaStore microservice application. It analyzes the trade-offs between fine-grained adaptation and system-wide control. The main conclusion is that these approaches can be combined into a multi-tiered architecture for self-adaptive microservices.

- **[arXiv251230] Bitcoin-IPC: Scaling Bitcoin with a Network of Proof-of-Stake Subnets**
  - **tags:** [sys], [blockchain scaling], [Bitcoin, Layer-2, Proof-of-Stake, subnet, SegWit, SWIFT messaging, virtual-byte cost]
  - **authors:** Marko Vukolić, Orestis Alpos, Jakov Mitrovski, Themis Papameletiou, Nikola Ristić, Dionysis Zindros
  - **institution:** Bitcoin Scaling Labs, Common Prefix
  - **link:** https://arxiv.org/pdf/2512.23439
  - **Simple LLM Summary:** The paper introduces Bitcoin-IPC, a protocol that scales Bitcoin by enabling the creation of permissionless Proof-of-Stake Layer-2 subnets whose stake is denominated in Bitcoin. Its design, inspired by SWIFT messaging and embedded in Bitcoin's SegWit, routes value transfers through Bitcoin L1, reducing transaction cost by up to 23x and increasing throughput to over 160 tps without modifying Bitcoin L1.

- **[arXiv251230] Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates**
  - **tags:** [sys], [distributed systems], [consistent hashing, rendezvous hashing, highest random weight, load balancing, minimal churn, cache locality]
  - **authors:** Yongjie Guan
  - **institution:** Zhejiang University of Technology
  - **link:** https://arxiv.org/pdf/2512.23434
  - **Simple LLM Summary:** This paper introduces Local Rendezvous Hashing (LRH), a method that combines a token ring with a cache-local Highest Random Weight (HRW) election among a fixed window of distinct neighboring nodes to improve load balance. It achieves near-optimal load distribution with minimal key movement during failures and significantly outperforms multi-probe consistent hashing in lookup throughput while maintaining similar balance.

- **[arXiv251230] Fancy Some Chips for Your TeaStore? Modeling the Control of an Adaptable Discrete System**
  - **tags:** [sys], [modeling language for distributed systems], [Chips, control theory, component-based models, adaptable systems, BIP translation]
  - **authors:** Anna Gallone, Simon Bliudze, Sophie Cerf, Olga Kouchnarenko
  - **institution:** Université Marie et Louis Pasteur, CNRS UMR6174, Institut FEMTO-ST; Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 CRIStAL
  - **link:** https://arxiv.org/pdf/2512.23496
  - **Simple LLM Summary:** This paper introduces Chips, a modeling language that combines control theory with programming concepts to design and analyze robust, component-based systems. It demonstrates the language's application through a case study of an adaptable TeaStore web application, showing how Chips facilitates systematic modeling and analysis of complex distributed systems.

- **[arXiv251230] AdaptiFlow: An Extensible Framework for Event-Driven Autonomy in Cloud Microservices**
  - **tags:** [sys], [autonomic computing], [MAPE-K loop, event-driven adaptation, rule-based mechanism, decentralized adaptation, self-healing, self-protection, self-optimization]
  - **authors:** Brice Arléon Zemtsop Ndadji, Simon Bliudze, Clément Quinton
  - **institution:** Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL
  - **link:** https://arxiv.org/pdf/2512.23499
  - **Simple LLM Summary:** This paper presents AdaptiFlow, a framework that provides abstraction layers for the Monitor and Execute phases of the MAPE-K loop to enable event-driven, decentralized autonomy in cloud microservices. It demonstrates that decentralized adaptation can be achieved with minimal service modification through localized decisions, bridging autonomic computing theory with cloud-native practice.

- **[arXiv251230] Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space**
  - **tags:** [sys], [signal processing], [power/energy consumption model, integer-friendly approximation, piecewise-polynomial, conflict-free data placement, mixed-radix streaming FFT, self-sorting FFT, fast Schur algorithm]
  - **authors:** Sergey Salishev
  - **institution:** Saint Petersburg State University
  - **link:** https://arxiv.org/pdf/2512.22676
  - **Simple LLM Summary:** This thesis develops signal-processing algorithms and hardware implementation schemes under constraints of minimal parallelism and memory to improve energy efficiency. It proposes models and methods including a power model for selecting optimal parallelism, approximation techniques to reduce lookup-table size, and conflict-free schedules for FFT computations. The results provide constructive theorems and design trade-offs for building efficient specialized accelerators for low-power computing.

- **[arXiv251230] Revisiting finite Abelian hidden subgroup problem and its distributed exact quantum algorithm**
  - **tags:** [ai], [quantum algorithms], [amplitude amplification, Chinese Remainder Theorem, exact quantum algorithm, distributed quantum algorithm, hidden subgroup problem]
  - **authors:** Ziyuan Dong, Xiang Fan, Tengxun Zhong, Daowen Qiu
  - **institution:** Sun Yat-sen University
  - **link:** https://arxiv.org/pdf/2512.22959
  - **Simple LLM Summary:** This paper revisits the finite Abelian hidden subgroup problem, presenting a new exact quantum algorithm using amplitude amplification and a distributed version leveraging the Chinese Remainder Theorem. The distributed algorithm requires fewer qudits, lower query complexity, and no quantum communication. The authors also extend the distributed approach to some non-Abelian groups and develop a parallel classical algorithm with reduced query complexity.

- **[arXiv251230] Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity**
  - **tags:** [mlsys], [others], [federated learning, L0 constraint, probabilistic gates, sparsity, federated stochastic gradient descent]
  - **authors:** Krishna Harsha Kovelakuntla Huthasana, Alireza Olama, Andreas Lundell
  - **institution:** Åbo Akademi University
  - **link:** https://arxiv.org/pdf/2512.23071
  - **Simple LLM Summary:** This paper proposes a federated learning method that enforces an L0 constraint on model parameters to achieve sparsity, using a reparameterization with probabilistic gates and continuous relaxation. The approach is shown to effectively reach a target parameter density under data and client heterogeneity with minimal performance loss, outperforming magnitude pruning-based methods in communication efficiency and statistical performance across various models and datasets.


**cs.AI/cs.LG contains "reinforcement learning" total: 37**
- [arXiv251230] Unbiased Visual Reasoning with Controlled Visual Inputs [link](https://arxiv.org/pdf/2512.22183)
- [arXiv251230] Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks [link](https://arxiv.org/pdf/2512.22186)
- [arXiv251230] Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part I: Basic Concepts, Neural Networks, and Variants [link](https://arxiv.org/pdf/2512.22190)
- [arXiv251230] Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents [link](https://arxiv.org/pdf/2512.22200)
- [arXiv251230] DiRL: An Efficient Post-Training Framework for Diffusion Language Models [link](https://arxiv.org/pdf/2512.22234)
- [arXiv251230] Masking Teacher and Reinforcing Student for Distilling Vision-Language Models [link](https://arxiv.org/pdf/2512.22238)
- [arXiv251230] Agentic Software Issue Resolution with Large Language Models: A Survey [link](https://arxiv.org/pdf/2512.22256)
- [arXiv251230] VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning [link](https://arxiv.org/pdf/2512.22315)
- [arXiv251230] SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents [link](https://arxiv.org/pdf/2512.22322)
- [arXiv251230] PHANTOM: Physics-Aware Adversarial Attacks against Federated Learning-Coordinated EV Charging Management System [link](https://arxiv.org/pdf/2512.22381)
- [arXiv251230] AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing [link](https://arxiv.org/pdf/2512.22455)
- [arXiv251230] Memento-II: Learning by Stateful Reflective Memory [link](https://arxiv.org/pdf/2512.22716)
- [arXiv251230] FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents [link](https://arxiv.org/pdf/2512.22733)
- [arXiv251230] ReDiF: Reinforced Distillation for Few Step Diffusion [link](https://arxiv.org/pdf/2512.22802)
- [arXiv251230] TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning [link](https://arxiv.org/pdf/2512.22824)
- [arXiv251230] AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning [link](https://arxiv.org/pdf/2512.22857)
- [arXiv251230] Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks [link](https://arxiv.org/pdf/2512.22860)
- [arXiv251230] Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks [link](https://arxiv.org/pdf/2512.22876)
- [arXiv251230] SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning [link](https://arxiv.org/pdf/2512.22895)
- [arXiv251230] Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning [link](https://arxiv.org/pdf/2512.22910)
- [arXiv251230] Heterogeneity in Multi-Agent Reinforcement Learning [link](https://arxiv.org/pdf/2512.22941)
- [arXiv251230] APO: Alpha-Divergence Preference Optimization [link](https://arxiv.org/pdf/2512.22953)
- [arXiv251230] Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning [link](https://arxiv.org/pdf/2512.23087)
- [arXiv251230] Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients [link](https://arxiv.org/pdf/2512.23090)
- [arXiv251230] A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms [link](https://arxiv.org/pdf/2512.23097)
- [arXiv251230] Evaluating Parameter Efficient Methods for RLVR [link](https://arxiv.org/pdf/2512.23165)
- [arXiv251230] ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing [link](https://arxiv.org/pdf/2512.23244)
- [arXiv251230] AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis [link](https://arxiv.org/pdf/2512.23366)
- [arXiv251230] The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis [link](https://arxiv.org/pdf/2512.23419)
- [arXiv251230] Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following [link](https://arxiv.org/pdf/2512.23457)
- [arXiv251230] Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance [link](https://arxiv.org/pdf/2512.23461)
- [arXiv251230] HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation [link](https://arxiv.org/pdf/2512.23464)
- [arXiv251230] Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation [link](https://arxiv.org/pdf/2512.23480)
- [arXiv251230] PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis [link](https://arxiv.org/pdf/2512.23545)
- [arXiv251230] Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning [link](https://arxiv.org/pdf/2512.23617)
- [arXiv251230] Training AI Co-Scientists Using Rubric Rewards [link](https://arxiv.org/pdf/2512.23707)
- [arXiv251230] Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning [link](https://arxiv.org/pdf/2512.23515)

**cs.AI/cs.LG contains "accelerate" total: 20**
- [arXiv251230] HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology [link](https://arxiv.org/pdf/2512.22188)
- [arXiv251230] DiRL: An Efficient Post-Training Framework for Diffusion Language Models [link](https://arxiv.org/pdf/2512.22234)
- [arXiv251230] Graph Attention-based Adaptive Transfer Learning for Link Prediction [link](https://arxiv.org/pdf/2512.22252)
- [arXiv251230] LuxIA: A Lightweight Unitary matriX-based Framework Built on an Iterative Algorithm for Photonic Neural Network Training [link](https://arxiv.org/pdf/2512.22264)
- [arXiv251230] DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations [link](https://arxiv.org/pdf/2512.22283)
- [arXiv251230] LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators [link](https://arxiv.org/pdf/2512.22307)
- [arXiv251230] AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents [link](https://arxiv.org/pdf/2512.22387)
- [arXiv251230] Quantum Generative Models for Computational Fluid Dynamics: A First Exploration of Latent Space Learning in Lattice Boltzmann Simulations [link](https://arxiv.org/pdf/2512.22672)
- [arXiv251230] Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach [link](https://arxiv.org/pdf/2512.22793)
- [arXiv251230] TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning [link](https://arxiv.org/pdf/2512.22824)
- [arXiv251230] SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals [link](https://arxiv.org/pdf/2512.23131)
- [arXiv251230] HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction [link](https://arxiv.org/pdf/2512.23175)
- [arXiv251230] KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta [link](https://arxiv.org/pdf/2512.23236)
- [arXiv251230] Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants [link](https://arxiv.org/pdf/2512.23312)
- [arXiv251230] SoulX-LiveTalk Technical Report [link](https://arxiv.org/pdf/2512.23379)
- [arXiv251230] AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis [link](https://arxiv.org/pdf/2512.23424)
- [arXiv251230] Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification [link](https://arxiv.org/pdf/2512.23436)
- [arXiv251230] HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation [link](https://arxiv.org/pdf/2512.23464)
- [arXiv251230] Space AI: Leveraging Artificial Intelligence for Space to Improve Life on Earth [link](https://arxiv.org/pdf/2512.22399)
- [arXiv251230] Constraint programming model and biased random-key genetic algorithm for the single-machine coupled task scheduling problem with exact delays to minimize the makespan [link](https://arxiv.org/pdf/2512.23150)
