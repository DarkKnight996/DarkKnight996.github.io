"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[25],{2091:(i,e,n)=>{n.r(e),n.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"daily/20251201-20251207","title":"20251201-20251207","description":"2025-12-01","source":"@site/docs/daily/20251201-20251207.md","sourceDirName":"daily","slug":"/daily/20251201-20251207","permalink":"/daily/20251201-20251207","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764647126000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251124-20251130","permalink":"/daily/20251124-20251130"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=n(4848),a=n(8453);const t={},o="20251201-20251207",l={},d=[{value:"2025-12-01",id:"2025-12-01",level:2},{value:"2025-12-02",id:"2025-12-02",level:2}];function c(i){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...i.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20251201-20251207",children:"20251201-20251207"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-01",children:"2025-12-01"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 23"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] An Empirical Study of Cross-Language Interoperability in Replicated Data Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed systems], [replicated data libraries, foreign-function interface, common data format, cross-language interoperability, empirical study]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Provakar Mondal, Eli Tilevich"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Virginia Tech"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22010",children:"https://arxiv.org/pdf/2511.22010"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper empirically compares two strategies for cross-language interoperability in replicated data systems: foreign-function interface (FFI) and common data format (CDF). The study found that CDF-based integration provides better software quality, lower latency, reduced memory consumption, and higher throughput. The authors validated their findings by implementing a CDF-based replicated data library that supports mixed language environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [prefix-aware attention, multi-tile kernel, KV cache optimization, pack-forward-merge, vLLM integration]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jinjun Yi, Zhixin Zhao, Yitao Hu, Ke Yan, Weiwei Sun, Hao Wang, Laiping Zhao, Yuhao Zhang, Wenxin Li, Keqiu Li"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tianjin University, Stevens Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22333",children:"https://arxiv.org/pdf/2511.22333"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," PAT introduces a prefix-aware attention kernel that organizes execution using a pack-forward-merge paradigm to reduce redundant KV cache loading. It employs multi-tile kernels and query packing to optimize resource utilization during LLM decoding. Evaluation shows PAT reduces attention latency by 67.4% on average and improves throughput compared to state-of-the-art attention kernels."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [latency-disaggregated architecture, bottleneck-based scheduler, Roofline-based performance model, fast preemption mechanism, Prefill/Decode disaggregation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Siyu Wu, Zihan Tang, Yuting Zeng, Hui Chen, Guiguang Ding, Tongxuan Liu, Ke Zhang, Hailong Yang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Beihang University, Tsinghua University, University of Science and Technology of China, JD Company"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21862",children:"https://arxiv.org/pdf/2511.21862"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a latency-disaggregated architecture that separates cluster resources into latency-strict and latency-relaxed pools for co-locating online and offline LLM workloads. It introduces a bottleneck-based scheduler with Roofline modeling and fast preemption mechanism to maintain online SLOs while improving resource utilization. Experiments show the method achieves up to 3\xd7 higher offline throughput while preserving online performance compared to existing approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] A Fast and Flat Federated Learning Method via Weighted Momentum and Sharpness-Aware Minimization"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, weighted momentum, sharpness-aware minimization, non-IID convergence, cosine-similarity adaptive rule]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tianle Li, Yongzhi Huang, Linshan Jiang, Chang Liu, Qipeng Xie, Wenfeng Du, Lu Wang, Kaishun Wu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Shenzhen University, The Hong Kong University of Science and Technology (Guangzhou), National University of Singapore, Nanyang Technological University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22080",children:"https://arxiv.org/pdf/2511.22080"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes FedWMSAM, a federated learning method that combines weighted momentum and sharpness-aware minimization to address local-global curvature misalignment and momentum-echo oscillation. It introduces momentum-guided global perturbation and a two-phase training schedule to improve optimization. Experimental results demonstrate the method's effectiveness in achieving fast convergence and robust generalization across non-IID data distributions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Optimality of Simultaneous Consensus with Limited Information Exchange (Extended Abstract)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed consensus], [epistemic logic, knowledge-based programs, simultaneous agreement, crash failures, limited information exchange]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kaya Alpturer, Ron van der Meyden, Sushmita Ruj, Godfrey Wong"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Princeton University, UNSW Sydney"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22380",children:"https://arxiv.org/pdf/2511.22380"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper develops optimal fault-tolerant simultaneous consensus protocols using epistemic logic and knowledge-based programming with limited information exchange. The authors introduce a new information exchange approach that achieves decisions at most one round later than the optimal Dwork-Moses protocol while reducing computation cost and space requirements. They derive protocols that are optimal for various limited information exchanges from the literature, including FloodSet variants and failure-counting approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [biophotonics simulation], [ray-tracing, GPU acceleration, Monte Carlo method, hardware RT-cores, OptiX platform]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shijie Yan, Douglas Dwyer, David R. Kaeli, Qianqian Fang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Northeastern University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22779",children:"https://arxiv.org/pdf/2511.22779"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces RT-MMC, a mesh-based Monte Carlo method that leverages modern GPU ray-tracing hardware for accelerated light transport simulations. By using NVIDIA's OptiX platform and RT-cores, the approach eliminates complex mesh generation while achieving 1.5x to 4.5x speed improvements over traditional methods. The hardware-based ray-tracing significantly simplifies simulation workflows and enhances practicality for biophotonics applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing], [Heard-Of model, asynchronous message-passing, model equivalence, colorless tasks, colored tasks, crash failures, message omissions, silencing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Hagit Attiya, Armando Casta\xf1eda, Dhrubajyoti Ghosh, Thomas Nowak"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Technion \u2013 Israel Institute of Technology, Instituto de Matem\xe1ticas, Universidad Nacional Aut\xf3noma de M\xe9xico, Universit\xe9 Paris-Saclay, CNRS, ENS Paris-Saclay, Laboratoire M\xe9thodes Formelles, Institut Universitaire de France"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21859",children:"https://arxiv.org/pdf/2511.21859"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper analyzes the relationship between the asynchronous message-passing model (AMP_f) and the Heard-Of model (HO_f) through bidirectional simulations and an intermediate model. It concludes that the models are equivalent for solving colorless tasks when n > 2f, but for colored tasks, equivalence holds only for f=1 due to the issue of silenced processes in HO_f."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] When AI Bends Metal: AI-Assisted Optimization of Design Parameters in Sheet Metal Forming"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Bayesian optimization, deep learning, active learning, design space exploration, numerical simulation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ahmad Tarraf, Koutaiba Kassem-Manthey, Seyed Ali Mohammadi, Philipp Martin, Lukas Moj, Semih Burak, Enju Park, Christian Terboven, Felix Wolf"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Technical University of Darmstadt, GNS Gesellschaft f\xfcr numerische Simulation mbH, RWTH Aachen University, GNS Systems GmbH"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22302",children:"https://arxiv.org/pdf/2511.22302"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces an AI-assisted workflow that combines deep learning and Bayesian optimization to automate the tuning of design parameters in sheet metal forming simulations. The method reduces expert involvement and accelerates design space exploration by using a deep learning model for initial parameter estimation followed by iterative refinement. The approach demonstrates significant potential to shorten design iterations and lower computational costs in simulation-driven industrial processes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Areon: Latency-Friendly and Resilient Multi-Proposer Consensus"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [blockchain consensus], [directed acyclic graph, proof-of-stake, multi-proposer, fork choice rule, VRF-based eligibility, sliding window]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," \xc1lvaro Castro-Castilla, Marcin Pawlowski, Hong-Sheng Zhou"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nomos Institute of Free Technology, Jagiellonian University, Virginia Commonwealth University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23025",children:"https://arxiv.org/pdf/2511.23025"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Areon introduces a multi-proposer consensus protocol that organizes blocks into a directed acyclic graph (DAG) with a sliding window reference mechanism and weighted fork choice rule. The protocol achieves bounded-latency finality with lower reorganization frequency compared to chain-based baselines. Experimental results show consistent performance improvements across various adversarial conditions and network delays."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cloud computing], [retry policy, distributed framework, analytical model, microservices, auto-scaling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jhonatan Tavori, Anat Bremler-Barr, Hanoch Levy, Ofek Lavi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tel Aviv University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23278",children:"https://arxiv.org/pdf/2511.23278"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces RetryGuard, a distributed framework that uses an analytical model to manage retry policies across microservices and prevent retry storms. Experimental results show RetryGuard significantly reduces resource usage and costs compared to AWS retry policies in both standard and complex Kubernetes deployments with Istio service mesh."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Silence Speaks Volumes: A New Paradigm for Covert Communication via History Timing Patterns"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [network security], [history covert channels, timing patterns, relative pointers, covert amplification factor, silent history protocol]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Christoph Weissenborn, Steffen Wendzel"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Federal Office for Information Security, Ulm University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22259",children:"https://arxiv.org/pdf/2511.22259"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a novel covert communication method called History Covert Channels (HCC), which embeds hidden messages by using relative pointers to past network timing patterns instead of directly manipulating traffic. This approach reduces reliance on centralized timekeeping and aims to evade standard detection tools. The authors' experiments demonstrate that their method achieves a higher bitrate compared to prior work."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Closing the Generalization Gap in Parameter-efficient Federated Edge Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [model pruning, client selection, joint resource management, generalization analysis, alternating optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xinnong Du, Zhonghao Lyu, Xiaowen Cao, Chunyang Wen, Shuguang Cui, Jie Xu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Chinese University of Hong Kong (Shenzhen), KTH Royal Institute of Technology, Shenzhen University, University of Science and Technology of China"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23282",children:"https://arxiv.org/pdf/2511.23282"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a parameter-efficient federated edge learning framework that jointly optimizes model pruning and client selection with communication-computation resources. The method formulates a generalization-aware optimization problem solved via alternating optimization. Experiments show the approach achieves superior learning performance compared to state-of-the-art baselines by coupling generalization analysis with system-level optimization."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Beyond 2-Edge-Connectivity: Algorithms and Impossibility for Content-Oblivious Leader Election"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing], [content-oblivious communication, leader election, topology knowledge, graph symmetry, message complexity]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yi-Jun Chang, Lyuting Chen, Haoran Zhou"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," National University of Singapore"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23297",children:"https://arxiv.org/pdf/2511.23297"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper studies leader election in content-oblivious networks where nodes can only send asynchronous, content-less pulses. It shows that with topology knowledge, leader election is possible in many non-2-edge-connected graphs like asymmetric trees, but impossible in graphs symmetric about an edge or when topology knowledge is insufficient. The work provides both impossibility results and algorithms with specific message complexity bounds for different graph classes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] ZipperChain: Transmuting Trusted Third-Party Services Into Trustless Atomic Broadcast"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed ledger technology], [ZipperChain, atomic broadcast, trustless, third-party services, distributed consensus, immutability, agreement, availability, pipeline, fast data center network]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Matteo Bjornsson, Taylor Hardin, Taylor Heinecke, Marcin Furtak, David L. Millman, Mike P. Wittie"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," BLOCKY, Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21969",children:"https://arxiv.org/pdf/2511.21969"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ZipperChain is a blockchain that replaces distributed consensus with a pipeline of specialized services on a small number of nodes, transferring trust from established third-party services to provide correctness guarantees. This approach enables high transaction throughput near network line speeds and block finality around 500 ms, without needing a native token for incentives."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [storage systems], [Clock2Q+, cache replacement algorithm, metadata cache, correlated references, S3-FIFO, three queues, correlation window]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yiyan Zhai, Bintang Dwi Marthen, Sarath Balivada, Vamsi Sudhakar Bojji, Eric Knauft, Jitender Rohilla, Jiaqi Zuo, Quanxing Liu, Maxime Austruy, Wenguang Wang, Juncheng Yang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Carnegie Mellon University, Bandung Institute of Technology, Broadcom Inc., Harvard University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21958",children:"https://arxiv.org/pdf/2511.21958"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Clock2Q+, a cache replacement algorithm designed for metadata caches that uses three queues and a correlation window to mitigate the negative impact of correlated references. It demonstrates superior performance, achieving up to a 28.5% lower miss ratio than S3-FIFO on metadata traces, while maintaining low overhead and ease of implementation for large-scale storage systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [blockchain, proof-of-work, trust rating, Bayesian-time-decay, high-performance cluster computing, reward incentive]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Murat Yaslioglu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Istanbul University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21844",children:"https://arxiv.org/pdf/2511.21844"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a novel blockchain-based framework that integrates high-performance cluster computing with AI, using an evolved proof-of-work consensus linked to computational effort and a dynamic Bayesian-time-decay trust rating for node selection. This mechanism aims to optimize resource use, incentivize broad participation, and create a merit-based system for block generation. The main conclusion is that this approach fosters a more sustainable, equitable, and energy-efficient environment for AI development by balancing computational power with inclusivity."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [Mixture-of-Experts scheduling, sparse attention acceleration, prefill-decode disaggregation, KV-cache reuse, continuous batching, load-aware scheduling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jun Wang, Yunxiang Yao, Wenwei Kuang, Runze Mao, Zhenhao Sun, Zhuang Tao, Ziyang Zhang, Dengyu Li, Jiajun Chen, Zhili Wang, Kai Cui, Congzhi Cai, Longwen Lan, Ken Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Huawei Technologies Co., Ltd."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22481",children:"https://arxiv.org/pdf/2511.22481"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," OmniInfer is a system-level acceleration framework built on vLLM that integrates three components\u2014OmniPlacement, OmniAttn, and OmniProxy\u2014to optimize LLM serving through expert placement, sparse attention, and disaggregation-aware scheduling. It achieves performance gains by adaptively disaggregating resources, exploiting sparsity, and coordinating prefill and decode phases. Evaluated on a 10-node cluster, it significantly reduces time-per-output-token and time-to-first-token while increasing query throughput."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] DisCEdge: Distributed Context Management for Large Language Models at the Edge"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [distributed context management, tokenization, geo-distributed storage, edge computing, data replication]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mohammadreza Malekabbasi, Minghe Wang, David Bermbach"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," TU Berlin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22599",children:"https://arxiv.org/pdf/2511.22599"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes DisCEdge, a system for managing LLM user context by storing and replicating it in tokenized form across distributed edge nodes. This approach reduces redundant computation and enables efficient synchronization. The evaluation shows it improves response times, lowers synchronization overhead, and significantly reduces client request sizes compared to existing methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, aggregation methods, personalization, optimization, robustness, heterogeneity, privacy-preserving, IID, non-IID]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Meriem Arbaoui, Mohamed-el-Amine Brahmia, Abdellatif Rahmoun, Mourad Zghal"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," LabRi-SBA Laboratory, CESI LINEACT UR 7527"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22616",children:"https://arxiv.org/pdf/2511.22616"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This survey paper provides a multi-level taxonomy of Federated Learning (FL) aggregation techniques, combining bibliometric analysis and systematic review to classify research in personalization, optimization, and robustness. It concludes by comparing aggregation methods under IID and non-IID data distributions and outlines future research directions to advance the field."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [LoRA, dynamic adapter placement, GPU Direct RDMA, multi-tenant serving, rank heterogeneity]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shashwat Jaiswal, Shrikara Arun, Anjaly Parayil, Ankur Mallick, Spyros Mastorakis, Alind Khare, Chloi Alverti, Renee St Amant, Chetan Bansal, Victor R\xfchle, Josep Torrellas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Illinois Urbana-Champaign, Microsoft, National Technical University of Athens"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22880",children:"https://arxiv.org/pdf/2511.22880"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents LoRAServe, a framework that dynamically places and routes heterogeneous LoRA adapters across GPUs to address performance skew in multi-tenant LLM inference. It uses workload-aware rebalancing and GPU Direct RDMA for remote access to improve resource utilization. The evaluation shows LoRAServe achieves higher throughput and lower latency while using fewer GPUs compared to state-of-the-art systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [split learning, pipeline parallelism, wireless edge networks, communication-computation overlap, alternating optimization, micro-batches]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chenyu Liu, Zhaoyang Zhang, Zirui Chen, Zhaohui Yang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Zhejiang University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23167",children:"https://arxiv.org/pdf/2511.23167"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes C\xb2P\xb2SL, a method that applies pipeline parallelism to split learning in wireless edge networks to overlap communication and computation processes, thereby reducing training latency. It formulates a joint optimization problem for task split and resource allocation, solved via alternating optimization. Experiments show the method reduces system training time by over 38% while maintaining accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Probabilistic Forward Pass, Bayesian Neural Networks, Gaussian propagation, TVM compiler, code generation, operator tuning, uncertainty estimation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bernhard Klein, Falk Selker, Hendrik Borras, Sophie Steger, Franz Pernkopf, Holger Fr\xf6ning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Heidelberg University, Graz University of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23440",children:"https://arxiv.org/pdf/2511.23440"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces an end-to-end pipeline for deploying Bayesian Neural Networks (BNNs) using a Probabilistic Forward Pass (PFP), which approximates inference by propagating Gaussian distributions through the network in a single deterministic pass. The method is implemented via custom operators in the TVM compiler and optimized for ARM CPUs, achieving speedups of up to 4200x compared to traditional sampling-based methods while maintaining comparable accuracy and uncertainty estimation. The results demonstrate that combining Bayesian approximations with code generation enables efficient BNN deployment on resource-constrained embedded systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251201] A lasso-alternative to Dijkstra's algorithm for identifying short paths in networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [graph theory and optimization], [lasso, LARS algorithm, ADMM, bi-directional Dijkstra, \u21131-regularized regression]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Anqi Dong, Amirhossein Taghvaei, Tryphon T. Georgiou"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," KTH Royal Institute of Technology, University of Washington, University of California, Irvine"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22745",children:"https://arxiv.org/pdf/2511.22745"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper formulates the shortest path problem in graphs as an \u21131-regularized regression (lasso). It connects this formulation, solved via the LARS algorithm, to the bi-directional Dijkstra algorithm and highlights the applicability of ADMM for efficient updates to network topology changes."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 33'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Heterogeneous Multi-Agent Reinforcement Learning with Attention for Cooperative and Scalable Feature Transformation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21934",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22101",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Factors That Support Grounded Responses in LLM Conversations: A Rapid Review ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21762",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22176",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Representative Action Selection for Large Action Space: From Bandits to MDPs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22104",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22105",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22210",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22226",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22018",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22235",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21726",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21928",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] An energy-efficient spiking neural network with continuous learning for self-adaptive brain-machine interface ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22108",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Hybrid Stackelberg Game and Diffusion-based Auction for Two-tier Agentic AI Task Offloading in Internet of Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22076",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22138",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] GPS: General Per-Sample Prompter ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21714",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22406",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22570",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22715",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] ORION: Teaching Language Models to Reason Efficiently in the Language of Thought ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22891",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Switching-time bioprocess control with pulse-width-modulated optogenetics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22893",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Language-conditioned world model improves policy generalization by reading environmental descriptions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22904",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22963",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Evolutionary Discovery of Heuristic Policies for Traffic Signal Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23122",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Peer-to-Peer Energy Trading in Dairy Farms using Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23148",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23158",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23193",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23262",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Emergent Coordination and Phase Structure in Independent Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23315",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23442",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] ThetaEvolve: Test-time Learning on Open Problems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23473",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] RELiQ: Scalable Entanglement Routing via Reinforcement Learning in Quantum Networks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22321",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23310",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 19'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] MRI-Based Brain Age Estimation with Supervised Contrastive Learning of Continuous Representation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22102",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21759",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] ResearchArcade: Graph Interface for Academic Tasks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22036",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21736",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Standardized Threat Taxonomy for AI Security, Governance, and Regulatory Compliance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21901",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21848",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Co-Evolving Agents: Learning from Failures as Hard Negatives ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22254",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21920",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] GLA-Grad++: An Improved Griffin-Lim Guided Diffusion Model for Speech Synthesis ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22293",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Cacheback: Speculative Decoding With Nothing But Cache ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21699",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] FastFHE: Packing-Scalable and Depthwise-Separable CNN Inference Over FHE ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22434",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22460",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22586",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23203",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Simultaneous Image Quality Improvement and Artefacts Correction in Accelerated MRI ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.23274",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] QuantumChem-200K: A Large-Scale Open Organic Molecular Dataset for Quantum-Chemistry Property Screening and Language Model Benchmarking ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21747",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Automated Statistical and Machine Learning Platform for Biological Research ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.21770",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Generative models for crystalline materials ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22652",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251201] Escaping Barren Plateaus in Variational Quantum Algorithms Using Negative Learning Rate in Quantum Internet of Things ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2511.22861",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-02",children:"2025-12-02"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 21"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] An optimization framework for task allocation in the edge/hub/cloud paradigm"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [edge computing optimization], [binary integer linear programming, task flow graph, latency optimization, energy optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Andreas Kouloumpris, Georgios L. Stavrinides, Maria K. Michael, Theocharis Theocharides"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Cyprus"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00029",children:"https://arxiv.org/pdf/2512.00029"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a binary integer linear programming (BILP) framework for optimal task allocation in edge/hub/cloud architectures, aiming to minimize either latency or energy consumption. The method is evaluated with real-world and synthetic benchmarks, showing it provides optimal and scalable results for design space exploration."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] A Parallel and Distributed Rust Library for Core Decomposition on Large Graphs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [graph algorithms], [k-core decomposition, parallel computing, shared-memory, Rust, message passing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Davide Rucci, Sebastian Parfeniuc, Matteo Mordacchini, Emanuele Carlini, Alfredo Cuzzocrea, Patrizio Dazzi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," ISTI-CNR, University of Pisa, IIT-CNR, University of Calabria"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00233",children:"https://arxiv.org/pdf/2512.00233"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper adapts a distributed k-core decomposition algorithm for shared-memory systems and implements three optimized versions in Rust. The fastest version, FastK, significantly reduces synchronization overhead and outperforms baseline sequential and parallel implementations, achieving up to an 11x speedup on 16 threads and being orders of magnitude faster than a reference Python implementation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Cross-Domain Federated Semantic Communication with Global Representation Alignment and Domain-Aware Aggregation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, semantic communication, joint source-channel coding, global representation alignment, domain-aware aggregation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Loc X. Nguyen, Ji Su Yoon, Huy Q. Le, Yu Qiao, Avi Deb Raha, Eui-Nam Huh, Walid Saad, Dusit Niyato, Zhu Han, Choong Seon Hong"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Kyung Hee University, Virginia Tech, Nanyang Technological University, University of Houston"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00711",children:"https://arxiv.org/pdf/2512.00711"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a novel federated learning framework to train semantic communication models across different data domains. It introduces global representation alignment to preserve domain semantics and a domain-aware aggregation method to prevent bias from dominant clients. The approach outperforms existing methods in image reconstruction quality, especially as channel conditions improve."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Steady and Energy-Efficient Multi-Hop Clustering for Flying Ad-Hoc Networks (FANETs)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [wireless networks], [multi-hop clustering, mobility-aware clustering, energy-centric CH selection, GS-assisted cluster maintenance]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Basilis Mamalis, Marios Perlitis"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of West Attica, Democritus University of Thrace"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00623",children:"https://arxiv.org/pdf/2512.00623"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a novel multi-hop clustering algorithm for FANETs that constructs stable clusters by selecting cluster heads based on high stability and energy, and employs a ground station-assisted maintenance mechanism. The method aims to enhance cluster longevity and communication efficiency. Experimental results demonstrate that the approach significantly outperforms existing schemes in terms of cluster stability, communication overhead, and security resilience."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] FlexiWalker: Extensible GPU Framework for Efficient Dynamic Random Walks with Runtime Adaptation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [dynamic random walks, rejection sampling, reservoir sampling, runtime adaptation, cost model, compile-time specialization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Seongyeon Park, Jaeyong Song, Changmin Shin, Sukjin Kim, Junguk Hong, Jinho Lee"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Seoul National University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00705",children:"https://arxiv.org/pdf/2512.00705"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," FlexiWalker is a GPU framework that introduces high-performance kernels for rejection and reservoir sampling to efficiently execute dynamic random walks. It uses a lightweight runtime cost model to select the optimal sampling kernel per node and a compile-time component to specialize user logic. The framework significantly outperforms existing CPU and GPU baselines and can handle workloads prior systems cannot support."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Heimdall++: Optimizing GPU Utilization and Pipeline Parallelism for Efficient Single-Pulse Detection"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [GPU parallelization, memory management, multi-threaded framework, pipeline parallelism]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bingzheng Xia, Zujie Ren, Kuang Ma, Xiaoqian Li, Wenda Li, Shuibing He"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Chinese Academy of Sciences, Zhejiang Lab, Zhejiang University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00398",children:"https://arxiv.org/pdf/2512.00398"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents Heimdall++, an optimized GPU-accelerated tool for single-pulse detection in radio astronomy. It improves upon the original Heimdall by implementing fine-grained GPU parallelization, enhanced memory management, and a multi-threaded framework to decouple processing stages and reduce GPU stalls. The results show that Heimdall++ achieves up to 2.66x speedup in processing while maintaining result consistency."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] IslandRun: Privacy-Aware Multi-Objective Orchestration for Distributed AI Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [multi-objective orchestration, agent-based routing, tiered island groups, typed placeholder sanitization, reversible anonymization, data locality routing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bala Siva Sai Akhil Malepati"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Independent researcher (based on email domain)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00595",children:"https://arxiv.org/pdf/2512.00595"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"}),' The paper presents IslandRun, a system for distributed AI inference that treats computational resources as autonomous "islands" and uses agent-based routing and reversible anonymization to orchestrate tasks across personal devices, edge servers, and the cloud. Its core method involves policy-constrained multi-objective optimization to balance performance, privacy, cost, and trust. The main conclusion is that this establishes a new paradigm for privacy-aware, decentralized inference orchestration across heterogeneous computing ecosystems.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Challenges of Heterogeneity in Big Data: A Comparative Study of Classification in Large-Scale Structured and Unstructured Domains"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [hyperparameter optimization, genetic algorithms, optuna, apache spark, transformer embeddings, roberta, bayesian target encoding, svm, logistic regression]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gonz\xe1lez Trigueros Jes\xfas Eduardo, Alonso S\xe1nchez Alejandro, Mu\xf1oz Rivera Emilio, Pe\xf1ar\xe1n Prieto Mariana Jaqueline, Mendoza Gonz\xe1lez Camila Natalia"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universidad de Guanajuato"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00298",children:"https://arxiv.org/pdf/2512.00298"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"}),' This paper compares classification strategies for structured and unstructured big data, using evolutionary/Bayesian hyperparameter optimization for numerical data and distributed Apache Spark processing for text. It finds a "complexity paradox" where optimized linear models outperform complex ones on structured data, while for text, robust feature engineering with Transformer embeddings allows simpler models to generalize better. The work provides a framework for algorithm selection based on data nature and infrastructure.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] LLaMCAT: Optimizing Large Language Model Inference with Cache Arbitration and Throttling"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [cache arbitration, thread throttling, MSHR contention, KV Cache, hybrid simulation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhongchun Zhou, Chengtao Lai, Wei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Hong Kong University of Science and Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00083",children:"https://arxiv.org/pdf/2512.00083"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces LLaMCAT, a method that optimizes LLM inference by combining Miss Status Holding Register (MSHR)- and load balance-aware cache arbitration with thread throttling to reduce stalls in KV Cache access. It demonstrates significant speedups over baselines, particularly when systems are bottlenecked by miss handling throughput or limited cache size, offering a practical hardware-level solution for accelerating LLM inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Quantum-Adversary-Resilient Evidence Structures and Migration Strategies for Regulated AI Audit Trails"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cryptographic audit trails], [post-quantum signatures, hash-and-sign, QROM, hybrid signatures, re-signing, Merkle-root anchoring, Q-Audit Integrity, Q-Non-Equivocation, Q-Binding]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Leo Kao"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Codebat Technologies Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00110",children:"https://arxiv.org/pdf/2512.00110"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper formalizes quantum-adversary-resilient security notions for constant-size cryptographic evidence structures used in AI audit trails and analyzes a post-quantum hash-and-sign instantiation. It proposes and evaluates three migration strategies\u2014hybrid signatures, re-signing, and Merkle-root anchoring\u2014for transitioning existing logs. The study concludes that quantum-safe audit trails are achievable with moderate overhead and that systematic migration can extend the evidentiary lifetime of deployments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] FC-ADL: Efficient Microservice Anomaly Detection and Localisation Through Functional Connectivity"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [functional connectivity, anomaly detection, root cause analysis, microservices, time-varying dependencies]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Giles Winchester, George Parisis, Luc Berthouze"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Sussex"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00844",children:"https://arxiv.org/pdf/2512.00844"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes FC-ADL, a method for microservice anomaly detection and localization by analyzing time-varying functional connectivity between service metrics. It shows this approach can effectively detect faults and identify root causes while being more scalable than causal inference methods. The method is validated on large-scale real-world deployments like Alibaba's."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [sampling, decision plane, tensor parallelism, pipeline parallelism, sequence-parallel sampling, speculative hot-vocab sampling, CPU-based algorithm]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bohan Zhao, Zane Cao, Yongchao He"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Not explicitly stated in the provided text. Author affiliations are not included."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00719",children:"https://arxiv.org/pdf/2512.00719"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes SIMPLE, a method that disaggregates the sampling step from GPU inference by moving it to a CPU-side service. This approach uses sequence-parallel sampling and speculative hot-vocab sampling to reduce the bottleneck caused by sampling in distributed LLM serving. The evaluation shows that SIMPLE significantly improves throughput and reduces latency while being compatible with existing data-plane optimizations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Delta Sum Learning: an approach for fast and global convergence in Gossip Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [gossip learning, delta sum learning, open application model, decentralized orchestration, federated averaging]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tom Goethals, Merlijn Sebrechts, Stijn De Schrijver, Filip De Turck, Bruno Volckaert"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Ghent University - imec, IDLab"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01549",children:"https://arxiv.org/pdf/2512.01549"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes Delta Sum Learning, a new aggregation method to improve global convergence in decentralized Gossip Learning. It also implements the method within a decentralized orchestration framework based on the Open Application Model. Evaluation shows that Delta Sum Learning maintains strong global convergence and scales better than alternatives, with a logarithmic versus linear accuracy loss as the network size increases."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Elastic Mixture of Rank-Wise Experts for Knowledge Reuse in Federated Fine-Tuning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [federated learning, LoRA, mixture of experts, knowledge reuse, parameter-efficient fine-tuning]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yebo Wu, Jingguang Li, Zhijiang Guo, Li Li"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Macau, HKUST, HKUST (Guangzhou)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00902",children:"https://arxiv.org/pdf/2512.00902"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes SmartFed, a federated fine-tuning framework that reuses knowledge from existing LoRA modules via a Mixture of Rank-Wise Experts (MoRE) and an Elastic Expert Quota Allocation (EEQA) mechanism to reduce computational and communication costs. It demonstrates that this approach significantly improves model performance and training efficiency compared to existing methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] StarDist: A Code Generator for Distributed Graph Algorithms"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed graph processing], [MPI, Remote Memory Access (RMA), code generation, communication aggregation, bulk-reduction substrate]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Barenya Kumar Nandy, Rupesh Nasre"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Indian Institute of Technology, Madras"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01646",children:"https://arxiv.org/pdf/2512.01646"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents StarDist, an analysis-transformation framework that optimizes the distributed backend of the StarPlat DSL compiler for graph algorithms. It aggregates communication by reordering neighborhood accesses and uses an optimized bulk-reduction substrate built on Open MPI's passive RMA. The optimized system outperforms d-Galois and DRONE in Single Source Shortest Path computations on large graphs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [GPU memory reuse, unified GPU memory pool, on-demand KV cache allocation, GPU-affinity-aware scheduling, cold-start optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Wenbin Zhu, Zhaoyan Shen, Zili Shao, Hongjun Dai, Feng Chen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Shandong University, The Chinese University of Hong Kong, Indiana University Bloomington"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01357",children:"https://arxiv.org/pdf/2512.01357"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents Tangram, a system that accelerates serverless LLM loading by reusing GPU memory to retain model parameters, reducing cold-start latency. Its key techniques include a unified memory pool for tensor sharing, dynamic KV cache management, and affinity-aware scheduling. Experiments show Tangram achieves up to 6.2x faster loading and reduces Time-To-First-Token by 23\u201355% compared to state-of-the-art methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Feature-Based Semantics-Aware Scheduling for Energy-Harvesting Federated Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, energy harvesting, client scheduling, version age of information, semantics-aware communication, feature-based proxy, non-IID data]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Eunjeong Jeong, Giovanni Perin, Howard H. Yang, Nikolaos Pappas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Link\xf6ping University, University of Brescia, Zhejiang University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01983",children:"https://arxiv.org/pdf/2512.01983"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a lightweight client scheduling framework for Energy-Harvesting Federated Learning that uses a feature-based proxy to efficiently estimate the Version Age of Information, a semantics-aware metric. This approach reduces the computational cost of predicting update significance, avoiding redundant local training. Experiments show the method achieves superior learning performance and energy reduction compared to baseline policies under extreme non-IID data and scarce energy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Joint Partitioning and Placement of Foundation Models for Real-Time Edge AI"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [dynamic graph re-partitioning, model-aware capacity profiling, distributed split inference, adaptive orchestration, runtime-resolved placement]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Aladin Djuhera, Fernando Koch, Alecio Binotto"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Technical University of Munich, Florida Atlantic University, Carl Zeiss AG"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01039",children:"https://arxiv.org/pdf/2512.01039"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a framework for dynamically partitioning and placing foundation model layers across heterogeneous edge nodes at runtime, formalized as a constrained optimization problem. It aims to adapt to fluctuating network and compute resources by integrating model-aware profiling with reactive graph re-partitioning. The main conclusion is that this approach enables efficient, low-latency inference for large models in volatile edge environments like 6G MEC, overcoming the limitations of static split inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Morphling: Fast, Fused, and Flexible GNN Training at Scale"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [domain-specific code synthesis, architecture-aware primitives, sparsity-aware execution, OpenMP, CUDA, MPI]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Anubhab, Rupesh Nasre"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," IIT Madras"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01678",children:"https://arxiv.org/pdf/2512.01678"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents Morphling, a domain-specific code synthesizer that compiles high-level GNN specifications into optimized, portable implementations for CPUs and GPUs. It uses a library of architecture-aware primitives and a runtime engine to dynamically choose dense or sparse execution paths. The results show that Morphling significantly outperforms existing frameworks in training throughput and memory efficiency on diverse datasets."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] Trace-based, time-resolved analysis of MPI application performance using standard metrics"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [MPI performance analysis], [trace-based analysis, time-resolved metrics, Paraver, critical path reconstruction, load balance, serialisation efficiency, transfer efficiency]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kingshuk Haldar"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," High Performance Computing Center Stuttgart (HLRS), University of Stuttgart"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01764",children:"https://arxiv.org/pdf/2512.01764"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a method for analyzing MPI application performance by calculating time-resolved standard metrics (like load balance and transfer efficiency) from execution traces, using fixed or adaptive time windows. It robustly processes Paraver traces to reconstruct critical paths and handle event anomalies. The approach reveals transient performance bottlenecks that are hidden by global, time-aggregated metrics, offering a scalable alternative to full trace visualization."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251202] UNIQ: Communication-Efficient Distributed Quantum Computing via Unified Nonlinear Integer Programming"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed quantum computing], [non-linear integer programming, greedy algorithm, JIT (Just-In-Time), EPR pair generation, qubit allocation, network scheduling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Hui Zhong, Jiachen Shen, Lei Fan, Xinyue Zhang, Hao Wang, Miao Pan, Zhu Han"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Houston"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00401",children:"https://arxiv.org/pdf/2512.00401"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes UNIQ, a unified optimization framework for distributed quantum computing that integrates qubit allocation, entanglement management, and network scheduling into a single non-linear integer programming model. It uses a greedy algorithm for qubit mapping and a JIT approach for parallel EPR pair generation to minimize communication costs and runtime. The method is shown to substantially outperform existing approaches across diverse circuits and topologies."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 61'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] NetDeTox: Adversarial and Efficient Evasion of Hardware-Security GNNs via RL-LLM Orchestration ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00119",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00545",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Socially aware navigation for mobile robots: a survey on deep reinforcement learning approaches ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00049",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Gradient Inversion in Federated Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00303",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] List Replicable Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00553",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00319",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00352",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Learning Causal States Under Partial Observability and Perturbation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00357",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00351",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00601",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Causal Reinforcement Learning based Agent-Patient Interaction with Clinical Domain Knowledge ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00048",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] InF-ATPG: Intelligent FFR-Driven ATPG with Advanced Circuit Representation Guided Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00079",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] ESPO: Entropy Importance Sampling Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00499",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] G-KV: Decoding-Time KV Cache Eviction with Global Attention ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00504",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00062",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00709",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00042",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Perturbation-mitigated USV Navigation with Distributionally Robust Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00030",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00249",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Reinforcement Learning from Implicit Neural Feedback for Human-Aligned Robot Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00050",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Upcycled and Merged MoE Reward Model for Mitigating Reward Hacking ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00724",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] AI Agent for Source Finding by SoFiA-2 for SKA-SDC2 ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00769",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] What Is Preference Optimization Doing, How and Why? ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00778",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00831",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00908",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00915",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00961",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00968",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] AltNet: Addressing the Plasticity-Stability Dilemma in Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01034",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01046",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Automating the Refinement of Reinforcement Learning Specifications ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01047",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01054",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] World Model Robustness via Surprise Recognition ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01119",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Mode-Conditioning Unlocks Superior Test-Time Scaling ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01127",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] A TinyML Reinforcement Learning Approach for Energy-Efficient Light Control in Low-Cost Greenhouse Systems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01167",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Sum Rate Maximization in STAR-RIS-UAV-Assisted Networks: A CA-DDPG Approach for Joint Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01202",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01224",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] On the Tension Between Optimality and Adversarial Robustness in Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01228",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01282",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01311",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Extending NGU to Multi-Agent RL: A Preliminary Study ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01321",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Directed evolution algorithm drives neural prediction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01362",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01366",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Stabilizing Reinforcement Learning with LLMs: Formulation and Practices ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01374",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Multi-Path Collaborative Reasoning via Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01485",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01661",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] How Does RL Post-training Induce Skill Composition? A Case Study on Countdown ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01775",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01801",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01878",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] New Spiking Architecture for Multi-Modal Decision-Making in Autonomous Vehicles ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01882",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Rectifying LLM Thought from Lens of Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01925",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Agentic Policy Optimization via Instruction-Policy Co-Evolution ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01945",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01952",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Learned-Rule-Augmented Large Language Model Evaluators ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01958",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Forecasting in Offline Reinforcement Learning for Non-stationary Environments ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01987",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01993",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Learning Sim-to-Real Humanoid Locomotion in 15 Minutes ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01996",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] A Diffusion Model Framework for Maximum Entropy Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02019",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Optimizing Information Asset Investment Strategies in the Exploratory Phase of the Oil and Gas Industry: A Reinforcement Learning Approach ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00243",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] How do trout regulate patterns of muscle contraction to optimize propulsive efficiency during steady swimming ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01218",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Formal Verification of Noisy Quantum Reinforcement Learning Policies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01502",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 29'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Socially aware navigation for mobile robots: a survey on deep reinforcement learning approaches ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00049",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] From RISC-V Cores to Neuromorphic Arrays: A Tutorial on Building Scalable Digital Neuromorphic Processors ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00113",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00396",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] A Comprehensive Survey on Surgical Digital Twin ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00019",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] SetupKit: Efficient Multi-Corner Setup/Hold Time Characterization Using Bias-Enhanced Interpolation and Active Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00044",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] A Rosetta Stone for AI Benchmarks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00193",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] SafeCiM: Investigating Resilience of Hybrid Floating-Point Compute-in-Memory Deep Learning Accelerators ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00059",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] GreenPlanner: Practical Floorplan Layout Generation via an Energy-Aware and Function-Feasible Generative Framework ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00406",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] ESPO: Entropy Importance Sampling Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00499",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00062",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] KAN-SAs: Efficient Acceleration of Kolmogorov-Arnold Networks on Systolic Arrays ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00055",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Faster Verified Explanations for Neural Networks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00164",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00722",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00829",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00931",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Diffusion Model in Latent Space for Medical Image Segmentation Task ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01292",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] The Necessity of Imperfection",":Reversing"," Model Collapse via Simulating Cognitive Boundedness ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01354",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Stabilizing Reinforcement Learning with LLMs: Formulation and Practices ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01374",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] RE-LLM: Integrating Large Language Models into Renewable Energy Systems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01392",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] hls4ml: A Flexible, Open-Source Platform for Deep Learning Acceleration on Reconfigurable Hardware ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01463",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] A unified framework for geometry-independent operator learning in cardiac electrophysiology simulations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01702",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Unifying Sign and Magnitude for Optimizing Deep Vision Networks via ThermoLion ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01881",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01888",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02010",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.02020",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.00299",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Building Trustworthy AI for Materials Discovery: From Autonomous Laboratories to Z-scores ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01080",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Deep FlexQP: Accelerated Nonlinear Programming via Deep Unfolding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01565",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251202] Cuffless Blood Pressure Estimation from Six Wearable Sensor Modalities in Multi-Motion-State Scenarios ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.01653",children:"link"})]}),"\n"]})]})}function h(i={}){const{wrapper:e}={...(0,a.R)(),...i.components};return e?(0,s.jsx)(e,{...i,children:(0,s.jsx)(c,{...i})}):c(i)}},8453:(i,e,n)=>{n.d(e,{R:()=>t,x:()=>o});var r=n(6540);const s={},a=r.createContext(s);function t(i){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof i?i(e):{...e,...i}},[e,i])}function o(i){let e;return e=i.disableParentContext?"function"==typeof i.components?i.components(s):i.components||s:t(i.components),r.createElement(a.Provider,{value:e},i.children)}}}]);