"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[25],{2091:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>a,metadata:()=>r,toc:()=>h});const r=JSON.parse('{"id":"daily/20251201-20251207","title":"20251201-20251207","description":"2025-12-01","source":"@site/docs/daily/20251201-20251207.md","sourceDirName":"daily","slug":"/daily/20251201-20251207","permalink":"/daily/20251201-20251207","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764564077000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251124-20251130","permalink":"/daily/20251124-20251130"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=n(4848),t=n(8453);const a={},o="20251201-20251207",l={},h=[{value:"2025-12-01",id:"2025-12-01",level:2}];function d(e){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"20251201-20251207",children:"20251201-20251207"})}),"\n",(0,s.jsx)(i.h2,{id:"2025-12-01",children:"2025-12-01"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"cs.DC total: 23"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] An Empirical Study of Cross-Language Interoperability in Replicated Data Systems"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [distributed systems], [replicated data libraries, foreign-function interface, common data format, cross-language interoperability, empirical study]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Provakar Mondal, Eli Tilevich"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Virginia Tech"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22010",children:"https://arxiv.org/pdf/2511.22010"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper empirically compares two strategies for cross-language interoperability in replicated data systems: foreign-function interface (FFI) and common data format (CDF). The study found that CDF-based integration provides better software quality, lower latency, reduced memory consumption, and higher throughput. The authors validated their findings by implementing a CDF-based replicated data library that supports mixed language environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm inference], [prefix-aware attention, multi-tile kernel, KV cache optimization, pack-forward-merge, vLLM integration]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Jinjun Yi, Zhixin Zhao, Yitao Hu, Ke Yan, Weiwei Sun, Hao Wang, Laiping Zhao, Yuhao Zhang, Wenxin Li, Keqiu Li"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Tianjin University, Stevens Institute of Technology"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22333",children:"https://arxiv.org/pdf/2511.22333"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," PAT introduces a prefix-aware attention kernel that organizes execution using a pack-forward-merge paradigm to reduce redundant KV cache loading. It employs multi-tile kernels and query packing to optimize resource utilization during LLM decoding. Evaluation shows PAT reduces attention latency by 67.4% on average and improves throughput compared to state-of-the-art attention kernels."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm inference], [latency-disaggregated architecture, bottleneck-based scheduler, Roofline-based performance model, fast preemption mechanism, Prefill/Decode disaggregation]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Siyu Wu, Zihan Tang, Yuting Zeng, Hui Chen, Guiguang Ding, Tongxuan Liu, Ke Zhang, Hailong Yang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Beihang University, Tsinghua University, University of Science and Technology of China, JD Company"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21862",children:"https://arxiv.org/pdf/2511.21862"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes a latency-disaggregated architecture that separates cluster resources into latency-strict and latency-relaxed pools for co-locating online and offline LLM workloads. It introduces a bottleneck-based scheduler with Roofline modeling and fast preemption mechanism to maintain online SLOs while improving resource utilization. Experiments show the method achieves up to 3\xd7 higher offline throughput while preserving online performance compared to existing approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] A Fast and Flat Federated Learning Method via Weighted Momentum and Sharpness-Aware Minimization"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [federated learning, weighted momentum, sharpness-aware minimization, non-IID convergence, cosine-similarity adaptive rule]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Tianle Li, Yongzhi Huang, Linshan Jiang, Chang Liu, Qipeng Xie, Wenfeng Du, Lu Wang, Kaishun Wu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Shenzhen University, The Hong Kong University of Science and Technology (Guangzhou), National University of Singapore, Nanyang Technological University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22080",children:"https://arxiv.org/pdf/2511.22080"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes FedWMSAM, a federated learning method that combines weighted momentum and sharpness-aware minimization to address local-global curvature misalignment and momentum-echo oscillation. It introduces momentum-guided global perturbation and a two-phase training schedule to improve optimization. Experimental results demonstrate the method's effectiveness in achieving fast convergence and robust generalization across non-IID data distributions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] Optimality of Simultaneous Consensus with Limited Information Exchange (Extended Abstract)"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [distributed consensus], [epistemic logic, knowledge-based programs, simultaneous agreement, crash failures, limited information exchange]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Kaya Alpturer, Ron van der Meyden, Sushmita Ruj, Godfrey Wong"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Princeton University, UNSW Sydney"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22380",children:"https://arxiv.org/pdf/2511.22380"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper develops optimal fault-tolerant simultaneous consensus protocols using epistemic logic and knowledge-based programming with limited information exchange. The authors introduce a new information exchange approach that achieves decisions at most one round later than the optimal Dwork-Moses protocol while reducing computation cost and space requirements. They derive protocols that are optimal for various limited information exchanges from the literature, including FloodSet variants and failure-counting approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [biophotonics simulation], [ray-tracing, GPU acceleration, Monte Carlo method, hardware RT-cores, OptiX platform]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Shijie Yan, Douglas Dwyer, David R. Kaeli, Qianqian Fang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Northeastern University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22779",children:"https://arxiv.org/pdf/2511.22779"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces RT-MMC, a mesh-based Monte Carlo method that leverages modern GPU ray-tracing hardware for accelerated light transport simulations. By using NVIDIA's OptiX platform and RT-cores, the approach eliminates complex mesh generation while achieving 1.5x to 4.5x speed improvements over traditional methods. The hardware-based ray-tracing significantly simplifies simulation workflows and enhances practicality for biophotonics applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [distributed computing], [Heard-Of model, asynchronous message-passing, model equivalence, colorless tasks, colored tasks, crash failures, message omissions, silencing]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Hagit Attiya, Armando Casta\xf1eda, Dhrubajyoti Ghosh, Thomas Nowak"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Technion \u2013 Israel Institute of Technology, Instituto de Matem\xe1ticas, Universidad Nacional Aut\xf3noma de M\xe9xico, Universit\xe9 Paris-Saclay, CNRS, ENS Paris-Saclay, Laboratoire M\xe9thodes Formelles, Institut Universitaire de France"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21859",children:"https://arxiv.org/pdf/2511.21859"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper analyzes the relationship between the asynchronous message-passing model (AMP_f) and the Heard-Of model (HO_f) through bidirectional simulations and an intermediate model. It concludes that the models are equivalent for solving colorless tasks when n > 2f, but for colored tasks, equivalence holds only for f=1 due to the issue of silenced processes in HO_f."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] When AI Bends Metal: AI-Assisted Optimization of Design Parameters in Sheet Metal Forming"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [Bayesian optimization, deep learning, active learning, design space exploration, numerical simulation]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Ahmad Tarraf, Koutaiba Kassem-Manthey, Seyed Ali Mohammadi, Philipp Martin, Lukas Moj, Semih Burak, Enju Park, Christian Terboven, Felix Wolf"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Technical University of Darmstadt, GNS Gesellschaft f\xfcr numerische Simulation mbH, RWTH Aachen University, GNS Systems GmbH"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22302",children:"https://arxiv.org/pdf/2511.22302"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces an AI-assisted workflow that combines deep learning and Bayesian optimization to automate the tuning of design parameters in sheet metal forming simulations. The method reduces expert involvement and accelerates design space exploration by using a deep learning model for initial parameter estimation followed by iterative refinement. The approach demonstrates significant potential to shorten design iterations and lower computational costs in simulation-driven industrial processes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] Areon: Latency-Friendly and Resilient Multi-Proposer Consensus"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [blockchain consensus], [directed acyclic graph, proof-of-stake, multi-proposer, fork choice rule, VRF-based eligibility, sliding window]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," \xc1lvaro Castro-Castilla, Marcin Pawlowski, Hong-Sheng Zhou"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Nomos Institute of Free Technology, Jagiellonian University, Virginia Commonwealth University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23025",children:"https://arxiv.org/pdf/2511.23025"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," Areon introduces a multi-proposer consensus protocol that organizes blocks into a directed acyclic graph (DAG) with a sliding window reference mechanism and weighted fork choice rule. The protocol achieves bounded-latency finality with lower reorganization frequency compared to chain-based baselines. Experimental results show consistent performance improvements across various adversarial conditions and network delays."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [cloud computing], [retry policy, distributed framework, analytical model, microservices, auto-scaling]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Jhonatan Tavori, Anat Bremler-Barr, Hanoch Levy, Ofek Lavi"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Tel Aviv University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23278",children:"https://arxiv.org/pdf/2511.23278"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces RetryGuard, a distributed framework that uses an analytical model to manage retry policies across microservices and prevent retry storms. Experimental results show RetryGuard significantly reduces resource usage and costs compared to AWS retry policies in both standard and complex Kubernetes deployments with Istio service mesh."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] Silence Speaks Volumes: A New Paradigm for Covert Communication via History Timing Patterns"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [network security], [history covert channels, timing patterns, relative pointers, covert amplification factor, silent history protocol]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Christoph Weissenborn, Steffen Wendzel"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Federal Office for Information Security, Ulm University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22259",children:"https://arxiv.org/pdf/2511.22259"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces a novel covert communication method called History Covert Channels (HCC), which embeds hidden messages by using relative pointers to past network timing patterns instead of directly manipulating traffic. This approach reduces reliance on centralized timekeeping and aims to evade standard detection tools. The authors' experiments demonstrate that their method achieves a higher bitrate compared to prior work."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] Closing the Generalization Gap in Parameter-efficient Federated Edge Learning"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [model pruning, client selection, joint resource management, generalization analysis, alternating optimization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Xinnong Du, Zhonghao Lyu, Xiaowen Cao, Chunyang Wen, Shuguang Cui, Jie Xu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," The Chinese University of Hong Kong (Shenzhen), KTH Royal Institute of Technology, Shenzhen University, University of Science and Technology of China"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23282",children:"https://arxiv.org/pdf/2511.23282"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a parameter-efficient federated edge learning framework that jointly optimizes model pruning and client selection with communication-computation resources. The method formulates a generalization-aware optimization problem solved via alternating optimization. Experiments show the approach achieves superior learning performance compared to state-of-the-art baselines by coupling generalization analysis with system-level optimization."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] Beyond 2-Edge-Connectivity: Algorithms and Impossibility for Content-Oblivious Leader Election"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [distributed computing], [content-oblivious communication, leader election, topology knowledge, graph symmetry, message complexity]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Yi-Jun Chang, Lyuting Chen, Haoran Zhou"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," National University of Singapore"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23297",children:"https://arxiv.org/pdf/2511.23297"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper studies leader election in content-oblivious networks where nodes can only send asynchronous, content-less pulses. It shows that with topology knowledge, leader election is possible in many non-2-edge-connected graphs like asymmetric trees, but impossible in graphs symmetric about an edge or when topology knowledge is insufficient. The work provides both impossibility results and algorithms with specific message complexity bounds for different graph classes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] ZipperChain: Transmuting Trusted Third-Party Services Into Trustless Atomic Broadcast"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [distributed ledger technology], [ZipperChain, atomic broadcast, trustless, third-party services, distributed consensus, immutability, agreement, availability, pipeline, fast data center network]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Matteo Bjornsson, Taylor Hardin, Taylor Heinecke, Marcin Furtak, David L. Millman, Mike P. Wittie"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," BLOCKY, Inc."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21969",children:"https://arxiv.org/pdf/2511.21969"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," ZipperChain is a blockchain that replaces distributed consensus with a pipeline of specialized services on a small number of nodes, transferring trust from established third-party services to provide correctness guarantees. This approach enables high transaction throughput near network line speeds and block finality around 500 ms, without needing a native token for incentives."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [storage systems], [Clock2Q+, cache replacement algorithm, metadata cache, correlated references, S3-FIFO, three queues, correlation window]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Yiyan Zhai, Bintang Dwi Marthen, Sarath Balivada, Vamsi Sudhakar Bojji, Eric Knauft, Jitender Rohilla, Jiaqi Zuo, Quanxing Liu, Maxime Austruy, Wenguang Wang, Juncheng Yang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Carnegie Mellon University, Bandung Institute of Technology, Broadcom Inc., Harvard University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21958",children:"https://arxiv.org/pdf/2511.21958"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces Clock2Q+, a cache replacement algorithm designed for metadata caches that uses three queues and a correlation window to mitigate the negative impact of correlated references. It demonstrates superior performance, achieving up to a 28.5% lower miss ratio than S3-FIFO on metadata traces, while maintaining low overhead and ease of implementation for large-scale storage systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [blockchain, proof-of-work, trust rating, Bayesian-time-decay, high-performance cluster computing, reward incentive]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Murat Yaslioglu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Istanbul University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21844",children:"https://arxiv.org/pdf/2511.21844"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes a novel blockchain-based framework that integrates high-performance cluster computing with AI, using an evolved proof-of-work consensus linked to computational effort and a dynamic Bayesian-time-decay trust rating for node selection. This mechanism aims to optimize resource use, incentivize broad participation, and create a merit-based system for block generation. The main conclusion is that this approach fosters a more sustainable, equitable, and energy-efficient environment for AI development by balancing computational power with inclusivity."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm inference], [Mixture-of-Experts scheduling, sparse attention acceleration, prefill-decode disaggregation, KV-cache reuse, continuous batching, load-aware scheduling]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Jun Wang, Yunxiang Yao, Wenwei Kuang, Runze Mao, Zhenhao Sun, Zhuang Tao, Ziyang Zhang, Dengyu Li, Jiajun Chen, Zhili Wang, Kai Cui, Congzhi Cai, Longwen Lan, Ken Zhang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Huawei Technologies Co., Ltd."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22481",children:"https://arxiv.org/pdf/2511.22481"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," OmniInfer is a system-level acceleration framework built on vLLM that integrates three components\u2014OmniPlacement, OmniAttn, and OmniProxy\u2014to optimize LLM serving through expert placement, sparse attention, and disaggregation-aware scheduling. It achieves performance gains by adaptively disaggregating resources, exploiting sparsity, and coordinating prefill and decode phases. Evaluated on a 10-node cluster, it significantly reduces time-per-output-token and time-to-first-token while increasing query throughput."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] DisCEdge: Distributed Context Management for Large Language Models at the Edge"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm inference], [distributed context management, tokenization, geo-distributed storage, edge computing, data replication]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Mohammadreza Malekabbasi, Minghe Wang, David Bermbach"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," TU Berlin"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22599",children:"https://arxiv.org/pdf/2511.22599"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes DisCEdge, a system for managing LLM user context by storing and replicating it in tokenized form across distributed edge nodes. This approach reduces redundant computation and enables efficient synchronization. The evaluation shows it improves response times, lowers synchronization overhead, and significantly reduces client request sizes compared to existing methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [federated learning, aggregation methods, personalization, optimization, robustness, heterogeneity, privacy-preserving, IID, non-IID]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Meriem Arbaoui, Mohamed-el-Amine Brahmia, Abdellatif Rahmoun, Mourad Zghal"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," LabRi-SBA Laboratory, CESI LINEACT UR 7527"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22616",children:"https://arxiv.org/pdf/2511.22616"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This survey paper provides a multi-level taxonomy of Federated Learning (FL) aggregation techniques, combining bibliometric analysis and systematic review to classify research in personalization, optimization, and robustness. It concludes by comparing aggregation methods under IID and non-IID data distributions and outlines future research directions to advance the field."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm inference], [LoRA, dynamic adapter placement, GPU Direct RDMA, multi-tenant serving, rank heterogeneity]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Shashwat Jaiswal, Shrikara Arun, Anjaly Parayil, Ankur Mallick, Spyros Mastorakis, Alind Khare, Chloi Alverti, Renee St Amant, Chetan Bansal, Victor R\xfchle, Josep Torrellas"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Illinois Urbana-Champaign, Microsoft, National Technical University of Athens"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22880",children:"https://arxiv.org/pdf/2511.22880"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper presents LoRAServe, a framework that dynamically places and routes heterogeneous LoRA adapters across GPUs to address performance skew in multi-tenant LLM inference. It uses workload-aware rebalancing and GPU Direct RDMA for remote access to improve resource utilization. The evaluation shows LoRAServe achieves higher throughput and lower latency while using fewer GPUs compared to state-of-the-art systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [split learning, pipeline parallelism, wireless edge networks, communication-computation overlap, alternating optimization, micro-batches]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Chenyu Liu, Zhaoyang Zhang, Zirui Chen, Zhaohui Yang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Zhejiang University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23167",children:"https://arxiv.org/pdf/2511.23167"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes C\xb2P\xb2SL, a method that applies pipeline parallelism to split learning in wireless edge networks to overlap communication and computation processes, thereby reducing training latency. It formulates a joint optimization problem for task split and resource allocation, solved via alternating optimization. Experiments show the method reduces system training time by over 38% while maintaining accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [Probabilistic Forward Pass, Bayesian Neural Networks, Gaussian propagation, TVM compiler, code generation, operator tuning, uncertainty estimation]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Bernhard Klein, Falk Selker, Hendrik Borras, Sophie Steger, Franz Pernkopf, Holger Fr\xf6ning"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Heidelberg University, Graz University of Technology"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23440",children:"https://arxiv.org/pdf/2511.23440"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces an end-to-end pipeline for deploying Bayesian Neural Networks (BNNs) using a Probabilistic Forward Pass (PFP), which approximates inference by propagating Gaussian distributions through the network in a single deterministic pass. The method is implemented via custom operators in the TVM compiler and optimized for ARM CPUs, achieving speedups of up to 4200x compared to traditional sampling-based methods while maintaining comparable accuracy and uncertainty estimation. The results demonstrate that combining Bayesian approximations with code generation enables efficient BNN deployment on resource-constrained embedded systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251201] A lasso-alternative to Dijkstra's algorithm for identifying short paths in networks"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [graph theory and optimization], [lasso, LARS algorithm, ADMM, bi-directional Dijkstra, \u21131-regularized regression]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Anqi Dong, Amirhossein Taghvaei, Tryphon T. Georgiou"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," KTH Royal Institute of Technology, University of Washington, University of California, Irvine"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22745",children:"https://arxiv.org/pdf/2511.22745"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper formulates the shortest path problem in graphs as an \u21131-regularized regression (lasso). It connects this formulation, solved via the LARS algorithm, to the bi-directional Dijkstra algorithm and highlights the applicability of ADMM for efficient updates to network topology changes."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 33'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Heterogeneous Multi-Agent Reinforcement Learning with Attention for Cooperative and Scalable Feature Transformation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21934",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22101",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Factors That Support Grounded Responses in LLM Conversations: A Rapid Review ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21762",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22176",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Representative Action Selection for Large Action Space: From Bandits to MDPs ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22104",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22105",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22210",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22226",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22018",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22235",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21726",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21928",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] An energy-efficient spiking neural network with continuous learning for self-adaptive brain-machine interface ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22108",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Hybrid Stackelberg Game and Diffusion-based Auction for Two-tier Agentic AI Task Offloading in Internet of Agents ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22076",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22138",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] GPS: General Per-Sample Prompter ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21714",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22406",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22570",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22715",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] ORION: Teaching Language Models to Reason Efficiently in the Language of Thought ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22891",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Switching-time bioprocess control with pulse-width-modulated optogenetics ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22893",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Language-conditioned world model improves policy generalization by reading environmental descriptions ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22904",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22963",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Evolutionary Discovery of Heuristic Policies for Traffic Signal Control ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23122",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Peer-to-Peer Energy Trading in Dairy Farms using Multi-Agent Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23148",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23158",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23193",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23262",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Emergent Coordination and Phase Structure in Independent Multi-Agent Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23315",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23442",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] ThetaEvolve: Test-time Learning on Open Problems ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23473",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] RELiQ: Scalable Entanglement Routing via Reinforcement Learning in Quantum Networks ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22321",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23310",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 19'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] MRI-Based Brain Age Estimation with Supervised Contrastive Learning of Continuous Representation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22102",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21759",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] ResearchArcade: Graph Interface for Academic Tasks ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22036",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21736",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Standardized Threat Taxonomy for AI Security, Governance, and Regulatory Compliance ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21901",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21848",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Co-Evolving Agents: Learning from Failures as Hard Negatives ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22254",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21920",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] GLA-Grad++: An Improved Griffin-Lim Guided Diffusion Model for Speech Synthesis ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22293",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Cacheback: Speculative Decoding With Nothing But Cache ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21699",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] FastFHE: Packing-Scalable and Depthwise-Separable CNN Inference Over FHE ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22434",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22460",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22586",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23203",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Simultaneous Image Quality Improvement and Artefacts Correction in Accelerated MRI ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.23274",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] QuantumChem-200K: A Large-Scale Open Organic Molecular Dataset for Quantum-Chemistry Property Screening and Language Model Benchmarking ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21747",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Automated Statistical and Machine Learning Platform for Biological Research ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.21770",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Generative models for crystalline materials ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22652",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251201] Escaping Barren Plateaus in Variational Quantum Algorithms Using Negative Learning Rate in Quantum Internet of Things ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.22861",children:"link"})]}),"\n"]})]})}function c(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>o});var r=n(6540);const s={},t=r.createContext(s);function a(e){const i=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(t.Provider,{value:i},e.children)}}}]);