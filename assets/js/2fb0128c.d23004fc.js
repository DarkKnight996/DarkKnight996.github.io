"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[639],{4644:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20250929-20251005","title":"20250929-20251005","description":"2025-09-29","source":"@site/docs/daily/20250929-20251005.md","sourceDirName":"daily","slug":"/daily/20250929-20251005","permalink":"/daily/20250929-20251005","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1761311518000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20250922-20250928","permalink":"/daily/20250922-20250928"},"next":{"title":"20251006-20251012","permalink":"/daily/20251006-20251012"}}');var s=i(4848),t=i(8453);const a={},o="20250929-20251005",l={},c=[{value:"2025-09-29",id:"2025-09-29",level:2},{value:"2025-09-30",id:"2025-09-30",level:2},{value:"2025-10-05",id:"2025-10-05",level:2}];function d(n){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20250929-20251005",children:"20250929-20251005"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-29",children:"2025-09-29"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Context-Driven Performance Modeling for Causal Inference Operators on\nNeural Processing Units"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [neural processing units, performance analysis, causal attention, sub-quadratic attention, edge computing, hardware-aware models]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Neelesh Gupta, Rakshith Jayanth, Dhruv Parikh, Viktor Prasanna"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Southern California"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.25155v1",children:"http://arxiv.org/pdf/2509.25155v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper conducts performance analysis of causal inference operators including standard quadratic attention and sub-quadratic alternatives on NPUs. The study reveals quadratic attention becomes memory-bound with cache inefficiency and pipeline stalls exceeding 95% at long contexts, while sub-quadratic models become compute-bound on vector cores. These findings provide insights for co-designing hardware-aware models and optimization strategies for on-device AI inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Intent-Driven Storage Systems: From Low-Level Tuning to High-Level\nUnderstanding"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [storage systems, workload intent, parameter reconfiguration, caching, prefetching]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shai Bergman, Won Wook Song, Lukas Cavigelli, Konstantin Berestizshevsky, Ke Zhou, Ji Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Huawei Zurich Research Center"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.15917v1",children:"http://arxiv.org/pdf/2510.15917v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes Intent-Driven Storage Systems (IDSS) that use LLMs to interpret workload intent from unstructured signals and generate adaptive storage configurations. The system improves IOPS by up to 2.45\xd7 on FileBench workloads through optimized caching and prefetching decisions. Results show LLMs can effectively bridge application semantics with low-level storage control when properly constrained within policy guardrails."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Asynchronous Policy Gradient Aggregation for Efficient Distributed\nReinforcement Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [distributed reinforcement learning, asynchronous policy gradient, heterogeneous computing, communication efficiency]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Alexander Tyurin, Andrei Spiridonov, Varvara Rudenko"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Opta"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.24305v1",children:"http://arxiv.org/pdf/2509.24305v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces two new algorithms, Rennala NIGT and Malenia NIGT, for asynchronous policy gradient aggregation in distributed reinforcement learning. The methods achieve state-of-the-art efficiency by handling both homogeneous and heterogeneous computing environments with improved computational and communication complexity. Experimental results demonstrate significant performance improvements over prior approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\nLong-Context LLM Serving"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [dynamic sparse attention, KV cache management, hierarchical storage, HBM-DRAM optimization, long-context LLM serving]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Qihui Zhou, Peiqi Yin, Pengfei Zuo, James Cheng"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Chinese University of Hong Kong, Huawei Cloud"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.24626v1",children:"http://arxiv.org/pdf/2509.24626v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," SparseServe introduces hierarchical HBM-DRAM management with fragmentation-aware KV cache transfer, working-set-aware batch control, and layer-segmented prefill to optimize dynamic sparse attention. It achieves up to 9.26x lower TTFT latency and 3.14x higher throughput compared to state-of-the-art systems. This enables efficient serving of long-context LLMs by addressing HBM capacity bottlenecks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Experience Deploying Containerized GenAI Services at an HPC Center"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [containerization, HPC-Kubernetes integration, GenAI deployment]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Angel M. Beltre, Jeff Ogden, Kevin Pedretti"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Sandia National Laboratories"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.20603v2",children:"http://arxiv.org/pdf/2509.20603v2"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents a converged computing architecture integrating HPC and Kubernetes platforms to deploy containerized GenAI workloads, using Llama LLM with vLLM inference server as a case study. It demonstrates successful deployment across multiple container runtimes and highlights practical considerations for reproducibility. The experience provides guidance for future HPC container tool development and research directions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [FPGA acceleration, graph construction, Vision GNNs, dynamic image processing, hardware optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Anvitha Ramachandran, Dhruv Parikh, Viktor Prasanna"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Southern California"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.25121v1",children:"http://arxiv.org/pdf/2509.25121v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a streaming FPGA accelerator for Dynamic Image Graph Construction (DIGC) in Vision GNNs, using on-chip buffers and parallel sorting to process features efficiently. The design minimizes memory traffic and achieves significant speedups over CPU and GPU baselines. It provides a scalable solution that maintains flexibility across different image resolutions and model variants."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] RServe: Overlapping Encoding and Prefill for Efficient LMM Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [multimodal models, scheduling, parallel processing, latency optimization, throughput improvement]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tianyu Guo, Tianming Xu, Xianjie Chen, Junru Chen, Nong Xiao, Xianwei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Sun Yat-sen University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.24381v1",children:"http://arxiv.org/pdf/2509.24381v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," REDServe proposes an efficient LMM inference system that overlaps multimodal encoding with language model computation using fine-grained scheduling. It employs chunked prefill and token budgeting to balance computational loads across requests. Experimental results show up to 66% latency reduction and 109% throughput improvement compared to existing approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts\nRouting"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [mixture-of-experts, load balancing, routing algorithm, inference optimization, plug-and-play]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Rana Shahout, Colin Cai, Yilun Du, Minlan Yu, Michael Mitzenmacher"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Harvard University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.03293v1",children:"http://arxiv.org/pdf/2510.03293v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," LASER is a plug-and-play routing algorithm that balances expert load in Mixture-of-Experts models by adapting to gate score distributions. It routes tokens to strongest experts when scores show clear preference, and to least-loaded experts when scores are uniform. The method improves inference latency and throughput while maintaining accuracy, requiring no model retraining or fine-tuning."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] HAPT: Heterogeneity-Aware Automated Parallel Training on Heterogeneous\nClusters"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [distributed training, heterogeneous clusters, parallel training, load balancing, communication optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Antian Liang, Zhigang Zhao, Kai Zhang, Xuri Shi, Chuantao Li, Chunxiao Wang, Zhenying He, Yinan Jing, X. Sean Wang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Fudan University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.24859v1",children:"http://arxiv.org/pdf/2509.24859v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Hapt introduces a fine-grained planner for inter-operator parallel strategy search and a heterogeneity-aware 1F1B scheduler to optimize distributed training on heterogeneous clusters. It achieves better load balancing and computation-communication overlap while minimizing memory overhead. Evaluation shows Hapt delivers 1.3x-1.6x higher performance compared to state-of-the-art frameworks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Zeppelin: Balancing Variable-length Workloads in Data Parallel Large\nModel Training"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [load balancing, variable-length sequences, communication optimization, hierarchical partitioning, attention mechanisms, data-parallel training]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chang Chen, Tiancheng Chen, Jiangfei Duan, Qianchao Zhu, Zerui Wang, Qinghao Hu, Peng Sun, Xiuhong Li, Chao Yang, Torsten Hoefler"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Peking University, ETH Zurich, The Chinese University of Hong Kong, Shanghai AI Laboratory, Nanyang Technological University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.21841v2",children:"http://arxiv.org/pdf/2509.21841v2"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Zeppelin introduces a hierarchical sequence partitioning method for attention modules, a routing layer for inter-node transfers, and a remapping layer for layout transformations. These techniques address load imbalance in large-scale data-parallel training with variable sequence lengths. The system achieves an average 2.80x speedup over state-of-the-art methods in comprehensive evaluations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in\nCloud Computing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [IoT data scheduling, reinforcement learning, ant colony optimization, cloud computing, resource optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Noor Islam S. Mohammad"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," IEEE (based on publication venue)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2508.04334v2",children:"http://arxiv.org/pdf/2508.04334v2"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a hybrid scheduling algorithm combining deep reinforcement learning and ant colony optimization for IoT data processing in cloud environments. The method achieves significant improvements in response time (18.4% reduction), resource utilization (12.7% improvement), and energy consumption (9.3% decrease) compared to existing approaches. The integration of model-free RL with swarm intelligence proves effective for scalable and energy-efficient IoT-cloud data scheduling."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] A Scalable Distributed Framework for Multimodal GigaVoxel Image\nRegistration"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [kernels], [image registration, distributed computing, GPU optimization, medical imaging, tensor sharding, memory efficiency]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Rohit Jena, Vedant Zope, Pratik Chaudhari, James C. Gee"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Pennsylvania"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.25044v1",children:"http://arxiv.org/pdf/2509.25044v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes FFDP, a distributed framework with optimized non-GEMM kernels for large-scale multimodal image registration. It enables convolution-aware tensor sharding and significantly accelerates existing pipelines while reducing memory usage. The method demonstrates unprecedented scalability by registering gigavoxel medical images efficiently on limited GPU resources."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-30",children:"2025-09-30"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Artificial Intelligence for Cost-Aware Resource Prediction in Big Data\nPipelines"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [trace analysis], [resource prediction, random forest, cloud computing, cost-aware autoscaling, big data pipelines]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Harshit Goyal"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," BITS Pilani"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.05127v1",children:"http://arxiv.org/pdf/2510.05127v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper uses Random Forest regression on Google Borg cluster traces to predict resource utilization in big data pipelines. The model achieves high accuracy (R\xb2\u22480.99) in capturing non-linear workload-resource relationships. Results demonstrate AI-driven prediction enables cost-aware autoscaling, reducing unnecessary provisioning while maintaining service quality."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Accelerating LLM Inference with Precomputed Query Storage"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [query caching, storage optimization, latency reduction, edge computing, vector database]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jay H. Park, Youngju Cho, Choungsol Lee, Moonwook Oh, Euiseong Seo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Samsung Electronics, Sungkyunkwan University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.25919v1",children:"http://arxiv.org/pdf/2509.25919v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," StorInfer accelerates LLM inference by precomputing and storing query-response pairs offline, using adaptive generation techniques to ensure diversity. When incoming queries match precomputed ones, it bypasses GPU computation for instant responses. This approach reduces latency by up to 17.3% without quality loss, demonstrating efficient storage-assisted inference for predictable query scenarios."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] SysMoBench: Evaluating AI on Formally Modeling Complex Real-World\nSystems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [formal modeling, system specification, TLA+, concurrent systems, distributed systems, benchmark evaluation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Qian Cheng, Ruize Tang, Emilie Ma, Finn Hackett, Peiyang He, Yiming Su, Ivan Beschastnikh, Yu Huang, Xiaoxing Ma, Tianyin Xu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nanjing University, University of British Columbia, University of Illinois Urbana-Champaign"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.23130v2",children:"http://arxiv.org/pdf/2509.23130v2"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces SysMoBench, a benchmark for evaluating AI's ability to generate formal models of complex concurrent and distributed systems using TLA+. It automates evaluation metrics including syntactic correctness and conformance to system code. The benchmark helps understand current LLM capabilities and limitations in formal system modeling, opening new research directions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Parallax: Efficient LLM Inference Service over Decentralized Environment"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [decentralized inference, GPU heterogeneity, two-phase scheduler, model allocation, pipeline selection]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chris Tong, Youhe Jiang, Gufeng Chen, Tianyi Zhao, Sibian Lu, Wenjie Qu, Eric Yang, Lynn Ai, Binhang Yuan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Gradient, HKUST, National University of Singapore"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.26182v1",children:"http://arxiv.org/pdf/2509.26182v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Parallax introduces a decentralized LLM serving system with a two-phase scheduler that optimizes model allocation across heterogeneous GPUs and performs request-time pipeline selection. It effectively handles GPU diversity and network constraints by distributing model layers and creating dynamic execution chains. The system demonstrates improved latency and throughput compared to baseline approaches, making volunteer computing viable for LLM inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Efficient Construction of Large Search Spaces for Auto-Tuning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [kernels], [auto-tuning, constraint satisfaction problem, search space construction, performance optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Floris-Jan Willemsen, Rob V. van Nieuwpoort, Ben van Werkhoven"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Leiden University, Netherlands eScience Center"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.26253v1",children:"http://arxiv.org/pdf/2509.26253v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper reformulates search space construction for auto-tuning as a Constraint Satisfaction Problem (CSP) and develops an optimized CSP solver with runtime parsing capabilities. The approach achieves 4 orders of magnitude speedup over brute-force methods and 1-2 orders of magnitude improvement over chain-of-trees frameworks. This breakthrough enables exploration of previously unattainable problem scales in auto-tuning domains."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] AGOCS -- Accurate Google Cloud Simulator Framework"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [trace analysis], [cloud workload simulation, google cluster traces, resource usage statistics, parallel execution, scala implementation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Leszek Sliwko, Vladimir Getov"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Westminster"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.26120v1",children:"http://arxiv.org/pdf/2509.26120v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," AGOCS is a high-fidelity cloud workload simulator that parses real Google cluster traces from 12,500 nodes over one month. The framework provides detailed parameters of jobs, tasks, and nodes with actual resource usage statistics. Implemented in Scala with parallel execution capabilities, it enables accurate cloud workload simulation on desktop machines for research purposes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] LoRAFusion: Efficient LoRA Fine-Tuning for LLMs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [finetuning], [LoRA, parameter-efficient fine-tuning, kernel optimization, scheduling algorithm, multi-job fine-tuning]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zhanda Zhu, Qidong Su, Yaoyao Ding, Kevin Song, Shang Wang, Gennady Pekhimenko"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Toronto, Vector Institute, NVIDIA"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.00206v1",children:"http://arxiv.org/pdf/2510.00206v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," LoRAFusion introduces kernel-level graph-splitting to fuse memory-bound operations and a scheduling-level adaptive batching algorithm for multi-job fine-tuning. This system eliminates redundant memory accesses and enables concurrent fine-tuning of multiple LoRA adapters. The approach achieves up to 1.96\xd7 speedup over existing systems while maintaining model quality."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] I Like To Move It -- Computation Instead of Data in the Brain"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [brain simulation, structural plasticity, spike exchange, computational neuroscience, Barnes-Hut algorithm, communication optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Fabian Czappa, Marvin Kaster, Felix Wolf"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Technical University of Darmstadt (based on authors' affiliations)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.26193v1",children:"http://arxiv.org/pdf/2509.26193v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents a novel algorithm that reduces communication overhead in brain simulations by moving computation instead of data. This approach decreases connectivity update time by 6x and spike exchange time by over 100x. The method enables more scalable simulation of neural networks with structural plasticity for studying brain functions like memory and learning."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Tuning the Tuner: Introducing Hyperparameter Optimization for\nAuto-Tuning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [hyperparameter optimization, auto-tuning, performance optimization, search space, statistical evaluation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Floris-Jan Willemsen, Rob V. van Nieuwpoort, Ben van Werkhoven"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Leiden University, Netherlands eScience Center"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.26300v1",children:"http://arxiv.org/pdf/2509.26300v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a novel method for hyperparameter tuning of optimization algorithms in auto-tuning systems. The authors propose a robust statistical evaluation approach, provide a FAIR dataset and simulation mode that reduces tuning costs by two orders of magnitude. Results show hyperparameter tuning improves auto-tuner performance by 94.8% on average, demonstrating its significant potential for advancing auto-tuning research and practice."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Efficient Distributed Training via Dual Batch Sizes and Cyclic\nProgressive Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [distributed training, batch size optimization, progressive learning, parameter server, ResNet-18]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kuan-Wei Lu, Ding-Yong Hong, Pangfeng Liu, Jan-Jan Wu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Academia Sinica"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.26092v1",children:"http://arxiv.org/pdf/2509.26092v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a dual batch size learning scheme that uses both large and small batch sizes simultaneously to improve generalization while maintaining efficiency, combined with cyclic progressive learning that adjusts image resolution during training. The hybrid approach achieves better accuracy and faster training times compared to conventional methods. Experimental results show 3.3% accuracy improvement with 10.6% faster training on CIFAR-100 and 0.1% accuracy improvement with 35.7% faster training on ImageNet using ResNet-18."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [datacenter lifecycle, TCO optimization, GPU infrastructure, hardware refresh, operational software]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jovan Stojkovic, Chaojie Zhang, \xcd\xf1igo Goiri, Ricardo Bianchini"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Microsoft Azure Research, University of Illinois Urbana-Champaign"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.26534v1",children:"http://arxiv.org/pdf/2509.26534v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a holistic framework for managing AI datacenter lifecycle across building, hardware refresh, and operation stages. The framework co-optimizes decisions across these stages considering workload dynamics and hardware evolution. It achieves up to 40% TCO reduction compared to traditional approaches."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-10-05",children:"2025-10-05"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Speculative Actions: A Lossless Framework for Faster Agentic Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [speculative execution, agent systems, parallel processing, latency optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Naimeng Ye, Arnav Ahuja, Georgios Liargkovas, Yunan Lu, Kostis Kaffes, Tianyi Peng"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Columbia University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.04371v1",children:"http://arxiv.org/pdf/2510.04371v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes speculative actions, a lossless framework inspired by speculative execution in microprocessors and speculative decoding in LLM inference. The method predicts likely agent actions using faster models to enable parallel execution of multiple steps. Evaluation across gaming, e-commerce, and web search environments shows up to 55% next-action prediction accuracy and significant latency reductions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] From Patchwork to Network: A Comprehensive Framework for Demand Analysis\nand Fleet Optimization of Urban Air Mobility"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [urban air mobility, fleet optimization, parallel simulation, demand forecasting, ground transportation integration]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xuan Jiang, Xuanyu Zhou, Yibo Zhao, Shangqing Cao, Jinhua Zhao, Mark Hansen, Raja Sengupta"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of California, Berkeley"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.04186v1",children:"http://arxiv.org/pdf/2510.04186v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces LPSim, a large-scale parallel simulation framework using multi-GPU computing to co-optimize Urban Air Mobility demand, fleet operations, and ground transportation interactions. The extended equilibrium search algorithm forecasts demand and determines optimal fleet composition. Results from the San Francisco Bay Area case study show over 20 minutes' travel time savings for 230,000 trips, but highlight critical dependence on ground access integration and dynamic scheduling."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>o});var r=i(6540);const s={},t=r.createContext(s);function a(n){const e=r.useContext(t);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),r.createElement(t.Provider,{value:e},n.children)}}}]);