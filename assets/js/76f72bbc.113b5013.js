"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[876],{5546:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20250901-20250907","title":"20250901-20250907","description":"2025-09-01","source":"@site/docs/daily/20250901-20250907.md","sourceDirName":"daily","slug":"/daily/20250901-20250907","permalink":"/daily/20250901-20250907","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1761624683000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Daily","permalink":"/category/daily"},"next":{"title":"20250908-20250914","permalink":"/daily/20250908-20250914"}}');var s=i(4848),t=i(8453);const a={},o="20250901-20250907",l={},c=[{value:"2025-09-01",id:"2025-09-01",level:2},{value:"2025-09-02",id:"2025-09-02",level:2},{value:"2025-09-03",id:"2025-09-03",level:2},{value:"2025-09-04",id:"2025-09-04",level:2},{value:"2025-09-05",id:"2025-09-05",level:2},{value:"2025-09-06",id:"2025-09-06",level:2},{value:"2025-09-07",id:"2025-09-07",level:2}];function d(n){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20250901-20250907",children:"20250901-20250907"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-01",children:"2025-09-01"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] LiquidGEMM: Hardware-Efficient W4A8 GEMM Kernel for High-Performance LLM\nServing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [kernels], [quantization, GEMM kernel, LLM inference, W4A8, hardware efficiency]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Huanqi Hu, Bowen Xiao, Shixuan Sun, Jianian Yin, Zhexi Zhang, Xiang Luo, Chengquan Jiang, Weiqi Xu, Xiaoying Jia, Xin Liu, Minyi Guo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Shanghai Jiao Tong University, ByteDance"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.01229v1",children:"http://arxiv.org/pdf/2509.01229v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," LiquidGEMM introduces a hardware-efficient W4A8 GEMM kernel with LiquidQuant for fast dequantization and an implicit pipeline to overlap operations. It achieves up to 2.90x speedup over existing W4A8 kernels and 4.94x system-level speedup, outperforming NVIDIA TensorRT-LLM kernels."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Optimal Parallel Scheduling under Concave Speedup Functions"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [parallel scheduling, concave speedup functions, resource allocation, cloud computing, edge computing, AI workloads]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chengzhang Li, Peizhong Ju, Atilla Eryilmaz, Ness Shroff"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The Ohio State University, University of Kentucky"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.01811v1",children:"http://arxiv.org/pdf/2509.01811v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes the SmartFill algorithm for optimal parallel job scheduling under general concave speedup functions. It introduces the Consistent Derivative Ratio (CDR) Rule and General Water-Filling method to determine resource allocations. The approach outperforms prior methods like heSRPT across various concave speedup functions commonly found in AI workloads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] HiCR, an Abstract Model for Distributed Heterogeneous Programming"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [distributed heterogeneous systems, runtime systems, plugin-based approach, hardware abstraction, cross-platform execution]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Sergio Miguel Martin, Luca Terracciano, Kiril Dichev, Noah Baumann, Jiashu Lin, Albert-Jan Yzelman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Huawei Zurich Research Center, HiSilicon Technologies"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.01425v1",children:"http://arxiv.org/pdf/2509.01425v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," HiCR presents an abstract model for distributed heterogeneous programming that defines minimal operations for hardware discovery, kernel execution, and memory management through a plugin-based approach. The model enables applications to run across diverse platforms without refactoring by separating semantics from implementation details. This allows HiCR-based code to seamlessly execute on current and future systems while supporting various parallel programming paradigms."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] LobRA: Multi-tenant Fine-tuning over Heterogeneous Data"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [finetuning], [multi-tenant fine-tuning, LoRA, sequence length heterogeneity, workload balancing, GPU efficiency]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Sheng Lin, Fangcheng Fu, Haoyang Li, Hao Ge, Xuanyu Wang, Jiawen Niu, Yaofeng Tu, Bin Cui"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Peking University, Shanghai Jiao Tong University, ZTE Corporation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.01193v1",children:"http://arxiv.org/pdf/2509.01193v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," LobRA introduces a framework for efficient multi-tenant fine-tuning using heterogeneous LoRA adapters to handle data sequence length variation and skewness. It deploys model replicas with tailored resource usage and balances workloads across them during training. Experiments show LobRA reduces GPU time for joint fine-tuning by 45.03%-60.67%."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-02",children:"2025-09-02"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] KubeIntellect: A Modular LLM-Orchestrated Agent Framework for End-to-End\nKubernetes Management"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [Kubernetes management, natural language interface, modular agents, code generation, workflow orchestration]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mohsen Seyedkazemi Ardebili, Andrea Bartolini"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Bologna"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.02449v1",children:"http://arxiv.org/pdf/2509.02449v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," KubeIntellect introduces an LLM-powered framework using modular agents and a supervisor to interpret natural language queries for comprehensive Kubernetes operations. The system features memory checkpoints, human-in-the-loop clarification, and dynamic task sequencing with a secure Code Generator Agent. Evaluation shows 93% tool synthesis success rate and 100% reliability across 200 queries, demonstrating effective infrastructure management through natural language interaction."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] An Efficient and Adaptive Watermark Detection System with Tile-based\nError Correction"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [kernels], [watermark detection, error correction, GPU optimization, tiling techniques]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xinrui Zhong, Xinze Feng, Jingwei Zuo, Fanjiang Ye, Yi Mu, Junfeng Guo, Heng Huang, Myungjin Lee, Yuke Wang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Rice University, University of Illinois Urbana-Champaign, University of Maryland, Cisco Research"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.02447v1",children:"http://arxiv.org/pdf/2509.02447v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," QRMark proposes an efficient watermark detection system combining QR Code-inspired error correction with tiling techniques and GPU optimization strategies. It uses Reed-Solomon error correction to maintain accuracy while employing resource-aware stream allocation and tile-based workload interleaving. The system achieves 2.43\xd7 speedup over baseline while preserving detection performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\nBreak the GPU Memory Wall"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [memory offloading, multi-tier storage, I/O optimization, GPU memory, large language models]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Avinash Maurya, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Argonne National Laboratory, Rochester Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.02480v1",children:"http://arxiv.org/pdf/2509.02480v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," MLP-Offload proposes a multi-level, multi-path offloading engine that optimizes LLM training by offloading optimizer states across multiple storage tiers with cache efficiency and concurrency control. It addresses I/O bottlenecks during backward and update phases by utilizing unused remote storage bandwidth. The system achieves 2.5\xd7 faster training iterations compared to state-of-the-art methods for models up to 280B parameters."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Efficient Pyramidal Analysis of Gigapixel Images on a Decentralized\nModest Computer Cluster"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [gigapixel image analysis, pyramidal processing, load balancing, decentralized computing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Marie Reinbigler, Rishi Sharma, Rafael Pires, Elisabeth Brunet, Anne-Marie Kermarrec, Catalin Fetita"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," SAMOVAR, Inria Saclay, T\xe9l\xe9com SudParis, IP Paris, EPFL"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.02440v1",children:"http://arxiv.org/pdf/2509.02440v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces PyramidAI, a pyramidal approach that analyzes gigapixel images by starting at low resolutions and progressively focusing on regions of interest at higher resolutions. This method reduces processed data by up to 2.65x while maintaining accuracy. When deployed on a decentralized cluster of 12 modest computers, analysis time drops from over an hour to just minutes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] HydroGAT: Distributed Heterogeneous Graph Attention Transformer for\nSpatiotemporal Flood Prediction"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [spatiotemporal flood prediction, graph neural networks, distributed training, heterogeneous graph attention, hydrological modeling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Aishwarya Sarkar, Autrin Hakimi, Xiaoqiong Chen, Hai Huang, Chaoqun Lu, Ibrahim Demir, Ali Jannesari"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Iowa State University, Tulane University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.02481v1",children:"http://arxiv.org/pdf/2509.02481v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," HydroGAT introduces a heterogeneous graph attention transformer that models every land and river pixel as nodes connected by hydrological flow directions, enabling simultaneous learning of spatiotemporal dependencies for flood prediction. The model achieves superior performance with NSE up to 0.97 and KGE up to 0.96 in hourly discharge forecasting while providing interpretable attention maps. A distributed training pipeline scales efficiently to 64 GPUs, demonstrating 15\xd7 speedup on the NERSC Perlmutter supercomputer."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Batch Query Processing and Optimization for Agentic Workflows"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [batch query processing, agentic workflows, KV-cache sharing, GPU optimization, DAG optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Junyi Shen, Noppanat Wadlom, Yao Lu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," National University of Singapore"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.02121v1",children:"http://arxiv.org/pdf/2509.02121v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Halo introduces batch query processing and optimization for agentic LLM workflows by representing workflows as structured query plan DAGs and performing plan-level optimization to minimize redundant execution. The system achieves up to 18.6x speedup for batch inference and 4.7x throughput improvement under online serving while maintaining output quality. These gains are accomplished through adaptive batching, KV-cache sharing, and compute-communication overlap techniques."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] A Continuous Energy Ising Machine Leveraging Difference-of-Convex\nProgramming"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [Ising machine, combinatorial optimization, difference-of-convex programming, GPU acceleration, continuous relaxation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Debraj Banerjee, Santanu Mahapatra, Kunal Narayan Chaudhury"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Indian Institute of Science"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.01928v1",children:"http://arxiv.org/pdf/2509.01928v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a continuous energy Ising machine that relaxes binary spins to continuous variables and uses a difference-of-convex programming approach with convergence guarantees. It implements efficient iterative algorithms requiring only single matrix-vector multiplication per iteration across GPU platforms. The method consistently outperforms existing solvers across problem sizes from 10\xb3 to 10\u2078 spins."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-03",children:"2025-09-03"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] CloudFormer: An Attention-based Performance Prediction for Public Clouds\nwith Unknown Workload"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [trace analysis], [performance prediction, cloud computing, transformer, virtual machines, interference mitigation, system metrics]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Amirhossein Shahbazinia, Darong Huang, Luis Costero, David Atienza"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," EPFL (\xc9cole Polytechnique F\xe9d\xe9rale de Lausanne)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.03394v1",children:"http://arxiv.org/pdf/2509.03394v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," CloudFormer proposes a dual-branch Transformer model that jointly models temporal dynamics and system-level interactions to predict VM performance degradation in public clouds. The method achieves state-of-the-art performance with 7.8% MAE and demonstrates robust generalization across diverse workloads without scenario-specific tuning. It addresses performance interference challenges in multi-tenant cloud environments using high-resolution system metrics."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] DPQuant: Efficient and Differentially-Private Model Training via Dynamic\nQuantization Scheduling"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [differential privacy, quantization, training efficiency, dynamic scheduling, privacy-preserving machine learning]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yubo Gao, Renbo Tu, Gennady Pekhimenko, Nandita Vijaykumar"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Toronto"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.03472v1",children:"http://arxiv.org/pdf/2509.03472v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," DPQuant introduces a dynamic quantization framework that adaptively selects layers for quantization each epoch using probabilistic sampling and loss-aware prioritization. The method reduces quantization variance amplified by DP-SGD noise injection while preserving privacy guarantees. Experimental results show DPQuant achieves near Pareto-optimal accuracy-compute trade-offs with up to 2.21\xd7 throughput improvements and less than 2% accuracy drop compared to static quantization baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Combining Performance and Productivity: Accelerating the Network Sensing\nGraph Challenge with GPUs and Commodity Data Science Software"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [kernels], [GPU acceleration, graph processing, data science tools, GraphBLAS, RAPIDS ecosystem, performance benchmarking]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Siddharth Samsi, Dan Campbell, Emanuel Scoullos, Oded Green"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," NVIDIA"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.03653v1",children:"http://arxiv.org/pdf/2509.03653v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents an alternative implementation of the Network Sensing Graph Challenge using NVIDIA's RAPIDS ecosystem (cuDF and cupy) instead of traditional HPC code. The authors reformulate GraphBLAS computations using data science terminology and demonstrate significant speedups of 147x-2185x on various NVIDIA GPUs compared to CPU-based Pandas implementations. The work shows that commodity data science software can effectively accelerate complex graph processing workloads without specialized HPC programming."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale\nTraining of LLMs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [failure recovery, checkpoint-free, scale-independent, fault-tolerance]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Haijun Zhang, Jinxiang Wang, Zhenhua Yu, Yanyong Zhang, Xuejie Ji, Kaining Mao, Jun Zhang, Yaqing Zhang, Ting Wu, Fei Jie, Xiemin Huang, Zhifang Cai, Junhua Cheng, Shuwei Wang, Wei Li, Xiaoming Bao, Hua Xu, Shixiong Zhao, Jun Li, Hongwei Sun, Ziyang Zhang, Yi Xiong, Chunsheng Li"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," iFLYTEK AI Engineering Institute, University of Science and Technology of China, Huawei Technologies Co., Ltd"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.03047v1",children:"http://arxiv.org/pdf/2509.03047v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," FlashRecovery introduces a fast failure recovery system with three core innovations: real-time failure detection, scale-independent task restart, and checkpoint-free single-step recovery. The system achieves optimal RTO and RPO by eliminating traditional checkpointing overhead. Experimental results show it can restore training on 4,800 devices within 150 seconds with consistent recovery time across different cluster scales."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Mycroft: Tracing Dependencies in Collective Communication Towards\nReliable LLM Training"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [collective communication, distributed tracing, root cause analysis, reliability, fault detection]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yangtao Deng, Lei Zhang, Qinlong Wang, Xiaoyun Zhi, Xinlei Zhang, Zhuo Jiang, Haohan Xu, Lei Wang, Zuquan Song, Gaohong Liu, Yang Bai, Shuguang Wang, Wencong Xiao, Jianxi Ye, Minlan Yu, Hong Xu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," ByteDance, The Chinese University of Hong Kong, Harvard University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.03018v1",children:"http://arxiv.org/pdf/2509.03018v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Mycroft is a lightweight distributed tracing system that tracks collective communication states and leverages internal dependencies to resolve reliability issues in LLM training. It detects anomalies within 15 seconds in 90% of cases and identifies root causes within 20 seconds in 60% of cases. The system has been successfully deployed at ByteDance for over six months, demonstrating effective debugging capabilities through extensive fault injection experiments."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-04",children:"2025-09-04"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Counterfactual simulations for large scale systems with burnout\nvariables"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [counterfactual simulation, burnout variables, parallel computing, online advertising, MapReduce, auction systems]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Benjamin Heymann"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," ENSAE, Criteo AI LAB"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.04038v1",children:"http://arxiv.org/pdf/2509.04038v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces uncertainty relaxation algorithms for efficient parallel computation of counterfactual simulations in large-scale systems with burnout variables. The method enables scalable estimation of alternative scenarios in online advertising platforms by overcoming sequential processing limitations. The approach significantly improves computational efficiency while maintaining accuracy in counterfactual analysis of auction systems with budget constraints."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Massively-Parallel Implementation of Inextensible Elastic Rods Using\nInter-block GPU Synchronization"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [kernels], [GPU computing, elastic rods simulation, medical simulation, parallel algorithms, CUDA, real-time systems]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Przemyslaw Korzeniowski, Niels Hald, Fernando Bello"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Imperial College London"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.04277v1",children:"http://arxiv.org/pdf/2509.04277v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a massively-parallel GPU implementation of inextensible elastic rods using inter-block synchronization, achieving significant speedups over CPU versions. The method enables multiple physics time-steps per kernel launch and maintains nearly constant computation time under certain constraints. This allows for accurate real-time simulation at haptic interactive rates for medical applications like catheter and guidewire simulation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for\nHigh-Performance Distributed Training Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [checkpointing], [distributed training, gradient compression, failure recovery, differential checkpointing, performance optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chenxuan Yao, Yuchong Hu, Feifan Liu, Zhengyu Liu, Dan Feng"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Huazhong University of Science and Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.04084v1",children:"http://arxiv.org/pdf/2509.04084v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," LowDiff proposes an efficient frequent checkpointing framework that reuses compressed gradients as differential checkpoints and employs batched gradient writes with dynamic tuning. The system achieves per-iteration checkpointing frequency with minimal runtime overhead by incorporating layer-wise gradient reusing and asynchronous persistence strategies. Experimental results demonstrate less than 3.1% performance overhead while enabling high-frequency checkpointing for distributed training systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Prob-GParareal: A Probabilistic Numerical Parallel-in-Time Solver for\nDifferential Equations"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [probabilistic numerical solver, parallel-in-time methods, uncertainty quantification, Gaussian processes, differential equations]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Guglielmo Gattiglio, Lyudmila Grigoryeva, Massimiliano Tamborrino"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Warwick, University of St. Gallen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.03945v1",children:"http://arxiv.org/pdf/2509.03945v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Prob-GParareal extends the GParareal algorithm using Gaussian processes to model correction functions, enabling uncertainty quantification in parallel-in-time solutions of differential equations. The method provides probabilistic forecasts while maintaining compatibility with classical solvers and handles probabilistic initial conditions. Theoretical analysis and numerical experiments demonstrate its accuracy and robustness across various benchmark problems including chaotic, stiff, and bifurcation systems."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-05",children:"2025-09-05"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] veScale: Consistent and Efficient Tensor Programming with Eager-Mode\nSPMD"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [SPMD, distributed training, eager execution, tensor programming, random number generation, performance optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Youjie Li, Cheng Wan, Zhiqi Lin, Hongyu Zhu, Jiacheng Yang, Ziang Song, Xinyi Di, Jiawei Wu, Huiyao Shu, Wenlei Bao, Yanghua Peng, Haibin Lin, Li-Wen Chang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," ByteDance"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.07003v1",children:"http://arxiv.org/pdf/2509.07003v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," veScale introduces an eager-mode training system that fully embraces the SPMD paradigm to simplify distributed tensor programming. It addresses consistency issues through a novel distributed RNG algorithm and improves performance by reducing PyTorch overhead and enhancing communication efficiency. Evaluations show veScale achieves up to 2.2x speedup over state-of-the-art systems while reducing code complexity by 78.4% and maintaining single-device-equivalent results."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface\nVisualization"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [distributed computing, 3D Gaussian splatting, scientific visualization, multi-GPU training, HPC systems]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mengjiao Han, Andres Sewell, Joseph Insley, Janet Knowles, Victor A. Mateevitsi, Michael E. Papka, Steve Petruzza, Silvio Rizzi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Argonne National Laboratory, Utah State University, University of Illinois Chicago"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.05216v1",children:"http://arxiv.org/pdf/2509.05216v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a multi-GPU extension of the 3D Gaussian Splatting pipeline for scientific visualization, adapting a distributed training backend from Grendel-GS. The distributed optimization approach achieves 5.6\xd7 speedup on benchmark datasets and enables training on large datasets that exceed single-GPU capacity. This work provides a foundation for integrating 3D-GS into HPC-based scientific workflows for real-time visualization of complex simulations."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-06",children:"2025-09-06"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Distributed Deep Learning using Stochastic Gradient Staleness"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [distributed deep learning, stochastic gradient staleness, data parallelism, parallel backpropagation, convergence analysis]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Viet Hoang Pham, Hyo-Sung Ahn"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Posts and Telecommunications Institute of Technology (PTIT), Gwangju Institute of Science and Technology (GIST)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.05679v1",children:"http://arxiv.org/pdf/2509.05679v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a distributed training method combining data parallelism and fully decoupled parallel backpropagation to accelerate deep learning. The approach uses multiple computational units to process more training data per iteration while avoiding locking issues in backpropagation. Theoretical convergence guarantees are provided and empirical evaluations on CIFAR-10 demonstrate improved training efficiency."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU\nLibraries"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [kernels], [GPU libraries, kernel fusion, C++ metaprogramming, SRAM optimization, CUDA]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Oscar Amoros, Albert Andaluz, Johnny Nunez, Antonio J. Pena"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universitat Politecnica de Catalunya, NVIDIA Computing SL, Barcelona Supercomputing Center"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2508.07071v2",children:"http://arxiv.org/pdf/2508.07071v2"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a C++ API methodology using metaprogramming to automatically generate fused GPU kernels at compile time, enabling horizontal and vertical fusion for arbitrary operation sequences. The approach eliminates manual kernel development while maximizing GPU resource utilization and keeping intermediate data in on-chip memory. Experimental results show performance improvements from 2x to over 1000x compared to traditional GPU libraries while maintaining high-level programmability."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Real-Time Analysis of Unstructured Data with Machine Learning on\nHeterogeneous Architectures"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [particle physics, graph neural networks, GPU acceleration, FPGA implementation, real-time processing, LHCb experiment, heterogeneous architectures]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Fotis I. Giasemis"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," CERN"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2508.07423v3",children:"http://arxiv.org/pdf/2508.07423v3"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper develops a graph neural network pipeline for real-time charged particle track reconstruction at the LHCb experiment. The system was implemented end-to-end on GPUs within the first-level trigger and also accelerated on FPGAs. Results show improved performance compared to classical tracking algorithms while evaluating power consumption and processing speed across different hardware architectures."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-07",children:"2025-09-07"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across\nThousands of Computers"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [distributed vector search, approximate nearest neighbor, graph index, key-value store, query latency, scalability]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Philip Adams, Menghao Li, Shi Zhang, Li Tan, Qi Chen, Mingqin Li, Zengzhong Li, Knut Risvik, Harsha Vardhan Simhadri"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Microsoft, Microsoft Research Asia, Shopify"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.06046v1",children:"http://arxiv.org/pdf/2509.06046v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," DISTRIBUTEDANN presents a distributed vector search system that scales a single DISKANN graph index across thousands of machines using a distributed key-value store and in-memory ANN index. It achieves 26ms median query latency and processes over 100,000 queries per second on a 50-billion vector dataset, showing 6x higher efficiency than traditional partitioning approaches. The system has been successfully deployed in Microsoft Bing, replacing conventional scale-out architectures."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Online Identification of IT Systems through Active Causal Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [causal learning, Gaussian process regression, system identification, active learning, intervention policy]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kim Hammar, Rolf Stadler"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," KTH Royal Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.02130v2",children:"http://arxiv.org/pdf/2509.02130v2"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents an active causal learning method that uses Gaussian process regression and rollout-based interventions to identify causal models of IT systems online. The approach is proven to be Bayesian-optimal and produces effective interventions. Experimental results show accurate system identification with minimal operational interference."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>o});var r=i(6540);const s={},t=r.createContext(s);function a(n){const e=r.useContext(t);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),r.createElement(t.Provider,{value:e},n.children)}}}]);