"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[513],{4793:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251027-20251102","title":"20251027-20251102","description":"2025-10-27","source":"@site/docs/daily/20251027-20251102.md","sourceDirName":"daily","slug":"/daily/20251027-20251102","permalink":"/daily/20251027-20251102","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1761709291000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251020-20251026","permalink":"/daily/20251020-20251026"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=i(4848),t=i(8453);const a={},o="20251027-20251102",l={},c=[{value:"2025-10-27",id:"2025-10-27",level:2},{value:"2025-10-28",id:"2025-10-28",level:2},{value:"2025-10-29",id:"2025-10-29",level:2}];function d(n){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20251027-20251102",children:"20251027-20251102"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-10-27",children:"2025-10-27"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Learning to Schedule: A Supervised Learning Framework for Network-Aware\nScheduling of Data-Intensive Workloads"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [network-aware scheduling, supervised learning, data-intensive workloads, Kubernetes, job completion time prediction]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Sankalpa Timilsina, Susmit Shannigrahi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tennessee Technological University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.21419v1",children:"http://arxiv.org/pdf/2510.21419v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a network-aware job scheduler using supervised learning to predict job completion times based on real-time cluster telemetry. The system employs a prediction-and-ranking mechanism that evaluates nodes and selects optimal placements for data-intensive workloads. Evaluation on a geo-distributed Kubernetes cluster showed 34-54% higher accuracy in node selection compared to the default Kubernetes scheduler."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] From SLA to vendor-neutral metrics: An intelligent knowledge-based\napproach for multi-cloud SLA-based broker"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [cloud computing, multi-cloud, SLA management, vendor-neutral metrics, intelligent knowledge-based system, auto-scaling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," V\xedctor Ramp\xe9rez, Javier Soriano, David Lizcano, Shadi Aljawarneh, Juan A. Lara"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universidad Polit\xe9cnica de Madrid (UPM), Madrid Open University (UDIMA)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.21173v1",children:"http://arxiv.org/pdf/2510.21173v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes an intelligent knowledge-based system that automatically translates high-level SLAs into vendor-neutral metrics for multi-cloud environments. The approach enables cross-provider metric measurement and provides consumer feedback through an intelligent tutoring system. Validation with IaaS and PaaS use cases demonstrates the system allows transparent multi-cloud exploitation across various application domains."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep\nLearning Training Workloads"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [GPU memory estimation, dynamic analysis, resource management, scheduling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jiabo Shi, Dimitrios Pezaros, Yehia Elkhatib"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Glasgow"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.21048v1",children:"http://arxiv.org/pdf/2510.21048v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," xMem proposes a CPU-based dynamic analysis framework to accurately estimate peak GPU memory requirements for deep learning training workloads without consuming GPU resources. The method achieves 91% reduction in median relative error and 75% reduction in OOM probability compared to existing solutions. This enables better GPU sharing and scheduling in cluster environments while significantly improving memory conservation potential."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Lincoln AI Computing Survey (LAICS) and Trends"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training, Other models inference], [AI accelerators, performance analysis, power consumption, market segmentation, computing architectures]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally, Jeremy Kepner"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," MIT Lincoln Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.20931v1",children:"http://arxiv.org/pdf/2510.20931v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper updates the Lincoln AI Computing Survey by collecting performance and power consumption data of commercial AI accelerators, plotting them on scatter graphs, and analyzing market trends. It introduces a new categorization of computing architectures and examines how GenAI models have shifted computational demands toward matrix-vector operations and high memory bandwidth. The survey highlights ongoing innovations in AI hardware across various deployment scales from embedded systems to data centers."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] ParaRNN: Unlocking Parallel Training of Nonlinear RNNs for Large\nLanguage Models"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [parallel training, nonlinear RNNs, sequence modeling, Newton's iterations, parallel reductions]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Federico Danieli, Pau Rodriguez, Miguel Sarabia, Xavier Suau, Luca Zappella"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Apple"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.21450v1",children:"http://arxiv.org/pdf/2510.21450v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ParaRNN enables parallel training of nonlinear RNNs by formulating recurrence relationships as a system of equations and solving them using Newton's iterations with parallel reductions. This approach achieves up to 665x speedup over sequential methods and allows training 7B parameter RNNs with performance comparable to Transformers and Mamba2. The framework is released as open-source to facilitate scalable nonlinear RNN research."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] REVE: A Foundation Model for EEG -- Adapting to Any Setup with\nLarge-Scale Pretraining on 25,000 Subjects"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [EEG foundation model, 4D positional encoding, masked autoencoding, brain-computer interfaces, clinical neuroscience]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yassine El Ouahidi, Jonathan Lys, Philipp Th\xf6lke, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon, Karim Jerbi, Giulia Lioi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," IMT Atlantique, Universit\xe9 de Montr\xe9al, Mila, UNIQUE"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.21585v1",children:"http://arxiv.org/pdf/2510.21585v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," REVE introduces a novel 4D positional encoding scheme and uses masked autoencoding pretraining on 60,000 hours of EEG data from 25,000 subjects. The model achieves state-of-the-art performance across 10 EEG tasks including motor imagery and seizure detection. It demonstrates strong generalization with minimal fine-tuning and enables standardized EEG research through released code and weights."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-10-28",children:"2025-10-28"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary\nWeights and Distilled Knowledge"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [model compression, ternary quantization, knowledge distillation, vision-language models, CLIP]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shu-Hao Zhang, Wei-Cheng Tang, Chen Wu, Peng Hu, Nan Li, Liang-Jie Zhang, Qi Zhang, Shao-Qun Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nanjing University, Microsoft"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.21879v1",children:"http://arxiv.org/pdf/2510.21879v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," TernaryCLIP converts CLIP model weights to ternary format using quantization-aware training and knowledge distillation. The method achieves 99% ternarization with 1.58-bit representation while maintaining performance on zero-shot tasks. This enables efficient deployment on resource-constrained devices with significant compression and acceleration benefits."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Encoder-Decoder Diffusion Language Models for Efficient Training and\nInference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [diffusion models, encoder-decoder architecture, efficient inference, parallel token sampling, block diffusion]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Marianne Arriola, Yair Schiff, Hao Phung, Aaron Gokaslan, Volodymyr Kuleshov"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Cornell University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.22852v1",children:"http://arxiv.org/pdf/2510.22852v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes an encoder-decoder architecture for discrete diffusion language models that separates clean token representation from denoising computation. The method enables faster training and inference by using a lightweight decoder for iterative refinement while periodically updating encoder representations. Experimental results show superior trade-offs between generation quality and inference throughput on summarization, translation, and mathematical reasoning tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Dopamine-driven synaptic credit assignment in neural networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [neural networks, credit assignment problem, dopamine optimizer, weight perturbation, reinforcement learning, adaptive learning rate, neuroAI]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Saranraj Nambusubramaniyan, Shervin Safavi, Raja Guru, Andreas Knoblauch"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Ulm, University of T\xfcbingen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.22178v1",children:"http://arxiv.org/pdf/2510.22178v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces a dopamine-inspired derivative-free optimizer for neural networks that uses weight perturbation and reward prediction error to adapt learning rates. This method achieves comparable performance to gradient-based algorithms while being more computationally efficient and biologically plausible. The optimizer demonstrates accelerated convergence in XOR tasks and chaotic time series forecasting with reduced memory and computation requirements."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Impact and Implications of Generative AI for Enterprise Architects in\nAgile Environments: A Systematic Literature Review"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [generative AI, enterprise architecture, agile environments, systematic literature review, architectural decision support, prompt engineering]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Stefan Julian Kooy, Jean Paul Sebastian Piest, Rob Henk Bemthuis"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Twente"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.22003v1",children:"http://arxiv.org/pdf/2510.22003v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper conducts a systematic literature review of 1,697 records to analyze the impact of Generative AI on enterprise architects in agile environments. The study identifies key use cases including design ideation, artifact creation, and architectural decision support, while highlighting risks like opacity, bias, and compliance concerns. The findings provide implications for capability building, governance, and establish a research agenda for human-AI collaboration in enterprise architecture."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] To Use or to Refuse? Re-Centering Student Agency with Generative AI in\nEngineering Design Education"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [generative AI, engineering education, design thinking, reflective practice, student agency]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Thijs Willems, Sumbul Khan, Qian Huang, Bradley Camburn, Nachamma Sockalingam, King Wang Poon"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Singapore University of Technology and Design"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.19342v1",children:"http://arxiv.org/pdf/2510.19342v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This study implemented a three-way framework (tool/teammate/non-use) in an AI-enhanced engineering design course to analyze student interactions with generative AI. Students developed practices like accelerated prototyping, prompt refinement, and hallucination recognition while learning to strategically reject AI outputs. The approach transformed AI efficiency into innovation by cultivating selective usage and deeper user research, making AI uptake an assessable design habit."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] PACR: Progressively Ascending Confidence Reward for LLM Reasoning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [reinforcement learning, reasoning, confidence reward, exploration efficiency, model-intrinsic supervision]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Eunseop Yoon, Hee Suk Yoon, Jaehyun Jang, SooHwan Eom, Qi Dai, Chong Luo, Mark A. Hasegawa-Johnson, Chang D. Yoo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Korea Advanced Institute of Science and Technology (KAIST), Microsoft Research Asia (MSRA), University of Illinois at Urbana-Champaign (UIUC)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.22255v1",children:"http://arxiv.org/pdf/2510.22255v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes PACR, a dense reward signal based on the model's progressively ascending confidence in the correct answer during reasoning. This method accelerates exploration and improves training efficiency in reinforcement learning with verifiable rewards. Results show PACR achieves better performance with fewer trajectories across multiple benchmarks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Step2Motion: Locomotion Reconstruction from Pressure Sensing Insoles"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [motion reconstruction, pressure sensing insoles, wearable sensors, locomotion analysis, IMU data]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jose Luis Ponton, Eduardo Alvarado, Lin Geng Foo, Nuria Pelechano, Carlos Andujar, Marc Habermann"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universitat Polit\xe8cnica de Catalunya, Max Planck Institute for Informatics"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.22712v1",children:"http://arxiv.org/pdf/2510.22712v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Step2Motion reconstructs human locomotion using multi-modal data from sensor insoles combining pressure measurements and inertial data (accelerations and angular rates). The method demonstrates robust performance across diverse locomotion styles including walking, jogging, dancing and crouching movements. This approach enables unconstrained motion capture in real-world environments without line-of-sight limitations or movement restrictions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] QuArch: A Benchmark for Evaluating LLM Reasoning in Computer\nArchitecture"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [computer architecture, benchmark, reasoning evaluation, question-answering]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shvetank Prakash, Andrew Cheng, Arya Tschand, Mark Mazumder, Varun Gohil, Jeffrey Ma, Jason Yik, Zishen Wan, Jessica Quaye, Elisavet Lydia Alvanaki, Avinash Kumar, Chandrashis Mazumdar, Tuhin Khare, Alexander Ingare, Ikechukwu Uchendu, Radhika Ghosal, Abhishek Tyagi, Chenyu Wang, Andrea Mattia Garavagno, Sarah Gu, Alice Guo, Grace Hur, Luca Carloni, Tushar Krishna, Ankita Nayak, Amir Yazdanbakhsh, Vijay Janapa Reddi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Harvard University, Massachusetts Institute of Technology, Georgia Institute of Technology, Columbia University, University of Texas at Austin, UC Santa Cruz, University of Rochester, University of Genoa, Scuola Superiore Sant'Anna, Qualcomm AI Research, Google DeepMind"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.22087v1",children:"http://arxiv.org/pdf/2510.22087v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces QuArch, the first benchmark with 2,671 expert-validated QA pairs for evaluating LLM reasoning in computer architecture. The evaluation shows frontier models possess domain knowledge but struggle with higher-order architectural reasoning, achieving accuracies between 34%-72%. This community-developed benchmark establishes foundational standards for measuring LLM capabilities in computer architecture."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] AndroidControl-Curated: Revealing the True Potential of GUI Agents\nthrough Benchmark Purification"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training], [GUI agents, benchmark purification, on-device models, AndroidControl-Curated, Magma-R1 model]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ho Fai Leung, Xiaoyan Xi, Fei Zuo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," BMW ArcherMind Information Technology Co. Ltd. (BA TechWorks)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.18488v1",children:"http://arxiv.org/pdf/2510.18488v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper identifies flaws in existing GUI agent benchmarks and creates AndroidControl-Curated through a purification pipeline. It develops Magma-R1-3B model via post-training on curated samples, achieving 75.3% success rate comparable to much larger models. The work demonstrates that GUI agents are more capable than previously measured and closer to practical deployment."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] CosmoCore Affective Dream-Replay Reinforcement Learning for Code\nGeneration"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training], [reinforcement learning, code generation, affective computing, hallucination mitigation, experience replay]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Santhosh Kumar Ravindran"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Microsoft Corporation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.18895v1",children:"http://arxiv.org/pdf/2510.18895v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," CosmoCore introduces an affective reinforcement learning framework that tags code generation trajectories with valence and surprise signals to prioritize replay of error episodes. This neuroscience-inspired approach reduces hallucinated code by 48% and accelerates self-correction by 45% in benchmarks. The system enhances traditional RLHF by incorporating emotional cues for more robust code generation in LLMs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] DynaQuery: A Self-Adapting Framework for Querying Structured and\nMultimodal Data"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [natural language querying, structured databases, multimodal data, schema linking, query planning, retrieval-augmented generation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Aymane Hassini"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Al Akhawayn University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.18029v1",children:"http://arxiv.org/pdf/2510.18029v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," DynaQuery introduces a self-adapting framework with a Schema Introspection and Linking Engine (SILE) that enhances schema linking in query planning for structured and multimodal databases. It demonstrates superior robustness compared to unstructured RAG approaches by nearly eliminating catastrophic failures like schema hallucination. The framework provides a validated basis for developing reliable natural language database interfaces."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Approximate Gradient Coding for Distributed Learning with Heterogeneous\nStragglers"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [gradient coding, distributed learning, straggler mitigation, optimization, convergence analysis]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Heekang Song, Wan Choi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Korea Advanced Institute of Science and Technology, Seoul National University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.22539v1",children:"http://arxiv.org/pdf/2510.22539v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes an optimized gradient coding scheme that minimizes residual error while ensuring unbiased gradient estimation by explicitly considering heterogeneous straggler probabilities. The authors derive closed-form solutions for encoding/decoding coefficients and develop data allocation strategies to reduce redundancy and computation load. Numerical results demonstrate that their approach significantly reduces straggler impact and accelerates convergence compared to existing methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] FineVision: Open Data Is All You Need"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [vision-language models, dataset curation, data decontamination, human-in-the-loop pipeline, multi-modal datasets]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Luis Wiedmann, Orr Zohar, Amir Mahla, Xiaohan Wang, Rui Li, Thibaud Frere, Leandro von Werra, Aritra Roy Gosthipaty, Andr\xe9s Marafioti"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Hugging Face, Technical University Munich, Stanford University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.17269v1",children:"http://arxiv.org/pdf/2510.17269v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," FineVision introduces a semi-automated human-in-the-loop pipeline to create a unified vision-language dataset of 24 million samples from over 200 sources, featuring rigorous de-duplication and decontamination. The method combines automated processing with human oversight for quality control and schema validation. Models trained on this curated dataset consistently outperform those using existing open mixtures, demonstrating the value of scale and data hygiene in vision-language model development."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-10-29",children:"2025-10-29"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Fault-Tolerant Multiparty Session Types with Global Escape Loops"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [fault-tolerance], [multiparty session types, distributed algorithms, global escape loops, fault-tolerant protocols]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Lukas Bartl, Julian Linne, Kirstin Peters"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universit\xe4t Augsburg"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.24203v1",children:"http://arxiv.org/pdf/2510.24203v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper extends fault-tolerant multiparty session types with a novel global escape loop construct that enables decentralized termination without global coordination. The approach uses non-blocking exit messages to allow processes to independently terminate loops while maintaining fault tolerance. The method is demonstrated through analysis of the rotating coordinator algorithm, providing formal verification for distributed algorithms under failure conditions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Odyssey: An End-to-End System for Pareto-Optimal Serverless Query\nProcessing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [serverless], [query processing, cost optimization, performance optimization, serverless computing, data analytics]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shyam Jesalpura, Shengda Zhu, Amir Shaikhha, Antonio Barbalace, Boris Grot"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Edinburgh"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.24307v1",children:"http://arxiv.org/pdf/2510.24307v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Odyssey introduces an end-to-end serverless data analytics system that automatically generates and evaluates query plans using state space pruning and a novel search algorithm to find Pareto-optimal plans balancing cost and performance. The system integrates query planning, cost modeling, and execution specifically for serverless environments. Evaluation shows Odyssey outperforms AWS Athena in cost and/or latency while accurately predicting both metrics."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] ARIMA_PLUS: Large-scale, Accurate, Automatic and Interpretable\nIn-Database Time Series Forecasting and Anomaly Detection in Google BigQuery"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [time series forecasting, anomaly detection, ARIMA_PLUS, BigQuery, interpretable models, scalable infrastructure]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xi Cheng, Weijie Shen, Haoming Chen, Chaoyi Shen, Jean Ortega, Jiashang Liu, Steve Thomas, Honglin Zheng, Haoyun Wu, Yuxiang Li, Casey Lichtendahl, Jenny Ortiz, Gang Liu, Haiyang Qi, Omid Fatemieh, Chris Fry, Jing Jing Long"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Google"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.24452v1",children:"http://arxiv.org/pdf/2510.24452v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ARIMA_PLUS introduces a modular framework combining interpretable time series models with scalable cloud infrastructure for forecasting and anomaly detection. It outperforms both statistical and neural network methods on benchmark datasets while enabling high-throughput processing. The system integrates directly into Google BigQuery with automated model selection and provides customizable business insights through SQL interface."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] MinatoLoader: Accelerating Machine Learning Training Through Efficient\nData Preprocessing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [data loader, data preprocessing, GPU utilization, batch construction, PyTorch]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Rahma Nouaji, Stella Bitchebe, Ricardo Macedo, Oana Balmau"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," McGill University, INESC TEC, University of Minho"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.10712v2",children:"http://arxiv.org/pdf/2509.10712v2"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," MinatoLoader introduces a novel data loader for PyTorch that prioritizes fast-to-preprocess samples and processes slower ones in parallel to reduce GPU idleness. It significantly accelerates ML training by up to 7.5\xd7 and improves GPU utilization from 46% to over 90% while maintaining model accuracy and convergence speed."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\nLearning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [kernels], [CUDA optimization, reinforcement learning, contrastive learning, GPU computing, automated code optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li, Chris Shum"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," DeepReinforce"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2507.14111v8",children:"http://arxiv.org/pdf/2507.14111v8"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," CUDA-L1 introduces a contrastive reinforcement learning framework that transforms LLMs into effective CUDA optimizers using speedup-based rewards. The method achieves significant performance improvements across multiple GPU architectures without requiring human expertise. It demonstrates capabilities in discovering optimization techniques, identifying bottlenecks, and strategically combining optimizations for substantial speedups."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine\nLearning Approach"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [electric vehicles, car-following behavior, random forest, intelligent driver model, traffic simulation, machine learning]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Md. Shihab Uddin, Md Nazmus Shakib, Rahul Bhadani"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Alabama in Huntsville"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.24085v1",children:"http://arxiv.org/pdf/2510.24085v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This study compares classical physics-based models (IDM, OVM, OVRV, CACC) with a machine learning approach using Random Forest to model electric vehicle car-following behavior. The Random Forest model demonstrated superior accuracy across different gap scenarios compared to classical models. These findings highlight machine learning's effectiveness in simulating EV behavior for mixed-autonomy traffic analysis."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Trajectory Design for UAV-Based Low-Altitude Wireless Networks in\nUnknown Environments: A Digital Twin-Assisted TD3 Approach"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [UAV trajectory design, digital twin, deep reinforcement learning, wireless networks, unknown environments]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jihao Luo, Zesong Fei, Xinyi Wang, Le Zhao, Yuanhao Cui, Guangxu Zhu, Dusit Niyato"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Beijing Institute of Technology, Beijing University of Posts and Telecommunications, The Chinese University of Hong Kong (Shenzhen), Nanyang Technological University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.24255v1",children:"http://arxiv.org/pdf/2510.24255v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a digital twin-assisted framework where UAVs use integrated sensing and communication signals to progressively build virtual environments, combined with simulated annealing and twin-delayed deep deterministic policy gradient algorithm for trajectory optimization. The approach achieves faster convergence, higher flight safety, and shorter mission completion time compared to baseline methods. It provides an efficient solution for low-altitude wireless network deployment in unknown environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Causal-Aware Generative Adversarial Networks with Reinforcement Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [generative adversarial networks, reinforcement learning, causal graphs, tabular data, privacy preservation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tu Anh Hoang Nguyen, Dang Nguyen, Tri-Nhan Vo, Thuc Duy Le, Sunil Gupta"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Deakin University, Adelaide University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.24046v1",children:"http://arxiv.org/pdf/2510.24046v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," CA-GAN introduces a two-step framework combining causal graph extraction with a custom Conditional WGAN-GP and reinforcement learning to align causal structures between real and synthetic data. It demonstrates superior performance over six state-of-the-art methods across 14 tabular datasets. The approach effectively preserves causal relationships, data utility, and privacy in synthetic data generation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] Fill in the Blanks: Accelerating Q-Learning with a Handful of\nDemonstrations in Sparse Reward Settings"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [reinforcement learning, sparse rewards, learning from demonstrations, Q-learning, sample efficiency]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Seyed Mahdi Basiri Azad, Joschka Boedecker"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Freiburg"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.24432v1",children:"http://arxiv.org/pdf/2510.24432v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a hybrid offline-to-online method that initializes Q-learning value functions using a small number of demonstrations to accelerate learning in sparse reward environments. By precomputing value estimates from offline demonstrations and using them as early learning targets, the approach reduces exploration burden and improves sample efficiency. Experiments show the method accelerates convergence and outperforms baselines even with minimal or suboptimal demonstration data."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] FALQON: Accelerating LoRA Fine-tuning with Low-Bit Floating-Point\nArithmetic"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [finetuning], [FP8 quantization, LoRA fine-tuning, low-rank adaptation, computational efficiency, memory optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kanghyun Choi, Hyeyoon Lee, SunJong Park, Dain Kwon, Jinho Lee"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Seoul National University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.24061v1",children:"http://arxiv.org/pdf/2510.24061v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," FALQON proposes a novel framework that merges LoRA adapters directly into an FP8-quantized backbone during fine-tuning, eliminating quantization overhead through reformulated forward/backward computations and a row-wise proxy update mechanism. The method achieves approximately 3\xd7 training speedup over existing quantized LoRA approaches while maintaining similar accuracy levels. This end-to-end FP8 workflow enables efficient large-scale model fine-tuning without requiring post-training quantization."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2510] RS-ORT: A Reduced-Space Branch-and-Bound Algorithm for Optimal\nRegression Trees"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [optimal regression trees, branch-and-bound algorithm, mixed-integer programming, large-scale datasets, parallel execution]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Cristobal Heredia, Pedro Chumpitaz-Flores, Kaixun Hua"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of South Florida"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.23901v1",children:"http://arxiv.org/pdf/2510.23901v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes RS-ORT, a specialized branch-and-bound algorithm that recasts optimal regression tree training as a two-stage optimization problem and branches exclusively on tree-structural variables. It introduces bound tightening techniques and decomposable bounding strategies to accelerate training while enabling parallel execution. Empirical results show RS-ORT achieves superior training and testing performance with simpler tree structures and better generalization on datasets with up to 2 million samples."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>o});var r=i(6540);const s={},t=r.createContext(s);function a(n){const e=r.useContext(t);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),r.createElement(t.Provider,{value:e},n.children)}}}]);