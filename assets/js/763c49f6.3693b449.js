"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[879],{4132:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"daily/20251229-20260104","title":"20251229-20260104","description":"2025-12-29","source":"@site/docs/daily/20251229-20260104.md","sourceDirName":"daily","slug":"/daily/20251229-20260104","permalink":"/daily/20251229-20260104","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766977171000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251222-20251228","permalink":"/daily/20251222-20251228"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=i(4848),a=i(8453);const t={},o="20251229-20260104",l={},d=[{value:"2025-12-29",id:"2025-12-29",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"20251229-20260104",children:"20251229-20260104"})}),"\n",(0,s.jsx)(n.h2,{id:"2025-12-29",children:"2025-12-29"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"cs.DC total: 16"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.strong,{children:["[arXiv251229] LIME",":Accelerating"," Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices"]})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [pipeline parallelism, model offloading, fine-grained offline allocation scheduler, online memory adaptation, collaborative inference]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Mingyu Sun, Xiao Zhang, Shen Qu, Yan Li, Mengbai Xiao, Yuan Yuan, Dongxiao Yu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Shandong University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21835",children:"https://arxiv.org/pdf/2512.21835"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes LIME, a collaborative system that uses interleaved pipeline parallelism and model offloading to enable lossless LLM inference across multiple memory-constrained edge devices. It introduces a fine-grained offline scheduler and online memory adaptation to optimize resource usage and minimize latency. Experiments show LIME achieves significant speedups over baselines on heterogeneous edge devices without compromising model accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [deep-surrogate, two-stage design, mixture-of-experts, error-bounded lossy compression, quality prediction]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Luki\u0107, Franck Cappello"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21433",children:"https://arxiv.org/pdf/2512.21433"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," DeepCQ is a deep-surrogate framework that predicts lossy compression quality using a two-stage design and mixture-of-experts approach for time-evolving data. It generalizes across compressors, metrics, and datasets, achieving prediction errors under 10% and reducing computational overhead in scientific data analysis."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Demystifying ARM SME to Optimize General Matrix Multiplications"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [cache-aware partitioning, data packing, micro-kernels, multi-vector loads, tile registers, ARM SME]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Chencheng Deng, Weiling Yang, Jianbin Fang, Dezun Dong"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," National University of Defense Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21473",children:"https://arxiv.org/pdf/2512.21473"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents MpGEMM, an open-source library that optimizes General Matrix Multiplication (GEMM) by leveraging ARM's Scalable Matrix Extension (SME) through techniques like cache-aware partitioning and specialized micro-kernels. It achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library on real-world workloads from DeepSeek and LLaMA."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [data spaces, cloud-edge continuum, edge computing, containerized microservices, AI/ML services, IoT testbed]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Spark Works Ltd, IONOS SE, Iquadrat Inform\xe1tica S.L."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21340",children:"https://arxiv.org/pdf/2512.21340"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a real-world use case of intelligent infrastructure monitoring implemented within a data space-enabled cloud-edge framework. It leverages edge computing, containerized microservices, and interoperable data sharing to address challenges like sensor integration and data privacy. The implementation demonstrates the transformative potential of combining AI, edge computing, and data spaces for building scalable and resilient smart city ecosystems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [LSTM, Random Forest, MQTT, InfluxDB, Streamlit, IoT monitoring, synthetic data, ASHRAE 2021]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Krishna Chaitanya Sunkara, Rambabu Konakanchi"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Oracle, Charles Schwab"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21801",children:"https://arxiv.org/pdf/2512.21801"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents a smart IoT system that uses LSTM networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. It demonstrates high accuracy on synthetic data and shows that proactive leak management can prevent significant energy waste. The work establishes a proof-of-concept for sustainable data center operations, though it requires empirical validation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [e-graph, term rewriting, equality saturation, auto vectorize, auto distribution, auto schedule, non-uniform memory access (NUMA) abstraction, roofline model]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Canaan Inc."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21571",children:"https://arxiv.org/pdf/2512.21571"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," nncase is an end-to-end compiler framework that uses an e-graph-based term rewriting engine to optimize LLM deployment across heterogeneous memory architectures. It unifies optimization through modules for auto vectorization, distribution, and scheduling. The evaluation shows it outperforms other frameworks and achieves performance comparable to hand-optimized solutions, demonstrating the viability of automated compilation for high-performance LLM inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Embedding Samples Dispatching for Recommendation Model Training in Edge Environments"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [embedding cache, parameter server, edge computing, sample dispatching, HybridDis]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Guopeng Li, Haisheng Tan, Chi Zhang, Hongqiu Ni, Zilong Wang, Xinyue Zhang, Yang Xu, Han Tian"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China, Hefei University of Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21615",children:"https://arxiv.org/pdf/2512.21615"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes ESD, a mechanism that optimizes the dispatch of input embedding samples to edge workers to minimize transmission costs in distributed DLRM training. It introduces HybridDis, a dispatch method combining an optimal and a heuristic algorithm to balance decision quality and resource use. Experiments show ESD reduces embedding transmission cost by up to 36.76% and achieves up to 1.74x training speedup."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol for Multicore Real-Time Systems"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [real-time systems], [lock-free protocol, fault tolerance, resource sharing, multicore, worst-case response time analysis]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Nan Chen, Xiaotian Dai, Tong Cheng, Alan Burns, Iain Bate, Shuai Zhao"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of York, Sun Yat-sen University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21701",children:"https://arxiv.org/pdf/2512.21701"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes LEFT-RS, a lock-free fault-tolerant resource sharing protocol for multicore real-time systems that allows concurrent read access to global resources and parallel entry into critical sections. It aims to improve efficiency and fault resilience by enabling tasks to complete earlier if others fail, while limiting overhead. The evaluation shows the method significantly outperforms existing approaches, achieving up to an 84.5% average improvement in schedulability."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Proceedings First Workshop on Adaptable Cloud Architectures"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," TBD"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Giuseppe De Palma, Saverio Giallorenzo"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," TBD"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22054",children:"https://arxiv.org/pdf/2512.22054"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [vision transformer, cloud-device collaboration, patch-level importance scoring, dynamic scheduling, weighted ensembling]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Linyi Jiang, Yifei Zhu, Hao Yin, Bo Li"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Tsinghua University, Hong Kong University of Science and Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21730",children:"https://arxiv.org/pdf/2512.21730"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," Hyperion is a cloud-device collaborative framework that enables low-latency inference on Ultra-HD video using vision transformers by identifying and transmitting critical image patches, dynamically adjusting transmission quality, and fusing edge and cloud results. It improves frame processing rate by up to 1.61\xd7 and accuracy by up to 20.2% compared to state-of-the-art baselines under dynamic network conditions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [mixture-of-experts (MoE), disaggregated expert parallelism (DEP), fine-grained task scheduling, FinDEP, task pipelining, optimization problem]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Xinglin Pan, Shaohuai Shi, Wenxiang Lin, Yuxin Wang, Zhenheng Tang, Wei Wang, Xiaowen Chu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," The Hong Kong University of Science and Technology (Guangzhou), Harbin Institute of Technology, Shenzhen, Hong Kong Baptist University, The Hong Kong University of Science and Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21487",children:"https://arxiv.org/pdf/2512.21487"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes FinDEP, a fine-grained task scheduling algorithm for disaggregated expert parallelism (DEP) to improve the inference throughput of MoE-based large language models. The method partitions computation and communication tasks and formulates an optimization problem to maximize task overlap. Experiments show FinDEP achieves up to 1.61x throughput improvement over prior methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [federated fine-tuning, adaptive aggregation, connection failures, data heterogeneity, convergence guarantee, LoRA]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yanmeng Wang, Zhiwen Dai, Shuai Wang, Jian Zhou, Fu Xiao, Tony Q. S. Quek, Tsung-Hui Chang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Nanjing University of Posts and Telecommunications, The Hong Kong University of Science and Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22035",children:"https://arxiv.org/pdf/2512.22035"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes FedAuto, a federated fine-tuning framework that uses adaptive aggregation to handle unreliable network connections and heterogeneous client data without prior knowledge of network conditions. It provides a strong per-round convergence guarantee. Experiments show it outperforms existing methods in various failure scenarios for both full and parameter-efficient fine-tuning like LoRA."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [model parallelism, block placement, request routing, mixed integer linear programming, performance modeling, distributed inference]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Tingyang Sun, Ting He, Bo Ji, Parimal Parag"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Pennsylvania State University, Virginia Tech, Indian Institute of Science"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21884",children:"https://arxiv.org/pdf/2512.21884"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents the first systematic study of resource allocation for distributed large language model inference, focusing on optimizing block placement and request routing. It develops performance models, formulates the problem as a mixed integer linear program, and provides efficient offline and online algorithms with performance guarantees. The proposed solution is shown to substantially reduce inference time compared to the state-of-the-art in geographically-distributed settings."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [agentic workflow, LLM-driven graph traversal, service dependency graph (SDG), hammock-block program dependence graph (PDG), root-cause analysis (RCA)]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Illinois at Urbana-Champaign, IBM Research"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22113",children:"https://arxiv.org/pdf/2512.22113"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces PRAXIS, an orchestrator that uses an LLM-driven structured traversal over service dependency and program dependence graphs to diagnose code-related cloud incidents. It significantly improves root-cause analysis accuracy and reduces computational cost compared to prior methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] BLEST: Blazingly Efficient BFS using Tensor Cores"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [Binarised Virtual Slice Sets (BVSS), graph reordering, batched SpMSpV multiplication, kernel fusion, lazy vertex update]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Deniz Elbek, Kamer Kaya"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Sabanci University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21967",children:"https://arxiv.org/pdf/2512.21967"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," BLEST is a GPU-accelerated BFS framework that leverages Tensor Cores by reformulating the pull-based BFS pipeline using a bitmap-oriented structure and a batched sparse matrix-sparse vector multiplication pattern. It introduces techniques like Binarised Virtual Slice Sets for load balancing and employs graph reordering for memory efficiency. The experiments show that BLEST achieves significant speedups over state-of-the-art frameworks like BerryBees, Gunrock, and GSWITCH."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251229] FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [expert parallelism, data shuffling, transformation-communication fusion, pipelined communication engine, load-balancing]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Zhuoran Zhu, Chunyang Zhu, Hao Lin, Xu Fu, Yiming Zhou, Quanlu Zhang, Zhenhua Li, Feng Qian, Chao Yu, Boxun Li, Guohao Dai, Yu Wang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Tsinghua University, Infinigence AI, University of Southern California, Zhongguancun Academy, Shanghai Jiaotong University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22036",children:"https://arxiv.org/pdf/2512.22036"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces FUSCO, a communication library that fuses data transformation and communication to optimize distributed data shuffling for Mixture-of-Experts (MoE) model training. It addresses the layout mismatch between expert-major data and device-major communication by using a pipelined engine with lightweight planning. The results show that FUSCO significantly outperforms existing libraries like NCCL and DeepEP in both training and inference latency for MoE models."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 12'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] A Reinforcement Learning Approach to Synthetic Data Generation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21395",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21648",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21540",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21351",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21446",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21586",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] Generative Actor Critic ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21527",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21412",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21514",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21717",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] A Comedy of Estimators: On KL Regularization in RL Training of LLMs ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21852",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] Meta-Learning-Based Handover Management in NextG O-RAN ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22022",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 12'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21648",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21446",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21544",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] Discovering Sparse Recovery Algorithms Using Neural Architecture Search ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21563",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21769",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] How Do Agents Perform Code Optimization? An Empirical Study ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21757",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21452",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21925",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22007",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22065",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.22066",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251229] Enabling Ultra-Fast Cardiovascular Imaging Across Heterogeneous Clinical Environments with a Generalist Foundation Model and Multimodal Database ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.21652",children:"link"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var r=i(6540);const s={},a=r.createContext(s);function t(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);