"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[220],{8453:(i,n,e)=>{e.d(n,{R:()=>t,x:()=>o});var r=e(6540);const s={},a=r.createContext(s);function t(i){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof i?i(n):{...n,...i}},[n,i])}function o(i){let n;return n=i.disableParentContext?"function"==typeof i.components?i.components(s):i.components||s:t(i.components),r.createElement(a.Provider,{value:n},i.children)}},9424:(i,n,e)=>{e.r(n),e.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251215-20251221","title":"20251215-20251221","description":"2025-12-15","source":"@site/docs/daily/20251215-20251221.md","sourceDirName":"daily","slug":"/daily/20251215-20251221","permalink":"/daily/20251215-20251221","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1765943222000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251208-20251214","permalink":"/daily/20251208-20251214"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=e(4848),a=e(8453);const t={},o="20251215-20251221",l={},c=[{value:"2025-12-15",id:"2025-12-15",level:2},{value:"2025-12-16",id:"2025-12-16",level:2},{value:"2025-12-17",id:"2025-12-17",level:2}];function d(i){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...i.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"20251215-20251221",children:"20251215-20251221"})}),"\n",(0,s.jsx)(n.h2,{id:"2025-12-15",children:"2025-12-15"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"cs.DC total: 15"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [distributed network algorithms], [multi-packet messaging, pruning, closeness centrality, decentralized computation, message efficiency]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Patrick D. Manya, Eugene M. Mbuyi, Gothy T. Ngoie, Jordan F. Masakuna"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Kinshasa"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11512",children:"https://arxiv.org/pdf/2512.11512"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper enhances a distributed pruning method for closeness centrality by using multi-packet messaging to batch data into larger blocks. This reduces the number of exchanged messages and improves communication efficiency, particularly for large networks, with a manageable trade-off in local memory overhead."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] Seamless Transitions: A Comprehensive Review of Live Migration Technologies"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [virtualization], [live migration, container migration, virtual machine migration, migration techniques, migration units, infrastructure characteristics]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Sima Attar-Khorasani, Lincoln Sherpa, Matthias Lieber, Siavash Ghiasvand"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," TUD Dresden University of Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.10979",children:"https://arxiv.org/pdf/2512.10979"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper provides a comprehensive review of live migration technologies, focusing on container-based and virtual machine-based approaches. It analyzes migration techniques, units, and infrastructure, concluding that practical challenges and resource demands can sometimes outweigh the benefits, making implementation difficult to justify."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] An Efficient Approach for Energy Conservation in Cloud Computing Environment"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [cloud computing], [task scheduling, resource utilization, fitness value, multi-criteria energy-efficient task scheduling (MCEETS), MaxUtil]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Sohan Kumar Pande, Sanjaya Kumar Panda, Preeti Ranjan Sahu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Not specified"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.10974",children:"https://arxiv.org/pdf/2512.10974"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a multi-criteria energy-efficient task scheduling (MCEETS) algorithm for cloud computing, which uses a fitness value based on CPU, disk, I/O utilization, and task processing time to improve resource utilization. Simulation results show that the proposed algorithm consumes less energy than the existing MaxUtil algorithm."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [dynamic scheduling, multi-objective scheduling, hybrid priority scheduler, predictive backfill, smart batch, fragmentation reduction, job starvation, GPU utilization]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Akhmadillo Mamirov"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," The College of Wooster"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.10980",children:"https://arxiv.org/pdf/2512.10980"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces three dynamic multi-objective schedulers (Hybrid Priority, Predictive Backfill, and Smart Batch) designed to reduce fragmentation and starvation in GPU clusters. Through simulation, these schedulers significantly outperform static baselines in utilization, throughput, and fairness, demonstrating that adaptive scheduling can meaningfully improve GPU efficiency in heterogeneous AI clusters."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] An LLVM-Based Optimization Pipeline for SPDZ"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [secure multiparty computation], [LLVM, SPDZ, secret sharing, batching, GPU kernels, non-blocking scheduler]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Tianye Dai, Hammurabi Mendes, Heuichan Lim"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Davidson College"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11112",children:"https://arxiv.org/pdf/2512.11112"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents an LLVM-based compiler pipeline for the SPDZ MPC protocol, which uses C with privacy annotations and LLVM IR to automatically batch operations and a runtime scheduler to overlap communication and computation, including GPU kernel mapping. The evaluation shows significant speedups over MP-SPDZ, indicating that leveraging LLVM with protocol-aware scheduling is effective for extracting parallelism without sacrificing usability."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [hybrid parallelism, data parallelism, pipeline parallelism, model partitioning, network scheduling, runtime adaptation]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Jianli Jin, Ziyang Lin, Qianli Dong, Yi Chen, Jayanth Srinivasa, Myungjin Lee, Zhaowei Tan, Fan Lai"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," UIUC, Northwestern University, University of California, Riverside, Cisco Research"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.10990",children:"https://arxiv.org/pdf/2512.10990"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," Dora is a framework that uses heterogeneity-aware model partitioning, contention-aware network scheduling, and a runtime adapter to achieve QoE-aware hybrid parallelism for distributed edge AI. It demonstrates significant improvements in execution speed and energy efficiency while maintaining quality of experience under dynamic runtime conditions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [GPU-native compilation, parallel traditional compilation, neural compilation, sequence-to-sequence translation, probabilistic verification, hybrid architecture, in-VRAM iteration]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Adilet Metinov, Gulida M. Kudakeeva, Gulnara D. Kabaeva"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Institute of Information Technology, Kyrgyz State Technical University named after I. Razzakov"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11200",children:"https://arxiv.org/pdf/2512.11200"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper establishes theoretical foundations for three GPU-native compilation approaches\u2014parallel traditional, neural, and hybrid\u2014to eliminate CPU-GPU data transfers during code iteration cycles. It demonstrates that these methods can achieve 10-100x speedups by keeping compilation and execution entirely in GPU memory. The main conclusion is that GPU-native compilation offers a path to drastically reduce latency and energy consumption in AI code generation systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training], [cluster scheduling, disaggregated architecture, co-execution group, two-tier scheduling, round-robin, warm-start context switching]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Tianyuan Wu, Lunxi Cao, Yining Wei, Wei Gao, Yuheng Zhao, Dakai An, Shaopan Xiong, Zhiqiang Lv, Ju Huang, Siran Yang, Yinghao Yu, Jiamang Wang, Lin Qu, Wei Wang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Hong Kong University of Science and Technology, UIUC, Alibaba Group"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11306",children:"https://arxiv.org/pdf/2512.11306"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces RollMux, a cluster scheduling framework that improves efficiency in disaggregated RL post-training by multiplexing jobs to utilize idle phases. Its core method involves a two-tier scheduler and a co-execution group abstraction to orchestrate cross-cluster execution. The evaluation shows RollMux significantly improves cost efficiency over standard disaggregated and co-located baselines while maintaining service-level objectives."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [hierarchical federated learning, aggregated federated learning, continual federated learning, decentralized aggregation, gossip-based protocols]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Sumit Chongder"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Maharashtra Institute of Technology - Art, Design and Technology University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.10987",children:"https://arxiv.org/pdf/2512.10987"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes an evaluation framework to compare centralized and decentralized aggregation algorithms in federated learning systems. It finds that decentralized methods (Aggregated and Continual Federated Learning) outperform centralized Hierarchical Federated Learning in metrics like precision and recall on standard datasets, highlighting the advantages of distributed computation and aggregation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] Agentic Operator Generation for ML ASICs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [Triton, ATen kernels, PyTorch OpInfo, JIT compilation, large language models, agentic AI, kernel generation]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Alec M. Hammond, Aram Markosyan, Aman Dontula, Simon Mahns, Zacharias Fisches, Dmitrii Pedchenko, Keyur Muzumdar, Natacha Supper, Mark Saroufim, Joe Isaacson, Laura Wang, Warren Hunt, Kaustubh Gondkar, Roman Levenstein, Gabriel Synnaeve, Richard Li, Jacob Kahn, Ajit Mathews"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Meta"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.10977",children:"https://arxiv.org/pdf/2512.10977"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents TritorX, an agentic AI system that uses large language models combined with a custom linter, JIT compilation, and a PyTorch OpInfo test harness to automatically generate functionally correct Triton ATen kernels for ML accelerators like MTIA. The system prioritizes broad operator coverage over performance for a limited set, successfully generating kernels for 481 unique operators that pass over 20,000 tests. This approach enables the rapid overnight generation of complete PyTorch ATen backends for new accelerator platforms."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [runtime parallelization, branch-aware memory management, adaptive scheduling, DAG partitioning, buffer reuse, heterogeneous inference]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Chong Tang, Hao Dai, Jagmohan Chauhan"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Southampton, University College London"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11532",children:"https://arxiv.org/pdf/2512.11532"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," Parallax is a framework that accelerates mobile DNN inference by partitioning the computation graph to expose parallelism and using branch-aware memory management with adaptive scheduling to handle operator fallbacks. It reduces latency by up to 46% and energy consumption by up to 30% compared to state-of-the-art frameworks, while controlling memory overhead, without requiring model refactoring."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [continuous learning, camera grouping, GPU allocation, transmission control, cross-camera correlation]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yuze He, Ferdi Kossmann, Srinivasan Seshan, Peter Steenkiste"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Carnegie Mellon University, Massachusetts Institute of Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11727",children:"https://arxiv.org/pdf/2512.11727"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," ECCO is a video analytics framework that improves resource efficiency by grouping cameras with similar data drift patterns to share retrained models. It uses a dynamic grouping algorithm, GPU allocator, and transmission controller to reduce compute and communication costs. The system increases retraining accuracy by 6.7%-18.1% or supports 3.3x more cameras at the same accuracy compared to baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [distributed systems], [Snowflake algorithm, network-derived identity, private IPv4 address, bit-allocation scheme (1-41-16-6), stateless microservices, container orchestration]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Manideep Reddy Chinthareddy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Independent researcher (based on email domain)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11643",children:"https://arxiv.org/pdf/2512.11643"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a stateless, cloud-agnostic distributed ID generator that eliminates the need for explicit worker IDs by deriving node uniqueness from a container's private IPv4 address. It introduces a modified bit-allocation scheme to incorporate this network-derived entropy while preserving monotonicity. The method demonstrates performance comparable to traditional stateful generators while offering improved scalability and operational simplicity in containerized environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [HPC resource access], [RESTful API, performance testing, proxy-based APIs, I/O bottlenecks, architectural redesign, security, authorization]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Elia Palme, Juan Pablo Dorsch, Ali Khosravi, Giovanni Pizzi, Francesco Pagnamenta, Andrea Ceriani, Eirini Koutsaniti, Rafael Sarmiento, Ivano Bonesana, Alejandro Dabin"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," CSCS \u2013 Swiss National Supercomputing Centre, PSI Center for Scientific Computing, Theory, and Data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11634",children:"https://arxiv.org/pdf/2512.11634"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents FirecREST v2, a redesigned RESTful API for programmatic access to HPC resources, focusing on integrating enhanced security and high throughput. Through systematic performance testing, the authors identified and addressed bottlenecks in proxy-based APIs, achieving a ~100x performance improvement. The key conclusion is that a ground-up architectural redesign was necessary to meet growing user demands for scalable and secure HPC resource access."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251215] Hypergraph based Multi-Party Payment Channel"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [blockchain scalability], [hypergraph, payment channel networks, multi-party channels, off-chain scaling, DAG updates]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ayush Nainwal, Atharva Kamble, Nitin Awathare"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Indian Institute of Technology, Jodhpur; Indian Institute of Technology, Bombay"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11775",children:"https://arxiv.org/pdf/2512.11775"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces traditional bilateral channels with collectively funded hyperedges to enable leaderless, concurrent payments. This design addresses liquidity fragmentation and channel depletion in existing payment networks. An implementation demonstrates a high transaction success rate of approximately 94%, highlighting the robustness of the approach."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 16'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] Bandwidth-constrained Variational Message Encoding for Cooperative Multi-agent Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11179",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11463",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] Rethinking Expert Trajectory Utilization in LLM Post-training ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11470",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] In-Context Multi-Objective Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11114",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11277",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11345",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11421",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] Multi-Objective Reinforcement Learning for Large-Scale Mixed Traffic Control ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11247",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] Three methods, one problem: Classical and AI approaches to no-three-in-line ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11469",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11169",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11270",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11342",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11391",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11558",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] Agile Flight Emerges from Multi-Agent Competitive Racing ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11781",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:['[arXiv251215] Marti-5: A Mathematical Model of "Self in the World" as a First Step Toward Self-Awareness ',(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.10985",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 6'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] A Scalable Multi-GPU Framework for Encrypted Large-Model Inference ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11269",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11187",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] Refining Graphical Neural Network Predictions Using Flow Matching for Optimal Power Flow with Constraint-Satisfaction Guarantee ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11127",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11282",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11342",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251215] Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11587",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"2025-12-16",children:"2025-12-16"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"cs.DC total: 26"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Strategic Server Deployment under Uncertainty in Mobile Edge Computing"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [mobile edge computing], [stochastic bilevel optimization, submodular maximization, greedy algorithm]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Duc A. Tran, Dung Truong, Duy Le"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Massachusetts Boston"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12532",children:"https://arxiv.org/pdf/2512.12532"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper formulates the strategic server deployment problem in mobile edge computing as a stochastic bilevel optimization and solves it by approximating the objective with submodular functions, enabling the use of greedy algorithms. The proposed method is evaluated with real-world data and shows significant performance improvements over alternative approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] A Conflict-Aware Resource Management Framework for the Computing Continuum"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [deep reinforcement learning, resource orchestration, conflict resolution, kubernetes, computing continuum]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Vlad Popescu-Vifor, Ilir Murturi, Praveen Kumar Donta, Schahram Dustdar"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," TU Wien, Stockholm University, University of Prishtina, ICREA"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12299",children:"https://arxiv.org/pdf/2512.12299"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a framework for adaptive conflict resolution in resource orchestration for the computing continuum using Deep Reinforcement Learning (DRL). The framework was prototyped on a Kubernetes testbed and shows efficient resource reallocation and adaptive learning, providing a scalable and resilient solution for conflict-aware orchestration."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Evaluating Asynchronous Semantics in Trace-Discovered Resilience Models: A Case Study on the OpenTelemetry Demo"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [fault-tolerance], [distributed tracing, Monte Carlo simulation, service dependency graph, chaos engineering, OpenTelemetry]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Anatoly A. Krasnovsky"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Innopolis University, MB3R Lab"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12314",children:"https://arxiv.org/pdf/2512.12314"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a method to model microservice resilience by automatically deriving a service dependency graph from OpenTelemetry traces and using Monte Carlo simulation to estimate endpoint availability under failures. It concludes that for the studied system, adding explicit asynchronous semantics for Kafka message queues provides negligible benefit to availability predictions, suggesting a simpler connectivity-only model is sufficient."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Beyond right or wrong : towards redefining adaptive learning indicators in virtual learning environments"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [educational technology], [Systematic Literature Review, adaptive learning, learning indicators, motivation, emotions, physiological responses, brain imaging, prior knowledge]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Andreia dos Santos Sachete, Alba Valeria de SantAnna de Freitas Loiola, Fabio Diniz Rossi, Jose Valdeni de Lima, Raquel Salcedo Gomes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Federal Institute Farroupilha, Federal University of Rio Grande do Sul"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12105",children:"https://arxiv.org/pdf/2512.12105"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper conducts a Systematic Literature Review to identify learning indicators beyond correctness for adaptive learning in Virtual Learning Environments. It concludes that indicators such as motivation, emotions, physiological responses, brain imaging, and prior knowledge are crucial for a more comprehensive assessment and adaptive training."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Reputation-Based Leader Election under Partial Synchrony: Towards a Protocol-Independent Abstraction with Enhanced Guarantees"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [distributed consensus], [Sliding Window Leader Election, reputation-based election, Byzantine fault tolerance, partial synchrony, protocol-independent abstraction]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Xuyang Liu, Zijian Zhang, Zhen Li, Jiahang Sun, Jiamou Liu, Peng Jiang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Beijing Institute of Technology, The University of Auckland"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12409",children:"https://arxiv.org/pdf/2512.12409"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a protocol-independent abstraction for leader election under partial synchrony and proposes the Sliding Window Leader Election (SWLE) mechanism, which uses consensus-behavior-based reputation scores to dynamically nominate leaders. The authors prove SWLE's correctness and demonstrate through deployment that it significantly outperforms the state-of-the-art in throughput, latency, and Byzantine leader frequency under common faults."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Accelerating Sparse Matrix-Matrix Multiplication on GPUs with Processing Near HBMs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [processing near memory, hardware-software co-design, sparse matrix-matrix multiplication, graph neural networks, hash-based multi-phase SpGEMM, acceleration of indirect memory access]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Shiju Li, Younghoon Min, Hane Yie, Hoshik Kim, Soohong Ahn, Joonseop Sim, Chul-Ho Lee, Jongryool Kim"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," SK hynix, Texas State University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12036",children:"https://arxiv.org/pdf/2512.12036"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a hardware-software co-designed framework for Sparse General Matrix-Matrix Multiplication (SpGEMM) on GPUs, featuring a novel near-memory processing technique called Acceleration of Indirect Memory Access (AIA). The method demonstrates significant performance improvements over state-of-the-art software libraries like cuSPARSE, particularly for graph analytics and Graph Neural Network training workloads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training], [reinforcement learning, heterogeneous environments, scheduling algorithm, multi-level search, successive halving, Proximal Policy Optimization (PPO)]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yongjun He, Shuai Zhang, Jiading Gai, Xiyuan Zhang, Boran Han, Bernie Wang, Huzefa Rangwala, George Karypis"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," ETH Zurich, Amazon Web Services (AWS)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12476",children:"https://arxiv.org/pdf/2512.12476"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents HetRL, a distributed system for efficient reinforcement learning (RL) training of large language models (LLMs) in environments with heterogeneous GPUs and networks. It formulates the scheduling problem as a constrained joint optimization and introduces a novel algorithm using multi-level search and successive halving. The evaluation shows HetRL achieves up to 9.17x higher throughput than state-of-the-art systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [bottleneck-aware tensor parallelism, low-rank bottleneck architectures, online-RMSNorm, linear layer grouping, low-rank activation checkpointing, 3D parallelism]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Zhengyang Wang, Ziyue Liu, Ruijie Zhang, Avinash Maurya, Paul Hovland, Bogdan Nicolae, Franck Cappello, Zheng Zhang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of California, Santa Barbara, Argonne National Laboratory"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12131",children:"https://arxiv.org/pdf/2512.12131"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes BOOST, a training framework optimized for low-rank bottleneck architectures in large language models. It introduces bottleneck-aware tensor parallelism and other optimizations to reduce communication overhead and improve GPU utilization. The framework achieves significant speedup over both full-rank baselines and low-rank models with standard parallelism."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Low-Rank Adaptation (LoRA), dynamic rank adaptation, NUMA-aware scheduling, embedding tables (EMTs), delta-update, parameter synchronization]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Wenjun Yu, Sitian Chen, Cheng Chen, Amelie Chi Zhou"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Hong Kong Baptist University, ByteDance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12295",children:"https://arxiv.org/pdf/2512.12295"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces LiveUpdate, a system that co-locates Low-Rank Adaptation (LoRA) trainers within inference nodes to eliminate inter-cluster synchronization for Deep Learning Recommendation Models. It uses dynamic rank adaptation and NUMA-aware resource scheduling to minimize memory overhead and latency impact. LiveUpdate reduces update costs by 2x and improves accuracy compared to delta-update baselines, enabling near-zero-overhead model freshness."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] MVP-ORAM: a Wait-free Concurrent ORAM for Confidential BFT Storage"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [secure storage systems], [Oblivious RAM, wait-free concurrency, Byzantine fault tolerance, secret sharing, access pattern hiding]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Robin Vassantlal, Hasan Heydari, Bernardo Ferreira, Alysson Bessani"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," LASIGE, Faculdade de Ci\xeancias, Universidade de Lisboa"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12006",children:"https://arxiv.org/pdf/2512.12006"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents MVP-ORAM, a wait-free concurrent Oblivious RAM protocol that allows clients to perform operations independently without locks or trusted proxies, merging conflicting updates on the fly. It introduces a weaker, practical notion of obliviousness for skewed workloads and demonstrates that the protocol can be integrated into confidential Byzantine fault-tolerant data stores. The prototype implementation shows it can process hundreds of 4KB accesses per second in cloud environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Ethical Risk Analysis of L2 Rollups"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [blockchain], [layer 2 rollups, ethical risk analysis, governance, upgrade timing, exit windows, proposer liveness, forced inclusion, data availability]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Georgy Ishmaev, Emmanuelle Anceaume, Davide Frey, Fran\xe7ois Ta\xefani"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Univ Rennes, Inria, CNRS, IRISA"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12732",children:"https://arxiv.org/pdf/2512.12732"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper adapts Ethical Risk Analysis to evaluate Layer 2 rollup architectures, using a role-based taxonomy and empirical data from 129 projects and incident reports. It finds widespread ethical hazards, such as instant upgrades without exit windows and proposer controls that can freeze withdrawals, and provides ethically grounded mitigation suggestions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [sketched random matrix theory, marchenko-pastur law, frequent directions sketching, byzantine-robust aggregation, decentralized federated learning, blockchain]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Animesh Mishra"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Department of Computer Science & Engineering (implied from author email domain: snu.edu.in, likely Shiv Nadar University)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12617",children:"https://arxiv.org/pdf/2512.12617"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes Spectral Sentinel, a Byzantine-robust decentralized federated learning framework that detects malicious clients by analyzing anomalies in the eigenspectra of gradient covariances using sketched random matrix theory. It achieves scalable detection for large models with provably optimal convergence and demonstrates practical deployment on blockchain networks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Fine-Grained Energy Prediction For Parallellized LLM Inference With PIE-P"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [energy prediction, multi-GPU inference, tensor parallelism, pipeline parallelism, data parallelism, inter-GPU communication modeling]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Anurag Dutt, Young Won Choi, Avirup Sil, Anshul Gandhi, Aruna Balasubramanian, Niranjan Balasubramanian"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Stony Brook University, IBM Research"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12801",children:"https://arxiv.org/pdf/2512.12801"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces PIE-P, a fine-grained energy prediction framework for multi-GPU LLM inference that addresses challenges like non-deterministic communication and parallelization overheads. It uses precise sampling and detailed modeling of inter-GPU communication to predict energy consumption across tensor, pipeline, and data parallelism strategies. The evaluation shows that PIE-P provides accurate predictions and significantly outperforms existing baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] SPARK: Igniting Communication-Efficient Decentralized Learning via Stage-wise Projected NTK and Accelerated Regularization"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [decentralized federated learning, neural tangent kernel, random projection, stage-wise annealed distillation, Nesterov momentum, Jacobian compression]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Li Xia"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Minzu University of China"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12737",children:"https://arxiv.org/pdf/2512.12737"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SPARK, a method for communication-efficient decentralized federated learning that combines random projection to compress Jacobian matrices, stage-wise annealed distillation to counteract compression noise, and Nesterov momentum for acceleration. It achieves a 98.7% reduction in communication overhead while maintaining convergence speed and superior accuracy, enabling practical deployment in bandwidth-limited edge environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] PROSERVE: Unified Multi-Priority Request Scheduling for LLM Serving"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [request scheduling, service gain maximization, two-tier scheduling, SlideBatching, GoRouting, SLO attainment]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Weizhe Huang, Tao Peng, Tongxuan Liu, Donghe Jin, Xianzhe Dong, Ke Zhang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," JD.com, USTC"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12928",children:"https://arxiv.org/pdf/2512.12928"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes PROSERVE, a two-tier scheduling framework for LLM serving that maximizes service gain by jointly optimizing for SLO attainment and client priorities. It uses SlideBatching for dynamic batch formation and GoRouting for gain-oriented request dispatching across distributed instances. Evaluations show it improves system gain by up to 35% and SLO attainment by up to 52% over baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Towards Secure Decentralized Applications and Consensus Protocols in Blockchains (on Selfish Mining, Undercutting Attacks, DAG-Based Blockchains, E-Voting, Cryptocurrency Wallets, Secure-Logging, and CBDC)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [blockchain security], [selfish mining, undercutting attacks, DAG-based blockchains, e-voting, cryptocurrency wallets, secure-logging, CBDC]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ivan Homoliak"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Brno University of Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13213",children:"https://arxiv.org/pdf/2512.13213"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This thesis proposes a holistic security reference architecture for analyzing vulnerabilities in blockchains and decentralized applications. It contributes methods for securing consensus protocols, wallets, e-voting, and logging, and introduces an interoperability protocol for central bank digital currencies. The main conclusion is that a comprehensive, standardized approach is needed to address the complex security challenges in full-stack decentralized systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [kernel fusion, distributed shared memory, inter-core connection, dataflow analysis, cost modeling]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ziyu Huang, Yangjie Zhou, Zihan Liu, Xinhao Luo, Yijia Diao, Minyi Guo, Jidong Zhai, Yu Feng, Chen Zhang, Anbang Wu, Jingwen Leng"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, Shanghai Qi Zhi Institute, National University of Singapore, Tsinghua University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12949",children:"https://arxiv.org/pdf/2512.12949"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," FlashFuser is a compiler framework that expands kernel fusion for memory-bound deep learning workloads by utilizing the inter-core Distributed Shared Memory (DSM) on modern GPUs. It introduces a DSM communication abstraction, a dataflow analyzer, and a unified search engine to optimize execution plans. Evaluation on an NVIDIA H100 shows it reduces memory access by 58% and achieves significant kernel speedups over existing libraries and compilers."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Toward Self-Healing Networks-on-Chip: RL-Driven Routing in 2D Torus Architectures"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [reinforcement learning, adaptive routing, 2D torus, networks-on-chip, packet delivery ratio, fault-adaptive score]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Mohammad Walid Charrwi, Zaid Hussain"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Kuwait University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13096",children:"https://arxiv.org/pdf/2512.13096"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a reinforcement learning (RL) based routing strategy for 2D torus Networks-on-Chip, where each router acts as an agent learning to forward packets based on network state. Compared to a traditional adaptive routing baseline, the RL method achieves significantly higher throughput and maintains better packet delivery under increasing node faults by exploiting path diversity. The results demonstrate that RL-driven routing offers superior throughput and fault resilience for torus NoC architectures."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Temporal parallelisation of continuous-time maximum-a-posteriori trajectory estimation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [optimal control and state estimation], [parallel-in-time, maximum-a-posteriori, Onsager-Machlup functional, associative scan, Kalman-Bucy filter, Rauch-Tung-Striebel smoother]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Hassan Razavi, \xc1ngel F. Garc\xeda-Fern\xe1ndez, Simo S\xe4rkk\xe4"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Aalto University, Universidad Polit\xb4ecnica de Madrid"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13319",children:"https://arxiv.org/pdf/2512.13319"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a parallel-in-time method for computing continuous-time MAP trajectory estimates by reformulating the problem as an optimal control problem using the Onsager-Machlup functional. The solution leverages parallel associative scan algorithms, extending to parallel Kalman-Bucy filters and smoothers for linear Gaussian cases and nonlinear models via Taylor expansions. GPU experiments demonstrate the framework achieves significant computational speedup while maintaining the accuracy of sequential algorithms."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] SPARS: A Reinforcement Learning-Enabled Simulator for Power Management in HPC Job Scheduling"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [reinforcement learning, discrete-event simulation, job scheduling, power management, energy efficiency, EASY Backfilling]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Muhammad Alfian Amrizal, Raka Satya Prasasta, Santana Yuda Pradata, Kadek Gemilang Santiyuda, Reza Pulungan, Hiroyuki Takizawa"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Universitas Gadjah Mada, Universitas Ahmad Dahlan, Institut Bisnis dan Teknologi Indonesia, National Taiwan University of Science and Technology, Tohoku University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13268",children:"https://arxiv.org/pdf/2512.13268"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces SPARS, a simulator that integrates reinforcement learning with discrete-event simulation to manage node power states in HPC job scheduling. It enables the evaluation of power-aware scheduling policies, showing a trade-off between energy savings and job performance. The tool is designed to be lightweight, reproducible, and extensible for developing sustainable HPC operations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] SIGMA: An AI-Empowered Training Stack on Early-Life Hardware"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [early-life hardware, fault-tolerance, distributed training, MoE, model training stability, accelerator utilization, training stack]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Lei Qu, Lianhai Ren, Peng Cheng, Rui Gao, Ruizhe Wang, Tianyu Chen, Xiao Liu, Xingjian Zhang, Yeyun Gong, Yifan Xiong, Yucheng Ding, Yuting Jiang, Zhenghao Lin, Zhongxin Guo, Ziyue Yang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Microsoft Research"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13488",children:"https://arxiv.org/pdf/2512.13488"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI accelerators. It introduces the LUCIA TRAINING PLATFORM (LTP) for system reliability and the LUCIA TRAINING FRAMEWORK (LTF) for stable and efficient model training, successfully demonstrating its capability by training a 200B MoE model with high utilization and minimal stability incidents. The work establishes a robust and cost-effective benchmark for AI infrastructure on emerging hardware."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] TreeVQA: A Tree-Structured Execution Framework for Shot Reduction in Variational Quantum Algorithms"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [variational quantum algorithms, tree-structured execution, shot reduction, VQA wrapper, joint execution, progressive branching]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yuewen Hou, Dhanvi Bharadwaj, Gokul Subramanian Ravi"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Michigan"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12068",children:"https://arxiv.org/pdf/2512.12068"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes TreeVQA, a tree-structured execution framework that reduces the shot count in Variational Quantum Algorithms by initially executing tasks jointly and branching only when their quantum executions diverge. Evaluations show the framework achieves significant shot count reductions, averaging 25.9x and over 100x for large-scale problems, while maintaining the same target accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [tile-based architecture, automated deployment, intermediate representation, network on chip, collective primitives, GEMM]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Aofeng Shen, Chi Zhang, Yakup Budanaz, Alexandru Calotoiu, Torsten Hoefler, Luca Benini"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," ETH Zurich"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13638",children:"https://arxiv.org/pdf/2512.13638"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper proposes "Design in Tiles (DiT)", an automated framework for deploying General Matrix Multiplication (GEMM) on tile-based many-PE accelerators by connecting a deployment toolchain with a configurable executable model. It demonstrates that this approach achieves higher PE utilization than an expert-tuned library on an NVIDIA GH200, resulting in a 1.2-2.0x speedup across diverse matrix shapes.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] SEDULity: A Proof-of-Learning Framework for Distributed and Secure Blockchains with Efficient Useful Work"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [proof-of-learning, consensus mechanism, incentive mechanism, distributed training, blockchain security]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Weihang Cao, Mustafa Doger, Sennur Ulukus"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Maryland, College Park"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13666",children:"https://arxiv.org/pdf/2512.13666"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes SEDULity, a Proof-of-Learning framework that replaces the traditional Proof-of-Work puzzle in blockchains with the useful work of training machine learning models. The framework is designed to be secure, efficient, and fully distributed, and it includes an incentive mechanism to ensure honest participation. Theoretical analysis and simulations demonstrate that the system can efficiently train models while maintaining blockchain security."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] Janus: Disaggregating Attention and Experts for Scalable MoE Inference"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [mixture-of-experts, disaggregated inference, adaptive communication, lightweight scheduler, fine-grained resource management, GPU kernel]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Zhexiang Zhang, Ye Wang, Xiangyu Wang, Yumiao Zhao, Jingzhe Jiang, Qizhen Weng, Shaohuai Shi, Yin Chen, Minchen Yu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," The Chinese University of Hong Kong, Shenzhen; Institute of Artificial Intelligence (TeleAI), China Telecom; Shenzhen Loop Area Institute; Harbin Institute of Technology, Shenzhen"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13525",children:"https://arxiv.org/pdf/2512.13525"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Janus, a scalable inference system for Mixture-of-Experts (MoE) models that disaggregates attention and expert modules onto separate GPU sub-clusters for independent management and scaling. Its key innovations include an adaptive two-phase communication scheme, a GPU-kernel-based scheduler for expert load balancing, and fine-grained resource management. The evaluation shows Janus achieves up to 3.9x higher per-GPU throughput than state-of-the-art systems while meeting latency requirements."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251216] astroCAMP: A Community Benchmark and Co-Design Framework for Sustainable SKA-Scale Radio Imaging"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [high-performance computing], [co-design, benchmarking, energy efficiency, carbon-aware computing, radio interferometric imaging, hardware-software co-design, multi-objective optimization]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Denisa-Andreea Constantinescu, Rub\xe9n Rodr\xedguez \xc1lvarez, Jacques Morin, Etienne Orliac, Micka\xebl Dardaillon, Sunrise Wang, Hugo Miomandre, Miguel Pe\xf3n-Quir\xf3s, Jean-Fran\xe7ois Nezan, David Atienza"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," EPFL, Univ Rennes, INSA Rennes, CNRS, Univ C\xf4te d\u2019Azur, OCA"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13591",children:"https://arxiv.org/pdf/2512.13591"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces astroCAMP, a framework for hardware-software co-design and benchmarking to improve the energy efficiency and computational performance of radio-interferometric imaging pipelines for the SKA telescope. It provides standardized metrics, datasets, and a multi-objective formulation to explore trade-offs between scientific fidelity, performance, and sustainability. The authors conclude that such a framework is essential for achieving sustainable, high-performance computing within the SKA's strict power and operational constraints."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 38'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11902",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11944",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11872",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Policy Gradient Algorithms for Age-of-Information Cost Minimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11990",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11862",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12230",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] ElasticVR: Elastic Task Computing in Multi-User Multi-Connectivity Wireless Virtual Reality (VR) Systems ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12366",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12046",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Reinforcement Learning for Latent-Space Thinking in LLMs ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11816",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Learning to Extract Context for Context-Aware LLM Inference ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11986",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11930",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12548",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Coupled Variational Reinforcement Learning for Language Model General Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12576",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12690",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12706",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Self-Motivated Growing Neural Network for Adaptive Architecture via Local Structural Plasticity ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12713",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Information-Consistent Language Model Recommendations through Group Relative Policy Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12858",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12922",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12987",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13043",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Deep Q-Learning-Based Intelligent Scheduling for ETL Optimization in Heterogeneous Data Environments ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13060",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13070",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13093",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13095",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13106",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13159",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] SACn: Soft Actor-Critic with n-step Returns ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13165",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13240",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13278",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13293",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3) ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13356",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13359",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Differentiable Evolutionary Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13399",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13510",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Memory in the Age of AI Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13564",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Image Diffusion Preview with Consistency Solver ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13592",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13607",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] A Scientific Reasoning Model for Organic Synthesis Procedure Generation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13668",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 20'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11935",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Neural CDEs as Correctors for Learned Time Series Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12116",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12142",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11893",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12365",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11870",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11920",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.11851",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12545",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Torch Geometric Pool: the Pytorch library for pooling in Graph Neural Networks ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12642",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Intelligent Scientific Literature Explorer using Machine Learning (ISLE) ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12760",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] HaShiFlex: A High-Throughput Hardened Shifter DNN Accelerator with Fine-Tuning Flexibility ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12847",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Distillation of Discrete Diffusion by Exact Conditional Distribution Matching ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12889",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12929",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12930",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13194",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13359",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Image Diffusion Preview with Consistency Solver ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13592",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12284",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251216] Efficient Level-Crossing Probability Calculation for Gaussian Process Modeled Data ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.12442",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"2025-12-17",children:"2025-12-17"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"cs.DC total: 7"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251217] Improving Slow Transfer Predictions: Generative Methods Compared"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [class imbalance, data augmentation, oversampling, SMOTE, CTGAN, stratified sampling]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Jacob Taegon Kim, Alex Sim, Kesheng Wu, Jinoh Kim"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of California, Berkeley, Lawrence Berkeley National Laboratory, Texas A&M University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14522",children:"https://arxiv.org/pdf/2512.14522"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper compares various data augmentation strategies, including traditional oversampling and generative techniques like CTGAN, to address class imbalance in predicting slow data transfers in scientific networks. It finds that while augmentation can improve performance, increasing the imbalance ratio does not lead to significant gains. The main conclusion is that even advanced generative methods do not substantially outperform simple stratified sampling for this task."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251217] Cornserve: Efficiently Serving Any-to-Any Multimodal Models"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [computation graph, disaggregation, distributed runtime, planner, Any-to-Any models]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Jeff J. Ma, Jae-Won Chung, Jisang Ahn, Yizhuo Liang, Akshay Jajoo, Myungjin Lee, Mosharaf Chowdhury"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Michigan, University of Southern California, Cisco Research"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14098",children:"https://arxiv.org/pdf/2512.14098"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents Cornserve, a serving system for Any-to-Any multimodal models. Its core method involves a planner that automatically generates optimized deployment plans by potentially disaggregating the model graph, and a distributed runtime to execute it. The main conclusion is that Cornserve efficiently handles heterogeneous model serving, significantly improving throughput and reducing latency compared to existing solutions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251217] Performance and Stability of Barrier Mode Parallel Systems with Heterogeneous and Redundant Jobs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [barrier execution mode, parallel task scheduling, stability analysis, performance bounds, Apache Spark]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Brenton Walker, Markus Fidler"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Institute of Communications Technology, Leibniz Universit\xe4t Hannover"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14445",children:"https://arxiv.org/pdf/2512.14445"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper analyzes the performance and stability penalties in parallel systems that use barrier synchronization, as in Apache Spark's Barrier Execution Mode. It develops models for systems with heterogeneous jobs and redundant tasks, deriving stability bounds and validating them against simulations and real Spark benchmarks. The main conclusion is that barriers introduce idle periods and scheduling overhead, which degrade system performance and stability compared to asynchronous execution."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251217] A Hybrid Reactive-Proactive Auto-scaling Algorithm for SLA-Constrained Edge Computing"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [auto-scaling, Kubernetes, LSTM, hybrid reactive-proactive algorithm, SLA compliance]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Suhrid Gupta, Muhammed Tawfiqul Islam, Rajkumar Buyya"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," The University of Melbourne"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14290",children:"https://arxiv.org/pdf/2512.14290"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a hybrid auto-scaling algorithm for edge computing that combines a machine learning-based proactive component using LSTM for demand prediction with a reactive component for immediate adjustments based on current resource utilization. The algorithm is integrated into Kubernetes and evaluated in an edge environment. The results show that the proposed solution reduces the SLA violation rate to 6%, significantly outperforming existing solutions which had violation rates up to 23%."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251217] Real-Time Service Subscription and Adaptive Offloading Control in Vehicular Edge Computing"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [deadline-constrained task offloading, resource allocation, linear program rounding, local-ratio techniques, approximation algorithm, vehicular edge computing, simulator]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Chuanchao Gao, Arvind Easwaran"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Nanyang Technological University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14002",children:"https://arxiv.org/pdf/2512.14002"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SARound, an approximation algorithm based on linear program rounding and local-ratio techniques, to solve a deadline-constrained task offloading and resource allocation problem in Vehicular Edge Computing. It also develops an online service subscription framework and a simulator called VecSim. Experimental results show that SARound outperforms existing baselines and improves the best-known approximation ratio for the problem."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251217] Q-IRIS: The Evolution of the IRIS Task-Based Runtime to Enable Classical-Quantum Workflows"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [quantum-classical hybrid runtime], [task-based runtime, quantum intermediate representation (QIR), quantum circuit cutting, heterogeneous scheduling, XACC, IRIS]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Narasinga Rao Miniskar, Mohammad Alaul Haque Monil, Elaine Wong, Vicente Leyton-Ortega, Jeffrey S. Vetter, Seth R. Johnson, Travis S. Humble"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Oak Ridge National Laboratory"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13931",children:"https://arxiv.org/pdf/2512.13931"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents Q-IRIS, a proof-of-concept hybrid execution framework that integrates the IRIS task-based runtime with the XACC quantum programming framework via QIR-EE to orchestrate classical and quantum workloads. It demonstrates the framework by asynchronously scheduling quantum tasks and using quantum circuit cutting to improve simulator throughput. The work concludes by outlining key challenges for scaling such hybrid runtimes, including coordinated scheduling and managing classical-quantum interactions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv251217] PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [structured pruning, ADMM, gradient compression, hierarchical communication, leader-follower model, buffer compaction]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Alireza Olama, Andreas Lundell, Izzat El Hajj, Johan Lilius, Jerker Bj\xf6rkqvist"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," \xc5bo Akademi University, American University of Beirut, CSC - IT Center for Science"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14628",children:"https://arxiv.org/pdf/2512.14628"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," PruneX is a distributed training system that co-designs a hierarchical structured pruning algorithm (H-SADMM) with cluster hierarchy to enforce node-level sparsity, enabling compacted gradient communication. This reduces inter-node bandwidth usage by ~60% and achieves better strong scaling speedup (6.75x) compared to dense baselines and Top-K compression on a 64-GPU supercomputer."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 18'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14157",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement Learning in Ride-Hailing ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13727",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Understanding and Improving Hyperbolic Deep Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14202",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Context-Picker: Dynamic context selection using multi-stage reinforcement learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14465",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13726",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14044",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13715",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Adaptive digital twins for predictive decision-making: Online Bayesian learning of transition dynamics ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13919",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14329",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14069",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] A Threshold-Triggered Deep Q-Network-Based Framework for Self-Healing in Autonomic Software-Defined IIoT-Edge Networks ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14297",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] A First-Order Logic-Based Alternative to Reward Models in RLHF ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14100",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Explainable reinforcement learning from human feedback to improve alignment ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13837",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Group-Theoretic Reinforcement Learning of Dynamical Decoupling Sequences ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13890",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14698",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13714",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14617",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14031",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 9'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13978",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14461",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14069",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13930",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14092",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13886",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13700",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] Informing Acquisition Functions via Foundation Models for Molecular Discovery ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.13935",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv251217] SuperWing: a comprehensive transonic wing dataset for data-driven aerodynamic design ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2512.14397",children:"link"})]}),"\n"]})]})}function h(i={}){const{wrapper:n}={...(0,a.R)(),...i.components};return n?(0,s.jsx)(n,{...i,children:(0,s.jsx)(d,{...i})}):d(i)}}}]);