"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[935],{8435:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251208-20251214","title":"20251208-20251214","description":"2025-12-08","source":"@site/docs/daily/20251208-20251214.md","sourceDirName":"daily","slug":"/daily/20251208-20251214","permalink":"/daily/20251208-20251214","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1765251861000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251201-20251207","permalink":"/daily/20251201-20251207"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=n(4848),t=n(8453);const a={},o="20251208-20251214",l={},c=[{value:"2025-12-08",id:"2025-12-08",level:2},{value:"2025-12-09",id:"2025-12-09",level:2}];function d(e){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"20251208-20251214",children:"20251208-20251214"})}),"\n",(0,s.jsx)(i.h2,{id:"2025-12-08",children:"2025-12-08"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"cs.DC total: 7"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251208] NVLang: Unified Static Typing for Actor-Based Concurrency on the BEAM"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [programming languages], [algebraic data types, Hindley-Milner type inference, typed process identifiers, typed futures, actor model, static typing, Core Erlang]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Miguel de Oliveira Guerreiro"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Lisbon"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05224",children:"https://arxiv.org/pdf/2512.05224"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," NVLang introduces a statically typed functional language for the BEAM virtual machine that uses algebraic data types to encode actor message protocols and extends Hindley-Milner type inference to enforce them at compile time. The main conclusion is that this approach eliminates message-passing errors while preserving the actor model's simplicity and maintains interoperability with the existing Erlang ecosystem."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251208] Metronome: Differentiated Delay Scheduling for Serverless Functions"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [delay scheduling, random forest regression, locality-aware scheduling, SLA compliance]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Zhuangbin Chen, Juzheng Zheng, Zibin Zheng"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Sun Yat-sen University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05703",children:"https://arxiv.org/pdf/2512.05703"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes Metronome, a differentiated delay scheduling framework for serverless functions that uses an online Random Forest Regression model to predict function execution times and identify optimal locality-aware nodes. The implementation on OpenLambda demonstrates that Metronome significantly reduces mean function execution time by 64.88%-95.83% compared to baselines while maintaining SLA compliance under high concurrency."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251208] Model Gateway: Model Management Platform for Model-Driven Drug Discovery"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [MLOps, LLM Agents, Generative AI, model registry, dynamic consensus model, asynchronous execution, cloud computing]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Yan-Shiun Wu, Nathan A. Morin"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Eli Lilly and Company"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05462",children:"https://arxiv.org/pdf/2512.05462"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper presents Model Gateway, a cloud-based MLOps platform for managing machine learning and scientific computational models in drug discovery. It integrates LLM Agents and Generative AI tools to handle tasks like model registration and asynchronous execution. The platform demonstrated scalability with a 0% failure rate under high load and is concluded to be a fundamental component for accelerating model-driven drug development."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251208] FedGMR: Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [federated learning, model heterogeneity, gradual model restoration, mask-aware aggregation, asynchronous training, bandwidth-constrained clients]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Chengjie Ma, Seungeun Oh, Jihong Park, Seong-Lyun Kim"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Yonsei University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05372",children:"https://arxiv.org/pdf/2512.05372"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes FedGMR, a federated learning method that progressively increases the density of clients' sub-models during training to keep bandwidth-constrained clients effective. It introduces a mask-aware aggregation rule for asynchronous, model-heterogeneous settings and provides convergence guarantees. Experiments show FedGMR achieves faster convergence and higher accuracy, especially under high heterogeneity and non-IID data."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251208] Compiler-supported reduced precision and AoS-SoA transformations for heterogeneous hardware"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [compiler optimization], [AoS-SoA transformation, reduced precision, GPU offloading, compiler annotations, unified memory]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Pawel K. Radtke, Tobias Weinzierl"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Durham University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05516",children:"https://arxiv.org/pdf/2512.05516"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces compiler annotations to support AoS-to-SoA data layout transformations and reduced precision for particle simulation codes on heterogeneous GPU platforms. It evaluates strategies for orchestrating these conversions between CPU and GPU, finding that NVIDIA's G200 platform achieved a speedup of around 2.6, while AMD's MI300A showed more robust but less pronounced benefits. The authors conclude that their compiler-based techniques are broadly applicable to Lagrangian codes and other domains."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251208] Are Bus-Mounted Edge Servers Feasible?"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [edge computing, vehicular networks], [bus-mounted edge servers, server placement, greedy heuristic algorithm, trace-driven simulation, coverage optimization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Xuezhi Li, Jiancong He, Ming Xie, Xuyang Chen, Le Chang, Li Jiang, Gui Gui"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Guangdong University of Technology, Central South University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05543",children:"https://arxiv.org/pdf/2512.05543"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper studies the feasibility of deploying edge servers on public buses to serve vehicular networks, using real-world mobility traces and a greedy algorithm to select buses that maximize coverage of demand points under budget constraints. The trace-driven simulations show that bus-mounted servers can effectively handle dynamic user demand, leading to the conclusion that they are a feasible, beneficial, and valuable solution for urban areas."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251208] InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [diffusion inference], [invariance caching, deterministic sampling, quantile-based change metrics, re-sampling correction, step-first caching, layer-wise caching]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Zihao Wu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Peking University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05134",children:"https://arxiv.org/pdf/2512.05134"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces InvarDiff, a training-free acceleration method for diffusion models that exploits temporal invariance across timesteps and layers to cache and reuse intermediate features. It uses a pre-computed binary cache plan and re-sampling correction to reduce redundant computation. Experiments show the method achieves 2\u20133\xd7 speed-ups on models like DiT and FLUX with minimal impact on output quality."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 9'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05946",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05962",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05711",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] Hierarchical Reinforcement Learning for the Dynamic VNE with Alternatives Problem ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05207",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05291",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05172",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05753",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05591",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] Enhancing Deep Deterministic Policy Gradients on Continuous Control Tasks with Decoupled Prioritized Experience Replay ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05320",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 9'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05341",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] Documenting SME Processes with Conversational AI: From Tacit Knowledge to BPMN ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05122",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] Trusted AI Agents in the Cloud ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05951",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05887",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05216",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] AI & Human Co-Improvement for Safer Co-Superintelligence ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05356",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05916",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] UniFS: Unified Multi-Contrast MRI Reconstruction via Frequency-Spatial Fusion ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05481",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251208] To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05925",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"2025-12-09",children:"2025-12-09"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"cs.DC total: 17"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm training], [A-3PO, proximal policy approximation, decoupled loss, asynchronous RL, trust region]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Xiaocan Li, Shiliang Wu, Zheng Shen"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Huawei Canada"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06547",children:"https://arxiv.org/pdf/2512.06547"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes A-3PO, a method that accelerates asynchronous LLM training by approximating the proximal policy through simple interpolation instead of an extra forward pass. This eliminates a computational bottleneck, reducing training time by 18% while maintaining performance comparable to methods like PPO."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] Cloud Revolution: Tracing the Origins and Rise of Cloud Computing"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [cloud computing], [virtualization, distributed systems, high-speed networking, edge computing, quantum computing services, data privacy, cloud security]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Deepa Gurung, S M Zia Ur Rashid, Zain ul Abdeen, Suman Rath"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Joongbu University, The University of Tulsa, Virginia Tech"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06800",children:"https://arxiv.org/pdf/2512.06800"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper reexamines the historical evolution of cloud computing, analyzing its technological foundations and economic impact. It concludes that cloud computing is a rapidly changing paradigm whose future depends on balancing scalability, openness, and trust, while highlighting challenges like security and vendor lock-in."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] PIR-DSN: A Decentralized Storage Network Supporting Private Information Retrieval"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [decentralized storage network], [Private Information Retrieval, secure mapping, Byzantine-robust retrieval, file replication]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Jiahao Zhang, Minghui Xu, Hechuan Guo, Xiuzhen Cheng"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Shandong University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07189",children:"https://arxiv.org/pdf/2512.07189"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces PIR-DSN, a decentralized storage network protocol that integrates Private Information Retrieval to protect user privacy during file retrieval. It uses a novel secure mapping method and file replication across miners to enable private, verifiable, and Byzantine-robust operations. The evaluation shows it achieves practical overhead and throughput, making it viable for privacy-sensitive applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] Optimizing video analytics inference pipelines: a case study"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [GPU acceleration, parallel processing, vectorized clustering, memory-efficient post-processing]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Saeid Ghafouri, Yuming Ding, Katerine Diaz Chito, Jes\xfas Martinez del Rinc\xf3n, Niamh O'Connell, Hans Vandierendonck"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Queen's University Belfast"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07009",children:"https://arxiv.org/pdf/2512.07009"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents a case study on optimizing a video analytics pipeline for poultry welfare monitoring through system-level improvements. The core methods include multi-level parallelization, GPU acceleration, vectorized clustering, and memory-efficient post-processing. These optimizations achieved up to a 2x speedup without compromising accuracy, highlighting practical strategies for building high-throughput, low-latency video inference systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] Stable-MoE: Lyapunov-based Token Routing for Distributed Mixture-of-Experts Training over Edge Networks"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm training], [Lyapunov optimization, token routing, mixture of experts, distributed training, edge networks]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Long Shi, Bingyan Ou, Kang Wei, Weihao Zhu, Zhe Wang, Zhiyong Chen"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Nanjing University of Science and Technology, Southeast University, Shanghai Jiao Tong University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06784",children:"https://arxiv.org/pdf/2512.06784"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes Stable-MoE, a Lyapunov-based token routing framework for distributed Mixture-of-Experts training over resource-heterogeneous edge networks. It formulates a stochastic optimization problem to maximize throughput and gating consistency while ensuring queue stability, transforming it into online per-slot decisions. Experiments show it outperforms baselines with significant gains in system throughput and test accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm inference], [shared system-level cache, predictive cache management, dead-block prediction, cache bypassing, thrashing mitigation, RTL implementation]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Zhongchun Zhou, Chengtao Lai, Yuhang Gu, Wei Zhang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," The Hong Kong University of Science and Technology, Southeast University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07312",children:"https://arxiv.org/pdf/2512.07312"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes DCO, a dynamic cache orchestration scheme for multi-core AI accelerators that uses dataflow information to guide predictive cache replacement and bypassing decisions. The method achieves up to 1.80x speedup over conventional caches and is implemented in RTL with a small area footprint, demonstrating the effectiveness of shared cache designs for LLM inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] A Chunked-Object Pattern for Multi-Region Large Payload Storage in Managed NoSQL Databases"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [distributed databases], [chunked-object pattern, multi-region replication, NoSQL, DynamoDB Global Tables, active-active architecture, time-to-consistency]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Manideep Reddy Chinthareddy"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Independent researcher (based on email domain)"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06852",children:"https://arxiv.org/pdf/2512.06852"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"}),' The paper proposes a "chunked-object" pattern to store large payloads by splitting them into ordered chunks within a managed NoSQL database itself, avoiding the need for separate object storage. This method eliminates replication lag hazards and reduces cross-region consistency time by keeping data and metadata within a single consistency domain, as demonstrated in a production system handling over 200,000 transactions per hour.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] ContinuumConductor : Decentralized Process Mining on the Edge-Cloud Continuum"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [distributed computing], [process mining, edge-cloud continuum, decentralized framework, IoT, IIoT]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Hendrik Reiter, Janick Edinger, Martin Kabierski, Agnes Koschmider, Olaf Landsiedel, Arvid Lepsien, Xixi Lu, Andrea Marrella, Estefania Serral, Stefan Schulte, Florian Tschorsch, Matthias Weidlich, Wilhelm Hasselbring"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Kiel University, University of Hamburg, University of Vienna, University of Bayreuth, Hamburg University of Technology, Utrecht University, Sapienza University of Rome, KU Leuven, Dresden University of Technology, Humboldt University of Berlin"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07280",children:"https://arxiv.org/pdf/2512.07280"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces ContinuumConductor, a layered decision framework for decentralizing process mining tasks across the edge-cloud continuum. It analyzes trade-offs between centralization and decentralization for steps like preprocessing and discovery to enable privacy-preserving and resource-efficient analysis. The method is demonstrated in a real-world inland port use case, establishing a foundation for computing-aware process mining in IIoT systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm inference], [vector lookup table, ultra-low-bit quantization, parallel inference, cache-aware streamed lookup, LUT-based inference]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Xiangyu Li, Chengyu Yin, Weijun Wang, Jianyu Wei, Ting Cao, Yunxin Liu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Tsinghua University, Beijing Jiaotong University, University of Science and Technology of China"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06443",children:"https://arxiv.org/pdf/2512.06443"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes Vec-LUT, a new vector lookup table paradigm to address the memory bandwidth underutilization of scalar LUT-based inference during parallel LLM inference on edge devices. It introduces a unified LUT across tokens and techniques like a vector LUT-centric tensor layout and cache-aware streamed lookup. Evaluations show Vec-LUT achieves up to 4.2x speedup over state-of-the-art baselines on various edge devices."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] A Performance Analyzer for a Public Cloud's ML-Augmented VM Allocator"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [bi-level optimization, VM placement, bin-packing, performance analysis, live migration]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Roozbeh Bostandoost, Pooria Namyar, Siva Kesava Reddy Kakarla, Ryan Beckett, Santiago Segarra, Eli Cortez, Ankur Mallick, Kevin Hsieh, Rodrigo Fonseca, Mohammad Hajiesmaili, Behnaz Arzani"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Massachusetts Amherst, Microsoft Research, Rice University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07750",children:"https://arxiv.org/pdf/2512.07750"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces SANJESH, a tool that uses a novel bi-level optimization approach to analyze the performance of cloud systems that use multiple interacting ML models, such as for VM placement. It finds scenarios where these models can cause significantly worse performance (up to 4x) than what simpler simulation-based methods detect."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] Bandwidth-Aware Network Topology Optimization for Decentralized Learning"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [ADMM, conjugate gradient, Mixed-Integer SDP, bandwidth allocation, consensus speed]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Yipeng Shen, Zehan Zhu, Yan Huang, Changzhi Yan, Cheng Zhuo, Jinming Xu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Zhejiang University, KTH Royal Institute of Technology"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07536",children:"https://arxiv.org/pdf/2512.07536"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes a bandwidth-aware network topology optimization framework for decentralized learning, which formulates the problem as a Mixed-Integer SDP and solves it using an ADMM-based method with a conjugate gradient substep. The resulting optimized topologies achieve higher consensus speed and reduce training time for decentralized learning tasks compared to benchmark topologies."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] Designing Co-operation in Systems of Hierarchical, Multi-objective Schedulers for Stream Processing"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [stream processing], [hierarchical schedulers, load balancing, constraint solver, Service Level Objective (SLO), greedy scheduler]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Animesh Dangwal, Yufeng Jiang, Charlie Arnold, Jun Fan, Mohamed Bassem, Aish Rajagopal"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Meta Platforms"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07792",children:"https://arxiv.org/pdf/2512.07792"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper designs a hierarchical, multi-objective scheduler system called SPTLB for automated load balancing across tiers in Meta's stream processing infrastructure. It uses a constraint solver to efficiently move applications while respecting properties like CPU/memory utilization and SLOs, and shows it outperforms a baseline greedy scheduler."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [reinforcement learning, deep learning, anomaly detection, honeynet, dynamic container deployment]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Lukas Johannes M\xf6ller"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Georgia Institute of Technology"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07827",children:"https://arxiv.org/pdf/2512.07827"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes ADLAH, an adaptive honeynet architecture that uses a reinforcement learning agent to dynamically escalate suspicious sessions from low-interaction to high-interaction honeypots. The core method integrates deep learning for anomaly detection with autonomous infrastructure orchestration to efficiently capture high-value threat intelligence. The main conclusion is that this AI-driven, multi-layered approach provides a practical and cost-effective blueprint for analyzing automated bot attacks and generating actionable cybersecurity insights."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] Communication-Efficient Serving for Video Diffusion Models with Latent Parallelism"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [diffusion inference], [latent parallelism, patch-aligned overlapping partition, position-aware latent reconstruction, communication-efficient serving, video diffusion models]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Zhiyuan Wu, Shuai Wang, Li Chen, Kaihui Gao, Dan Li, Yanyu Ren, Qiming Zhang, Yong Wang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Tsinghua University, Zhongguancun Laboratory, ZTE Corporation"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07350",children:"https://arxiv.org/pdf/2512.07350"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes Latent Parallelism (LP), a novel parallelism strategy for serving Video Diffusion Models (VDMs) that reduces communication overhead by dynamically rotating partition dimensions across diffusion timesteps. It introduces a patch-aligned overlapping partition and position-aware latent reconstruction to maintain generation quality. Experiments show LP reduces communication by up to 97% compared to baseline methods while preserving video quality."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] Quantifying the Carbon Reduction of DAG Workloads: A Job Shop Scheduling Perspective"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [carbon-aware scheduling, job shop scheduling, DAG workloads, makespan, carbon intensity, energy efficiency]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Roozbeh Bostandoost, Adam Lechowicz, Walid A. Hanafy, Prashant Shenoy, Mohammad Hajiesmaili"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Massachusetts Amherst"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07799",children:"https://arxiv.org/pdf/2512.07799"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper models the scheduling of batch workloads with task dependencies (DAGs) as a flexible job-shop scheduling problem to quantify the maximum potential carbon reduction. Using an offline solver, the study finds that a dependency-aware approach can reduce carbon emissions by up to 25% on average without increasing the optimal makespan, though it highlights a trade-off between carbon savings, energy use, and completion time."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] Otus Supercomputer"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [high-performance computing], [supercomputer, HPC cluster, CPU nodes, GPU nodes, FPGA nodes, energy efficiency, Top500, Green500]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Sadaf Ehtesabi, Manoar Hossain, Tobias Kenter, Andreas Krawinkel, Holger Nitsche, Lukas Ostermann, Christian Plessl, Heinrich Riebler, Stefan Rohde, Robert Schade, Michael Schwarz, Jens Simon, Nils Winnwa, Alex Wiens, Xin Wu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Paderborn University, Paderborn Center for Parallel Computing (PC2)"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07401",children:"https://arxiv.org/pdf/2512.07401"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper provides a comprehensive overview of the Otus supercomputer, detailing its hardware, software, and system integration for energy-efficient operation. It describes Otus as a high-performance computing cluster that complements the Noctua 2 system with increased computing power and specialized node types. The main conclusion is that Otus is a highly energy-efficient system, ranking 5th on the Green500 list with its GPU partition."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251209] Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [edge-cloud disaggregated architecture, scene segmentation and clustering, hierarchical memory, threshold-based progressive sampling]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Shengyuan Ye, Bei Ouyang, Tianyi Qian, Liekang Zeng, Mu Yuan, Xiaowen Chu, Weijie Hong, Xu Chen"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Sun Yat-sen University, The Chinese University of Hong Kong, HKUST (Guangzhou), Shenzhen Smart City Communications Co., Ltd."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07344",children:"https://arxiv.org/pdf/2512.07344"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes Venus, a system for efficient online video understanding that uses an edge-cloud architecture to process video streams. Its core method involves building a hierarchical memory from keyframes at the edge and using a progressive sampling algorithm for retrieval. The system achieves a significant speedup in response latency while maintaining high reasoning accuracy."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 37'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06920",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06392",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] The Role of Entropy in Visual Grounding: Analysis and Optimization ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06726",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06102",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07135",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06747",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06218",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06982",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07342",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06912",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] LightSearcher: Efficient DeepSearch via Experiential Memory ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06653",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06471",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Auto-exploration for online reinforcement learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06244",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06250",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Towards Robust Protective Perturbation against DeepFake Face Swapping ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07228",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07266",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Learning Without Time-Based Embodiment Resets in Soft-Actor Critic ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06252",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06835",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06240",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06533",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05969",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06060",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06859",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07200",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06917",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06351",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07417",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07419",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07437",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07497",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Model-Based Reinforcement Learning Under Confounding ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07528",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] ReLaX: Reasoning with Latent Exploration for Large Reasoning Models ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07558",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07611",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07631",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07761",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Learning to Hedge Swaptions ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06639",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Statistical analysis of Inverse Entropy-regularized Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06956",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 25'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06676",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] FOAM: Blocked State Folding for Memory-Efficient LLM Training ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07112",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06392",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Block Sparse Flash Attention ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07011",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06404",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] A Patient-Doctor-NLP-System to contest inequality for less privileged ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06734",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06113",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06417",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07173",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Flash Multi-Head Feed-Forward Network ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06989",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06337",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] BitStopper: An Efficient Transformer Attention Accelerator via Stage-fusion and Early Termination ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06457",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06530",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Learning Without Time-Based Embodiment Resets in Soft-Actor Critic ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06252",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] A self-driving lab for solution-processed electrochromic thin films ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.05989",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06038",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Approximate Multiplier Induced Error Propagation in Deep Neural Networks ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.06537",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Leveraging KV Similarity for Online Structured Pruning in LLMs ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07090",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07371",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07437",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Artificial Intelligence and Nuclear Weapons Proliferation: The Technological Arms Race for (In)visibility ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07487",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07490",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07650",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] A scalable and real-time neural decoder for topological quantum codes ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07737",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251209] LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2512.07808",children:"link"})]}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>o});var r=n(6540);const s={},t=r.createContext(s);function a(e){const i=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(t.Provider,{value:i},e.children)}}}]);