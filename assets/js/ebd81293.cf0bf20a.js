"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[91],{4341:(n,i,e)=>{e.r(i),e.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>r,toc:()=>h});const r=JSON.parse('{"id":"daily/20250915-20250921","title":"20250915-20250921","description":"2025-09-15","source":"@site/docs/daily/20250915-20250921.md","sourceDirName":"daily","slug":"/daily/20250915-20250921","permalink":"/daily/20250915-20250921","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1761311518000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20250908-20250914","permalink":"/daily/20250908-20250914"},"next":{"title":"20250922-20250928","permalink":"/daily/20250922-20250928"}}');var s=e(4848),a=e(8453);const t={},o="20250915-20250921",l={},h=[{value:"2025-09-15",id:"2025-09-15",level:2},{value:"2025-09-16",id:"2025-09-16",level:2},{value:"2025-09-17",id:"2025-09-17",level:2},{value:"2025-09-18",id:"2025-09-18",level:2},{value:"2025-09-19",id:"2025-09-19",level:2},{value:"2025-09-20",id:"2025-09-20",level:2},{value:"2025-09-21",id:"2025-09-21",level:2}];function c(n){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"20250915-20250921",children:"20250915-20250921"})}),"\n",(0,s.jsx)(i.h2,{id:"2025-09-15",children:"2025-09-15"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] Machine Learning-Driven Predictive Resource Management in Complex\nScience Workflows"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [scheduling], [machine learning, resource management, workflow management, predictive modeling, distributed computing, science workflows]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Tasnuva Chowdhury, Tadashi Maeno, Fatih Furkan Akman, Joseph Boudreau, Sankha Dutta, Shengyu Feng, Adolfy Hoisie, Kuan-Chieh Hsu, Raees Khan, Jaehyung Kim, Ozgur O. Kilic, Scott Klasky, Alexei Klimentov, Tatiana Korchuganova, Verena Ingrid Martinez Outschoorn, Paul Nilsson, David K. Park, Norbert Podhorszki, Yihui Ren, John Rembrandt Steele, Fr\xe9d\xe9ric Suter, Sairam Sri Vatsavai, Torre Wenaus, Wei Yang, Yiming Yang, Shinjae Yoo"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Brookhaven National Laboratory"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.11512v1",children:"http://arxiv.org/pdf/2509.11512v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces a machine learning pipeline within the PanDA workflow management system to predict resource requirements for complex science workflows. The models overcome limited upfront knowledge challenges by forecasting key resource needs. This enables proactive decision-making and enhances workflow efficiency across heterogeneous computing resources."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\nHeterogeneous Precision LLM Serving"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [KV cache management, memory fragmentation, GPU sharing, mixed-precision models, scheduling optimization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Kyungmin Bin, Seungbeom Choi, Jimyoung Son, Jieun Choi, Daseul Bae, Daehyeon Baek, Kihyo Moon, Minsung Jang, Hyojung Lee"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Samsung SDS"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.06261v2",children:"http://arxiv.org/pdf/2509.06261v2"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," FineServe introduces a precision-aware KV Slab memory management technique and two-level scheduling framework for serving mixed-precision LLMs. It dynamically allocates KV cache based on quantization characteristics and adapts batch sizes to request fluctuations. The system achieves up to 2.2\xd7 higher SLO attainment and 1.8\xd7 higher throughput compared to state-of-the-art GPU sharing systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code\nTranslation in HPC"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [finetuning], [parallel code translation, HPC, LLM evaluation, compiler-guided repair]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Tomer Bitan, Tal Kadosh, Erel Kaplan, Shira Meiri, Le Chen, Peter Morales, Niranjan Hasabnis, Gal Oren"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Technion"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.12136v1",children:"http://arxiv.org/pdf/2509.12136v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," UniPar introduces a systematic framework for evaluating LLM-based parallel code translation between serial code, CUDA, and OpenMP using strategies like fine-tuning and compiler-guided repair. The approach significantly improves performance over off-the-shelf models, achieving up to 2x gains in compilation and functional correctness. This demonstrates the potential of enhanced LLM methodologies for high-performance computing code translation tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous\nGPUs"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models inference], [diffusion models, parallel inference, heterogeneous GPUs, workload balancing, scheduling]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Han Liang, Jiahui Zhou, Zicheng Zhou, Xiaoxi Zhang, Xu Chen"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Sun Yat-sen University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.04719v2",children:"http://arxiv.org/pdf/2509.04719v2"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," STADI introduces a hybrid scheduler with fine-grained parallelism across temporal and spatial dimensions, using computation-aware step allocation and elastic patch parallelism for heterogeneous GPUs. The method achieves up to 45% latency reduction and improved resource utilization compared to existing approaches. Experiments demonstrate effective load balancing and bottleneck mitigation in heterogeneous multi-GPU environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] Distributed 3D Gaussian Splatting for High-Resolution Isosurface\nVisualization"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models training], [distributed computing, 3D Gaussian Splatting, high-performance computing, scientific visualization, parallel training]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Mengjiao Han, Andres Sewell, Joseph Insley, Janet Knowles, Victor A. Mateevitsi, Michael E. Papka, Steve Petruzza, Silvio Rizzi"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Argonne National Laboratory"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.12138v1",children:"http://arxiv.org/pdf/2509.12138v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents a distributed 3D Gaussian Splatting pipeline for high-performance computing systems that partitions data across nodes, trains Gaussian splats in parallel using multi-nodes and multi-GPUs, and merges splats for global rendering. The method employs ghost cells at partition boundaries and background masks to eliminate artifacts during distributed processing. Results demonstrate up to 3X speedup across 8 nodes while preserving image quality, enabling scalable visualization of large-scale scientific data for future in situ applications."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"2025-09-16",children:"2025-09-16"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] Scaling Up Throughput-oriented LLM Inference Applications on\nHeterogeneous Opportunistic GPU Clusters with Pervasive Context Management"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [throughput-oriented inference, opportunistic GPU clusters, pervasive context management, dynamic resource allocation, context reuse]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Thanh Son Phung, Douglas Thain"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Notre Dame"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.13201v1",children:"http://arxiv.org/pdf/2509.13201v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces pervasive context management, a technique that exploits common computational contexts in LLM applications to enable efficient context reuse on heterogeneous opportunistic GPU clusters. The approach allows dynamic resource allocation for throughput-oriented LLM inference, avoiding static allocations and long queues. Evaluation shows this method reduces execution time by 98.1% compared to traditional approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] AI Factories: It's time to rethink the Cloud-HPC divide"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [AI Factories, HPC-cloud integration, dual-stack approach, EuroHPC, cloud-native technologies]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Pedro Garcia Lopez, Daniel Barcelona Pons, Marcin Copik, Torsten Hoefler, Eduardo Qui\xf1ones, Maciej Malawski, Peter Pietzutch, Alberto Marti, Thomas Ohlson Timoudas, Aleksander Slominski"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," URV, ETH Zurich, BSC, AGH, ICL, OpenNebula, RISE, IBM Research"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.12849v1",children:"http://arxiv.org/pdf/2509.12849v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a dual-stack approach integrating HPC and cloud-native technologies in supercomputers to bridge the performance-usability gap for AI Factories. It advocates combining HPC's raw performance with cloud's accessibility through service-oriented interfaces. The convergence aims to support Sovereign AI initiatives by enabling both high-performance computing and user-friendly AI services on shared infrastructure."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models training], [diffusion models, weather forecasting, climate modeling, high-performance computing, transformer architecture]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," V\xe4in\xf6 Hatanp\xe4\xe4, Eugene Ku, Jason Stock, Murali Emani, Sam Foreman, Chunyong Jung, Sandeep Madireddy, Tung Nguyen, Varuni Sastry, Ray A. O. Sinurat, Sam Wheeler, Huihuo Zheng, Troy Arcomano, Venkatram Vishwanath, Rao Kotamarthi"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Argonne National Laboratory"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.13523v1",children:"http://arxiv.org/pdf/2509.13523v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces AERIS, a billion-parameter Swin diffusion transformer for Earth system modeling, and SWiPe, a parallelization technique for efficient scaling. It demonstrates state-of-the-art performance on the Aurora supercomputer, outperforming traditional weather forecasting systems and maintaining stability for seasonal predictions up to 90 days. This highlights the potential of large-scale diffusion models for climate and weather prediction tasks."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"2025-09-17",children:"2025-09-17"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] Julia GraphBLAS with Nonblocking Execution"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [kernels], [GraphBLAS, nonblocking execution, Julia programming language, DAG optimization, PageRank]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Pascal Costanza, Timothy G. Mattson, Raye Kimmerer, Benjamin Brock"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Lawrence Berkeley National Laboratory"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.14211v1",children:"http://arxiv.org/pdf/2509.14211v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper implements GraphBLAS with aggressive nonblocking execution using Julia's language features, which simplifies building operation DAGs for optimization. The approach enables function fusion, object elision, and parallelism exploitation while preserving DAG semantics. Current work demonstrates potential through PageRank implementation, showing Julia's advantages for nonblocking GraphBLAS execution."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [kernels], [GPU performance, Zero-Knowledge Proofs, Number-Theoretic Transform, Multi-Scalar Multiplication, cryptographic proofs]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Tarunesh Verma, Yichao Yuan, Nishil Talati, Todd Austin"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Michigan"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.22684v1",children:"http://arxiv.org/pdf/2509.22684v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents ZKProphet, a comprehensive performance analysis of Zero-Knowledge Proofs on GPUs, identifying Number-Theoretic Transform as the primary bottleneck after MSM optimizations. The study reveals that current NTT implementations underutilize GPU resources and lack architectural optimizations like asynchronous operations. The authors propose runtime parameter tuning and alternative data representations as methods to improve ZKP performance on GPU architectures."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] GPU Programming for AI Workflow Development on AWS SageMaker: An\nInstructional Approach"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models training], [GPU programming, AWS SageMaker, parallel computing, AI workflow development, HPC profiling]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Sriram Srinivasan, Hamdan Alabsi, Rand Obeidat, Nithisha Ponnala, Azene Zenebe"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Bowie State University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.13703v1",children:"http://arxiv.org/pdf/2509.13703v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents a specialized course design that teaches GPU programming and parallel computing concepts using AWS SageMaker for developing AI agents. The course evaluation showed AWS served as an effective platform for practical GPU programming, experiential learning enhanced technical proficiency, and the approach strengthened students' problem-solving skills. The findings advocate for integrating parallel computing into STEM education to prepare students for compute-intensive fields."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] FLAME: A Serving System Optimized for Large-Scale Generative\nRecommendation with Efficiency"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models inference], [generative recommendation, serving system, CPU-GPU heterogeneous computing, memory optimization, kernel fusion, request orchestration]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Xianwen Guo, Bin Huang, Xiaomeng Wu, Guanlin Wu, Fangjian Li, Shijia Wang, Qiang Xiao, Chuanjiang Luo, Yong Li"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Netease Cloud Music"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.22681v1",children:"http://arxiv.org/pdf/2509.22681v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," FLAME introduces a CPU-GPU heterogeneous serving system with three key modules: Proximal Data Accelerator for memory optimization, Fused Kernel Engine for computation acceleration using TensorRT, and Dynamic Stream Orchestrator for request coordination. The system achieves significant performance improvements including 1.9x throughput gain, 1.7x latency reduction, and 4.6x-6.1x computation speedup. This enables efficient large-scale deployment of generative recommendation models that require high computational resources."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"2025-09-18",children:"2025-09-18"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] Conditional Prior-based Non-stationary Channel Estimation Using\nAccelerated Diffusion Models"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models inference], [diffusion models, channel estimation, non-stationary channels, MIMO systems, wireless communication]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Muhammad Ahmed Mohsin, Ahsan Bilal, Muhammad Umer, Asad Aali, Muhammad Ali Jamshed, Dean F. Hougen, John M. Cioffi"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Stanford University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.15182v1",children:"http://arxiv.org/pdf/2509.15182v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a conditional prior diffusion method for non-stationary channel estimation that uses a temporal encoder with cross-time attention and accelerated diffusion with SNR-matched initialization. The approach achieves lower NMSE than baseline methods across all SNRs on 3GPP benchmarks, demonstrating stable performance and strong high-SNR fidelity in wireless channel estimation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] Channel Prediction under Network Distribution Shift Using Continual\nLearning-based Loss Regularization"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models training], [continual learning, channel prediction, wireless networks, loss regularization, catastrophic forgetting, Elastic Weight Consolidation, Synaptic Intelligence]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Muhammad Ibtsaam Qadir, Muhammad Ali Jamshed, Dean F. Hougen, John M. Cioffi"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Stanford University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.15192v1",children:"http://arxiv.org/pdf/2509.15192v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a continual learning framework using loss regularization to address catastrophic forgetting in wireless channel prediction under network distribution shifts. The method employs Elastic Weight Consolidation and Synaptic Intelligence to preserve important parameters from previous configurations while adapting to new environments. Results show SI reduces NMSE by up to 1.8 dB with better memory efficiency than EWC, making it suitable for resource-constrained wireless systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine\nLearning"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models inference], [zero-knowledge proofs, distributed machine learning, cryptographic verification, modular framework, trust minimization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Dan Ivanov, Tristan Freiberg, Shirin Shahabi, Jonathan Gold, Haruna Isah"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Inference Labs Inc."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2508.06972v3",children:"http://arxiv.org/pdf/2508.06972v3"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"}),' DSperse introduces a modular framework for distributed ML inference that uses targeted cryptographic verification of strategic subcomputations called "slices" to avoid full-model circuitization costs. It enables flexible proof boundaries aligned with model structure and enforces consistency through audit, replication, or economic incentives. Empirical evaluation shows the framework supports scalable verification strategies with practical trust minimization.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] PCCL: Photonic circuit-switched collective communication for distributed\nML"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [photonic communication, collective communication, distributed ML, network optimization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Abhishek Vijaya Kumar, Arjun Devraj, Rachee Singh"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Cornell University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.15450v1",children:"http://arxiv.org/pdf/2509.15450v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," PCCL introduces a photonic circuit-switched collective communication library that dynamically reconfigures network topology to match communication patterns, eliminating congestion and dilation. It uses a hardware-agnostic optimization framework to balance reconfiguration delays with performance gains. The approach achieves up to 3\xd7 speedup over state-of-the-art algorithms and 1.3\xd7 improvement in end-to-end training throughput on 128 GPUs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless\nand GPU-Based Training Architectures"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models training], [serverless computing, distributed machine learning, cost-performance analysis, training architectures, communication overhead]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Amine Barrak, Fabio Petrillo, Fehmi Jaafar"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Oakland University, \xc9cole de technologie sup\xe9rieure (ETS), University of Quebec at Chicoutimi"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.14920v1",children:"http://arxiv.org/pdf/2509.14920v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper compares serverless distributed ML architectures (SPIRT, MLLess, AllReduce, ScatterReduce) with GPU-based training using CNN models on CIFAR-10. The study evaluates training time, cost, and communication efficiency, finding that while GPU training achieves fastest convergence, serverless frameworks offer cost advantages for lightweight models. Optimizations like gradient accumulation and in-database computation improve serverless performance, revealing key trade-offs between different architectures."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"2025-09-19",children:"2025-09-19"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] Characterizing the Efficiency of Distributed Training: A Power,\nPerformance, and Thermal Perspective"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM training], [distributed training, parallelism strategies, power-performance-thermal analysis, hardware utilization, optimization techniques]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Seokjin Go, Joongun Park, Spandan More, Hanjiang Wu, Irene Wang, Aaron Jezghani, Tushar Krishna, Divya Mahajan"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Georgia Institute of Technology"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.10371v2",children:"http://arxiv.org/pdf/2509.10371v2"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper comprehensively characterizes LLM training efficiency across different hardware platforms and parallelism strategies. It analyzes how hardware utilization, power consumption, and thermal behavior are affected by various configurations. The study reveals that performance depends on complex interactions between hardware, system topology, and model execution, with scale-up systems sometimes outperforming scale-out systems in communication-bound scenarios."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] RLinf: Flexible and Efficient Large-scale Reinforcement Learning via\nMacro-to-Micro Flow Transformation"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models training], [reinforcement learning, system optimization, workflow transformation, scheduling policy, elastic pipelining, context switching]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, Zixiao Huang, Mingjie Wei, Yuqing Xie, Ke Yang, Bo Dai, Zhexuan Xu, Xiangyuan Wang, Xu Fu, Zhihao Liu, Kang Chen, Weilin Liu, Gang Liu, Boxun Li, Jianlei Yang, Zhi Yang, Guohao Dai, Yu Wang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Tsinghua University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.15965v1",children:"http://arxiv.org/pdf/2509.15965v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," RLinf introduces a macro-to-micro flow transformation (M2Flow) paradigm that decomposes and recomposes RL workflows for optimized execution. It employs context switching, elastic pipelining, and profiling-guided scheduling to enhance system flexibility and efficiency. Evaluations show RLinf achieves 1.1x-2.13x speedup in training throughput compared to state-of-the-art systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] Efficient Pre-Training of LLMs via Topology-Aware Communication\nAlignment on More Than 9600 GPUs"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [scheduling], [LLM pre-training, communication patterns, network topology, GPU clusters, resource scheduling]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Guoliang He, Youhe Jiang, Wencong Xiao, Kaihua Jiang, Shuguang Wang, Jun Wang, Zixian Du, Zhuo Jiang, Xinlei Zhang, Binhang Yuan, Eiko Yoneki"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Cambridge, ByteDance Seed, HKUST"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.15940v1",children:"http://arxiv.org/pdf/2509.15940v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents Arnold, a topology-aware scheduling system that aligns LLM communication patterns with data center network topology. The system reduces communication group spread by up to 1.67x and improves end-to-end training performance by 10.6% when scaling to over 9600 GPUs. The approach effectively addresses bandwidth contention issues in large-scale LLM pre-training through intelligent resource scheduling."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] LongCat-Flash Technical Report"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM training, LLM inference], [Mixture-of-Experts, computational efficiency, agentic capabilities, scaling framework, open-source]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han, Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen, Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong, Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu, Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan, Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang, Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang, Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu, Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, Liang Shi, Lianhui Ma, Lin Qiu, Lingbin Kong, Lingtong Si, Linkun Lyu, Linsen Guo, Liqi Yang, Lizhi Yan, Mai Xia, Man Gao, Manyuan Zhang, Meng Zhou, Mengxia Shen, Mingxiang Tuo, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pingwei Sun, Qi Gu, Qianyun Li, Qingyuan Li, Qiong Huang, Qiyuan Duan, Ran Meng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shizhe Wu, Shuai Liang, Shuo Wang, Suogui Dang, Tao Fang, Tao Li, Tefeng Chen, Tianhao Bai, Tianhao Zhou, Tingwen Xie, Wei He, Wei Huang, Wei Liu, Wei Shi, Wei Wang, Wei Wu, Weikang Zhao, Wen Zan, Wenjie Shi, Xi Nan, Xi Su, Xiang Li, Xiang Mei, Xiangyang Ji, Xiangyu Xi, Xiangzhou Huang, Xianpeng Li, Xiao Fu, Xiao Liu, Xiao Wei, Xiaodong Cai, Xiaolong Chen, Xiaoqing Liu, Xiaotong Li, Xiaowei Shi, Xiaoyu Li, Xili Wang, Xin Chen, Xing Hu, Xingyu Miao, Xinyan He, Xuemiao Zhang, Xueyuan Hao, Xuezhi Cao, Xunliang Cai, Xurui Yang, Yan Feng, Yang Bai, Yang Chen, Yang Yang, Yaqi Huo, Yerui Sun, Yifan Lu, Yifan Zhang, Yipeng Zang, Yitao Zhai, Yiyang Li, Yongjing Yin, Yongkang Lv, Yongwei Zhou, Yu Yang, Yuchen Xie, Yueqing Sun, Yuewen Zheng, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunfang Tai, Yunke Zhao, Zeyang Yu, Zhao Zhang, Zhaohua Yang, Zhenchao Zhang, Zhikang Xia, Zhiye Zou, Zhizhao Zeng, Zhongda Su, Zhuofan Chen, Zijian Zhang, Ziwen Wang, Zixu Jiang, Zizhe Zhao, Zongyu Wang, Zunhai Su"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Meituan"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.01322v2",children:"http://arxiv.org/pdf/2509.01322v2"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," LongCat-Flash introduces a 560B parameter MoE model with Zero-computation Experts and Shortcut-connected MoE for dynamic computation allocation and improved inference efficiency. It was trained on over 20T tokens in 30 days using a comprehensive scaling framework, achieving competitive performance with strengths in agentic tasks. The model is open-sourced to support community research."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"2025-09-20",children:"2025-09-20"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] Trace Replay Simulation of MIT SuperCloud for Studying Optimal\nSustainability Policies"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [scheduling], [digital twin, power simulation, reinforcement learning, job scheduling, sustainability policies, trace replay]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Wesley Brewer, Matthias Maiterth, Damien Fay"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Oak Ridge National Laboratory"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.16513v1",children:"http://arxiv.org/pdf/2509.16513v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper extends the ExaDigiT digital twin framework to simulate MIT SuperCloud workloads using trace replay and reinforcement learning. The RAPS module enables experimentation with energy-aware scheduling policies through Proximal Policy Optimization. Preliminary results demonstrate the feasibility of learning sustainability-focused scheduling decisions that improve datacenter efficiency and throughput."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\nDynamic Workloads"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [parallelism, dynamic workloads, latency-throughput tradeoff, KV cache, tensor parallelism, sequence parallelism]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Mert Hidayetoglu, Aurick Qiao, Michael Wyatt, Jeff Rasley, Yuxiong He, Samyam Rajbhandari"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Snowflake AI Research"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.16495v1",children:"http://arxiv.org/pdf/2509.16495v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," Shift Parallelism combines tensor parallelism and sequence parallelism to dynamically adapt to varying traffic loads, achieving both low latency and high throughput. It enables up to 1.51x faster response times in interactive workloads and 50% higher throughput in batch workloads compared to TP-only solutions. The method demonstrates superior latency-throughput tradeoffs across diverse models and traffic patterns."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"2025-09-21",children:"2025-09-21"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] MoA-Off: Adaptive Heterogeneous Modality-Aware Offloading with\nEdge-Cloud Collaboration for Efficient Multimodal LLM Inference"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [multimodal LLM, edge-cloud collaboration, adaptive offloading, inference optimization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Zheming Yang, Qi Guo, Yunqing Hu, Chang Zhao, Chang Zhang, Jian Zhao, Wen Ji"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Institute of Computing Technology, Chinese Academy of Sciences"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.16995v1",children:"http://arxiv.org/pdf/2509.16995v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," MoA-Off introduces a modality-aware complexity estimation module and adaptive edge-cloud offloading strategy for multimodal LLM inference. It dynamically schedules workloads between edge and cloud based on input complexity and system states. The framework achieves over 30% latency reduction and 30%-65% resource savings while maintaining competitive accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2509] ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\nCaching"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [KV cache, prefix caching, SmartNIC, distributed systems, performance optimization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Xingyu Xiang, Raj Joshi, Yuhan Liu, Jiayi Yao, Chenxingyu Zhao, Junchen Jiang, Yang Zhou, Eddie Kohler, Minlan Yu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Harvard University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.16857v1",children:"http://arxiv.org/pdf/2509.16857v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," ShadowServe introduces a SmartNIC-accelerated system that separates control and data planes to eliminate interference during KV cache fetching for distributed prefix caching. It employs a chunked pipeline and minimal-copy memory management to overcome SmartNIC resource limitations. The system achieves up to 2.2\xd7 lower TPOT and 1.38\xd7 lower TTFT in low-bandwidth scenarios, improving throughput by up to 1.35\xd7 compared to state-of-the-art solutions."]}),"\n"]}),"\n"]}),"\n"]})]})}function d(n={}){const{wrapper:i}={...(0,a.R)(),...n.components};return i?(0,s.jsx)(i,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},8453:(n,i,e)=>{e.d(i,{R:()=>t,x:()=>o});var r=e(6540);const s={},a=r.createContext(s);function t(n){const i=r.useContext(a);return r.useMemo(function(){return"function"==typeof n?n(i):{...i,...n}},[i,n])}function o(n){let i;return i=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:t(n.components),r.createElement(a.Provider,{value:i},n.children)}}}]);