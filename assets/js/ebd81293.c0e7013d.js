"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[91],{4341:(n,i,e)=>{e.r(i),e.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>r,toc:()=>h});const r=JSON.parse('{"id":"daily/20250915-20250921","title":"20250915-20250921","description":"2025-09-15","source":"@site/docs/daily/20250915-20250921.md","sourceDirName":"daily","slug":"/daily/20250915-20250921","permalink":"/daily/20250915-20250921","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1761709291000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20250908-20250914","permalink":"/daily/20250908-20250914"},"next":{"title":"20250922-20250928","permalink":"/daily/20250922-20250928"}}');var a=e(4848),s=e(8453);const t={},o="20250915-20250921",l={},h=[{value:"2025-09-15",id:"2025-09-15",level:2},{value:"2025-09-16",id:"2025-09-16",level:2},{value:"2025-09-17",id:"2025-09-17",level:2},{value:"2025-09-18",id:"2025-09-18",level:2},{value:"2025-09-19",id:"2025-09-19",level:2},{value:"2025-09-20",id:"2025-09-20",level:2},{value:"2025-09-21",id:"2025-09-21",level:2}];function c(n){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.header,{children:(0,a.jsx)(i.h1,{id:"20250915-20250921",children:"20250915-20250921"})}),"\n",(0,a.jsx)(i.h2,{id:"2025-09-15",children:"2025-09-15"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\nHeterogeneous Precision LLM Serving"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [KV cache management, memory fragmentation, quantization, GPU sharing, scheduling, mixed-precision models]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Kyungmin Bin, Seungbeom Choi, Jimyoung Son, Jieun Choi, Daseul Bae, Daehyeon Baek, Kihyo Moon, Minsung Jang, Hyojung Lee"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Samsung SDS"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.06261v2",children:"http://arxiv.org/pdf/2509.06261v2"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," FineServe proposes a precision-aware KV slab memory management technique and two-level scheduling framework for serving mixed-precision LLMs. The KV slab dynamically allocates KV cache based on quantization characteristics to reduce memory fragmentation, while the scheduler optimizes model placement and batch sizing. Experiments show FineServe achieves up to 2.2\xd7 higher SLO attainment and 1.8\xd7 higher throughput compared to state-of-the-art systems."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code\nTranslation in HPC"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [finetuning], [parallel code translation, HPC, LLM evaluation, compiler-guided repair]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Tomer Bitan, Tal Kadosh, Erel Kaplan, Shira Meiri, Le Chen, Peter Morales, Niranjan Hasabnis, Gal Oren"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Technion, Ben-Gurion University, IAEC, Argonne National Laboratory, Code Metal, Stanford University"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.12136v1",children:"http://arxiv.org/pdf/2509.12136v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," UniPar introduces a systematic framework for evaluating LLM-based parallel code translation between serial code, CUDA, and OpenMP. The methodology combines fine-tuning, hyperparameter optimization, and compiler-guided repair to improve translation performance. Results show that while off-the-shelf models struggle, UniPar's approach doubles compilation and functional correctness rates, achieving up to 69% compilation and 33% correctness."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] Machine Learning-Driven Predictive Resource Management in Complex\nScience Workflows"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [scheduling], [machine learning, resource management, workflow management, predictive modeling, distributed computing]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Tasnuva Chowdhury, Tadashi Maeno, Fatih Furkan Akman, Joseph Boudreau, Sankha Dutta, Shengyu Feng, Adolfy Hoisie, Kuan-Chieh Hsu, Raees Khan, Jaehyung Kim, Ozgur O. Kilic, Scott Klasky, Alexei Klimentov, Tatiana Korchuganova, Verena Ingrid Martinez Outschoorn, Paul Nilsson, David K. Park, Norbert Podhorszki, Yihui Ren, John Rembrandt Steele, Fr\xe9d\xe9ric Suter, Sairam Sri Vatsavai, Torre Wenaus, Wei Yang, Yiming Yang, Shinjae Yoo"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Brookhaven National Laboratory, Oak Ridge National Laboratory, University of Pittsburgh, Carnegie Mellon University, University of Massachusetts Amherst, SLAC National Accelerator Laboratory"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.11512v1",children:"http://arxiv.org/pdf/2509.11512v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces a machine learning pipeline within the PanDA workflow management system to predict resource requirements for complex science workflows. The models overcome challenges of limited upfront knowledge by forecasting key resource needs using advanced ML techniques. This enables proactive decision-making and enhances workflow efficiency across heterogeneous computing resources."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous\nGPUs"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models inference], [diffusion models, parallel inference, heterogeneous GPUs, load balancing, scheduling]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Han Liang, Jiahui Zhou, Zicheng Zhou, Xiaoxi Zhang, Xu Chen"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Sun Yat-sen University"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.04719v2",children:"http://arxiv.org/pdf/2509.04719v2"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," STADI introduces a hybrid scheduler with fine-grained parallelism across temporal and spatial dimensions for diffusion model inference. It uses computation-aware step allocation and elastic patch parallelism to balance workloads across heterogeneous GPUs. Experiments show up to 45% latency reduction and improved resource utilization compared to existing methods."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] Distributed 3D Gaussian Splatting for High-Resolution Isosurface\nVisualization"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models training], [3D Gaussian Splatting, distributed computing, scientific visualization, high-performance computing, parallel training]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Mengjiao Han, Andres Sewell, Joseph Insley, Janet Knowles, Victor A. Mateevitsi, Michael E. Papka, Steve Petruzza, Silvio Rizzi"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Argonne National Laboratory, Utah State University, University of Illinois Chicago"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.12138v1",children:"http://arxiv.org/pdf/2509.12138v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents a distributed 3D Gaussian Splatting pipeline for high-performance computing systems that partitions data across nodes, trains Gaussian splats in parallel using multi-node/multi-GPU setups, and merges results for global rendering. The method incorporates ghost cells and background masking to eliminate rendering artifacts while maintaining image quality. Results demonstrate up to 3x speedup across 8 nodes while enabling scalable visualization of large-scale scientific datasets."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"2025-09-16",children:"2025-09-16"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] Scaling Up Throughput-oriented LLM Inference Applications on\nHeterogeneous Opportunistic GPU Clusters with Pervasive Context Management"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [throughput-oriented inference, opportunistic resource allocation, context management, GPU clusters, dynamic resource pooling]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Thanh Son Phung, Douglas Thain"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," University of Notre Dame"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.13201v1",children:"http://arxiv.org/pdf/2509.13201v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes pervasive context management to enable efficient LLM inference on heterogeneous opportunistic GPU clusters by reusing computational contexts across dynamically allocated resources. This approach allows throughput-oriented applications to utilize idle resources without requiring static allocations. Evaluation shows 98.1% execution time reduction compared to traditional approaches."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] AI Factories: It's time to rethink the Cloud-HPC divide"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [AI Factories, HPC-cloud convergence, dual-stack approach, EuroHPC, Sovereign AI]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Pedro Garcia Lopez, Daniel Barcelona Pons, Marcin Copik, Torsten Hoefler, Eduardo Qui\xf1ones, Maciej Malawski, Peter Pietzutch, Alberto Marti, Thomas Ohlson Timoudas, Aleksander Slominski"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," URV, ETH Zurich, BSC, Sano & AGH, ICL, OpenNebula, RISE, IBM Research"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.12849v1",children:"http://arxiv.org/pdf/2509.12849v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a dual-stack approach integrating HPC and cloud-native technologies in supercomputers to bridge the performance-usability divide for AI Factories. The method combines high-performance computing with cloud-native tools like Kubernetes to create service-oriented AI platforms. The conclusion advocates for this convergence to support Sovereign AI initiatives while maintaining both computational power and accessibility."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models training], [diffusion transformer, weather forecasting, high-performance computing, ensemble prediction, climate modeling]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," V\xe4in\xf6 Hatanp\xe4\xe4, Eugene Ku, Jason Stock, Murali Emani, Sam Foreman, Chunyong Jung, Sandeep Madireddy, Tung Nguyen, Varuni Sastry, Ray A. O. Sinurat, Sam Wheeler, Huihuo Zheng, Troy Arcomano, Venkatram Vishwanath, Rao Kotamarthi"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Argonne National Laboratory, University of Chicago, Allen Institute for AI, University of California, Los Angeles"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.13523v1",children:"http://arxiv.org/pdf/2509.13523v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces AERIS, a billion-parameter Swin diffusion transformer for Earth system modeling, and SWiPe, a parallelization technique for efficient scaling. It achieves record computational performance on the Aurora supercomputer and demonstrates superior forecasting skill compared to traditional methods, maintaining stability for seasonal-scale predictions up to 90 days. This work highlights the potential of large-scale diffusion models for advancing weather and climate prediction capabilities."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"2025-09-17",children:"2025-09-17"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] Julia GraphBLAS with Nonblocking Execution"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [kernels], [GraphBLAS, nonblocking execution, Julia programming, multi-stage programming, DAG optimization]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Pascal Costanza, Timothy G. Mattson, Raye Kimmerer, Benjamin Brock"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," University of Bristol, National Energy Research Scientific Computing Center, Lawrence Berkeley National Laboratory, Intel"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.14211v1",children:"http://arxiv.org/pdf/2509.14211v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper implements nonblocking GraphBLAS execution in Julia using multi-stage programming to generate and compile symbolic representations of computation DAGs at runtime. The approach enables aggressive optimization strategies like operation fusion and parallelization while maintaining DAG semantics. Julia's language features significantly simplify implementing this nonblocking execution model, demonstrating promising results for GraphBLAS operations like PageRank."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] FLAME: A Serving System Optimized for Large-Scale Generative\nRecommendation with Efficiency"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models inference], [generative recommendation, serving system optimization, CPU-GPU heterogeneous computing, memory optimization, kernel fusion, request orchestration]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Xianwen Guo, Bin Huang, Xiaomeng Wu, Guanlin Wu, Fangjian Li, Shijia Wang, Qiang Xiao, Chuanjiang Luo, Yong Li"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Netease Cloud Music"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.22681v1",children:"http://arxiv.org/pdf/2509.22681v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," FLAME introduces an optimized serving system for large-scale generative recommendation models using CPU-GPU heterogeneous hardware, memory optimization through PDA, kernel fusion via FKE, and dynamic request coordination with DSO. The system achieves significant performance improvements including 1.9x throughput gain, 1.7x latency reduction, and 4.6x-6.1x computation speedup. Comprehensive evaluations confirm FLAME effectively supports large-scale online deployment of GR models with remarkable system performance enhancements."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] GPU Programming for AI Workflow Development on AWS SageMaker: An\nInstructional Approach"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models training], [GPU programming, AI workflow development, AWS SageMaker, parallel computing, HPC profiling, RAG]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Sriram Srinivasan, Hamdan Alabsi, Rand Obeidat, Nithisha Ponnala, Azene Zenebe"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Bowie State University"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.13703v1",children:"http://arxiv.org/pdf/2509.13703v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper presents a specialized course design that teaches GPU programming and parallel computing concepts using AWS SageMaker for developing AI workflows. Students gained hands-on experience with cloud GPU instances, parallel algorithms, and performance optimization tools. Evaluation showed AWS provided an effective platform for practical learning, experiential methods enhanced technical skills, and the course improved problem-solving abilities through exposure to performance analysis tools."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [kernels], [GPU performance, Zero-Knowledge Proofs, Number-Theoretic Transform, Multi-Scalar Multiplication, cryptographic protocols]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Tarunesh Verma, Yichao Yuan, Nishil Talati, Todd Austin"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," University of Michigan"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.22684v1",children:"http://arxiv.org/pdf/2509.22684v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," ZKProphet presents a comprehensive performance analysis of Zero-Knowledge Proofs on GPUs, identifying NTT as the primary bottleneck after MSM optimizations. The study shows that current implementations underutilize GPU resources and lack architectural optimizations like asynchronous operations. It provides optimization guidelines including parameter tuning and alternative data representations to improve ZKP performance on GPUs."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"2025-09-18",children:"2025-09-18"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] Conditional Prior-based Non-stationary Channel Estimation Using\nAccelerated Diffusion Models"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models inference], [diffusion models, channel estimation, non-stationary channels, MIMO systems, wireless communication]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Muhammad Ahmed Mohsin, Ahsan Bilal, Muhammad Umer, Asad Aali, Muhammad Ali Jamshed, Dean F. Hougen, John M. Cioffi"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Stanford University, University of Oklahoma, University of Glasgow"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.15182v1",children:"http://arxiv.org/pdf/2509.15182v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a conditional diffusion model for non-stationary channel estimation that uses historical channel observations to guide the denoising process. The method employs cross-time attention and SNR-matched initialization to accelerate inference while maintaining accuracy. Experimental results show superior performance over traditional baselines across all SNR ranges with strong high-SNR fidelity."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] Channel Prediction under Network Distribution Shift Using Continual\nLearning-based Loss Regularization"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models training], [channel prediction, continual learning, loss regularization, wireless networks, catastrophic forgetting, EWC, Synaptic Intelligence]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Muhammad Ibtsaam Qadir, Muhammad Ali Jamshed, Dean F. Hougen, John M. Cioffi"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Stanford University, University of Oklahoma, Purdue University, University of Glasgow"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.15192v1",children:"http://arxiv.org/pdf/2509.15192v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a continual learning framework using loss regularization to address catastrophic forgetting in wireless channel prediction under network distribution shifts. The method employs Elastic Weight Consolidation and Synaptic Intelligence to preserve important parameters from previous configurations while adapting to new environments. Results show SI reduces NMSE by up to 1.8 dB with better memory efficiency than EWC, making it suitable for resource-constrained wireless infrastructure."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] PCCL: Photonic circuit-switched collective communication for distributed\nML"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [collective communication, photonic networks, distributed machine learning, network topology, GPU clusters]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Abhishek Vijaya Kumar, Arjun Devraj, Rachee Singh"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Cornell University"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.15450v1",children:"http://arxiv.org/pdf/2509.15450v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," PCCL introduces a photonic circuit-switched collective communication library that dynamically reconfigures network topology to match communication patterns, eliminating congestion and dilation. It achieves up to 3\xd7 speedup over state-of-the-art algorithms on 128 GPUs by creating direct contention-free circuits between communicating GPUs. The hardware-agnostic optimization framework makes it practical across different optical hardware with varying switching speeds."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless\nand GPU-Based Training Architectures"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models training], [serverless computing, distributed machine learning, cost-performance analysis, training architectures, communication overhead]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Amine Barrak, Fabio Petrillo, Fehmi Jaafar"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Oakland University, \xc9cole de technologie sup\xe9rieure (ETS), University of Quebec at Chicoutimi"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.14920v1",children:"http://arxiv.org/pdf/2509.14920v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper compares serverless distributed ML architectures (SPIRT, MLLess, AllReduce, ScatterReduce) with GPU-based training, evaluating training time, cost, and communication overhead. The proposed SPIRT architecture demonstrates improved performance through parallel batch processing and RedisAI integration. While GPU training achieves fastest convergence, serverless frameworks offer cost advantages for lightweight models with optimized configurations."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"2025-09-19",children:"2025-09-19"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] Characterizing the Efficiency of Distributed Training: A Power,\nPerformance, and Thermal Perspective"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM training], [distributed training, GPU clusters, parallelism strategies, power consumption, thermal behavior, scale-up vs scale-out]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Seokjin Go, Joongun Park, Spandan More, Hanjiang Wu, Irene Wang, Aaron Jezghani, Tushar Krishna, Divya Mahajan"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Georgia Institute of Technology"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.10371v2",children:"http://arxiv.org/pdf/2509.10371v2"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper comprehensively characterizes LLM training efficiency across multiple hardware platforms and parallelism strategies, analyzing power, performance and thermal behavior. The study reveals that performance depends on complex interactions between hardware, topology and model execution rather than just hardware scaling. Key findings show scale-up systems can outperform scale-out in communication-bound regimes, while certain parallelism combinations cause bandwidth underutilization and large microbatch sizes worsen thermal throttling."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] Efficient Pre-Training of LLMs via Topology-Aware Communication\nAlignment on More Than 9600 GPUs"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM training], [scheduling, communication patterns, network topology, GPU clusters, resource allocation]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Guoliang He, Youhe Jiang, Wencong Xiao, Kaihua Jiang, Shuguang Wang, Jun Wang, Zixian Du, Zhuo Jiang, Xinlei Zhang, Binhang Yuan, Eiko Yoneki"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," University of Cambridge, ByteDance Seed, HKUST"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.15940v1",children:"http://arxiv.org/pdf/2509.15940v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents Arnold, a topology-aware scheduling system that aligns LLM communication patterns with data center network topology. The system reduces communication group spread by up to 1.67x and improves end-to-end training performance by 10.6% when scaling to over 9600 GPUs, demonstrating significant efficiency gains for large-scale LLM pre-training."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] RLinf: Flexible and Efficient Large-scale Reinforcement Learning via\nMacro-to-Micro Flow Transformation"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [scheduling], [reinforcement learning, system optimization, workflow transformation, elastic pipelining, context switching]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, Zixiao Huang, Mingjie Wei, Yuqing Xie, Ke Yang, Bo Dai, Zhexuan Xu, Xiangyuan Wang, Xu Fu, Zhihao Liu, Kang Chen, Weilin Liu, Gang Liu, Boxun Li, Jianlei Yang, Zhi Yang, Guohao Dai, Yu Wang"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Tsinghua University, Zhongguancun Academy, Infinigence AI, Peking University, UC Berkeley, Beihang University, Shanghai Jiaotong University"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.15965v1",children:"http://arxiv.org/pdf/2509.15965v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," RLinf introduces a macro-to-micro flow transformation (M2Flow) paradigm that automatically decomposes and recomposes RL workflows with context switching and elastic pipelining. The system achieves 1.1x-2.13x speedup in training throughput through profiling-guided scheduling and adaptive communication. Evaluations demonstrate consistent performance improvements over state-of-the-art systems on both reasoning and embodied RL tasks."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] LongCat-Flash Technical Report"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM training], [Mixture-of-Experts, computational efficiency, agentic capabilities, scaling framework, inference throughput]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han, Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen, Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong, Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu, Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan, Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang, Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang, Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu, Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, Liang Shi, Lianhui Ma, Lin Qiu, Lingbin Kong, Lingtong Si, Linkun Lyu, Linsen Guo, Liqi Yang, Lizhi Yan, Mai Xia, Man Gao, Manyuan Zhang, Meng Zhou, Mengxia Shen, Mingxiang Tuo, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pingwei Sun, Qi Gu, Qianyun Li, Qingyuan Li, Qiong Huang, Qiyuan Duan, Ran Meng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shizhe Wu, Shuai Liang, Shuo Wang, Suogui Dang, Tao Fang, Tao Li, Tefeng Chen, Tianhao Bai, Tianhao Zhou, Tingwen Xie, Wei He, Wei Huang, Wei Liu, Wei Shi, Wei Wang, Wei Wu, Weikang Zhao, Wen Zan, Wenjie Shi, Xi Nan, Xi Su, Xiang Li, Xiang Mei, Xiangyang Ji, Xiangyu Xi, Xiangzhou Huang, Xianpeng Li, Xiao Fu, Xiao Liu, Xiao Wei, Xiaodong Cai, Xiaolong Chen, Xiaoqing Liu, Xiaotong Li, Xiaowei Shi, Xiaoyu Li, Xili Wang, Xin Chen, Xing Hu, Xingyu Miao, Xinyan He, Xuemiao Zhang, Xueyuan Hao, Xuezhi Cao, Xunliang Cai, Xurui Yang, Yan Feng, Yang Bai, Yang Chen, Yang Yang, Yaqi Huo, Yerui Sun, Yifan Lu, Yifan Zhang, Yipeng Zang, Yitao Zhai, Yiyang Li, Yongjing Yin, Yongkang Lv, Yongwei Zhou, Yu Yang, Yuchen Xie, Yueqing Sun, Yuewen Zheng, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunfang Tai, Yunke Zhao, Zeyang Yu, Zhao Zhang, Zhaohua Yang, Zhenchao Zhang, Zhikang Xia, Zhiye Zou, Zhizhao Zeng, Zhongda Su, Zhuofan Chen, Zijian Zhang, Ziwen Wang, Zixu Jiang, Zizhe Zhao, Zongyu Wang, Zunhai Su"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Meituan"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.01322v2",children:"http://arxiv.org/pdf/2509.01322v2"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," LongCat-Flash introduces a 560B parameter Mixture-of-Experts model with Zero-computation Experts and Shortcut-connected MoE for dynamic computation allocation and improved inference efficiency. It achieves training on 20+ trillion tokens in 30 days with over 100 TPS inference at $0.70 per million tokens. The model demonstrates competitive performance, especially in agentic tasks, and is open-sourced for community research."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"2025-09-20",children:"2025-09-20"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] Trace Replay Simulation of MIT SuperCloud for Studying Optimal\nSustainability Policies"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [scheduling], [digital twin, power simulation, reinforcement learning, job scheduling, energy efficiency, trace replay]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Wesley Brewer, Matthias Maiterth, Damien Fay"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Oak Ridge National Laboratory, Hewlett Packard Enterprise"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.16513v1",children:"http://arxiv.org/pdf/2509.16513v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper extends the ExaDigiT digital twin framework to simulate MIT SuperCloud workloads using trace replay and reinforcement learning. The method enables experimentation with energy-aware scheduling policies through Proximal Policy Optimization. Preliminary results demonstrate feasibility for learning optimal sustainability policies that improve datacenter throughput and efficiency."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\nDynamic Workloads"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [parallelism, dynamic workloads, latency-throughput tradeoff, tensor parallelism, sequence parallelism, KV cache]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Mert Hidayetoglu, Aurick Qiao, Michael Wyatt, Jeff Rasley, Yuxiong He, Samyam Rajbhandari"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Snowflake AI Research"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.16495v1",children:"http://arxiv.org/pdf/2509.16495v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces Shift Parallelism, a method that dynamically switches between tensor parallelism and sequence parallelism to optimize LLM inference. It achieves up to 1.51x faster response in interactive workloads and 50% higher throughput in batch workloads compared to TP-only solutions. The approach provides better latency-throughput tradeoffs for dynamic workloads without sacrificing performance in either low or high traffic scenarios."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"2025-09-21",children:"2025-09-21"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] MoA-Off: Adaptive Heterogeneous Modality-Aware Offloading with\nEdge-Cloud Collaboration for Efficient Multimodal LLM Inference"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [multimodal LLM, edge-cloud collaboration, adaptive offloading, inference optimization]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Zheming Yang, Qi Guo, Yunqing Hu, Chang Zhao, Chang Zhang, Jian Zhao, Wen Ji"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Peng Cheng Laboratory, Institute of AI for Industries"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.16995v1",children:"http://arxiv.org/pdf/2509.16995v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," MoA-Off proposes an adaptive edge-cloud collaborative framework with modality-aware complexity estimation for efficient multimodal LLM inference. It dynamically schedules workloads between edge and cloud based on input complexity and system states. The method achieves over 30% latency reduction and 30%-65% resource savings while maintaining competitive accuracy."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(i.li,{children:["\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"[arXiv2509] ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\nCaching"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [distributed prefix caching, KV cache fetching, SmartNIC acceleration, interference-free, chunked pipeline, minimal-copy memory management]"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"authors:"})," Xingyu Xiang, Raj Joshi, Yuhan Liu, Jiayi Yao, Chenxingyu Zhao, Junchen Jiang, Yang Zhou, Eddie Kohler, Minlan Yu"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"institution:"})," Harvard University, University of Chicago, University of Washington, UC Davis"]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"link:"})," ",(0,a.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.16857v1",children:"http://arxiv.org/pdf/2509.16857v1"})]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)(i.strong,{children:"Simple LLM Summary:"})," ShadowServe introduces a SmartNIC-accelerated system that separates control and data planes to eliminate interference during KV cache fetching for distributed prefix caching. It employs a chunked pipeline and minimal-copy memory management to overcome SmartNIC resource limitations. The system achieves up to 2.2x lower TPOT and 1.38x lower TTFT in low-bandwidth scenarios, improving throughput by up to 1.35x."]}),"\n"]}),"\n"]}),"\n"]})]})}function d(n={}){const{wrapper:i}={...(0,s.R)(),...n.components};return i?(0,a.jsx)(i,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},8453:(n,i,e)=>{e.d(i,{R:()=>t,x:()=>o});var r=e(6540);const a={},s=r.createContext(a);function t(n){const i=r.useContext(s);return r.useMemo(function(){return"function"==typeof n?n(i):{...i,...n}},[i,n])}function o(n){let i;return i=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:t(n.components),r.createElement(s.Provider,{value:i},n.children)}}}]);