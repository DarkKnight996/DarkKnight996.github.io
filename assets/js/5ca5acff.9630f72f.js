"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[513],{4793:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"daily/20251027-20251102","title":"20251027-20251102","description":"2025-10-27","source":"@site/docs/daily/20251027-20251102.md","sourceDirName":"daily","slug":"/daily/20251027-20251102","permalink":"/daily/20251027-20251102","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1761624683000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251020-20251026","permalink":"/daily/20251020-20251026"},"next":{"title":"Paper","permalink":"/category/paper"}}');var r=i(4848),t=i(8453);const a={},o="20251027-20251102",l={},c=[{value:"2025-10-27",id:"2025-10-27",level:2},{value:"2025-10-28",id:"2025-10-28",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"20251027-20251102",children:"20251027-20251102"})}),"\n",(0,r.jsx)(n.h2,{id:"2025-10-27",children:"2025-10-27"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] Learning to Schedule: A Supervised Learning Framework for Network-Aware\nScheduling of Data-Intensive Workloads"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [scheduling], [network-aware scheduling, supervised learning, data-intensive workloads, Kubernetes, job completion time prediction]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Sankalpa Timilsina, Susmit Shannigrahi"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Tennessee Technological University"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.21419v1",children:"http://arxiv.org/pdf/2510.21419v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a network-aware job scheduler using supervised learning to predict job completion times based on real-time cluster telemetry. The system employs a prediction-and-ranking mechanism that evaluates nodes and selects optimal placements for data-intensive workloads. Evaluation on a geo-distributed Kubernetes cluster showed 34-54% higher accuracy in node selection compared to the default Kubernetes scheduler."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] From SLA to vendor-neutral metrics: An intelligent knowledge-based\napproach for multi-cloud SLA-based broker"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [cloud computing, multi-cloud, SLA management, vendor-neutral metrics, intelligent knowledge-based system, auto-scaling]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," V\xedctor Ramp\xe9rez, Javier Soriano, David Lizcano, Shadi Aljawarneh, Juan A. Lara"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Universidad Polit\xe9cnica de Madrid (UPM), Madrid Open University (UDIMA)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.21173v1",children:"http://arxiv.org/pdf/2510.21173v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes an intelligent knowledge-based system that automatically translates high-level SLAs into vendor-neutral metrics for multi-cloud environments. The approach enables cross-provider metric measurement and provides consumer feedback through an intelligent tutoring system. Validation with IaaS and PaaS use cases demonstrates the system allows transparent multi-cloud exploitation across various application domains."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep\nLearning Training Workloads"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [GPU memory estimation, dynamic analysis, resource management, scheduling]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jiabo Shi, Dimitrios Pezaros, Yehia Elkhatib"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Glasgow"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.21048v1",children:"http://arxiv.org/pdf/2510.21048v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," xMem proposes a CPU-based dynamic analysis framework to accurately estimate peak GPU memory requirements for deep learning training workloads without consuming GPU resources. The method achieves 91% reduction in median relative error and 75% reduction in OOM probability compared to existing solutions. This enables better GPU sharing and scheduling in cluster environments while significantly improving memory conservation potential."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] Lincoln AI Computing Survey (LAICS) and Trends"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [Other models training, Other models inference], [AI accelerators, performance analysis, power consumption, market segmentation, computing architectures]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally, Jeremy Kepner"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," MIT Lincoln Laboratory"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.20931v1",children:"http://arxiv.org/pdf/2510.20931v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper updates the Lincoln AI Computing Survey by collecting performance and power consumption data of commercial AI accelerators, plotting them on scatter graphs, and analyzing market trends. It introduces a new categorization of computing architectures and examines how GenAI models have shifted computational demands toward matrix-vector operations and high memory bandwidth. The survey highlights ongoing innovations in AI hardware across various deployment scales from embedded systems to data centers."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] ParaRNN: Unlocking Parallel Training of Nonlinear RNNs for Large\nLanguage Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [LLM training], [parallel training, nonlinear RNNs, sequence modeling, Newton's iterations, parallel reductions]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Federico Danieli, Pau Rodriguez, Miguel Sarabia, Xavier Suau, Luca Zappella"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Apple"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.21450v1",children:"http://arxiv.org/pdf/2510.21450v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," ParaRNN enables parallel training of nonlinear RNNs by formulating recurrence relationships as a system of equations and solving them using Newton's iterations with parallel reductions. This approach achieves up to 665x speedup over sequential methods and allows training 7B parameter RNNs with performance comparable to Transformers and Mamba2. The framework is released as open-source to facilitate scalable nonlinear RNN research."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] REVE: A Foundation Model for EEG -- Adapting to Any Setup with\nLarge-Scale Pretraining on 25,000 Subjects"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [Other models training], [EEG foundation model, 4D positional encoding, masked autoencoding, brain-computer interfaces, clinical neuroscience]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yassine El Ouahidi, Jonathan Lys, Philipp Th\xf6lke, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon, Karim Jerbi, Giulia Lioi"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," IMT Atlantique, Universit\xe9 de Montr\xe9al, Mila, UNIQUE"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.21585v1",children:"http://arxiv.org/pdf/2510.21585v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," REVE introduces a novel 4D positional encoding scheme and uses masked autoencoding pretraining on 60,000 hours of EEG data from 25,000 subjects. The model achieves state-of-the-art performance across 10 EEG tasks including motor imagery and seizure detection. It demonstrates strong generalization with minimal fine-tuning and enables standardized EEG research through released code and weights."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"2025-10-28",children:"2025-10-28"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream\nProcessing Pipelines"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [LLM inference], [stream processing, data pipelines, workflow automation, hypergraph of thoughts, multi-agent systems]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Abolfazl Younesi, Zahra Najafabadi Samani, Thomas Fahringer"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Innsbruck"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.23408v1",children:"http://arxiv.org/pdf/2510.23408v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," AutoStreamPipe introduces a framework that uses Large Language Models with Hypergraph of Thoughts to automatically generate and deploy stream processing pipelines. It bridges the semantic gap between user intent and platform-specific implementations through structured multi-agent reasoning. Experimental results show significant reductions in development time (6.3\xd7) and error rates (5.19\xd7) compared to traditional LLM code-generation methods."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with\nLLMs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [trace analysis], [log-based anomaly detection, rule synthesis, large language models, automated code generation, hierarchical clustering, agentic workflow]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Junjie Huang, Minghua He, Jinyang Liu, Yintong Huo, Domenico Bianculli, Michael R. Lyu"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," The Chinese University of Hong Kong, Peking University, Singapore Management University, University of Luxembourg"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.22986v1",children:"http://arxiv.org/pdf/2510.22986v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," CodeAD introduces a framework that uses LLMs to automatically synthesize lightweight Python rule functions for log-based anomaly detection through hierarchical clustering and iterative agentic workflows. The method achieves 3.6% higher F1 score than state-of-the-art baselines while being 4x faster and costing under $4 per dataset. This enables efficient, interpretable, and scalable online anomaly detection directly on raw logs."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] Simopt-Power: Leveraging Simulation Metadata for Low-Power Design\nSynthesis"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [compiler], [FPGA, low-power design, simulation metadata, power optimization, Shannon Decomposition]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Eashan Wadhwa, Shanker Shreejith"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Trinity College Dublin"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.21745v1",children:"http://arxiv.org/pdf/2510.21745v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," Simopt-Power introduces a simulation-driven optimization framework that leverages activity profiles to identify high-toggle paths in FPGA designs. It uses Shannon Decomposition to insert duplicate truth table logic and relocate critical nets, reducing switching activity without altering functionality. The method achieves approximately 9% power reduction with only 19% additional LUT resources, demonstrating effective use of simulation metadata in FPGA-CAD flows."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] A Feature Engineering Approach for Business Impact-Oriented Failure\nDetection in Distributed Instant Payment Systems"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [trace analysis], [anomaly detection, distributed systems, payment systems, feature engineering, observability]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Lorenzo Porcelli"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Bank of Italy, University of Salerno"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.21710v1",children:"http://arxiv.org/pdf/2510.21710v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a feature engineering approach using processing times between ISO 20022 message exchanges to create system state representations for anomaly detection in instant payment systems. The method enables early failure detection and incident classification by mapping features to distinct processing phases. Experimental evaluation on TARGET Instant Payment Settlement demonstrates effectiveness in detecting diverse anomaly patterns while providing interpretable explanations for business impact assessment."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] Accelerating IC Thermal Simulation Data Generation via Block Krylov and\nOperator Action"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [ai for science], [thermal simulation, data generation, neural operators, block Krylov, integrated circuits]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hong Wang, Wenkai Yang, Jie Wang, Huanshuo Dong, Zijie Geng, Zhen Huang, Depeng Xie, Zhezheng Hao, Hande Dong"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.23221v1",children:"http://arxiv.org/pdf/2510.23221v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes BlocKOA, a novel algorithm that combines block Krylov methods with operator actions to accelerate thermal simulation data generation for integrated circuits. This method achieves 420x speedup in data generation while maintaining comparable accuracy to existing methods. Data-driven approaches trained on BlocKOA-generated data perform similarly to those using conventional data with only 4% of the generation time."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] PAHQ: Accelerating Automated Circuit Discovery through Mixed-Precision\nInference Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [LLM inference], [automated circuit discovery, mixed-precision quantization, activation patching, mechanistic interpretability, computational efficiency]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Xinhai Wang, Shu Yang, Liangyu Wang, Lin Zhang, Huanyi Xie, Lijie Hu, Di Wang"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," King Abdullah University of Science and Technology, Harbin Institute of Technology"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.23264v1",children:"http://arxiv.org/pdf/2510.23264v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," PAHQ accelerates automated circuit discovery in large language models through per-attention-head mixed-precision quantization, maintaining high precision only for investigated components while reducing precision elsewhere. This training-free method reduces runtime by up to 80% and memory consumption by up to 30% while preserving analytical faithfulness. The approach integrates with existing circuit discovery techniques by modifying attention computation mechanisms."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] Progressive Growing of Patch Size: Curriculum Learning for Accelerated\nand Improved Medical Image Segmentation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [Other models training], [medical image segmentation, curriculum learning, patch size optimization, 3D segmentation, training efficiency]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Stefan M. Fischer, Johannes Kiechle, Laura Daza, Lina Felsner, Richard Osuala, Daniel M. Lang, Karim Lekadir, Jan C. Peeken, Julia A. Schnabel"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technical University of Munich, Klinikum rechts der Isar, University of Barcelona"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.23241v1",children:"http://arxiv.org/pdf/2510.23241v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Progressive Growing of Patch Size (PGPS), a curriculum learning method that gradually increases patch size during 3D medical image segmentation training. The approach achieves either faster training (44% time reduction) with comparable performance or improved segmentation accuracy (1.28% Dice gain) with reduced training time (89%). The method demonstrates consistent benefits across multiple architectures and is particularly effective for class-imbalanced tasks like lesion segmentation."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] Accelerating Eigenvalue Dataset Generation via Chebyshev Subspace Filter"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [ai for science], [eigenvalue computation, Chebyshev subspace filter, data generation acceleration, neural eigenvalue methods, scientific computing]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Hong Wang, Jie Wang, Jian Luo, huanshuo dong, Yeqiu Chen, Runmin Jiang, Zhen huang"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," University of Science and Technology of China"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.23215v1",children:"http://arxiv.org/pdf/2510.23215v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes Sorting Chebyshev Subspace Filter (SCSF), a method that accelerates eigenvalue dataset generation by grouping operators with similar eigenvalue distributions and reusing eigenpairs from solved problems. This approach reduces redundant computations in generating training data for neural eigenvalue solvers. Experimental results demonstrate that SCSF achieves up to 3.5\xd7 speedup compared to traditional numerical solvers."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv2510] Guiding Skill Discovery with Foundation Models"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [Other models training], [reinforcement learning, skill discovery, foundation models, human preferences, reward functions]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Zhao Yang, Thomas M. Moerland, Mike Preuss, Aske Plaat, Vincent Fran\xe7ois-Lavet, Edward S. Hu"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Vrije Universiteit Amsterdam, Leiden University, University of Pennsylvania"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"http://arxiv.org/pdf/2510.23167v1",children:"http://arxiv.org/pdf/2510.23167v1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," FoG skill discovery method uses foundation models to incorporate human preferences by extracting score functions that evaluate states, then re-weights skill discovery rewards. This approach successfully eliminates undesirable behaviors like flipping and avoids hazardous areas while discovering skills with hard-to-define behaviors. The method demonstrates effectiveness in both state-based and pixel-based tasks."]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);