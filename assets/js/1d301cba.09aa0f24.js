"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[478],{8453:(i,n,e)=>{e.d(n,{R:()=>t,x:()=>o});var r=e(6540);const s={},a=r.createContext(s);function t(i){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof i?i(n):{...n,...i}},[n,i])}function o(i){let n;return n=i.disableParentContext?"function"==typeof i.components?i.components(s):i.components||s:t(i.components),r.createElement(a.Provider,{value:n},i.children)}},9750:(i,n,e)=>{e.r(n),e.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20260126-20260201","title":"20260126-20260201","description":"2026-01-26","source":"@site/docs/daily/20260126-20260201.md","sourceDirName":"daily","slug":"/daily/20260126-20260201","permalink":"/daily/20260126-20260201","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1771990223000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20260119-20260125","permalink":"/daily/20260119-20260125"},"next":{"title":"20260202-20260208","permalink":"/daily/20260202-20260208"}}');var s=e(4848),a=e(8453);const t={},o="20260126-20260201",l={},c=[{value:"2026-01-26",id:"2026-01-26",level:2},{value:"2026-01-27",id:"2026-01-27",level:2},{value:"2026-01-28",id:"2026-01-28",level:2},{value:"2026-01-29",id:"2026-01-29",level:2},{value:"2026-01-30",id:"2026-01-30",level:2}];function h(i){const n={a:"a",annotation:"annotation",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mi:"mi",mn:"mn",mrow:"mrow",msup:"msup",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...i.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"20260126-20260201",children:"20260126-20260201"})}),"\n",(0,s.jsx)(n.h2,{id:"2026-01-26",children:"2026-01-26"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"cs.DC total: 6"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260126] Consensus In Asynchrony"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [distributed consensus], [events-based synchronisation, vector agreement, fault-tolerant consensus, FLP impossibility, binary agreement, deterministic consensus]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ivan Klianev"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Transactum Pty Ltd"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16460",children:"https://arxiv.org/pdf/2601.16460"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a deterministic fault-tolerant consensus algorithm for asynchronous systems using events-based synchronization, which achieves vector agreement. It argues that the classic FLP impossibility result relies on implicit assumptions and presents experimental evidence against one of them, suggesting consensus in asynchrony is possible."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260126] Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [space filling curves, communication-avoiding algorithms, generalized Hilbert curves, cache blocking, data locality]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Evangelos Georganas, Alexander Heinecke, Pradeep Dubey"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Intel Corporation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16294",children:"https://arxiv.org/pdf/2601.16294"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a matrix multiplication method using space filling curves (specifically generalized Hilbert curves) to partition computation, achieving platform- and shape-oblivious high data locality. It extends this approach with communication-avoiding algorithms to minimize data movement, resulting in compact code that outperforms vendor libraries by up to 2x on various CPU platforms."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260126] Artifact for Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [cloud-native systems], [Kubernetes, Kepler, cAdvisor, additive energy model, service-level energy measurement]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Julian Legler"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Berlin"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16635",children:"https://arxiv.org/pdf/2601.16635"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents GOXN, an experimentation engine for measuring the energy consumption of Kubernetes-based microservices, using an additive model to derive service-level energy from container-level metrics. The main conclusion is that excluding network and storage energy can lead to significant underestimation of consumption, especially for auxiliary services, and that high tracing loads shift energy dominance towards network and storage components."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260126] W4A16 Mixed-Precision Matrix Multiplication on Decoupled Architecture: Kernel Design and Memory Bottleneck Analysis for Ascend NPUs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [weight-only quantization, W4A16, mixed-precision matrix multiplication, kernel design, Split-K parallelization, on-the-fly dequantization, memory bottleneck analysis]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yuanhong He, Peiyu Niu, Jun Chen, Chenchen Zhang, Chao Yang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," PKU-Changsha Institute of Computing and Digital Economy, Peking University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16536",children:"https://arxiv.org/pdf/2601.16536"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents a custom W4A16 matrix multiplication kernel for Huawei Ascend 910 NPUs, using vector cores for dequantization and cube cores with Split-K parallelization to optimize performance. It finds that memory transfer, not dequantization computation, is the primary bottleneck, achieving up to 1.48x speedup over FP16 baseline in LLM decoding scenarios."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260126] GPU-Accelerated Selected Basis Diagonalization with Thrust for SQD-based Algorithms"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [Selected Basis Diagonalization, Sample-based Quantum Diagonalization, GPU acceleration, Thrust library, Davidson method, parallel algorithms]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Jun Doi, Tomonori Shirakawa, Yukio Kawashima, Seiji Yunoki, Hiroshi Horii"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," IBM Quantum, IBM Research - Tokyo, RIKEN, Center for Computational Science"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16637",children:"https://arxiv.org/pdf/2601.16637"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a GPU-accelerated implementation of Selected Basis Diagonalization (SBD) for Sample-based Quantum Diagonalization (SQD) algorithms using the Thrust library. The method restructures key computational components into fine-grained, data-parallel primitives with GPU-friendly data layouts. The results show a speedup of up to ~40x over CPU execution, demonstrating that GPU-native primitives provide a high-performance foundation for accelerating quantum-classical workflows."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260126] DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [checkpointing, state providers, asynchronous snapshots, hybrid parallelism, data parallelism, tensor parallelism, pipeline parallelism, ZeRO, FSDP]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Avinash Maurya, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Argonne National Laboratory, Rochester Institute of Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16956",children:"https://arxiv.org/pdf/2601.16956"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper introduces DataStates-LLM, a checkpointing architecture that uses composable State Providers to decouple state abstraction from data movement, enabling lazy, non-blocking asynchronous snapshots. It addresses the "3D heterogeneity" of distributed model states to reduce serialization and I/O bottlenecks. The evaluation shows it achieves up to 4x higher checkpointing throughput and reduces end-to-end training time by up to 2.2x compared to state-of-the-art solutions.']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 10'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Towards a Theoretical Understanding to the Generalization of RLHF ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16403",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Boosting Deep Reinforcement Learning with Semantic Knowledge for Robotic Manipulators ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16866",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Sim-to-Real Transfer via a Style-Identified Cycle Consistent Generative Adversarial Network: Zero-Shot Deployment on Robotic Manipulators through Visual Domain Adaptation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16677",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16486",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16906",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16399",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Reasoning Promotes Robustness in Theory of Mind Tasks ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16853",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Reinforcement Learning-Based Energy-Aware Coverage Path Planning for Precision Agriculture ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16405",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Endless Terminals: Scaling RL Environments for Terminal Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16443",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] LongCat-Flash-Thinking-2601 Technical Report ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16725",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 6'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] kNN-Graph: An adaptive graph model for ",(0,s.jsxs)(n.span,{className:"katex",children:[(0,s.jsx)(n.span,{className:"katex-mathml",children:(0,s.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(n.semantics,{children:[(0,s.jsx)(n.mrow,{children:(0,s.jsx)(n.mi,{children:"k"})}),(0,s.jsx)(n.annotation,{encoding:"application/x-tex",children:"k"})]})})}),(0,s.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(n.span,{className:"base",children:[(0,s.jsx)(n.span,{className:"strut",style:{height:"0.6944em"}}),(0,s.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.03148em"},children:"k"})]})})]}),"-nearest neighbors ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16509",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] E2Former-V2: On-the-Fly Equivariant Attention with Linear Activation Memory ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16622",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] DSGym: A Holistic Framework for Evaluating and Training Data Science Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16344",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Bayesian Experimental Design for Model Discrepancy Calibration: A Rivalry between Kullback--Leibler Divergence and Wasserstein Distance ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16425",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Auto-Regressive Masked Diffusion Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16971",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Active learning for photonics ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16287",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"2026-01-27",children:"2026-01-27"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"cs.DC total: 24"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Context Lake: A System Class Defined by Decision Coherence"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [decision coherence, transactional consistency, semantic operations, operational envelopes, composition impossibility theorem]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Xiaowei Jiang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Tacnode"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17019",children:"https://arxiv.org/pdf/2601.17019"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper introduces the "Decision Coherence Law" and argues that existing data systems are inadequate for AI agents making concurrent, irreversible decisions. It proposes a new system class called "Context Lake," which requires semantic operations, transactional consistency, and bounded operational envelopes to ensure correctness in collective AI systems.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Athena: Synergizing Data Prefetching and Off-Chip Prediction via Online Reinforcement Learning"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [reinforcement learning, data prefetching, off-chip prediction, cache management, coordination policy]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Rahul Bera, Zhenrong Lang, Caroline Hengartner, Konstantinos Kanellopoulos, Rakesh Kumar, Mohammad Sadrosadati, Onur Mutlu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," ETH Z\xfcrich, NTNU"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17615",children:"https://arxiv.org/pdf/2601.17615"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Athena, a framework that uses online reinforcement learning to autonomously coordinate data prefetchers and an off-chip predictor in high-performance processors. It demonstrates that Athena consistently outperforms prior coordination policies across diverse workloads and system configurations by learning to synergize these techniques effectively."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [federated learning, decentralized federated learning, peer-to-peer, knowledge distillation, parameter-efficient fine-tuning, contextual bandit, LinUCB]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Inderjeet Singh, Eleonore Vissol-Gaudin, Andikan Otung, Motoyoshi Sekiya"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Fujitsu Research of Europe"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17133",children:"https://arxiv.org/pdf/2601.17133"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces KNEXA-FL, a framework for orchestrated decentralization in federated learning. It uses a central matchmaker to intelligently pair heterogeneous LLM agents for peer-to-peer knowledge exchange via secure distillation, which significantly improves performance and stability compared to random collaboration or centralized baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Scaling All-to-all Operations Across Emerging Many-Core Supercomputers"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [HPC communication algorithms], [all-to-all, MPI, locality-aware, hierarchical algorithms, Sapphire Rapids, NUMA]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Shannon Kinkead, Jackson Wesley, Whit Schonbein, David DeBonis, Matthew G. F. Dosanjh, Amanda Bienz"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Sandia National Laboratories, University of New Mexico, Los Alamos National Laboratory"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17606",children:"https://arxiv.org/pdf/2601.17606"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces novel locality-aware and hierarchical algorithms for optimizing MPI all-to-all collective operations on emerging many-core supercomputers. It presents a performance analysis comparing these new algorithms against existing implementations. The novel algorithms achieve up to 3x speedup over the system MPI on state-of-the-art Sapphire Rapids systems at 32 nodes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Lightspeed Data Compute for the Space Era"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [distributed computing], [MapReduce, inter-satellite laser links, distance-aware routing, bipartite match scheduling, LEO satellite mesh]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Thomas Sandholm, Bernardo A. Huberman, Klas Segeljakt, Paris Carbone"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," RISE - Research Institutes of Sweden, KTH Royal Institute of Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17589",children:"https://arxiv.org/pdf/2601.17589"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SpaceCoMP, a MapReduce-inspired processing model for Low Earth Orbit satellite networks. It uses inter-satellite laser links for cooperative in-orbit data processing and a scheduling strategy to minimize aggregation costs. The method demonstrates significant improvements in efficiency, showing that orbital mesh networks can enable faster data processing in space."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Conduit: Programmer-Transparent Near-Data Processing Using Multiple Compute-Capable Resources in Solid State Drives"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [near-data processing, in-storage processing, in-flash processing, vectorization, SIMD, instruction-granularity offloading, cost function, SSD simulator]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Rakesh Nadig, Vamanan Arulchelvan, Mayank Kabra, Harshita Gupta, Rahul Bera, Nika Mansouri Ghiasi, Nanditha Rao, Qingcai Jiang, Andreas Kosmas Kakolyris, Yu Liang, Mohammad Sadrosadati, Onur Mutlu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," ETH Z\xfcrich, Inria Paris"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17633",children:"https://arxiv.org/pdf/2601.17633"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," Conduit is a programmer-transparent near-data processing framework for SSDs that uses compile-time vectorization and runtime instruction-granularity offloading to leverage multiple heterogeneous compute resources within an SSD. It outperforms prior offloading techniques by 1.8x and reduces energy consumption by 46% across data-intensive workloads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Push Down Optimization for Distributed Multi Cloud Data Integration"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [cloud data integration], [push-down optimization, ETL, multi-cloud, data federation, SQL engines, Redshift, BigQuery]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ravi Kiran Kodali, Vinoth Punniyamoorthy, Akash Kumar Agarwal, Bikesh Kumar, Balakrishna Pothineni, Aswathnarayan Muthukrishnan Kirubakaran, Sumit Saha, Nachiappan Chockalingam"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Cognizant Technology Solutions, Albertsons Companies, East West Bank"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17546",children:"https://arxiv.org/pdf/2601.17546"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper examines the feasibility and strategies of push-down optimization for ETL pipelines in multi-cloud environments, evaluating techniques like localized push-down and data federation. It concludes that these approaches, demonstrated in a case study across Redshift and BigQuery, can reduce cross-cloud data traffic, lower runtime, and improve cost efficiency for distributed data integration."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] A Unified Approach to Concurrent, Parallel Map-Reduce in R using Futures"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [parallel computing], [map-reduce, futures, R programming, transpilation, concurrent processing]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Henrik Bengtsson"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Not explicitly stated; author email domain not provided. Likely independent or academic affiliation based on context."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17578",children:"https://arxiv.org/pdf/2601.17578"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces the futurize package for R, which provides a unified function to automatically transpile sequential map-reduce code into parallel equivalents using the future ecosystem. This allows users to parallelize existing code with minimal changes, abstracting away the complexities of diverse parallel APIs. The main conclusion is that this approach simplifies parallel computing in R by separating the declaration of what to parallelize from the choice of how to execute it."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Communication-Avoiding Linear Algebraic Kernel K-Means on GPUs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [distributed linear algebra, 1.5D algorithm, communication-avoiding, multi-GPU, kernel k-means]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Julian Bellavita, Matthew Rubino, Nakul Iyer, Andrew Chang, Aditya Devarakonda, Flavio Vella, Giulia Guidi"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Cornell University, Meta, Wake Forest University, University of Trento, Epic Systems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17136",children:"https://arxiv.org/pdf/2601.17136"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces distributed-memory parallel algorithms for large-scale Kernel K-means clustering on multi-GPU systems, mapping its expensive computations onto communication-efficient distributed linear algebra primitives. The core innovation is a 1.5D partitioning scheme that enables highly scalable clustering of million-scale datasets. The results show that the 1.5D algorithm achieves substantial performance improvements, scaling to data one to two orders of magnitude larger than previous single-GPU methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Decentralized Multi-Agent Swarms for Autonomous Grid Security in Industrial IoT: A Consensus-based Approach"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [decentralized multi-agent swarm, consensus-based threat validation, peer-to-peer protocol, edge computing, swarm intelligence, byzantine fault-tolerant consensus]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Samaresh Kumar Singh, Joyjit Roy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," IEEE (affiliation inferred from membership; no specific academic/research institution provided)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17303",children:"https://arxiv.org/pdf/2601.17303"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a Decentralized Multi-Agent Swarm (DMAS) architecture using AI agents at edge gateways and a Consensus-based Threat Validation (CVT) protocol for Industrial IoT security. The method enables cooperative, real-time threat detection without a central cloud, achieving sub-millisecond response times and high accuracy in experiments. The results show the approach significantly outperforms centralized and edge-based baselines in speed, accuracy, and bandwidth reduction."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Kareus: Joint Reduction of Dynamic and Static Energy in Large Model Training"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [kernel scheduling, frequency scaling, multi-pass multi-objective optimization, dynamic energy, static energy]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ruofan Wu, Jae-Won Chung, Mosharaf Chowdhury"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Michigan"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17654",children:"https://arxiv.org/pdf/2601.17654"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," Kareus is a training system that jointly optimizes dynamic and static energy consumption by decomposing the problem into local subproblems and using a multi-pass multi-objective algorithm to find optimal kernel execution schedules and GPU frequencies. It demonstrates that fine-grained kernel scheduling and frequency scaling interdependently impact energy use, enabling reductions in training energy by up to 28.3% at the same time or training time by up to 27.5% at the same energy compared to prior work."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] CondenseGraph: Communication-Efficient Distributed GNN Training via On-the-Fly Graph Condensation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [graph condensation, error feedback, distributed training, communication compression]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Zizhao Zhang, Yihan Xue, Haotian Zhu, Sijia Li, Zhijun Wang, Yujie Xiao"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Michigan, University of Southern California, New York University, Rice University, University of California, Berkeley"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17774",children:"https://arxiv.org/pdf/2601.17774"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes CondenseGraph, a framework that reduces communication overhead in distributed GNN training by dynamically compressing boundary node features into compact super nodes. It uses a gradient-based error feedback mechanism to compensate for information loss. Experiments show it reduces communication volume by 40-60% while maintaining accuracy comparable to full-precision baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Multi-core & GPU-based Balanced Butterfly Counting in Signed Bipartite Graphs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [graph algorithms], [vertex-level parallelism, tile-based parallel approach, dynamic scheduling, wedge-based counting]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Mekala Kiran, Apurba Das, Suman Banerjee, Tathagata Ray"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," BITS Pilani Hyderabad Campus, Indian Institute of Technology Jammu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17707",children:"https://arxiv.org/pdf/2601.17707"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents highly parallel algorithms for balanced butterfly counting in signed bipartite graphs, including a multi-core CPU method (M-BBC) and two GPU-based methods (G-BBC and G-BBC++). The core methods employ fine-grained vertex-level parallelism and tile-based approaches with dynamic scheduling to accelerate computation. The experimental results show substantial speedups over the sequential baseline, demonstrating the scalability and efficiency of the proposed parallel algorithms for large-scale graph analysis."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] On the Extension of Private Distributed Matrix Multiplication Schemes to the Grid Partition"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [distributed computing, coding theory], [polynomial codes, private distributed matrix multiplication, grid partitioning, outer product partitioning, inner product partitioning, collusion parameter]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Christoph Hofmeister, Razane Tajeddine, Antonia Wachter-Zeh, Rawad Bitar"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Technical University of Munich (TUM), American University of Beirut (AUB)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17834",children:"https://arxiv.org/pdf/2601.17834"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes extension operations to adapt existing polynomial codes designed for outer product partitioning to the more general grid partitioning for private distributed matrix multiplication. It shows these extensions improve performance for certain parameters but also impose combinatorial constraints, and introduces a new grid partitioning scheme that bypasses these constraints to outperform existing methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Action Authorization Boundary, Canonical Action Representation, deterministic authorization, non-bypassable enforcement, provenance logging]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Amjad Fatmi"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," The Faramesh Labs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17744",children:"https://arxiv.org/pdf/2601.17744"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Faramesh, a protocol-agnostic execution control plane that establishes a mandatory Action Authorization Boundary (AAB) to enforce deterministic, non-bypassable authorization for actions proposed by autonomous agents before they are executed. Its core method involves canonicalizing agent intent and providing fail-closed, replayable decision artifacts. The main conclusion is that such an architectural boundary is a necessary structural guarantee for safe, auditable, and trustworthy deployment of autonomous agents in real-world systems, distinct from observability-only or protocol-based solutions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] LLM-42: Enabling Determinism in LLM Inference with Verified Speculation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [speculative decoding, dynamic batching, verify-rollback loop, batch-invariant computation, GPU kernels, floating-point non-associativity]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Raja Gond, Aditya K Kamath, Arkaprava Basu, Ramachandran Ramjee, Ashish Panwar"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Microsoft Research, University of Washington, Indian Institute of Science"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17768",children:"https://arxiv.org/pdf/2601.17768"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents LLM-42, a scheduling-based method that enables deterministic LLM inference by using a speculative fast path with dynamic batching and a lightweight verify-rollback loop to check and correct for non-determinism. This approach reuses existing kernels and incurs overhead only for the portion of the workload requiring determinism, avoiding the throughput degradation of disabling dynamic batching or the fixed overhead of batch-invariant kernels."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] An MLIR Lowering Pipeline for Stencils at Wafer-Scale"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [MLIR, stencil computations, domain-specific languages, CSL, xDSL, compiler pipeline, asynchronous execution model]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Nicolai Stawinoga, David Katz, Anton Lydike, Justs Zarins, Nick Brown, George Bisbas, Tobias Grosser"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Berlin, The University of Edinburgh, Imperial College London, University of Cambridge"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17754",children:"https://arxiv.org/pdf/2601.17754"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents an MLIR-based compiler pipeline that automatically transforms stencil-based kernels into optimized CSL code for the Cerebras Wafer-Scale Engine (WSE), bridging the semantic gap between mathematical problem descriptions and the WSE's asynchronous execution model. The approach achieves performance comparable to or better than manually optimized code on WSE2 and WSE3, and significantly outperforms traditional GPU and CPU-based supercomputers without requiring application-level code changes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] A Universal Load Balancing Principle and Its Application to Large Language Model Serving"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [load balancing, barrier synchronization, integer optimization, data-parallel decoding, straggler mitigation]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Zixi Chen, Tianci Bu, Chendong Song, Xin Lu, Yinyu Ye, Zijie Zhou"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," HKUST, National University of Defense Technology, Peking University, Stanford University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17855",children:"https://arxiv.org/pdf/2601.17855"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a universal load-balancing principle formulated as a step-wise finite-horizon integer optimization to address the straggler problem in barrier-synchronized systems like LLM serving. It demonstrates that this method significantly reduces idle time, improves throughput and latency, and lowers energy consumption for data-parallel decoding."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] MultiChain Blockchain Data Provenance for Deterministic Stream Processing with Kafka Streams: A Weather Data Case Study"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [streaming data systems], [MultiChain blockchain, Kafka Streams, Merkle Trees, data provenance, deterministic serialization, cryptographic anchoring]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Niaz Mohammad Ramaki, Florian Schintke"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Berlin, Zuse Institute Berlin"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18011",children:"https://arxiv.org/pdf/2601.18011"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a blockchain-based architecture using MultiChain and Kafka Streams to ensure deterministic reproducibility and auditability in stream processing. The method involves canonicalizing, deduplicating, and aggregating weather data, then storing Merkle roots and Kafka offset boundaries on-chain as cryptographic checkpoints. The evaluation with real weather station data shows the system provides linear verification cost, deterministic reproducibility, and scalable off-chain storage with satisfactory transaction throughput."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Types for Grassroots Logic Programs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [programming languages], [Grassroots Logic Programs, type system, moded paths, concurrent logic programming, regular sets, well-typing, covariance, contravariance]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ehud Shapiro"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," London School of Economics, Weizmann Institute of Science"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17957",children:"https://arxiv.org/pdf/2601.17957"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a type system for Grassroots Logic Programs (GLP), a concurrent logic programming language, by defining types as regular sets of moded paths that capture communication directionality. It provides a syntactic definition of well-typing and proves a semantic characterization linking it to covariance and contravariance conditions. The type system was implemented in Dart with AI assistance to help ensure correctness when programming complex communication modalities."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] On the Bandwidth Consumption of Blockchains"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [blockchain networking], [bandwidth measurement, transport protocol, node role segregation, block propagation, polling, WebSockets]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Andrei Lebedev, Vincent Gramoli"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," The University of Sydney, Redbelly Network"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18400",children:"https://arxiv.org/pdf/2601.18400"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper conducts the first empirical comparison of bandwidth consumption by measuring the network traffic of five blockchain protocols (Algorand, Aptos, Avalanche, Redbelly, Solana). It concludes that the transport protocol and block propagation strategy are the main factors impacting network traffic, and that segregating node roles helps reduce traffic."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] An Initial Evaluation of Distributed Graph Algorithms using NWGraph and HPX"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [distributed graph processing], [NWGraph, HPX, asynchronous many-task model, Breadth-First Search, PageRank, Boost Graph Library]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Karame Mohammadiporshokooh, Panagiotis Syskakis, Hartmut Kaiser"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Louisiana State University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18158",children:"https://arxiv.org/pdf/2601.18158"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a distributed graph processing framework by integrating the NWGraph library with the HPX runtime system, leveraging its asynchronous many-task model to reduce synchronization overhead. The evaluation shows that the approach achieves better performance than the distributed Boost Graph Library for Breadth-First Search, but not yet for PageRank, highlighting the promise and ongoing challenges of applying task-based runtimes to graph algorithms."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Rhea: Detecting Privilege-Escalated Evasive Ransomware Attacks Using Format-Aware Validation in the Cloud"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [ransomware detection], [format-aware validation, mutation snapshots, cloud-offloaded defense, file format specification, privilege-escalated evasive ransomware (PEER)]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Beom Heyn Kim, Seok Min Hong, Mohammad Mannan"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Hanyang University, Concordia University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18216",children:"https://arxiv.org/pdf/2601.18216"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents Rhea, a cloud-offloaded ransomware defense system that uses format-aware validation to detect evasive attacks by checking the syntactic and semantic correctness of file formats in replicated data snapshots. It concludes that this method significantly outperforms existing I/O-pattern and statistical content-based approaches against modern, privilege-escalated ransomware."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] An Adaptive Purification Controller for Quantum Networks: Dynamic Protocol Selection and Multipartite Distillation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [quantum networks], [adaptive purification controller, dynamic programming, Pareto pruning, entanglement distillation, BBPSSW, DEJMPS, goodput optimization]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Pranav Kulkarni, Leo S\xfcnkel, Michael K\xf6lle"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," AIGNOSCO GmbH, LMU Munich"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18351",children:"https://arxiv.org/pdf/2601.18351"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes an Adaptive Purification Controller (APC) that uses dynamic programming with Pareto pruning to autonomously select and sequence entanglement purification protocols in a quantum network. It demonstrates that this adaptive approach eliminates fidelity cliffs of static protocols and prevents resource wastage, achieving real-time decision-making with millisecond latencies."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 46'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Breaking Task Impasses Quickly: Adaptive Neuro-Symbolic Learning for Open-World Robotics ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16985",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Cognitive Platform Engineering for Autonomous Cloud Operations ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17542",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Deep Intrinsic Surprise-Regularized Control (DISRC): A Biologically Inspired Mechanism for Efficient Deep Q-Learning in Sparse Environments ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17598",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Quantum-Inspired Episode Selection for Monte Carlo Reinforcement Learning via QUBO Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17570",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Multi-Agent Deep Reinforcement Learning Under Constrained Communications ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17069",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Structure-Aware NL-to-SQL for SFC Provisioning via AST-Masking Empowered Language Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17295",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17223",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17329",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Beyond Instrumental and Substitutive Paradigms: Introducing Machine Culture as an Emergent Phenomenon in Large Language Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17096",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17275",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Retell, Reward, Repeat: Reinforcement Learning for Narrative Theory-Informed Story Generation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17226",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Embodiment-Induced Coordination Regimes in Tabular Multi-Agent Q-Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17454",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17678",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17687",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17699",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17920",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] SD-E",(0,s.jsxs)(n.span,{className:"katex",children:[(0,s.jsx)(n.span,{className:"katex-mathml",children:(0,s.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(n.semantics,{children:[(0,s.jsx)(n.mrow,{children:(0,s.jsxs)(n.msup,{children:[(0,s.jsx)(n.mrow,{}),(0,s.jsx)(n.mn,{children:"2"})]})}),(0,s.jsx)(n.annotation,{encoding:"application/x-tex",children:"^2"})]})})}),(0,s.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(n.span,{className:"base",children:[(0,s.jsx)(n.span,{className:"strut",style:{height:"0.8141em"}}),(0,s.jsxs)(n.span,{className:"mord",children:[(0,s.jsx)(n.span,{}),(0,s.jsx)(n.span,{className:"msupsub",children:(0,s.jsx)(n.span,{className:"vlist-t",children:(0,s.jsx)(n.span,{className:"vlist-r",children:(0,s.jsx)(n.span,{className:"vlist",style:{height:"0.8141em"},children:(0,s.jsxs)(n.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,s.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsx)(n.span,{className:"mord mtight",children:"2"})})]})})})})})]})]})})]}),": Semantic Exploration for Reasoning Under Token Budgets ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17982",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18107",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18142",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18150",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18207",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18217",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18225",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18292",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18296",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18381",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] daVinci-Dev: Agent-native Mid-training for Software Engineering ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18418",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18467",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18479",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18510",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18580",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18586",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18626",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18631",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18681",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18706",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18730",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18734",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18751",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18771",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18778",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18779",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18783",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18795",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Exact Minimum-Volume Confidence Set Intersection for Multinomial Outcomes ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18145",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Emergent Cooperation in Quantum Multi-Agent Reinforcement Learning Using Communication ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18419",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 17'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Unrolled Neural Networks for Constrained Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17274",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Cognitive Platform Engineering for Autonomous Cloud Operations ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17542",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17364",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Accelerated Sinkhorn Algorithms for Partial Optimal Transport ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17196",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] PALMA: A Lightweight Tropical Algebra Library for ARM-Based Embedded Systems ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17028",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17426",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17687",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17868",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17917",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Multimodal Machine Learning for Soft High-k Elastomers under Data Scarcity ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18032",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18067",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18150",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Frequency-Based Hyperparameter Selection in Games ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18409",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Gradient Regularized Natural Gradients ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18420",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Physics-Informed Uncertainty Enables Reliable AI-driven Design ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18638",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] EveNet: A Foundation Model for Particle Collision Data Analysis ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17126",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] GenAI-Net: A Generative AI Framework for Automated Biomolecular Network Design ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17582",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"2026-01-28",children:"2026-01-28"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"cs.DC total: 12"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260128] Convex Hull 3D Filtering with GPU Ray Tracing and Tensor Cores"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [computational geometry], [GPU ray tracing, tensor cores, convex hull, parallel reduction, Manhattan distance]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Roberto Carrasco, Enzo Meneses, Hector Ferrada, Cristobal A. Navarro, Nancy Hitschfeld"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Universidad de Chile, Universidad Austral de Chile"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19647",children:"https://arxiv.org/pdf/2601.19647"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a 3D preprocessing filter for the convex hull algorithm that uses GPU ray tracing and tensor cores to build a delimiter polyhedron based on Manhattan distances, discarding interior points. The method accelerates 3D convex hull computation by up to 200x compared to a CPU parallel implementation while maintaining controlled energy consumption, demonstrating significant speed and energy efficiency gains."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260128] Trustworthy Scheduling for Big Data Applications"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [counterfactual explanations, random forests, task scheduling, explainability, resource configuration]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Dimitrios Tomaras, Vana Kalogeraki, Dimitrios Gunopulos"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Athens University of Economics and Business, National and Kapodistrian University of Athens"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18983",children:"https://arxiv.org/pdf/2601.18983"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes X-Sched, a middleware that integrates counterfactual explanations with machine learning models like Random Forests to provide actionable guidance for optimal resource configuration in containerized environments. The experimental results demonstrate that this approach efficiently identifies configurations to meet performance goals while offering transparency into scheduling decisions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260128] Knowledge-Aware Evolution for Streaming Federated Continual Learning with Category Overlap and without Task Identifiers"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [federated continual learning, adaptive inference model switching, gradient-balanced replay, kernel spectral boundary buffer, category overlap, streaming data]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Sixing Tan, Xianmin Liu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Harbin Institute of Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19788",children:"https://arxiv.org/pdf/2601.19788"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes FedKACE, a method for streaming federated continual learning that handles category overlap and lacks task identifiers. Its core techniques include an adaptive model switching mechanism, a gradient-balanced replay scheme, and a kernel spectral boundary buffer to balance new learning and old knowledge retention. Experiments show the method is effective in this challenging setting."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260128] Self-Sovereign Identity and eIDAS 2.0: An Analysis of Control, Privacy, and Legal Implications"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [digital identity regulation], [Self-Sovereign Identity, eIDAS 2.0, European Digital Identity Architecture and Reference Framework, EUDI Wallet, Blockchain, Decentralized Identity]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Nacereddine Sitouah, Marco Esposito, Francesco Bruschi"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Polytechnic University of Milan"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19837",children:"https://arxiv.org/pdf/2601.19837"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper analyzes the key provisions of the eIDAS 2.0 Regulation and the European Digital Identity Architecture and Reference Framework (ARF) to assess legislative gaps and implementation challenges. It concludes by evaluating the extent to which these emerging European digital identity implementations align with the principles of Self-Sovereign Identity (SSI) models."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260128] KUBEDIRECT: Unleashing the Full Power of the Cluster Manager for Serverless Computing"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [serverless computing], [Kubernetes, cluster manager, direct message passing, hierarchical write-back cache, narrow waist]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Sheng Qi, Zhiquan Zhang, Xuanzhe Liu, Xin Jin"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Peking University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19160",children:"https://arxiv.org/pdf/2601.19160"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper presents KUBEDIRECT, a system that enhances Kubernetes for serverless computing by bypassing the API Server for direct message passing between controllers, using a "narrow waist" structure as a hierarchical write-back cache for state management. It achieves this with minimal code changes (~150 LoC per controller) while maintaining compatibility. Experiments show it reduces serving latency by 26.7x compared to Knative and matches the performance of clean-slate designs.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260128] Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [Axe Layout, tiling, sharding, replication, DSL, compiler, collective primitives]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Bohan Hou, Hongyi Jin, Guanjie Wang, Jinqi Chen, Yaxing Cai, Lijie Yang, Zihao Ye, Yaoyao Ding, Ruihang Lai, Tianqi Chen"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Carnegie Mellon University, Shanghai Jiao Tong University, NVIDIA, Princeton University, University of Toronto"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19092",children:"https://arxiv.org/pdf/2601.19092"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Axe Layout, a hardware-aware abstraction that unifies data layout management (like tiling and sharding) across distributed devices and on-device memory hierarchies. It also presents a compiler and DSL built on this abstraction to generate efficient kernels. Experiments show this unified approach achieves performance close to hand-tuned kernels on modern GPUs and multi-device environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260128] Revisiting Parameter Server in LLM Post-Training"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [post-training], [parameter server, fully sharded data parallel, on-demand communication, point-to-point communication, load balancing]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Xinyi Wan, Penghui Qi, Guangxing Huang, Chaoyi Ruan, Min Lin, Jialin Li"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Sea AI Lab, National University of Singapore"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19362",children:"https://arxiv.org/pdf/2601.19362"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes On-Demand Communication (ODC), a method that adapts the parameter server paradigm into Fully Sharded Data Parallel training by replacing collective communication with direct point-to-point communication. This approach reduces synchronization barriers and improves load balancing under imbalanced workloads caused by variable sequence lengths in LLM post-training. The results show that ODC consistently improves device utilization and training throughput, achieving up to a 36% speedup over standard FSDP."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260128] Native LLM and MLLM Inference at Scale on Apple Silicon"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [content-based prefix caching, continuous batching, unified memory architecture, MLX, vision encoding, content hashing]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Wayner Barrios"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Wiqonn Technologies"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19139",children:"https://arxiv.org/pdf/2601.19139"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents vllm-mlx, a framework for efficient LLM and multimodal LLM inference on Apple Silicon built natively on the MLX framework. Its key innovations include continuous batching for text models and a content-based prefix caching mechanism that eliminates redundant vision encoding for multimodal inputs by using content hashing. The evaluation shows significant throughput improvements over llama.cpp for text models and up to 28x speedup for repeated image queries, demonstrating efficient inference on consumer Apple hardware."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260128] Modular Foundation Model Inference at the Edge: Network-Aware Microservice Optimization"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [microservice architecture, Lyapunov optimization, effective capacity theory, integer programming, edge computing, QoS, network-aware deployment]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Juan Zhu, Zixin Wang, Shenghui Song, Jun Zhang, Khaled Ben Letaief"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," The Hong Kong University of Science and Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19563",children:"https://arxiv.org/pdf/2601.19563"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a microservice-based framework for deploying foundation models at the edge, using a two-tier strategy with statically placed core services and dynamically orchestrated light services to manage resource contention. It employs a network-aware integer program for long-term core service placement and an online Lyapunov-based controller for light service orchestration to provide probabilistic latency guarantees. The simulation results show the framework achieves over 84% on-time task completion with moderate cost and maintains robustness under scaling system loads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260128] Enabling SSI-Compliant Use of EUDI Wallet Credentials through Trusted Execution Environment and Zero-Knowledge Proof"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [digital identity systems], [Trusted Execution Environment, Zero-Knowledge Proof, Self-Sovereign Identity, eIDAS, blockchain, Decentralized Identifiers, Verifiable Credentials]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Nacereddine Sitouah, Francesco Bruschi, Stefano De Cillis"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Polytechnic University of Milan"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19893",children:"https://arxiv.org/pdf/2601.19893"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a new architecture that combines Trusted Execution Environments (TEEs) and Zero-Knowledge Proofs (ZKPs) to enable the use of credentials from the centralized European Digital Identity Wallet (EUDIW) in a Self-Sovereign Identity (SSI) compliant environment. The main conclusion is that this approach can bridge the gap between the current eIDAS 2.0 implementation and true SSI principles, allowing for secure and legally compliant credentials to be used in a decentralized, user-controlled manner."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260128] Decentralized Nonsmooth Nonconvex Optimization with Client Sampling"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [decentralized optimization], [Goldstein stationary point, client sampling, stochastic first-order method, spectral gap, zeroth-order optimization]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Xinyan Chen, Weiguo Gao, Luo Luo"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Not specified"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19381",children:"https://arxiv.org/pdf/2601.19381"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a decentralized stochastic first-order method with client sampling for nonsmooth nonconvex optimization, achieving optimal sample complexity and improved communication complexity for finding Goldstein stationary points. The method is also extended to zeroth-order optimization, and numerical experiments demonstrate its empirical advantage."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260128] Accelerating radio astronomy imaging with RICK: a step towards SKA-Mid and SKA-Low"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [high-performance computing], [distributed Fast Fourier Transform, HeFFTe library, GPU acceleration, communication overhead reduction]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Giovanni Lacopo, Emanuele De Rubeis, Claudio Gheller, Giuliano Taffoni, Luca Tornatore"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," INAF-Osservatorio astronomico di Brera, ICSC \u2013 Centro Nazionale di Ricerca in High Performance Computing, Big Data e Quantum Computing, INAF-Osservatorio Astronomico di Trieste, Universit\xe0 di Bologna, Istituto di Radioastronomia INAF"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19714",children:"https://arxiv.org/pdf/2601.19714"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents RICK 2.0, a radio astronomy imaging software that uses the HeFFTe library for distributed Fast Fourier Transforms to achieve high performance and portability across CPUs and GPUs. The new implementation significantly reduces communication overheads that previously dominated runtime, enabling efficient scaling for large datasets. The authors validate the software with data from MeerKAT and LOFAR, demonstrating it as a scalable solution for future SKA observatories."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 29'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19099",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19280",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Output Feedback Stabilization of Linear Systems via Policy Gradient Methods ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19284",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19122",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] LLM-Enhanced Reinforcement Learning for Long-Term User Satisfaction in Interactive Recommendation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19585",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Save the Good Prefix: Precise Error Penalization via Process-Supervised RL to Enhance LLM Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18984",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19620",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19707",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Safe Exploration via Policy Priors ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19612",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Vector-Valued Distributional Reinforcement Learning Policy Evaluation: A Hilbert Space Embedding Approach ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18952",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Tracking Drift: Variation-Aware Entropy Scheduling for Non-Stationary Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19624",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Improving Policy Exploitation in Online Reinforcement Learning with Instant Retrospect Action ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19720",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Analysis of Control Bellman Residual Minimization for Markov Decision Problem ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18840",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] CHEHAB RL: Learning to Optimize Fully Homomorphic Encryption Computations ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19367",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Reimagining Peer Review Process Through Multi-Agent Mechanism Design ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19778",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Task-Centric Policy Optimization from Misaligned Motion Priors ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19411",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19697",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Differential Voting: Loss Functions For Axiomatically Diverse Aggregation of Heterogeneous Preferences ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18824",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] OSIRIS: Bridging Analog Circuit Design and Machine Learning with Scalable Dataset Generation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19439",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19810",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] A Unifying View of Coverage in Linear Off-Policy Evaluation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19030",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Variational Quantum Circuit-Based Reinforcement Learning for Dynamic Portfolio Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18811",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] APC-RL: Exceeding Data-Driven Behavior Priors with Adaptive Policy Composition ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19452",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Innovator-VL: A Multimodal Large Language Model for Scientific Discovery ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19325",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Structure-based RNA Design by Step-wise Optimization of Latent Diffusion Model ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19232",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] From Observations to Events: Event-Aware World Model for Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19336",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Self-Distillation Enables Continual Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19897",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Implicit Q-Learning and SARSA: Liberating Policy Control from Step-Size Calibration ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18907",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Reinforcement Learning for Quantum Technology ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18953",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 13'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18963",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] In-Network Collective Operations: Game Changer or Challenge for AI Workloads? ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19132",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Tracking Drift: Variation-Aware Entropy Scheduling for Non-Stationary Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19624",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] RPO",":Reinforcement"," Fine-Tuning with Partial Reasoning Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19404",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Cross-Domain Offshore Wind Power Forecasting: Transfer Learning Through Meteorological Clusters ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19674",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Whitespaces Don't Lie: Feature-Driven and Embedding-Based Approaches for Detecting Machine-Generated Code ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19264",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Fuzzy expert system for the process of collecting and purifying acidic water: a digital twin approach ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19527",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Accelerated Multiple Wasserstein Gradient Flows for Multi-objective Distributional Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19220",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19810",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] APC-RL: Exceeding Data-Driven Behavior Priors with Adaptive Policy Composition ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19452",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] Randomization Boosts KV Caching, Learning Balances Query Load: A Joint Perspective ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18999",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] E-QRGMM: Efficient Generative Metamodeling for Covariate-Dependent Uncertainty Quantification ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19256",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260128] LabelKAN -- Kolmogorov-Arnold Networks for Inter-Label Learning: Avian Community Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18818",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"2026-01-29",children:"2026-01-29"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"cs.DC total: 19"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] Delta Fair Sharing: Performance Isolation for Multi-Tenant Storage Systems"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [storage systems], [delta-fairness, delta-Pareto-efficiency, FAIRDB, RocksDB, performance isolation, preemption delays]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Tyler Griggs, Soujanya Ponnapalli, Dev Bali, Wenjie Ma, James DeLoye, Audrey Cheng, Jaewan Hong, Natacha Crooks, Scott Shenker, Ion Stoica, Matei Zaharia"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of California, Berkeley"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20030",children:"https://arxiv.org/pdf/2601.20030"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Delta Fair Sharing, a family of algorithms designed to provide performance isolation in multi-tenant storage systems by bounding the delay in receiving a fair share of resources. The method is implemented in FAIRDB, an extension of RocksDB, and evaluation shows it isolates well-behaved clients from high-demand workloads better than existing alternatives."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] A Data-Informed Local Subspaces Method for Error-Bounded Lossy Compression of Large-Scale Scientific Datasets"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [data compression], [error-bounded lossy compression, localized subspaces, distributed computing, MPI]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Arshan Khan, Rohit Deshmukh, Ben O'Neill"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Central Florida, RNET Technologies Inc."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20113",children:"https://arxiv.org/pdf/2601.20113"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Discontinuous DLS, a data-driven error-bounded lossy compression method that uses localized spatial and temporal subspaces informed by the underlying data structure to improve compression efficiency. The method is implemented in a distributed MPI environment and is shown to significantly reduce storage requirements while preserving data fidelity for large-scale scientific datasets, outperforming other state-of-the-art compressors."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] DecHW: Heterogeneous Decentralized Federated Learning Exploiting Second-Order Information"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [decentralized federated learning, second-order information, Hessian matrix, consensus weights, robust aggregation]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Adnan Ahmad, Chiara Boldrini, Lorenzo Valerio, Andrea Passarella, Marco Conti"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Deakin University, Italian National Research Council (CNR)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19938",children:"https://arxiv.org/pdf/2601.19938"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes DecHW, a novel decentralized federated learning method that addresses data and model heterogeneity by using second-order information (Hessian approximations) to generate consensus weights for robustly aggregating neighbor updates. This approach reduces parameter variations across devices. Experiments show it achieves strong model generalizability with lower communication costs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] DABench-LLM: Standardized and In-Depth Benchmarking of Post-Moore Dataflow AI Accelerators for LLMs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [benchmarking, dataflow architecture, performance profiling, scalability analysis, resource efficiency, load balance]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ziyu Hu, Zhiqing Zhong, Weijian Zheng, Zhijing Ye, Xuwei Tan, Xueru Zhang, Zheng Xie, Rajkumar Kettimuthu, Xiaodong Yu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Stevens Institute of Technology, Binghamton University, The Ohio State University, Argonne National Laboratory"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19904",children:"https://arxiv.org/pdf/2601.19904"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces DABench-LLM, a benchmarking framework for evaluating LLM training workloads on dataflow-based AI accelerators. It combines intra-chip profiling and inter-chip scalability analysis to assess performance metrics like resource efficiency. The framework was validated on three commercial accelerators, revealing bottlenecks and providing optimization strategies, demonstrating its effectiveness across diverse hardware platforms."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] GPU-Augmented OLAP Execution Engine: GPU Offloading"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [database systems], [GPU offloading, vectorized execution, key-only transfer, late materialization, risk-aware gating, hybrid execution engine]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ilsun Chang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Independent Researcher"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19911",children:"https://arxiv.org/pdf/2601.19911"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper proposes a hybrid OLAP execution engine that selectively offloads high-impact primitives like Top-K selection and join probe to the GPU, using key-only transfer and late materialization to reduce data movement. It introduces a "Risky Gate" to trigger offloading only when beneficial based on cost analysis. The main conclusion is that this risk-aware, gated offloading approach improves tail latency (P95/P99) compared to always-on GPU offloading.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] StreamFusion: Scalable Sequence Parallelism for Distributed Inference of Diffusion Transformers on GPUs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion inference], [sequence parallelism, topology-aware communication, Torus Attention, one-sided communication, Ulysses Attention, Ring Attention]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Jiacheng Yang, Jun Wu, Yaoyao Ding, Zhiying Xu, Yida Wang, Gennady Pekhimenko"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Toronto, Vector Institute, Amazon Web Services, NVIDIA"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20273",children:"https://arxiv.org/pdf/2601.20273"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces StreamFusion, a distributed inference engine for Diffusion Transformers that uses a topology-aware sequence parallelism technique, a novel Torus Attention method to overlap communication with computation, and one-sided communication to reduce overhead. It aims to overcome the communication bottlenecks of existing methods like Ulysses and Ring Attention. The experiments show that StreamFusion outperforms the state-of-the-art by an average of 1.35x."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] Understanding Bottlenecks for Efficiently Serving LLM Inference With KV Offloading"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [KV cache offloading, PCIe bandwidth, analytical framework, memory-bound execution, critical ratio \u03ba_crit, empirical characterization, hardware interconnects, scheduling algorithms]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," William Meng, Benjamin Lee, Hong Wang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Stanford University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19910",children:"https://arxiv.org/pdf/2601.19910"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper develops an analytical framework to model the performance bottleneck in LLM inference when using KV cache offloading to CPU DRAM, defining a critical token ratio (\u03ba_crit) where execution becomes memory-bound due to PCIe bandwidth limitations. It concludes that typical workloads far exceed this threshold, causing severe GPU underutilization and latency dominated by data transfers, motivating optimizations in hardware, model architecture, and scheduling."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] SuperInfer: SLO-Aware Rotary Scheduling and Memory Management for LLM Inference on Superchips"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [rotary scheduling, KV cache offloading, NVLink-C2C, SLO-aware scheduling, memory management]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Jiahuan Yu, Mingtao Hu, Zichao Lin, Minjia Zhang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Illinois Urbana-Champaign"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20309",children:"https://arxiv.org/pdf/2601.20309"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces SuperInfer, a system for LLM inference on Superchips like NVIDIA GH200. It proposes RotaSched, an SLO-aware rotary scheduler, and DuplexKV, an engine for efficient KV cache transfer over NVLink-C2C, to address memory bottlenecks and meet latency SLOs. The evaluation shows it significantly improves TTFT SLO attainment rates while maintaining throughput, demonstrating the effectiveness of co-designing scheduling and memory management for Superchips."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] RAPID-Graph: Recursive All-Pairs Shortest Paths Using Processing-in-Memory for Dynamic Programming on Graphs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [graph processing], [processing-in-memory, phase-change memory, Floyd-Warshall, Min-Plus, recursion-aware partitioner, 2.5D PIM stack]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yanru Chen, Zheyu Li, Keming Fan, Runyang Tian, John Hsu, Weihong Xu, Minxuan Zhou, Tajana Rosing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of California San Diego, Ecole Polytechnique F\xe9d\xe9rale de Lausanne, Illinois Institute of Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19907",children:"https://arxiv.org/pdf/2601.19907"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes RAPID-Graph, a co-designed processing-in-memory system for computing all-pairs shortest paths on large graphs. It uses a recursion-aware partitioner to decompose graphs for in-place execution and a 2.5D PIM stack with phase-change memory. The system demonstrates significant speedups and energy efficiency compared to GPU clusters and prior PIM accelerators."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] Securing AI Agents in Cyber-Physical Systems: A Survey of Environmental Interactions, Deepfake Threats, and Defenses"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [SENTINEL framework, deepfake detection, provenance-grounded trust, physics-grounded trust, defense-in-depth, Model Context Protocol (MCP)]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Mohsen Hatami, Van Tuan Pham, Hozefa Lakadawala, Yu Chen"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Binghamton University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20184",children:"https://arxiv.org/pdf/2601.20184"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This survey paper reviews security threats to AI agents in cyber-physical systems, focusing on deepfake attacks and vulnerabilities from protocols like MCP. It organizes the literature using the SENTINEL framework, a lifecycle-aware methodology for threat analysis and defense selection. The main conclusion is that detection mechanisms alone are insufficient for safety-critical systems, highlighting the need for provenance- and physics-grounded trust mechanisms within a defense-in-depth architecture."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] Meeting SLOs, Slashing Hours: Automated Enterprise LLM Optimization with OptiKIT"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [model compression, dynamic resource allocation, pipeline orchestration, automated optimization, GPU throughput]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Nicholas Santavas, Kareem Eissa, Patrycja Cieplicka, Piotr Florek, Matteo Nulli, Stefan Vasilev, Seyyed Hadi Hashemi, Antonios Gasteratos, Shahram Khadivi"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," eBay, Democritus University of Thrace"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20408",children:"https://arxiv.org/pdf/2601.20408"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces OptiKIT, an automated distributed framework for optimizing Large Language Models (LLMs) in enterprise settings. It automates complex workflows like model compression and tuning, enabling non-expert teams to achieve significant performance gains. In production, the system delivers over 2x GPU throughput improvement, democratizing efficient LLM deployment."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] OnePiece: A Large-Scale Distributed Inference System with RDMA for Complex AI-Generated Content (AIGC) Workflows"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [diffusion inference], [microservices, RDMA, double-ring buffer, dynamic resource allocation]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," June Chen, Neal Xu, Gragas Huang, Bok Zhou, Stephen Liu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Tencent"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20655",children:"https://arxiv.org/pdf/2601.20655"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces OnePiece, a distributed inference system that decomposes AIGC workflows into fine-grained microservices and uses RDMA for efficient communication. Its key innovations include a double-ring buffer to prevent deadlocks and a dynamic Node Manager for elastic resource allocation. Experiments show it reduces GPU consumption by 16x in an image-to-video generation task compared to monolithic pipelines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] Rethinking Thread Scheduling under Oversubscription: A User-Space Framework for Coordinating Multi-runtime and Multi-process Workloads"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [user-space scheduling, cooperative scheduling, oversubscription, runtime systems, SCHED_COOP]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Aleix Roca, Vicen\xe7 Beltran"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Barcelona Supercomputing Center"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20435",children:"https://arxiv.org/pdf/2601.20435"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents the User-space Scheduling Framework (USF) and its default cooperative policy SCHED_COOP, which reduces interference by switching threads only upon blocking. The framework is implemented by extending the GNU C library with the nOS-V runtime to coordinate multiple runtimes without invasive application changes. Evaluations show performance gains up to 2.4x in oversubscribed multi-process scenarios."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] LIFT: Byzantine Resilient Hub-Sampling"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [peer-to-peer networks], [hub-sampling, Byzantine fault tolerance, cryptographically secure PRNG, peer sampling protocol]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Mohamed Amine Legheraba, Nour Rachdi, Maria Gradinariu Potop-Butucaru, S\xe9bastien Tixeuil"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," LIP6-NPA, Sorbonne University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20368",children:"https://arxiv.org/pdf/2601.20368"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces LIFT, a protocol that extends the Elevator hub-sampling protocol by using a cryptographically secure pseudo-random number generator for hub selection to resist Byzantine attacks. It shows that the original Elevator is vulnerable to manipulation by even 2% of Byzantine nodes, while LIFT remains robust with up to 10% Byzantine nodes, highlighting the necessity of secure randomness in decentralized systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] Graph-Structured Deep Learning Framework for Multi-task Contention Identification with High-dimensional Metrics"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [graph-based modeling, multi-task classification, representation transformation, adaptive loss weighting, high-dimensional metrics]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Xiao Yang, Yinan Ni, Yuqi Tang, Zhimin Qiu, Chen Wang, Tingzhou Yuan"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Santa Clara University, University of Illinois at Urbana-Champaign, New York University, University of Southern California, University of Missouri\u2013Kansas City, Boston University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20389",children:"https://arxiv.org/pdf/2601.20389"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a graph-structured deep learning framework that uses representation transformation and graph-based modeling to capture dependencies in high-dimensional system metrics for multi-task contention identification. It employs a task decoupling mechanism and adaptive multi-task loss weighting to distinguish different contention patterns. Experiments show the method improves accuracy and pattern recognition for performance management in complex computing environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] SA-PEF: Step-Ahead Partial Error Feedback for Efficient Federated Learning"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [federated learning, gradient compression, error feedback, step-ahead correction, partial error feedback, non-convex optimization, communication efficiency]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Dawit Kiros Redie, Reza Arablouei, Stefan Werner"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Norwegian University of Science and Technology (NTNU), Aalto University, CSIRO's Data61"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20738",children:"https://arxiv.org/pdf/2601.20738"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SA-PEF, a method combining step-ahead correction with partial error feedback to improve communication efficiency in federated learning under non-IID data. It accelerates early training by controlling residual error contraction while maintaining long-term stability. Experiments show SA-PEF consistently reaches target accuracy faster than standard error feedback."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] AutoOverlap: Enabling Fine-Grained Overlap of Computation and Communication with Chunk-Based Scheduling"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [communication overlap, chunk-based scheduling, source-to-source compiler, Triton, multi-GPU, kernel fusion]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Xinwei Qiang, Yue Guan, Zhengding Hu, Yufei Ding, Adnan Aziz"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of California, San Diego, Meta"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20595",children:"https://arxiv.org/pdf/2601.20595"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," AutoOverlap is a compiler and runtime that enables fine-grained overlap of computation and communication within a single fused kernel using a chunk-based scheduling abstraction. It transforms Triton kernels to align computation with chunk availability, reducing synchronization and launch overhead. The approach achieves an average speedup of 1.3\xd7 and up to 4.7\xd7 on multi-GPU workloads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] Agentic Fog: A Policy-driven Framework for Distributed Intelligence in Fog Computing"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [policy-driven agents, exact potential game, bounded-rational best-response dynamics, peer-to-peer coordination, shared memory, asynchronous updates]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Saeed Akbar, Muhammad Waqas, Rahmat Ullah"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Sarhad University of Science & Information Technology, GIK Institute of Engineering Sciences & Technology, University of Essex"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20764",children:"https://arxiv.org/pdf/2601.20764"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Agentic Fog, a framework where fog nodes act as policy-driven autonomous agents coordinating via peer-to-peer interactions and shared memory, formalized as an exact potential game. The system is proven to converge and remain stable under asynchronous updates and node failures. Simulations show it achieves lower latency and better adapts to dynamic demand than greedy heuristics and integer linear programming."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260129] Beyond GEMM-Centric NPUs: Enabling Efficient Diffusion LLM Sampling"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [non-GEMM vector primitives, in-place memory reuse, decoupled mixed-precision memory hierarchy, iterative denoising, vocabulary-wide logits reduction]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Binglei Lou, Haoran Wu, Yao Lai, Jiayi Nie, Can Xiao, Xuan Guo, Rika Antonova, Robert Mullins, Aaron Zhao"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Imperial College London, University of Cambridge"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20706",children:"https://arxiv.org/pdf/2601.20706"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a new NPU architecture optimized for the sampling phase of Diffusion LLMs, which is bottlenecked by irregular memory accesses and large logits tensors. The design uses lightweight vector instructions, memory reuse strategies, and a specialized memory hierarchy to accelerate sampling. The optimizations achieve up to 2.53x speedup over a comparable GPU, addressing a key inefficiency in dLLM inference."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 26'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20305",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20193",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20088",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] In-Context Reinforcement Learning From Suboptimal Historical Data ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20116",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20209",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20103",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] A Reinforcement Learning Based Universal Sequence Design for Polar Codes ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20118",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20126",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20221",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19969",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Techno-economic optimization of a heat-pipe microreactor, part II: multi-objective optimization analysis ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20079",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Distributional value gradients for stochastic environments ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20071",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Proactive SFC Provisioning with Forecast-Driven DRL in Data Centers ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20229",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] PsychePass: Calibrating LLM Therapeutic Competence via Trajectory-Anchored Tournaments ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20330",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Fair Recourse for All: Ensuring Individual and Group Fairness in Counterfactual Explanations ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20449",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Inequality in Congestion Games with Learning Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20578",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Ranking-aware Reinforcement Learning for Ordinal Ranking ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20585",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20614",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20687",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20714",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20753",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Less is More: Clustered Cross-Covariance Control for Offline RL ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20765",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] SERA: Soft-Verified Efficient Repository Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20789",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Reinforcement Learning via Self-Distillation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20802",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20829",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Exploring the holographic entropy cone via reinforcement learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19979",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 17'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Order-Optimal Sample Complexity of Rectified Flows ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20250",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] How Much Progress Has There Been in NVIDIA Datacenter GPUs? ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20115",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] NeuraLSP: An Efficient and Rigorous Neural Left Singular Subspace Preconditioner for Conjugate Gradient Methods ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20174",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] CHIME: Chiplet-based Heterogeneous Near-Memory Acceleration for Edge Multimodal LLM Inference ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19908",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] PiC-BNN: A 128-kbit 65 nm Processing-in-CAM-Based End-to-End Binary Neural Network Accelerator ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19920",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Continuous-Flow Data-Rate-Aware CNN Inference on FPGA ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19940",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Bench4HLS: End-to-End Evaluation of LLMs in High-Level Synthesis Code Generation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19941",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] MeanCache: From Instantaneous to Average Velocity for Accelerating Flow Matching Inference ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.19961",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] MALLOC: Benchmarking the Memory-aware Long Sequence Compression for Large Sequential Recommendation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20234",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding in LVLMs ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20357",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20375",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20637",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] REASON: Accelerating Probabilistic Logical Reasoning for Scalable Neuro-Symbolic Intelligence ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20784",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] SERA: Soft-Verified Efficient Repository Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20789",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Reinforcement Learning via Self-Distillation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20802",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Leveraging Second-Order Curvature for Efficient Learned Image Compression: Theory and Empirical Evidence ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20769",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260129] Neural Quantum States in Mixed Precision ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20782",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"2026-01-30",children:"2026-01-30"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"cs.DC total: 15"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] ZipMoE: Efficient On-Device MoE Serving via Lossless Compression and Cache-Affinity Scheduling"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [lossless compression, cache-affinity scheduling, on-device serving, mixture-of-experts, memory optimization]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yuchen Yang, Yaru Zhao, Pu Yang, Shaowei Wang, Zhi-Hua Zhou"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Nanjing University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21198",children:"https://arxiv.org/pdf/2601.21198"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents ZipMoE, a system for efficient on-device inference of Mixture-of-Experts models. It uses a co-design of lossless compression and cache-aware scheduling to shift the bottleneck from I/O to computation, significantly reducing latency and increasing throughput compared to prior systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] Belief Propagation Converges to Gaussian Distributions in Sparsely-Connected Factor Graphs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [probabilistic inference], [belief propagation, gaussian belief propagation, central limit theorem, factor graphs, stereo depth estimation]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Tom Yates, Yuzhou Cheng, Ignacio Alzugaray, Danyal Akarca, Pedro A.M. Mediano, Andrew J. Davison"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Imperial College London"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21935",children:"https://arxiv.org/pdf/2601.21935"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper provides a theoretical guarantee for the use of Gaussian Belief Propagation (GBP) in non-Gaussian problems. It proves, using the Central Limit Theorem, that variable beliefs in sparsely-connected factor graphs converge to Gaussian distributions under certain assumptions. The conclusion is validated experimentally in a stereo depth estimation task, showing beliefs become Gaussian after few iterations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] Self-Adaptive Probabilistic Skyline Query Processing in Distributed Edge Computing via Deep Reinforcement Learning"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [deep deterministic policy gradient, markov decision process, probabilistic skyline query, edge computing, self-adaptive framework]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Chuan-Chi Lai"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," National Chung Cheng University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21855",children:"https://arxiv.org/pdf/2601.21855"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes SA-PSKY, a self-adaptive framework that uses a Deep Deterministic Policy Gradient (DDPG) agent to dynamically adjust filtering thresholds for probabilistic skyline queries in distributed edge computing. The method formulates the problem as a Markov Decision Process to optimize computation and communication costs in real-time. The experiments show the framework significantly reduces communication overhead and total response time compared to static baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] Maxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [distributed systems coordination], [maxwait, Lingua Franca, logical-time consistency, bounded communication latencies, fault detection, PTIDES, Chandy-and-Misra, Time-Warp]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Francesco Paladino, Shulu Li, Edward A. Lee"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," UC Berkeley"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21146",children:"https://arxiv.org/pdf/2601.21146"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces maxwait, a generalized coordination mechanism for distributed time-sensitive systems that explicitly configures trade-offs between timing and consistency. It subsumes classical distributed methods and enables real-time behavior, implemented as an extension of the Lingua Franca language. The mechanism enforces logical-time consistency under bounded latencies and provides structured fault handling when bounds are violated."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [invocation distance, proactive prefetching, priority-based eviction, agent-specific memory, sparse agent activation]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Zaifeng Pan, Yipeng Shen, Zhengding Hu, Zhuang Wang, Aninda Manocha, Zheng Wang, Zhongkai Yu, Yue Guan, Yufei Ding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of California, San Diego, Amazon Web Services"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21473",children:"https://arxiv.org/pdf/2601.21473"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"}),' The paper introduces ScaleSim, a system for efficient LLM serving in large-scale multi-agent simulations. It proposes a memory management technique based on "invocation distance" to predict agent request order, enabling proactive prefetching and eviction to reduce GPU memory pressure. The system achieves up to 1.74x speedup over existing methods like SGLang.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] Ira: Efficient Transaction Replay for Distributed Systems"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [distributed systems], [primary-backup replication, hint-based replay, cache management, Belady algorithm, Ethereum block execution]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Adithya Bhat, Harshal Bhadreshkumar Shah, Mohsen Minaei"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Visa Research"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21286",children:"https://arxiv.org/pdf/2601.21286"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Ira, a framework that accelerates transaction replay in primary-backup systems by having the primary send compact hints about future access patterns to backups. In a case study on Ethereum (Ira-L), these hints enable backups to prefetch data and optimize cache management, reducing replay time significantly. The evaluation shows that Ira-L achieves a median 25\xd7 speedup per block with minimal overhead, demonstrating its effectiveness in improving consensus latency."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] EWSJF: An Adaptive Scheduler with Hybrid Partitioning for Mixed-Workload LLM Inference"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [EWSJF, Refine-and-Prune, Dynamic Queue Routing, Density-Weighted Scoring, Bayesian Meta-Optimization, vLLM]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Bronislav Sidik, Chaya Levi, Joseph Kampeas"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Toga Networks (Huawei)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21758",children:"https://arxiv.org/pdf/2601.21758"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces EWSJF, an adaptive request-level scheduler for mixed LLM workloads that uses real-time learning to group requests and prioritize them, improving both throughput and latency. It integrates components like unsupervised partitioning and Bayesian optimization to tune scheduling parameters dynamically. The results show significant improvements in throughput and latency reduction for short requests compared to standard FCFS scheduling."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] Nimbus: A Unified Embodied Synthetic Data Generation Framework"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [modular four-layer architecture, decoupled execution model, dynamic pipeline scheduling, global load balancing, distributed fault tolerance, backend-specific rendering optimizations]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Zeyu He, Yuchang Zhang, Yuanzhen Zhou, Miao Tao, Hengjie Li, Yang Tian, Jia Zeng, Tai Wang, Wenzhe Cai, Yilun Chen, Ning Gao, Jiangmiao Pang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Shanghai Artificial Intelligence Laboratory, Shanghai Innovation Institute"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21449",children:"https://arxiv.org/pdf/2601.21449"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents Nimbus, a unified framework for generating embodied synthetic data. Its core method is a modular architecture that decouples trajectory planning, rendering, and storage into asynchronous stages, employing dynamic scheduling and load balancing. The main conclusion is that this system achieves a 2-3x throughput improvement and robust operation for large-scale data generation, serving as the backbone for the InternData suite."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] Deep Reinforcement Learning for Fault-Adaptive Routing in Eisenstein-Jacobi Interconnection Topologies"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [reinforcement learning, greedy adaptive routing, Dijkstra's algorithm, Eisenstein-Jacobi networks, fault-adaptive routing]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Mohammad Walid Charrwi, Zaid Hussain"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Kuwait University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21090",children:"https://arxiv.org/pdf/2601.21090"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a reinforcement learning-based approach for adaptive routing in faulty Eisenstein-Jacobi interconnection networks. The RL agent, trained with a multi-objective reward function, learns to navigate around clustered failures and significantly outperforms greedy routing while nearly matching the theoretical optimum of Dijkstra's algorithm without requiring global topology knowledge. The results demonstrate that RL-based policies provide a robust and practical solution for self-healing communication in fault-prone many-core networks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] DASH: Deterministic Attention Scheduling for High-throughput Reproducible LLM Training"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [deterministic attention, scheduling, DAG, FlashAttention, gradient accumulation, pipeline stalls]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Xinwei Qiang, Hongmin Chen, Shixuan Sun, Jingwen Leng, Xin Liu, Minyi Guo"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Shanghai Jiao Tong University, ByteDance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21824",children:"https://arxiv.org/pdf/2601.21824"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces DASH, a method that formulates the backward pass of deterministic attention as a scheduling problem on a Directed Acyclic Graph (DAG) and proposes two scheduling strategies to minimize critical path length. The strategies, Descending Q-Tile Iteration and Shift Scheduling, reduce pipeline stalls and hardware underutilization. Empirical evaluation shows DASH improves the throughput of the deterministic attention backward pass by up to 1.28x, narrowing the performance gap for reproducible LLM training."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] Heterogeneous Computing: The Key to Powering the Future of AI Agent Inference"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [operational intensity, capacity footprint, roofline model, KV cache, disaggregated serving, heterogeneous computing, memory bandwidth, memory capacity, optical I/O]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yiren Zhao, Junyi Liu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Imperial College London, Microsoft Research"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.22001",children:"https://arxiv.org/pdf/2601.22001"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Operational Intensity (OI) and Capacity Footprint (CF) as metrics to analyze AI agent inference workloads, revealing bottlenecks in memory capacity and bandwidth. It concludes that future systems require disaggregated serving and system-level heterogeneity, including specialized accelerators and decoupled compute-memory, to efficiently handle diverse agentic workflows."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] Where Do the Joules Go? Diagnosing Inference Energy Consumption"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [energy measurement, framework, GPU utilization, memory usage, throughput per watt]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Jae-Won Chung, Ruofan Wu, Jeff J. Ma, Mosharaf Chowdhury"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Michigan"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.22076",children:"https://arxiv.org/pdf/2601.22076"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper conducts a large-scale measurement study of inference time and energy across 46 generative AI models and 7 tasks on NVIDIA GPUs, revealing order-of-magnitude variations. Based on these observations, it proposes a diagnostic framework that attributes time and energy consumption to latent metrics like memory and utilization, which are affected by factors across algorithm, software, and hardware layers."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [federated learning, LoRA, parameter-efficient fine-tuning, adaptive aggregation, clinical information extraction]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Anran Li, Yuanyuan Chen, Wenjun Long, Yu Yin, Yan Hu, Hyunjae Kim, Weipeng Zhou, Yujia Zhou, Hongyi Peng, Yang Ren, Xuguang Ai, Zhenyue Qin, Ming Hu, Xiaoxiao Li, Han Yu, Yih-Chung Tham, Lucila Ohno-Machado, Hua Xu, Qingyu Chen"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Yale University, Nanyang Technological University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.22124",children:"https://arxiv.org/pdf/2601.22124"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Fed-MedLoRA, a federated and parameter-efficient framework for training large language models on medical data. It uses Low-Rank Adaptation (LoRA) to reduce communication overhead and incorporates adaptive aggregation to handle data heterogeneity across institutions. The framework demonstrates effectiveness in clinical information extraction tasks, improving generalizability and safety compared to single-institution training."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] bigMICE: Multiple Imputation of Big Data"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [multiple imputation, MICE, Apache Spark, memory efficiency, big data]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Hugo Morvan, Jonas Agholme, Bjorn Eliasson, Katarina Olofsson, Ludger Grote, Fredrik Iredahl, Oleg Sysoev"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Link\xf6ping University, G\xf6teborg University, Region V\xe4stra G\xf6taland"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21613",children:"https://arxiv.org/pdf/2601.21613"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents the bigMICE package, which adapts the Multiple Imputation by Chained Equations (MICE) framework for big data using Apache Spark to control memory usage and enable processing on hardware with limited memory. The method was tested on a large medical registry and was found to be more memory-efficient and faster than a common MICE implementation, while also producing high-quality imputations even with high proportions of missing data."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260130] Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [Multi-Agent Reinforcement Learning (MARL), Multi-Agent Actor-Critic (MAAC), Centralized Critic, Decentralized Critics, Monte Carlo methods]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Shuo Liu, Tianle Chen, Ryan Amiri, Christopher Amato"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Northeastern University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21972",children:"https://arxiv.org/pdf/2601.21972"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes Multi-Agent Actor-Critic (MAAC) methods, specifically CoLLM-CC and CoLLM-DC, to optimize decentralized collaboration among Large Language Models. The main conclusion is that while Monte Carlo methods and the decentralized critic approach can match the centralized critic's performance on short-horizon tasks, the centralized critic (CoLLM-CC) significantly outperforms them on long-horizon or sparse-reward tasks."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 50'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Intrinsic Reward Policy Optimization for Sparse-Reward Environments ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21391",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21468",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21711",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Log2Motion: Biomechanical Motion Synthesis from Touch Logs ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21043",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21548",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Language-based Trial and Error Falls Behind in the Era of Experience ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21754",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] The Surprising Difficulty of Search in Model-Based Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21306",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21713",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21051",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Few-Shot Learning for Dynamic Operations of Automated Electric Taxi Fleets under Evolving Charging Infrastructure: A Meta-Deep Reinforcement Learning Approach ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21312",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Mean-Field Control on Sparse Graphs: From Local Limits to GNNs via Neighborhood Distributions ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21477",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21909",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21316",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Explicit Credit Assignment through Local Rewards and Dependence Graphs in Multi-Agent Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21523",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Signal-Adaptive Trust Regions for Gradient-Free Optimization of Recurrent Spiking Neural Networks ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21572",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Error Amplification Limits ANN-to-SNN Conversion in Continuous Control ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21778",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21484",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21737",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21912",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Self-Improving Pretraining: using post-trained models to pretrain better models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21343",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21459",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21590",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21208",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Factored Causal Representation Learning for Robust Reward Modeling in RLHF ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21350",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Epistemic Context Learning: Building Trust the Right Way in LLM-Based Multi-Agent Systems ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21742",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21919",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Beyond Imitation: Reinforcement Learning for Active Latent Planning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21598",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21212",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Constrained Meta Reinforcement Learning with Provable Test-Time Safety ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21845",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Safety Generalization Under Distribution Shift in Safe Reinforcement Learning: A Diabetes Testbed ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21094",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21609",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21083",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] READY: Reward Discovery for Meta-Black-Box Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21847",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21244",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] EGAM: Extended Graph Attention Model for Solving Routing Problems ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21281",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Distributional Active Inference ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20985",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21031",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21669",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Do Reasoning Models Enhance Embedding Models? ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21192",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Optimistic Transfer under Task Shift via Bellman Alignment ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21924",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21418",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21872",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21969",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Elign: Equivariant Diffusion Model Alignment from Foundational Machine Learning Force Fields ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21985",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Geometry of Drifting MDPs with Path-Integral Stability Certificates ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21991",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] SymbXRL: Symbolic Explainable Deep Reinforcement Learning for Mobile Networks ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.22024",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] SIA: Symbolic Interpretability for Anticipatory Deep Reinforcement Learning in Network Control ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.22044",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] DynaWeb: Model-Based Reinforcement Learning of Web Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.22149",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Exploring Reasoning Reward Model for Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.22154",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Reinforcement Learning for Adaptive Composition of Quantum Circuit Optimisation Passes ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21629",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 27'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Bi-Anchor Interpolation Solver for Accelerating Generative Modeling ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21542",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Improving Classifier-Free Guidance of Flow Matching via Manifold Projection ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21892",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Hardware-Triggered Backdoors ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21902",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21570",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Snowball: A Scalable All-to-All Ising Machine with Dual-Mode Markov Chain Monte Carlo Spin Selection and Asynchronous Spin Updates for Fast Combinatorial Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21058",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Procedural Pretraining: Warming Up Language Models with Abstract Data ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21725",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21484",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21737",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] More Code, Less Reuse: Investigating Code Quality and Reviewer Sentiment towards AI-generated Pull Requests ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21276",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Delegation Without Living Governance ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21226",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Faster Predictive Coding Networks via Better Initialization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20895",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] READY: Reward Discovery for Meta-Black-Box Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21847",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] A generative machine learning model for designing metal hydrides applied to hydrogen storage ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20892",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21526",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Spava: Accelerating Long-Video Understanding via Sequence-Parallelism-aware Approximate Attention ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21444",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21531",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] FBS: Modeling Native Parallel Reading inside a Transformer ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21708",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Uncertainty-Aware Data-Based Method for Fast and Reliable Shape Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21956",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Generalized Information Gathering Under Dynamics Uncertainty ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21988",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Geometry of Drifting MDPs with Path-Integral Stability Certificates ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21991",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.21996",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.22131",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.22137",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Discovering Hidden Gems in Model Repositories ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.22157",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Distributed Causality in the SDG Network: Evidence from Panel VAR and Conditional Independence Analysis ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20875",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] Denoising and Baseline Correction of Low-Scan FTIR Spectra: A Benchmark of Deep Learning Models Against Traditional Signal Processing ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.20905",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260130] MEIDNet: Multimodal generative AI framework for inverse materials design ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.22009",children:"link"})]}),"\n"]})]})}function d(i={}){const{wrapper:n}={...(0,a.R)(),...i.components};return n?(0,s.jsx)(n,{...i,children:(0,s.jsx)(h,{...i})}):h(i)}}}]);