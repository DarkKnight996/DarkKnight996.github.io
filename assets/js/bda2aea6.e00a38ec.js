"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[619],{6624:(i,e,n)=>{n.r(e),n.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251222-20251228","title":"20251222-20251228","description":"2025-12-22","source":"@site/docs/daily/20251222-20251228.md","sourceDirName":"daily","slug":"/daily/20251222-20251228","permalink":"/daily/20251222-20251228","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766462051000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251215-20251221","permalink":"/daily/20251215-20251221"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=n(4848),a=n(8453);const t={},l="20251222-20251228",o={},c=[{value:"2025-12-22",id:"2025-12-22",level:2},{value:"2025-12-23",id:"2025-12-23",level:2}];function d(i){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...i.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20251222-20251228",children:"20251222-20251228"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-22",children:"2025-12-22"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 13"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Sedna: Sharding transactions in multiple concurrent proposer blockchains"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [blockchain consensus], [verifiable rateless coding, transaction sharding, multi-proposer consensus, until-decode privacy]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Alejandro Ranchal-Pedrosa, Benjamin Marsh, Lefteris Kokoris-Kogias, Alberto Sonnino"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Sei Labs, Mysten Labs, University of Portsmouth, University College London"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17045",children:"https://arxiv.org/pdf/2512.17045"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Sedna is a user-facing protocol that uses verifiable, rateless coding to shard transactions across multiple concurrent proposers in a blockchain, replacing naive replication. It guarantees liveness and until-decode privacy, reducing MEV exposure and approaching the information-theoretic lower bound for bandwidth overhead, yielding a 2\u20133x efficiency improvement. The protocol requires no consensus modifications, enabling incremental deployment."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [dimensionality reduction, Byzantine-robust aggregation, privacy-preserving federated learning, secure multi-party computation, adaptive tuning]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Baolei Zhang, Minghong Fang, Zhuqing Liu, Biao Yi, Peizhao Zhou, Yuan Wang, Tong Li, Zheli Liu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nankai University, University of Louisville, University of North Texas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17254",children:"https://arxiv.org/pdf/2512.17254"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes ABBR, a practical framework for federated learning that integrates privacy-preserving and Byzantine-robust defenses. It uses dimensionality reduction to speed up secure computations for model filtering and an adaptive tuning strategy to mitigate the impact of undetected malicious models. The framework demonstrates significantly faster performance with minimal overhead while maintaining strong resilience against attacks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Dion2: A Simple Method to Shrink Matrix in Muon"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [Muon optimizer, orthonormalization, matrix shrinking, sampling, Newton-Schulz iterations, FSDP]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kwangjun Ahn, Noah Amsel, John Langford"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Microsoft Research, AI Frontiers, NYU"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16928",children:"https://arxiv.org/pdf/2512.16928"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces Dion2, a simple method to improve the scalability of the Muon optimizer by reducing the computational cost of its orthonormalization step. It works by sampling a fraction of rows or columns at each iteration for orthonormalization, making the update sparse. The method maintains update quality close to full Muon while significantly reducing time per step, as demonstrated in large-scale model training."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Democratizing Scalable Cloud Applications: Transactional Stateful Functions on Streaming Dataflows"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed systems, cloud computing], [streaming dataflow, stateful functions, serializable transactions, fault-tolerance, serverless, Apache Flink, Stateflow, Styx]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kyriakos Psarakis"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Unknown (Institution not provided in the given text)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17429",children:"https://arxiv.org/pdf/2512.17429"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This thesis proposes using the streaming dataflow execution model to simplify building scalable cloud applications. It introduces Stateflow, a high-level programming model, and Styx, a distributed engine that provides deterministic, serializable transactions with strong fault tolerance. The work concludes that this approach democratizes development by improving programmability and performance over existing systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Scalable Distributed Vector Search via Accuracy Preserving Index Construction"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [SPIRE, hierarchical vector index, partition granularity, accuracy-preserving recursive construction, approximate nearest neighbor search, distributed index]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yuming Xu, Qianxi Zhang, Qi Chen, Baotong Lu, Menghao Li, Philip Adams, Mingqin Li, Zengzhong Li, Jing Liu, Cheng Li, Fan Yang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Science and Technology of China, Microsoft Research, Shopify, Microsoft AI and Research"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17264",children:"https://arxiv.org/pdf/2512.17264"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces SPIRE, a scalable distributed vector index system designed for Approximate Nearest Neighbor Search (ANNS). Its core method involves identifying a balanced partition granularity to avoid read-cost explosion and using an accuracy-preserving recursive construction to build a multi-level index. The main conclusion is that SPIRE achieves high scalability and up to 9.64x higher throughput than state-of-the-art systems in experiments with billions of vectors."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] The HEAL Data Platform"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [data platform], [Gen3 platform, federated system, cloud-based, FAIR principles, mesh architecture, persistent identifiers, metadata services, APIs]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Brienna M. Larrick, L. Philip Schumm, Mingfei Shao, Craig Barnes, Anthony Juehne, Hara Prasad Juvvla, Michael B. Kranz, Michael Lukowski, Clint Malson, Jessica N. Mazerik, Christopher G. Meyer, Jawad Qureshi, Erin Spaniol, Andrea Tentner, Alexander VanTol, Peter Vassilatos, Sara Volk de Garcia, Robert L. Grossman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Chicago"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17506",children:"https://arxiv.org/pdf/2512.17506"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents the HEAL Data Platform, a cloud-based federated system built on the open-source Gen3 platform to serve as a single point of search, discovery, and analysis for data from the NIH HEAL Initiative. It interoperates with multiple data repositories using framework services for authentication, metadata, and persistent identifiers. The platform maximizes data value by ensuring data are Findable, Accessible, Interoperable, and Reusable (FAIR), facilitating secondary analysis."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [Logit-Aware Activation Budgeting, Phase-Multiplexed Scheduler, Head-Centric Sparse Attention, parallel decoding, memory footprint optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jiakun Fan, Yanglin Zhang, Xiangchen Li, Dimitrios S. Nikolopoulos"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Virginia Tech"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17077",children:"https://arxiv.org/pdf/2512.17077"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces dLLM-Serve, a serving system that addresses the memory footprint crisis in Diffusion LLMs by co-optimizing memory, scheduling, and generation quality. It proposes techniques like Logit-Aware Activation Budgeting and a Phase-Multiplexed Scheduler to manage resource oscillation and improve efficiency. The system significantly improves throughput and reduces latency across different hardware, establishing a blueprint for scalable dLLM inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [real-time systems], [fixed-priority scheduling, EDF, DAG task models, events executor, LIFO queue]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Oren Bell, Harun Teper, Mario G\xfcnzel, Chris Gill, Jian-Jia Chen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Washington University in St Louis, TU Dortmund University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16926",children:"https://arxiv.org/pdf/2512.16926"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel scheduling approach for ROS2 applications by using an events executor to implement fixed-job-level-priority schedulers for arbitrary Directed Acyclic Graph (DAG) tasks on uniprocessor systems. The method abstracts ROS2 applications as forests of trees and maps them to traditional real-time DAG models, requiring a special LIFO-ordered events queue. The authors conclude that their implementation generates schedules equivalent to a conventional fixed-priority DAG scheduler, helping to bridge the gap between real-time systems theory and ROS2 scheduling."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [C++, MPI, OpenMP, parallel programming, code generation, scalability, ChatGPT, Claude, LLaMA]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Patrick Diehl, Noujoud Nader, Deepti Gupta"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Los Alamos National Laboratory, Louisiana State University, Texas A&M University-Central Texas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17023",children:"https://arxiv.org/pdf/2512.17023"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper systematically evaluates large language models (LLMs) like ChatGPT, Claude, and LLaMA on generating correct and scalable parallel C++ code using MPI and OpenMP for Mandelbrot set computation. The method involves compiling and executing the generated programs to assess correctness, robustness, and performance. The main conclusion is that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance in this HPC code generation task."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [adaptive graph pruning, Spatio-Temporal Graph Neural Networks (ST-GNNs), Federated Learning (FL), Gossip Learning, Sudden Event Prediction Accuracy (SEPA), online semi-decentralized training]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ivan Kralj, Lodovico Giaretta, Gordan Je\u017ei\u0107, Ivana Podnar \u017darko, \u0160ar\u016bnas Girdzijauskas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Zagreb, RISE Research Institutes of Sweden, KTH Royal Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17352",children:"https://arxiv.org/pdf/2512.17352"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes an adaptive graph pruning algorithm for Spatio-Temporal Graph Neural Networks (ST-GNNs) to reduce communication overhead in online semi-decentralized traffic prediction systems. It also introduces a novel event-focused metric called SEPA to better evaluate responsiveness to sudden traffic changes. The experiments demonstrate that the method significantly lowers communication costs without compromising prediction accuracy or responsiveness to critical traffic events."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Torrent: A Distributed DMA for Efficient and Flexible Point-to-Multipoint Data Movement"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [distributed dma, chainwrite, network-on-chip, point-to-multipoint, scheduling algorithms]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yunhao Deng, Fanchen Kong, Xiaoling Yi, Ryan Antonio, Marian Verhelst"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," KU Leuven"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17589",children:"https://arxiv.org/pdf/2512.17589"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Torrent, a distributed DMA architecture that performs efficient point-to-multipoint data transfers by forming logical data chains (Chainwrite) over a standard Network-on-Chip without modifying its hardware or protocol. It uses scheduling algorithms to optimize chain order and demonstrates significant performance improvements, achieving up to 7.88x speedup over unicast with minimal area and power overhead."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [GPU-internal scheduling, resource sharing, collaborative multi-GPU video decoding, logically decoupled execution, inter-stage blocking elimination]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Wuhan University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17574",children:"https://arxiv.org/pdf/2512.17574"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes FlashCodec and UnifiedServe, a framework that optimizes multi-stage MLLM inference by accelerating video decoding and enabling resource sharing between vision and LLM stages. This approach reduces latency and increases throughput by eliminating inter-stage blocking and improving GPU utilization. The system achieves significantly higher throughput and can serve more requests compared to existing state-of-the-art systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Asymptotic behaviour of galactic small-scale dynamos at modest magnetic Prandtl number"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [astrophysics], [Pencil Code, Astaroth, GPU acceleration, magnetohydrodynamics (MHD), supernova-driven dynamo, magnetic Prandtl number]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Frederick A. Gent, Mordecai-Mark Mac Low, Maarit J. Korpi-Lagg, Touko Puro, Matthias Reinhardt"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nordita, KTH Royal Institute of Technology and Stockholm University, Aalto University, Newcastle University, American Museum of Natural History"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17885",children:"https://arxiv.org/pdf/2512.17885"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper uses high-resolution GPU-accelerated simulations with the Pencil Code and Astaroth to model a supernova-driven galactic dynamo. The main finding is that the strength of the turbulent magnetic field from the small-scale dynamo reaches an asymptotic limit at a modest magnetic Prandtl number of only a few hundred, which is far below physical interstellar values. This asymptotic behavior allows the model's characteristics to be incorporated into larger-scale galactic simulations."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 22'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17034",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17213",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17194",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17091",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17180",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17444",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Value Under Ignorance in Universal Artificial Intelligence ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17086",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Reinforcement Learning for Self-Improving Agent with Skill Library ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17102",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Understanding Generalization in Role-Playing Models via Information Theory ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17270",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] A Theoretical Analysis of State Similarity Between Markov Decision Processes ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17265",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16969",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Large Language Models as Pok\xe9mon Battle Agents: Strategic Play and Content Generation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17308",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17008",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17043",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Learning Safe Autonomous Driving Policies Using Predictive Safety Representations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17586",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] SCOPE: Sequential Causal Optimization of Process Interventions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17629",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Trust-Region Adaptive Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17636",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] About Time: Model-free Reinforcement Learning with Timed Reward Machines ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17637",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17846",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17853",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17899",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] HydroGym: A Reinforcement Learning Platform for Fluid Dynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17534",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 8'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17267",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17281",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17461",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17091",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17137",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17273",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17299",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17659",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-23",children:"2025-12-23"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 21"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [Byzantine fault tolerance, gossip protocol, consensus algorithms, cryptographic signatures, SHA-256, timestamp validation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nihir Chadderwala"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Independent researcher (based on email domain)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17913",children:"https://arxiv.org/pdf/2512.17913"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a Byzantine fault-tolerant multi-agent system for healthcare that uses a gossip protocol and cryptographic validation to securely propagate medical messages. The system achieves consensus with up to 33% faulty nodes and maintains 100% accuracy in experiments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [FPGA, mobile GPU, digital twin, model recovery, hardware acceleration, edge AI]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bin Xu, Ayan Banerjee, Midhat Urooj, Sandeep K.S. Gupta"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Arizona State University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17941",children:"https://arxiv.org/pdf/2512.17941"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a digital twin learning framework accelerated on reconfigurable hardware (FPGA) and compares it to a mobile GPU implementation for edge AI. The FPGA implementation achieves superior performance-per-watt, reduced DRAM footprint, and faster runtime compared to both mobile GPU and cloud GPU baselines, demonstrating its efficiency for mission-critical healthcare applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] Fast Online Digital Twinning on FPGA for Mission Critical Applications"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [FPGA acceleration, digital twinning, model recovery, GRU, dense layers, hardware acceleration, edge computing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Bin Xu, Ayan Banerjee, Sandeep K. S. Gupta"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Arizona State University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17942",children:"https://arxiv.org/pdf/2512.17942"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces an FPGA-accelerated framework for fast online digital twinning, offloading key neural components like GRUs and dense layers to reconfigurable hardware for parallel execution. It demonstrates that this approach enables real-time, low-latency operation for mission-critical applications, such as collision avoidance, achieving responsiveness significantly faster than human reaction times."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [quantum reinforcement learning, parameterized quantum circuits, policy gradient, deep q-learning, quantum cloud computing, resource allocation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Irwindeep Singh, Sukhpal Singh Gill, Jinzhao Sun, Jan Mol"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Indian Institute of Technology Jodhpur, Queen Mary University of London"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17918",children:"https://arxiv.org/pdf/2512.17918"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes QAISim, a Python toolkit for simulating quantum artificial intelligence models to manage resource allocation in quantum cloud environments. It uses quantum reinforcement learning with parameterized quantum circuits to optimize resource allocation for IoT applications. The approach reduces model complexity compared to classical methods, requiring fewer trainable variables."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [CXL shared memory, KV cache, disaggregated serving, two-tier synchronization, RDMA, Dynamo framework]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Dongha Yoon, Younghoon Min, Hoshik Kim, Sam H. Noh, Jongryool Kim"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Virginia Tech, SK Hynix America"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18194",children:"https://arxiv.org/pdf/2512.18194"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," TraCT is a rack-scale LLM serving system that uses CXL shared memory as a substrate for transferring and caching KV tensors, eliminating the network bottleneck of traditional RDMA-based approaches. It addresses challenges like synchronization and consistency on non-coherent CXL memory through software mechanisms like a two-tier inter-node synchronization scheme. The system significantly reduces latency and improves throughput compared to existing baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] Constrained Cuts, Flows, and Lattice-Linearity"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [graph theory, combinatorial optimization], [lattice-linearity, distributive lattices, min-cuts, max-flow, parallel algorithms, poset slicing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Robert Streit, Vijay K. Garg"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Texas at Austin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18141",children:"https://arxiv.org/pdf/2512.18141"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces methods using lattice-linear predicates and parallel advancement to efficiently compute min-cuts under additional constraints, representing them via distributive lattices. It shows that while constrained min-cut problems are generally NP-hard, exact algorithms with improved complexity over exhaustive search can be achieved using techniques like poset slicing. The work also contributes new concepts like k-transition predicates and strong advancement for better parallel complexity analysis."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [attention-based gradient importance predictor, differentiated parameter compression, knapsack-based optimization, hierarchical cloud-edge coordination, residual-based error compensation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yi Yang, Ziyu Lin, Liesheng Wei"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Sichuan Agricultural University, Google LLC, College of Information Technology, ShangHai Ocean University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18127",children:"https://arxiv.org/pdf/2512.18127"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ACE-Sync is an adaptive cloud-edge synchronization framework that reduces communication overhead in distributed training by dynamically selecting and compressing parameter groups using an attention-based importance predictor and a knapsack-based optimization strategy. It maintains model accuracy through residual error compensation and device clustering. Experiments show it reduces communication cost by 60% while preserving competitive model accuracy, demonstrating efficiency for large-scale distributed training."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [pipeline parallelism, asynchronous execution, message-queue decoupling, graph compilation, mixed-precision quantization, kernel fusion, silence-detection, Transformer]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Yildiz Technical University, Aktif Investment Bank Inc."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18318",children:"https://arxiv.org/pdf/2512.18318"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes an asynchronous pipeline-parallel Transformer framework for real-time multilingual lip synchronization, which decouples translation, speech, and lip-sync modules using message queues and optimizes them with graph compilation and quantization. The method reduces end-to-end latency by up to 3.1x compared to sequential approaches while maintaining accuracy. It is designed for resource-constrained AIoT communication systems like telemedicine and multilingual kiosks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [graph algorithms], [GPU, vertex cover, branch-and-reduce, component-aware branching, load balancing, non-tail-recursive parallelization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Hussein Amro, Basel Fakhri, Amer E. Mouawad, Izzat El Hajj"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," American University of Beirut, University of Waterloo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18334",children:"https://arxiv.org/pdf/2512.18334"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a new GPU algorithm for the vertex cover problem that improves scalability by detecting when a graph splits into independent components and processing them separately to avoid redundant work. It overcomes the load balancing challenge of non-tail-recursive branching by delegating solution aggregation to the last descendant of each branch. The method significantly outperforms the prior state-of-the-art GPU solution, reducing runtime from hours to seconds on large graphs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [KV-cache reuse, Activated LoRA (aLoRA), vLLM, base-aligned block hashing, activation-aware masking]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Allison Li, Kristjan Greenewald, Thomas Parnell, Navid Azizan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Massachusetts Institute of Technology, IBM Research"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17910",children:"https://arxiv.org/pdf/2512.17910"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a method for efficient multi-adapter LLM serving by enabling cross-model KV-cache reuse between a base model and its LoRA adapters via a technique called Activated LoRA (aLoRA). The approach, integrated into the vLLM framework, uses base-aligned block hashing and activation-aware masking to avoid recomputation when switching adapters. The evaluation shows significant latency and time-to-first-token improvements, demonstrating a practical bridge between parameter-efficient adaptation and high-performance inference serving."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] Snowveil: A Framework for Decentralised Preference Discovery"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [decentralised governance], [gossip-based protocol, potential function, submartingale theory, Constrained Hybrid Borda (CHB), axiomatic analysis, scalability simulation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Grammateia Kotsialou"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," King\u2019s College London"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18444",children:"https://arxiv.org/pdf/2512.18444"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Snowveil, a gossip-based protocol for decentralised preference discovery, where voters iteratively sample random peers to converge on a collective outcome. It proposes a novel aggregation rule (CHB) and uses potential functions and submartingale theory to prove convergence to a stable winner in finite time. The framework is shown to be scalable and offers a formal toolkit for decentralised governance in large systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] A Real-Time Digital Twin for Adaptive Scheduling"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [high-performance computing scheduling], [digital twin, discrete-event simulation, adaptive scheduling, PBS scheduler, what-if evaluation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yihe Zhang, Yash Kurkure, Yiheng Tao, Michael E. Papka, Zhiling Lan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Illinois Chicago, Argonne National Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18894",children:"https://arxiv.org/pdf/2512.18894"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents SchedTwin, a real-time digital twin that uses a high-fidelity discrete-event simulator to perform rapid what-if evaluations of multiple scheduling policies and dynamically selects the best one for adaptive HPC scheduling. The method is integrated with the PBS scheduler, and preliminary results show it consistently outperforms static policies while maintaining low overhead."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Multi-Player Multi-Armed Bandit (MP-MAB), Kernel Density Estimation (KDE), QoS-aware load balancing, decentralized proxy, adaptive exploration]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ivan \u010cili\u0107, Ivana Podnar \u017darko, Pantelis Frangoudis, Schahram Dustdar"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Zagreb, TU Wien"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18915",children:"https://arxiv.org/pdf/2512.18915"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes QEdgeProxy, a decentralized load balancer for the Computing Continuum that formulates the load balancing problem as a Multi-Player Multi-Armed Bandit with heterogeneous rewards, using Kernel Density Estimation to estimate QoS success probabilities. It demonstrates that this approach significantly outperforms baseline methods in per-client QoS satisfaction and effectively adapts to dynamic changes like load surges."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] Timely Parameter Updating in Over-the-Air Federated Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [over-the-air computation, federated learning, gradient selection, FAIR-k, parameter staleness, convergence analysis]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jiaqi Zhu, Zhongyuan Zhao, Xiao Li, Ruihao Du, Shi Jin, Howard H.Yang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Zhejiang University, Beijing University of Posts and Telecommunications, Southeast University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19103",children:"https://arxiv.org/pdf/2512.19103"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes the FAIR-k algorithm for Over-the-Air Federated Learning, which selects a subset of gradients for transmission by balancing update freshness and gradient magnitude. The analysis shows that FAIR-k accelerates convergence and improves communication efficiency by mitigating the dimension-waveform disparity and the effects of data heterogeneity and channel noise."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [Similar Prompts Searching (SPS), Main Model Pre-allocation (MMP), Lagrangian duality, Longest Processing Time (LPT), heterogeneous inference, expert offloading]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Wentao Liu, Yuhao Hu, Ruiting Zhou, Baochun Li, Ne Wang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Southeast University, University of Toronto, The Hong Kong Polytechnic University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18674",children:"https://arxiv.org/pdf/2512.18674"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Remoe, a heterogeneous MoE inference system for serverless computing that assigns non-expert modules to GPUs and expert modules to CPUs, offloading infrequently activated experts to separate functions. It uses techniques like SPS for activation prediction and MMP for memory management to optimize cost and latency. The system reduces inference cost by up to 57% and cold start latency by 47% compared to baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [decentralized federated learning, evidential deep learning, Dirichlet-based evidential models, trust-aware aggregation, peer compatibility scoring, model personalization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Murtaza Rangwala, Richard O. Sinnott, Rajkumar Buyya"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Melbourne"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19131",children:"https://arxiv.org/pdf/2512.19131"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents Murmura, a framework for decentralized federated learning that uses evidential deep learning to measure epistemic uncertainty and compute peer compatibility scores for trust-aware model aggregation. This enables nodes to personalize their models by selectively collaborating with compatible peers while excluding those with mismatched data distributions. The method significantly reduces performance degradation in non-IID settings and achieves faster convergence compared to baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [length-aware scheduling, dynamic programming, runtime range refinement, decentralized load balancing, multi-instance serving, continuous batching]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yitao Yuan, Chenqi Zhao, Bohan Zhao, Zane Cao, Yongchao He, Wenfei Wu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Peking University, ScitiX AI"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19179",children:"https://arxiv.org/pdf/2512.19179"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents L4, a runtime system for LLM serving that dynamically reschedules requests across multiple instances to reduce length heterogeneity within batches. It partitions instances into length-specialized groups forming a pipeline and uses a dynamic programming algorithm for optimal partitioning. The evaluation shows L4 significantly reduces latency and improves throughput compared to state-of-the-art multi-instance schedulers."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [bounded lag synchronous, alltoallv, distributed inference, embedding tables, PyTorch Distributed]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kiril Dichev, Filip Pawlowski, Albert-Jan Yzelman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Huawei Technologies Switzerland AG"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19342",children:"https://arxiv.org/pdf/2512.19342"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a bounded lag synchronous (BLS) version of the alltoallv collective operation to reduce synchronization overhead in distributed deep learning recommender model inference. This method allows slower processes to lag behind within a bounded number of iterations, preserving accuracy while improving performance. The authors demonstrate that BLS significantly improves latency and throughput for unbalanced inference runs, effectively masking process delays."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] Simulations between Strongly Sublinear MPC and Node-Capacitated Clique"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing], [MPC, Node-Capacitated Clique, simulation, round-preserving, strongly sublinear regime]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Philipp Schneider, Julian Werthmann"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," CISPA Helmholtz Center for Information Security, Paderborn University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19326",children:"https://arxiv.org/pdf/2512.19326"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper studies round-preserving simulations between the strongly sublinear Massively Parallel Computation (MPC) model and the Node-Capacitated Clique (NCC) model under matched total resources. It provides techniques for efficient, constant-overhead simulations between the models and also proves impossibility results that show the limitations of such simulations are inherent."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [performance modeling, hybrid parallelism, congestion-aware routing, ZeRO/FDSP sharding, Astra-Sim, DeepFlow, Chakra execution traces]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," George Karfakis, Faraz Tahmasebi, Binglu Chen, Lime Yao, Saptarshi Mitra, Tianyue Pan, Hyoukjun Kwon, Puneet Gupta"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of California, Los Angeles"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19606",children:"https://arxiv.org/pdf/2512.19606"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," RAPID-LLM is a performance modeling framework that combines a DeepFlow-based frontend to generate hardware-aware execution traces from an abstract LLM specification with an extended Astra-Sim backend to simulate these traces on explicit network topologies. It accurately predicts LLM training and inference performance on GPU clusters and enables exploration of hybrid parallelism, network faults, and hardware design variants."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251223] EuroHPC SPACE CoE: Redesigning Scalable Parallel Astrophysical Codes for Exascale"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [astrophysics simulation], [exascale computing, parallel code redesign, high-performance data analysis, machine learning, visualization, software repositories, data sharing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nitin Shukla, Alessandro Romeo, Caterina Caravita, Lubomir Riha, Ondrej Vysocky, Petr Strakos, Milan Jaros, Jo\xe3o Barbosa, Radim Vavrik, Andrea Mignone, Marco Rossazza, Stefano Truzzi, Vittoria Berta, Iacopo Colonnelli, Doriana Medi\u0107, Elisabetta Boella, Daniele Gregori, Eva Sciacca, Luca Tornatore, Giuliano Taffoni, Pranab J. Deka, Fabio Bacchini, Rostislav-Paul Wilhelm, Georgios Doulis, Khalil Pierre, Luciano Rezzolla, Tine Colman, Beno\xeet Commer\xe7on, Othman Bouizi, Matthieu Kuhn, Erwan Raffin, Marc Sergent, Robert Wissing, Guillermo Marin, Klaus Dolag, Geray S. Karademir, Gino Perna, Marisa Zanotti, Sebastian Trujillo-Gomez"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," CINECA, IT4Innovations, University of Turin, E4 COMPUTER ENGINEERING SpA, INAF, KU Leuven, Goethe-Universit\xe4t, CRAL, CNRS, ENS Lyon, Eviden, University of Oslo, Barcelona Supercomputing Center, ENGINSOFT SpA, Ludwig-Maximilians-Universit\xe4t, Heidelberg Institute for Theoretical Studies"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18883",children:"https://arxiv.org/pdf/2512.18883"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The SPACE Centre of Excellence aims to re-engineer key astrophysical simulation codes to tackle the architectural complexity of exascale systems by adopting innovative programming paradigms and software solutions. It concludes that this collaborative effort, which also addresses high-performance data analysis using machine learning, is essential for enabling and promoting the use of exascale and post-exascale computing capabilities in Astrophysics and Cosmology."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 40'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18135",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18317",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18014",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] On Swarm Leader Identification using Probing Policies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18146",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17979",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18189",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Stable and Efficient Single-Rollout RL for Multimodal Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18215",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17943",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Monitoring Monitorability ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18311",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18309",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17912",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC) ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18333",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18336",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] On the Universality of Transformer Architectures; How Much Attention Is Enough? ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18445",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18540",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Toward Training Superintelligent Software Agents through Self-Play SWE-RL ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18552",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18564",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18604",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18622",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18623",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18670",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18730",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18745",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18763",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18857",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18956",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18957",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19001",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19093",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] First-Order Representation Languages for Goal-Conditioned RL ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19355",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19361",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Learning General Policies with Policy Gradient Methods ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19366",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19516",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19554",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19576",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19673",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19691",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17929",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18892",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19104",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 12'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17958",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18034",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] SoK: Understanding (New) Security Issues Across AI4Code Use Cases ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18456",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18604",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18670",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Generative Modeling through Spectral Analysis of Koopman Operator ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18837",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Merging of Kolmogorov-Arnold networks trained on disjoint datasets ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.18921",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19004",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] Phase-space entropy at acquisition reflects downstream learnability ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19223",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] An Agentic Framework for Autonomous Materials Computation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19458",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.19526",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251223] The Subject of Emergent Misalignment in Superintelligence: An Anthropological, Cognitive Neuropsychological, Machine-Learning, and Ontological Perspective ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17989",children:"link"})]}),"\n"]})]})}function h(i={}){const{wrapper:e}={...(0,a.R)(),...i.components};return e?(0,s.jsx)(e,{...i,children:(0,s.jsx)(d,{...i})}):d(i)}},8453:(i,e,n)=>{n.d(e,{R:()=>t,x:()=>l});var r=n(6540);const s={},a=r.createContext(s);function t(i){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof i?i(e):{...e,...i}},[e,i])}function l(i){let e;return e=i.disableParentContext?"function"==typeof i.components?i.components(s):i.components||s:t(i.components),r.createElement(a.Provider,{value:e},i.children)}}}]);