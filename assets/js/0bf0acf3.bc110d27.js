"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[774],{46:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"daily/20250908-20250914","title":"20250908-20250914","description":"2025-09-08","source":"@site/docs/daily/20250908-20250914.md","sourceDirName":"daily","slug":"/daily/20250908-20250914","permalink":"/daily/20250908-20250914","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1761311518000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20250901-20250907","permalink":"/daily/20250901-20250907"},"next":{"title":"20250915-20250921","permalink":"/daily/20250915-20250921"}}');var r=i(4848),a=i(8453);const t={},o="20250908-20250914",l={},c=[{value:"2025-09-08",id:"2025-09-08",level:2},{value:"2025-09-09",id:"2025-09-09",level:2},{value:"2025-09-10",id:"2025-09-10",level:2},{value:"2025-09-11",id:"2025-09-11",level:2},{value:"2025-09-12",id:"2025-09-12",level:2},{value:"2025-09-13",id:"2025-09-13",level:2},{value:"2025-09-14",id:"2025-09-14",level:2}];function h(n){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"20250908-20250914",children:"20250908-20250914"})}),"\n",(0,r.jsx)(e.h2,{id:"2025-09-08",children:"2025-09-08"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] Research on fault diagnosis and root cause analysis based on full stack\nobservability"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [trace analysis], [fault diagnosis, root cause analysis, causal discovery, multi-modal fusion, observability data]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jian Hou"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Huazhong University of Science and Technology"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.12231v1",children:"http://arxiv.org/pdf/2509.12231v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes KylinRCA framework that integrates temporal causal discovery and cross-modal graph learning for fault diagnosis. It achieves global root cause localization and generates auditable evidence chains using mask-based explanation methods. The framework provides an effective solution for fault diagnosis under full-stack observability by combining dynamic causal analysis with multi-modal data fusion."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] Several Performance Bounds on Decentralized Online Optimization are\nHighly Conservative and Potentially Misleading"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [decentralized optimization, performance analysis, algorithm tuning, worst-case analysis, online optimization]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Erwan Meunier, Julien M. Hendrickx"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Universit\xe9 catholique de Louvain"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.06466v1",children:"http://arxiv.org/pdf/2509.06466v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper uses the Performance Estimation Problem approach to analyze decentralized online optimization algorithms, revealing that existing performance bounds are highly conservative and can lead to poor algorithm selection. The authors demonstrate how step-size tuning can improve classical methods, achieving up to 20% reduction in worst-case performance regret. Their analysis suggests some algorithms may not benefit from inter-agent communications for significant periods in worst-case scenarios."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World\nServing"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [speculative decoding, dynamic adaptation, KLD stability, large-batch serving, post-hoc signals]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Mingyu Yang, Jae-Young Choi, Kihyo Moon, Minsung Jang, Eunjoo Jeon"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Samsung SDS"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.01083v2",children:"http://arxiv.org/pdf/2509.01083v2"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," DSDE proposes a training-free framework using KLD variance as a stability signal to dynamically adjust speculation length during LLM inference. The method achieves competitive latency and superior robustness across diverse workloads, particularly in low-acceptance-rate scenarios. This demonstrates the value of post-hoc signals for building more adaptive and robust LLM inference systems."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [MaaS, SLO-aware orchestration, heterogeneous model instances, parallelism strategies, continuous batching]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Mo Xuan, Zhang yue, Wu Weigang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Sun Yat-sen University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.06362v1",children:"http://arxiv.org/pdf/2509.06362v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," MaaSO introduces a novel orchestrator for Model-as-a-Service platforms that optimizes heterogeneous LLM instance configurations through profiling, placement optimization, and SLO-aware request distribution. The system leverages different parallelism strategies and batch sizes to better meet diverse latency requirements. Experimental results show MaaSO improves SLO satisfaction by 15-30% and reduces latency by 40-60% compared to existing approaches while lowering orchestration overhead."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"2025-09-09",children:"2025-09-09"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\nEfficient MoE LLM Inference"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [Mixture of Experts, expert scheduling, memory optimization, prefetching, cache management, CUDA pipeline]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Yuning Zhang, Grant Pinkert, Nan Yang, Yanli Li, Dong Yuan"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," The University of Sydney"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.07379v1",children:"http://arxiv.org/pdf/2509.07379v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," DuoServe-MoE proposes a dual-phase inference system that separates prefill and decode stages with tailored expert scheduling strategies. It uses a two-stream CUDA pipeline for prefill and a lightweight predictor for decode-stage expert prefetching. Experiments show 1.42-7.54\xd7 latency improvements while maintaining only 15% peak memory usage of full model size."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] Towards Scalable Proteomics: Opportunistic SMC Samplers on HTCondor"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [Sequential Monte Carlo, proteomics, HTCondor, opportunistic computing, Bayesian inference, distributed computing]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Matthew Carter, Lee Devlin, Alexander Philips, Edward Pyzer-Knapp, Paul Spirakis, Simon Maskell"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of Liverpool"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.08020v1",children:"http://arxiv.org/pdf/2509.08020v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces an opportunistic computing framework using HTCondor to deploy Sequential Monte Carlo samplers for large-scale proteomics inference. The proposed Coordinator-Manager-Follower architecture reduces synchronization overhead and enables scalable Bayesian inference using idle university computing resources. Results show the framework achieves accurate inference with weak scaling, generating more samples under fixed time constraints as resources increase."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] Astra: A Multi-Agent System for GPU Kernel Performance Optimization"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [kernels], [GPU kernel optimization, multi-agent LLM system, CUDA code generation, performance optimization, SGLang framework]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Anjiang Wei, Tianran Sun, Yogesh Seenichamy, Hang Song, Anne Ouyang, Azalia Mirhoseini, Ke Wang, Alex Aiken"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Stanford University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.07506v1",children:"http://arxiv.org/pdf/2509.07506v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Astra introduces a multi-agent LLM system that collaboratively optimizes GPU kernels through iterative code generation, testing, profiling, and planning. Starting from existing CUDA implementations in SGLang, it achieves an average 1.32x speedup using zero-shot prompting. The system autonomously applies optimizations like loop transformations and memory access improvements, demonstrating LLMs' potential for high-performance kernel optimization."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] Accelerating Frontier MoE Training with 3D Integrated Optics"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [3D integrated optics, photonic interconnects, mixture of experts, scale-up domain, high-performance computing]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Mikhail Bernadskiy, Peter Carson, Thomas Graham, Taylor Groves, Ho John Lee, Eric Yeh"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Lightmatter"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.15893v1",children:"http://arxiv.org/pdf/2510.15893v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes using 3D-stacked photonic interconnects to overcome electrical interconnect limitations in large-scale AI training. The method enables connecting thousands of GPUs across multiple racks with 8X scale-up capability. Results show 2.7X faster training time for trillion-parameter MoE models through improved bandwidth and multi-dimensional parallelism."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with\nFaaS-hosted MCP Services"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [Agentic AI, Workflow Patterns, FaaS, MCP, Model Context Protocol, Cloud Computing]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Shiva Sai Krishna Anand Tokal, Vaibhav Jha, Anand Eswaran, Praveen Jayachandran, Yogesh Simmhan"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Indian Institute of Science, Bangalore and IBM India Research Lab, Bangalore"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.07595v1",children:"http://arxiv.org/pdf/2509.07595v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," AgentX introduces a novel agentic workflow pattern with stage designer, planner, and executor agents that outperforms state-of-the-art patterns like ReAct and Magentic One. It leverages Model Context Protocol tools deployed via FaaS for improved performance. Empirical evaluation shows competitive success rates with better latency and cost efficiency in practical applications."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] MoE-Compression: How the Compression Error of Experts Affects the\nInference Accuracy of MoE Model?"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [Mixture of Experts, model compression, error-bounded lossy compression, inference accuracy, expert layers]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Songkai Ma, Zhaorui Zhang, Sheng Di, Benben Liu, Xiaodong Yu, Xiaoyi Lu, Dan Wang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Hong Kong Polytechnic University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.07727v1",children:"http://arxiv.org/pdf/2509.07727v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes using error-bounded lossy compression algorithms to compress non-activated experts in MoE models for efficient inference under GPU memory constraints. Experiments show compression errors in shallow layers cause minimal accuracy degradation, while middle-layer errors significantly impair performance, and deep-layer errors can sometimes improve accuracy. The study provides a comprehensive analysis of how compression-induced errors affect MoE inference accuracy across different expert layers."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"2025-09-10",children:"2025-09-10"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] Design and Implementation of Code Completion System Based on LLM and\nCodeBERT Hybrid Subsystem"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [code completion, hybrid model, CodeBERT, GPT-3.5, code generation]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Bingbing Zhang, Ziyu Lin, Yingxin Su"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Xiamen Institute of Technology, Google LLC, University of California Davis"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.08215v1",children:"http://arxiv.org/pdf/2509.08215v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a hybrid model combining CodeBERT and GPT-3.5 for code completion tasks, leveraging CodeBERT's semantic understanding and GPT-3.5's generation capabilities. The hybrid approach demonstrates superior performance in accuracy, code quality, and efficiency compared to benchmarks. Robustness testing confirms the model's reliability and stability in practical applications."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] Optimizing the Variant Calling Pipeline Execution on Human Genomes Using\nGPU-Enabled Machines"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [variant calling pipeline, GPU optimization, machine learning prediction, job shop scheduling, genome analysis]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Ajay Kumar, Praveen Rao, Peter Sanders"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," The University of Missouri, Karlsruhe Institute of Technology"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.09058v1",children:"http://arxiv.org/pdf/2509.09058v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a machine learning-based approach to optimize variant calling pipeline execution on GPU-enabled machines. The method uses ML to predict execution times of pipeline stages and generates optimal execution plans inspired by flexible job shop scheduling. Experimental results show the approach achieved 2X speedup over greedy methods and effectively predicted execution times using genomic sequence features."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and\nDynamic Parallelism"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [heterogeneous GPU clusters, fine-grained parallelism, dynamic load balancing, attention mechanism optimization]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Zizhao Mo, Jianxiong Liao, Huanle Xu, Zhi Zhou, Chengzhong Xu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of Macau, Sun Yat-sen University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.08309v1",children:"http://arxiv.org/pdf/2509.08309v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Hetis introduces fine-grained and dynamic parallelism for LLM serving in heterogeneous GPU clusters, selectively parallelizing compute-intensive operations and distributing attention computations at head granularity. The system employs an online load dispatching policy to balance network latency, computational load, and memory intensity. Evaluation shows Hetis improves throughput by up to 2.25\xd7 and reduces latency by 1.49\xd7 compared to existing systems."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"2025-09-11",children:"2025-09-11"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] TrEnv: Transparently Share Serverless Execution Environments Across\nDifferent Functions and Nodes"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [serverless computing, execution environment sharing, memory optimization, cold start reduction, LLM agents]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jialiang Huang, Teng Ma, Zheng Liu, Sixing Lin, Kang Chen, Jinlei Jiang, Xia Liao, Yingdi Shan, Yongwei Wu, Ning Zhang, Mengting Lu, Tao Ma, Haifeng Gong, Mingxing Zhang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Tsinghua University, Alibaba Group"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.09525v1",children:"http://arxiv.org/pdf/2509.09525v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," TrEnv introduces a serverless platform that transparently shares execution environments across functions and nodes using repurposable sandboxes and memory templates. It reduces startup latency and memory usage for both container-based and VM-based environments. The system achieves up to 7\xd7 lower P99 latency and 61% memory savings compared to state-of-the-art systems like E2B for LLM agent workloads."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] Coherence-Aware Task Graph Modeling for Realistic Application"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [trace analysis], [cache coherence, task graph modeling, multicore systems, runtime behavior, system performance]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Guochu Xiong, Xiangzhong Luo, Weichen Liu"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Nanyang Technological University, Southeast University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.09094v1",children:"http://arxiv.org/pdf/2509.09094v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," CoTAM proposes a coherence-aware task graph modeling framework that constructs unified task graphs reflecting runtime behavior by analyzing cache coherence impacts through decoupling and learned weighting schemes. The framework outperforms implicit methods and bridges the gap between dynamic workload behavior and existing designs. Experimental results demonstrate the importance of incorporating cache coherence into task graph modeling for accurate system-level analysis."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"2025-09-12",children:"2025-09-12"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards\nAutonomous Science"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [Agentic AI, Scientific Workflows, Autonomous Science, Swarm Intelligence, Workflow Evolution]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Woong Shin, Renan Souza, Daniel Rosendo, Fr\xe9d\xe9ric Suter, Feiyi Wang, Prasanna Balaprakash, Rafael Ferreira da Silva"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Oak Ridge National Laboratory"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.09915v1",children:"http://arxiv.org/pdf/2509.09915v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a conceptual framework for evolving scientific workflows along intelligence and composition dimensions, transitioning from static systems to intelligent swarm-based approaches. It presents an architectural blueprint for autonomous science laboratories that integrate AI agents as ecosystem components. This framework aims to enable 100x acceleration in scientific discovery through distributed, autonomous workflow coordination."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] SynergAI: Edge-to-Cloud Synergy for Architecture-Driven High-Performance\nOrchestration for AI Inference"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [AI inference, edge computing, cloud computing, performance optimization, Kubernetes]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Foteini Stathopoulou, Aggelos Ferikoglou, Manolis Katsaragakis, Dimosthenis Masouros, Sotirios Xydis, Dimitrios Soudris"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," National Technical University of Athens"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.12252v1",children:"http://arxiv.org/pdf/2509.12252v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," SynergAI introduces an architecture-aware framework for AI inference serving across edge-to-cloud infrastructures using offline and online scheduling policies. It dynamically allocates workloads across heterogeneous hardware to minimize QoS violations. The system achieves 2.4x fewer QoS violations compared to state-of-the-art solutions when implemented in a Kubernetes ecosystem."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] MinatoLoader: Accelerating Machine Learning Training Through Efficient\nData Preprocessing"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [data loader, data preprocessing, GPU utilization, batch construction, PyTorch]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Rahma Nouaji, Stella Bitchebe, Ricardo Macedo, Oana Balmau"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," McGill University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.10712v1",children:"http://arxiv.org/pdf/2509.10712v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," MinatoLoader introduces a new data loading approach that prioritizes fast-to-preprocess samples and processes slower samples in parallel to reduce GPU idleness. It achieves up to 7.5\xd7 training speedup and improves GPU utilization from 46.4% to 90.45% while maintaining model accuracy across various workloads."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] Coordinated Reinforcement Learning Prefetching Architecture for\nMulticore Systems"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [reinforcement learning, hardware prefetching, multicore systems, coordinated learning, memory optimization]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Mohammed Humaid Siddiqui, Fernando Guzman, Yufei Wu, Ruishu Ann"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Fairleigh Dickinson University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.10719v1",children:"http://arxiv.org/pdf/2509.10719v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes CRL-Pythia, a coordinated reinforcement learning prefetcher that enables cross-core information sharing and cooperative prefetching decisions in multicore systems. This approach significantly reduces redundant prefetch requests and improves learning convergence across cores. Experimental results show CRL-Pythia outperforms single Pythia configurations with approximately 12% IPC improvement for bandwidth-constrained workloads while maintaining moderate hardware overhead."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"2025-09-13",children:"2025-09-13"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"[arXiv2509] FastTrack: GPU-Accelerated Tracking for Visual SLAM"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [GPU acceleration, visual SLAM, stereo feature matching, local map tracking, CUDA implementation, real-time performance]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Kimia Khabiri, Parsa Hosseininejad, Shishir Gopinath, Karthik Dantu, Steven Y. Ko"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University at Buffalo (based on author affiliations: 1 - University at Buffalo, 2 - University of Texas at Austin)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.10757v1",children:"http://arxiv.org/pdf/2509.10757v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents FastTrack, a GPU-accelerated tracking system for visual-inertial SLAM that uses CUDA to speed up stereo feature matching and local map tracking. The implementation within ORB-SLAM3 demonstrates up to 2.8\xd7 performance improvement on both desktop and embedded platforms. Evaluation using EuRoC and TUM-VI datasets confirms the effectiveness of GPU acceleration for real-time SLAM applications."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"2025-09-14",children:"2025-09-14"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] Parallel/Distributed Tabu Search for Scheduling Microprocessor Tasks in\nHybrid Flowshop"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [tabu search, hybrid flow shop, multiprocessor tasks, distributed computing, parallel computing]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Adam Janiak, Damian Kowalczyk, Maciej Lichtenstein"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Wroc\u0142aw University of Science and Technology (Politechnika Wroc\u0142awska)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.11396v1",children:"http://arxiv.org/pdf/2509.11396v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a parallel/distributed tabu search algorithm for scheduling microprocessor tasks in hybrid flow shops to minimize makespan. The algorithm effectively utilizes parallel and distributed mechanisms for neighborhood evaluation and balances heterogeneous network environments. The approach demonstrates efficient performance for solving this NP-hard scheduling problem."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] GFS: A Preemption-aware Scheduling Framework for GPU Clusters with\nPredictive Spot Instance Management"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [GPU scheduling, spot instances, preemption management, resource allocation, cluster management]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jiaang Duan, Shenglin Xu, Shiyou Qian, Dingyu Yang, Kangjin Wang, Chenzhi Liao, Yinghao Yu, Qin Hua, Hanwen Hu, Qi Wang, Wenchao Wu, Dongqing Bao, Tianyu Lu, Jian Cao, Guangtao Xue, Guodong Yang, Liping Zhang, Gang Chen"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Shanghai Jiao Tong University, Zhejiang University, Alibaba Group"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.11134v1",children:"http://arxiv.org/pdf/2509.11134v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," GFS introduces a preemption-aware scheduling framework with predictive spot instance management for GPU clusters. It uses demand forecasting, dynamic quota allocation, and preemptive scheduling policies to balance high-priority and low-priority tasks. The framework reduces eviction rates by 33.0%, cuts queuing delays by 44.1%, and improves GPU allocation rates by up to 22.8% in production clusters."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] Chameleon: Taming Dynamic Operator Sequences for Memory-Intensive LLM\nTraining"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [memory optimization, swap-based training, dynamic operator sequences, eager mode]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Zibo Wang, Yuhang Zhou, Zhibin Wang, Shipeng Li, Xinjing Huang, Chendong Cai, Bingxu Mu, Yuqing Sun, Zhiheng Hu, Bin She, Shu You, Guanghuan Fang, Rong Gu, Wanchun Dou, Guihai Chen, Chen Tian"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," Nanjing University, Huawei Technologies Co., Ltd"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.11076v1",children:"http://arxiv.org/pdf/2509.11076v1"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," Chameleon introduces a lightweight online profiler and optimized swap policy execution to handle dynamic operator sequences in LLM training. It enables training models up to 4x larger than hardware memory capacity while reducing profiling overhead by 84.25%. The system achieves up to 38.94% performance improvement compared to recomputation or high-degree parallelism methods."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"[arXiv2509] VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing\nfor Energy-Efficient LLM Serving"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [energy efficiency, frequency scaling, request routing, prefill-decode disaggregation, SLO-aware serving]"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"authors:"})," Jiahuan Yu, Aryan Taneja, Junfeng Lin, Minjia Zhang"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"institution:"})," University of Illinois Urbana-Champaign, Tsinghua University"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"link:"})," ",(0,r.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.04827v2",children:"http://arxiv.org/pdf/2509.04827v2"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simple LLM Summary:"})," VoltanaLLM introduces a control-theoretic approach combining feedback-driven frequency control and state-space routing for LLM serving. It dynamically adjusts GPU frequencies for prefill/decode phases and optimizes request routing to minimize energy consumption. The system achieves up to 36.3% energy savings while maintaining near-perfect SLO attainment rates."]}),"\n"]}),"\n"]}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(h,{...n})}):h(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>o});var s=i(6540);const r={},a=s.createContext(r);function t(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),s.createElement(a.Provider,{value:e},n.children)}}}]);