"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[873],{8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>o});var r=i(6540);const s={},t=r.createContext(s);function a(n){const e=r.useContext(t);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),r.createElement(t.Provider,{value:e},n.children)}},9238:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20250922-20250928","title":"20250922-20250928","description":"2025-09-22","source":"@site/docs/daily/20250922-20250928.md","sourceDirName":"daily","slug":"/daily/20250922-20250928","permalink":"/daily/20250922-20250928","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1761311518000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20250915-20250921","permalink":"/daily/20250915-20250921"},"next":{"title":"20250929-20251005","permalink":"/daily/20250929-20251005"}}');var s=i(4848),t=i(8453);const a={},o="20250922-20250928",l={},c=[{value:"2025-09-22",id:"2025-09-22",level:2},{value:"2025-09-23",id:"2025-09-23",level:2},{value:"2025-09-24",id:"2025-09-24",level:2},{value:"2025-09-25",id:"2025-09-25",level:2},{value:"2025-09-26",id:"2025-09-26",level:2},{value:"2025-09-27",id:"2025-09-27",level:2},{value:"2025-09-28",id:"2025-09-28",level:2}];function h(n){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20250922-20250928",children:"20250922-20250928"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-22",children:"2025-09-22"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via\nPartially Disaggregated Prefill"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [heterogeneous GPU clusters, workload balancing, partially disaggregated prefill]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yunzhao Liu, Qiang Xu, Y. Charlie Hu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Purdue University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.17357v1",children:"http://arxiv.org/pdf/2509.17357v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Cronus introduces a partially disaggregated prefill approach that partitions and overlaps prefill and decode stages across heterogeneous GPUs to balance workloads. It significantly improves throughput over disaggregated prefill and reduces latency metrics compared to data and pipeline parallelism while maintaining or enhancing throughput in evaluations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Enhancing Cluster Scheduling in HPC: A Continuous Transfer Learning for\nReal-Time Optimization"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [cluster scheduling, transfer learning, real-time optimization, node-affinity constraints, Kubernetes]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Leszek Sliwko, Jolanta Mizera-Pietraszko"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Westminster, Opole University of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.22701v1",children:"http://arxiv.org/pdf/2509.22701v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a continuous transfer learning model for optimizing task scheduling in cluster systems with node-affinity constraints. The model dynamically evolves during operations, reducing retraining needs while achieving over 99% accuracy on Google Cluster Data. It significantly improves scheduling latency and computational overhead compared to traditional schedulers like Kubernetes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] High-Performance Statistical Computing (HPSC): Challenges,\nOpportunities, and Future Directions"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [statistical computing, high-performance computing, community integration, scalable applications, HPC technologies]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Sameh Abdulah, Mary Lai O. Salvana, Ying Sun, David E. Keyes, Marc G. Genton"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," King Abdullah University of Science and Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2508.04013v2",children:"http://arxiv.org/pdf/2508.04013v2"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes bridging the gap between statistical computing and high-performance computing communities through technical innovation and collaboration. It outlines challenges and opportunities for integrating statistical methods with modern HPC platforms to enable scalable statistical applications. The authors present a roadmap for developing a thriving high-performance statistical computing community that leverages HPC technologies for faster statistical insights."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Odyssey: Adaptive Policy Selection for Resilient Distributed Training"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [fault tolerance, distributed training, performance optimization, adaptive policy selection]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yuhang Zhou, Zhibin Wang, Peng Jiang, Haoran Xia, Junhe Lu, Qianyu Jiang, Rong Gu, Hengxi Xu, Xinjing Huang, Guanghuan Fang, Zhiheng Hu, Jingyi Zhang, Yongjin Cai, Jian He, Chen Tian"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nanjing University, Huawei"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2508.21613v3",children:"http://arxiv.org/pdf/2508.21613v3"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Odyssey proposes an adaptive fault-tolerant system that intelligently selects optimal recovery strategies through performance modeling and communication optimizations. It maintains within 11.00% performance gap between post-recovery and failure-free training while achieving up to 1.355x higher throughput than state-of-the-art methods. The system preserves model convergence and efficient memory usage during distributed LLM training."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [semantic caching, cross-region caching, LLM agents, tool access, performance optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chaoyi Ruan, Chao Bi, Kaiwen Zheng, Ziji Shi, Xinyi Wan, Jialin Li"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," National University of Singapore (NUS)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.17360v1",children:"http://arxiv.org/pdf/2509.17360v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Asteria introduces a semantic-aware cross-region caching architecture with Semantic Elements and Semantic Retrieval Index for LLM agent tool access. It uses two-stage retrieval combining vector similarity and lightweight LLM judgment. Evaluation shows 3.6\xd7 throughput improvement and 85%+ cache hit rates while maintaining accuracy comparable to non-cached baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Cluster Workload Allocation: A Predictive Approach Leveraging Machine\nLearning Efficiency"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [workload allocation, task constraints, ensemble classifiers, Google Cluster Data, node affinity]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Leszek Sliwko"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Westminster"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.17695v1",children:"http://arxiv.org/pdf/2509.17695v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This research uses machine learning classifiers to predict suitable node-task pairings in cluster workload allocation by analyzing task constraint operators from Google Cluster Data. Various ML models were evaluated, with an ensemble voting classifier achieving 98% accuracy in identifying tasks with single suitable nodes. The approach demonstrates ML's effectiveness in optimizing workload allocation strategies for constrained tasks in large-scale computing clusters."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale\nMoE Serving"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [Mixture-of-Experts, model serving, distributed systems, fault tolerance, resource scaling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ziming Liu, Boyu Tian, Guoteng Wang, Zhen Jiang, Peng Sun, Zhenhua Han, Tian Tang, Xiaohe Hu, Yanmin Jia, Yan Zhang, He Liu, Mingjun Zhang, Yiqi Zhang, Qiaoling Chen, Shenggan Cheng, Mingyu Gao, Yang You, Siyuan Feng"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," National University of Singapore, Shanghai Qiji Zhifeng Co., Ltd."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.17863v1",children:"http://arxiv.org/pdf/2509.17863v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes EaaS, a serving system that disaggregates MoE modules into independent stateless services for efficient large-scale deployment. It achieves fine-grained resource scaling and inherent fault tolerance through decoupled compute units. Experiments show comparable performance to monolithic systems with 37.5% resource savings and robust fault tolerance under hardware failures."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Intelligent Load Balancing in Cloud Computer Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [load balancing, cloud computing, task scheduling, resource management, virtual machine migration]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Leszek Sliwko"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Westminster"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.22704v1",children:"http://arxiv.org/pdf/2509.22704v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This research developed intelligent load balancing strategies for cloud systems, including centralized metaheuristic and decentralized agent-based approaches. The study created a high-fidelity cloud workload simulator using Google's workload traces and proposed models for resource utilization and VM migration traffic estimation. Experimental results demonstrated effective dynamic task allocation that maintains system stability while minimizing costs."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-23",children:"2025-09-23"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Accelerating Gravitational $N$-Body Simulations Using the RISC-V-Based\nTenstorrent Wormhole"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [RISC-V accelerator, gravitational N-body simulation, high-performance computing, energy efficiency, astrophysical application]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jenny Lynn Almerol, Elisabetta Boella, Mario Spera, Daniele Gregori"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," E4 Computer Engineering, Scuola Internazionale Superiore di Studi Avanzati"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.19294v1",children:"http://arxiv.org/pdf/2509.19294v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper implements gravitational N-body simulations on the RISC-V-based Tenstorrent Wormhole accelerator. The method achieves over 2\xd7 speedup and approximately 2\xd7 energy savings compared to optimized CPU implementations. Results demonstrate RISC-V accelerators' competitiveness for high-performance scientific computing beyond AI workloads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] In-Transit Data Transport Strategies for Coupled AI-Simulation Workflow\nPatterns"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [AI-Simulation workflows, data transport strategies, performance benchmarking]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Harikrishna Tummalapalli, Riccardo Balin, Christine M. Simpson, Andrew Park, Aymen Alsaadi, Andrew E. Shao, Wesley Brewer, Shantenu Jha"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Argonne National Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.19150v1",children:"http://arxiv.org/pdf/2509.19150v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces SimAI-Bench to prototype and evaluate coupled AI-simulation workflows, benchmarking data transport strategies on the Aurora supercomputer. For one-to-one workflows, node-local and DragonHPC staging outperform Redis and Lustre, while for many-to-one workflows, file systems become optimal as ensemble sizes increase. The study highlights data transport bottlenecks and provides performance insights for different workflow patterns."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] TD3-Sched: Learning to Orchestrate Container-based Cloud-Edge Resources\nvia Distributed Reinforcement Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [distributed reinforcement learning, cloud-edge computing, resource orchestration, TD3 algorithm, container scheduling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shengye Song, Minxian Xu, Kan Hu, Wenxia Guo, Kejiang Ye"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.18957v1",children:"http://arxiv.org/pdf/2509.18957v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes TD3-Sched, a distributed reinforcement learning scheduler using Twin Delayed Deep Deterministic Policy Gradient for continuous CPU and memory allocation in cloud-edge environments. The method demonstrates significant latency reductions (17.9%-38.6%) and superior SLO compliance (only 0.47% violations) compared to baseline approaches, showing faster convergence and more stable performance while maintaining service quality in container-based cloud-edge systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Whack-a-Mole: Deterministic Packet Spraying Across Multiple Network\nPaths"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [packet spraying, multipath transport, network load balancing, deterministic algorithm, distributed AI/ML training]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Michael Luby, John Byers"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," BitRipple, Inc. and Boston University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.18519v1",children:"http://arxiv.org/pdf/2509.18519v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Whack-a-Mole is a deterministic packet spraying algorithm that distributes packets across multiple network paths using a bit-reversal counter and discrete path allocations. It provides provably tight discrepancy bounds of O(log m) between expected and actual packet counts per path. The algorithm responds quickly to congestion feedback and effectively minimizes collective completion time while maximizing GPU utilization for distributed AI/ML workloads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Supercomputing for High-speed Avoidance and Reactive Planning in Robots"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [robotic control, high-performance computing, trajectory planning, real-time systems, hybrid architecture]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kieran S. Lachmansingh, Jos\xe9 R. Gonz\xe1lez-Estrada, Ryan E. Grant, Matthew K. X. J. Pan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Queen's University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.19486v1",children:"http://arxiv.org/pdf/2509.19486v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents SHARP, a system that offloads robotic trajectory planning to high-performance computing clusters using parallelized multi-goal A* search with MPI. The approach achieves millisecond-scale planning latencies (22.9-30.0 ms) for a 7-DOF manipulator dodging high-speed projectiles. Results demonstrate that HPC offloading enables reactive performance below human reaction times when network latency remains in tens of milliseconds, supporting hybrid control architectures for dynamic robotics."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Towards the Distributed Large-scale k-NN Graph Construction by Graph\nMerge"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [k-NN graph construction, graph merge, distributed computing, indexing graph, parallel algorithms]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Cheng Zhang, Wan-Lei Zhao, Shihai Xiao, Jiajie Yao, Xuecang Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Xiamen University, Huawei Technologies Ltd."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.11697v3",children:"http://arxiv.org/pdf/2509.11697v3"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes efficient graph merge algorithms (Two-way Merge and Multi-way Merge) for constructing large-scale k-NN graphs and indexing graphs in distributed environments. The methods enable building billion-scale graphs in parallel with significantly reduced construction time. Experimental results show the merged graphs achieve similar nearest neighbor search performance as original graphs while being much faster to construct."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] LLM Agents for Interactive Workflow Provenance: Reference Architecture\nand Evaluation Methodology"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [workflow provenance, natural language queries, metadata-driven design, Retrieval-Augmented Generation, interactive analysis]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Renan Souza, Timothy Poteet, Brian Etz, Daniel Rosendo, Amal Gueroudji, Woong Shin, Prasanna Balaprakash, Rafael Ferreira da Silva"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Oak Ridge National Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.13978v2",children:"http://arxiv.org/pdf/2509.13978v2"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a reference architecture using interactive LLM agents for workflow provenance analysis through natural language queries. The approach employs a lightweight metadata-driven design with Retrieval-Augmented Generation to translate natural language into structured queries. Evaluations across multiple LLMs demonstrate that modular design, prompt tuning, and RAG enable accurate responses beyond recorded provenance data."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Scheduler-Driven Job Atomization"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [GPU scheduling, job atomization, resource utilization, Multi-Instance GPU, cluster management]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Michal Konopa, Jan Fesl, Ladislav Ber\xe1nek"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of South Bohemia"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.19086v1",children:"http://arxiv.org/pdf/2509.19086v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Scheduler-Driven Job Atomization (SJA), a new paradigm that enables bidirectional interaction between schedulers and jobs to dynamically create subjobs fitting available execution gaps. Unlike traditional approaches, SJA proactively shapes workloads before execution to avoid costly state transfers and interruptions. The method aims to increase GPU utilization, reduce wait times, and minimize migration overhead by ensuring each subjob is correct by construction."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] 6G Twin: Hybrid Gaussian Radio Fields for Channel Estimation and\nNon-Linear Precoder Design for Radio Access Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [6G, channel estimation, Gaussian Radio Fields, precoder design, continual learning, radio access networks]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Muhammad Ali Jamshed, Dean F. Hougen, John M. Cioffi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Stanford University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.18735v1",children:"http://arxiv.org/pdf/2509.18735v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes 6G Twin, an AI-native radio access network framework combining neural Gaussian Radio Fields for compressed channel acquisition, continual learning for mobility handling, and an energy-optimal nonlinear precoder. The method achieves 100x pilot reduction with 1.1ms inference time and maintains robust performance during handovers while significantly reducing transmit energy. The integrated system demonstrates superior throughput-energy tradeoffs in 3GPP-style environments with real-time operational capability."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] On The Reproducibility Limitations of RAG Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [retrieval-augmented generation, reproducibility, benchmarking, embedding models, vector retrieval, uncertainty quantification]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Baiqiang Wang, Dongfang Zhao, Nathan R Tallent, Luanzheng Guo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Washington, Pacific Northwest National Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.18869v1",children:"http://arxiv.org/pdf/2509.18869v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces ReproRAG, a comprehensive benchmarking framework that systematically measures reproducibility in RAG systems by analyzing embedding models, retrieval algorithms, and hardware configurations. The study reveals that different embedding models significantly impact RAG reproducibility. The framework provides tools for validating deployments and making informed design decisions to enhance trustworthy AI for science."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-24",children:"2025-09-24"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\nArchitectures"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [kernels], [molecular dynamics, performance portability, exascale computing, GPU optimization, Kokkos library]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Anders Johansson, Evan Weinberg, Christian R. Trott, Megan J. McCarthy, Stan G. Moore"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Sandia National Laboratories"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2508.13523v2",children:"http://arxiv.org/pdf/2508.13523v2"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper integrates the Kokkos performance portability library into LAMMPS molecular dynamics code to achieve cross-platform compatibility across exascale architectures. It demonstrates strong scaling performance on multiple supercomputers using various interatomic potentials. The approach enables efficient execution on diverse GPU hardware while maintaining computational performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] FZModules: A Heterogeneous Computing Framework for Customizable\nScientific Data Compression Pipelines"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [kernels], [data compression, lossy compression, heterogeneous computing, GPU acceleration, scientific data]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Skyler Ruiter, Jiannan Tian, Fengguang Song"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Indiana University, Oakland University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.20563v1",children:"http://arxiv.org/pdf/2509.20563v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," FZModules introduces a heterogeneous computing framework that enables customizable scientific data compression pipelines using high-performance modules and asynchronous task execution. The framework allows assembling error-bounded compression pipelines that achieve comparable speed to fused-kernel GPU compressors while maintaining similar rate-distortion performance to higher-fidelity CPU/hybrid compressors. This enables rapid development of domain-tailored compression solutions for large-scale scientific data."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\nLLM Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [parallelism transformation, KV cache optimization, scheduling optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Haoyu Chen, Xue Li, Kun Qian, Yu Guan, Jin Zhao, Xin Wang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Fudan University and Alibaba Group"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.19729v1",children:"http://arxiv.org/pdf/2509.19729v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Gyges introduces dynamic cross-instance parallelism transformation that adaptively adjusts parallelism strategies to match incoming request dynamics. It employs page-friendly KV cache layouts, dedicated weight padding, and transformation-aware scheduling to optimize performance. Evaluations show throughput improvements of 1.75x-6.57x compared to state-of-the-art solutions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] BurstEngine: an Efficient Distributed Framework for Training\nTransformers on Extremely Long Sequences of over 1M Tokens"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [distributed training, long sequence, attention optimization, memory optimization, workload balance]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ao Sun, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong sun"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tsinghua University, Beijing University of Posts and Telecommunications"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.19836v1",children:"http://arxiv.org/pdf/2509.19836v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," BurstEngine introduces BurstAttention with topology-aware ring communication and fine-grained computation-communication overlap for efficient distributed training of transformers on sequences over 1M tokens. It also employs sequence-level selective checkpointing and workload balancing optimizations to reduce memory overhead. The framework achieves 1.2\xd7 speedup with lower memory usage compared to state-of-the-art baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [energy efficiency, token throughput, test-time compute, GPU utilization, model serving]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Felipe Oviedo, Fiodar Kazhamiaka, Esha Choukse, Allen Kim, Amy Luers, Melanie Nakagawa, Ricardo Bianchini, Juan M. Lavista Ferres"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Microsoft"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.20241v1",children:"http://arxiv.org/pdf/2509.20241v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a bottom-up methodology to estimate AI inference energy consumption based on token throughput. It finds current energy estimates often overstate usage by 4-20x, with median energy of 0.34 Wh per query for large models. The study identifies efficiency interventions that could reduce energy consumption by 8-20x through combined model, platform, and hardware improvements."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Adaptive Approach to Enhance Machine Learning Scheduling Algorithms\nDuring Runtime Using Reinforcement Learning in Metascheduling Applications"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [reinforcement learning, metascheduling, online learning, multi-schedule graph, adaptive scheduling, real-time optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Samer Alshaer, Ala Khalifeh, Roman Obermaisser"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Siegen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.20520v1",children:"http://arxiv.org/pdf/2509.20520v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes an adaptive online learning unit using reinforcement learning to enhance machine learning scheduling algorithms during runtime in metascheduling applications. The RL models continuously explore new scheduling solutions and expand the Multi-Schedule Graph to handle unexpected events and complex scenarios. This approach improves system performance and flexibility in dynamic environments while optimizing existing schedulers for stricter deadlines and new criteria."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge\nAccelerators"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [edge computing, DNN training, DNN inference, power optimization, time-slicing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Prashanthi S. K., Saisamarth Taluri, Pranav Gupta, Amartya Ranjan Saikia, Kunal Kumar Sahoo, Atharva Vinay Joshi, Lakshya Karwa, Kedar Dhule, Yogesh Simmhan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Indian Institute of Science"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.20205v1",children:"http://arxiv.org/pdf/2509.20205v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Fulcrum scheduler with GMD and ALS techniques for optimizing concurrent DNN training and inference on edge GPUs. These methods intelligently time-slice workloads and select power modes while meeting latency/power constraints. The solutions achieve >97% constraint satisfaction and are within 7% of optimal throughput with minimal profiling overhead."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Reconstruction-Based Adaptive Scheduling Using AI Inferences in\nSafety-Critical Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [adaptive scheduling, safety-critical systems, time-triggered systems, schedule reconstruction, AI-generated priorities]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Samer Alshaer, Ala Khalifeh, Roman Obermaisser"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Siegen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.20513v1",children:"http://arxiv.org/pdf/2509.20513v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a reconstruction framework that transforms AI-generated scheduling priorities into executable schedules while ensuring safety constraints like precedence rules and collision-free communication. The method incorporates safety checks, allocation algorithms, and recovery mechanisms for handling unexpected events. Results show the framework enhances system adaptability, operational integrity, and runtime performance while maintaining computational efficiency in safety-critical time-triggered systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] A Theory of Multi-Agent Generative Flow Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [multi-agent systems, generative flow networks, reinforcement learning, probabilistic modeling, decentralized execution]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Leo Maxime Brunswic, Haozhi Wang, Shuang Luo, Jianye Hao, Amir Rasouli, Yinchuan Li"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Huawei Technologies"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.20408v1",children:"http://arxiv.org/pdf/2509.20408v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a theoretical framework for multi-agent generative flow networks (MA-GFlowNets) that enables multiple agents to collaboratively generate objects through joint actions. It introduces four algorithms including centralized, independent, joint, and conditional flow networks with theoretical guarantees. Experimental results show the proposed framework outperforms reinforcement learning and MCMC-based methods in generating samples proportional to reward functions."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-25",children:"2025-09-25"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Guiding Application Users via Estimation of Computational Resources for\nMassively Parallel Chemistry Computations"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [machine learning, computational chemistry, resource prediction, coupled-cluster methods, supercomputing, active learning, execution time optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Tanzila Tabassum, Omer Subasi, Ajay Panyala, Epiya Ebiapia, Gerald Baumgartner, Erdal Mutlu, P., Sadayappan, Karol Kowalski"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Pacific Northwest National Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.20667v1",children:"http://arxiv.org/pdf/2509.20667v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper develops machine learning models to predict computational resources for chemistry simulations on supercomputers. It uses gradient boosting and active learning to optimize runtime parameters like node count and tile sizes. The approach achieves low prediction errors and helps users minimize execution time and resource costs for coupled-cluster computations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL\nPost-Training"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training], [reinforcement learning, LLM post-training, GPU utilization, synchronous RL, tail batching, rollout scheduling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Wei Gao, Yuheng Zhao, Dakai An, Tianyuan Wu, Lunxi Cao, Shaopan Xiong, Ju Huang, Weixun Wang, Siran Yang, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Hong Kong University of Science and Technology, Alibaba Group"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.21009v1",children:"http://arxiv.org/pdf/2509.21009v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," RollPacker introduces tail batching, a novel rollout scheduling strategy that groups long-tail responses into designated rounds to reduce GPU idle time in synchronous RL post-training. The system implements holistic optimizations across rollout, reward, and training stages. Empirical results show 2.03x-2.56x faster training compared to existing systems while maintaining accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Communication Bias in Large Language Models: A Regulatory Perspective"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [AI regulation, bias mitigation, fairness assessment, EU AI Act, Digital Services Act, transparency requirements]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Adrian Kuenzler, Stefan Schmid"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Hong Kong, TU Berlin, Weizenbaum Institute"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.21075v1",children:"http://arxiv.org/pdf/2509.21075v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper analyzes communication bias in LLMs from a regulatory perspective, examining frameworks like the EU AI Act and Digital Services Act. It discusses methods for bias assessment and mitigation while highlighting limitations of current approaches. The authors conclude that beyond regulation, greater attention to competition and design governance is needed to ensure fair and trustworthy AI systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Redesigning GROMACS Halo Exchange: Improving Strong Scaling with\nGPU-initiated NVSHMEM"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [kernels], [GPU-initiated communication, halo exchange, molecular dynamics, NVSHMEM, strong scaling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mahesh Doijade, Andrey Alekseenko, Ania Brown, Alan Gray, Szil\xe1rd P\xe1ll"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," NVIDIA, KTH Royal Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.21527v1",children:"http://arxiv.org/pdf/2509.21527v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper redesigns GROMACS halo exchange using GPU-initiated NVSHMEM communication, fusing data packing and communication in tuned GPU kernels. The approach improves communication-computation overlap and achieves up to 2x performance gains in strong scaling. Results demonstrate significant benefits of GPU-initiated communication for latency-sensitive applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM\nTraining"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [pipeline parallelism, sequence scheduling, gradient checkpointing, distributed training, long-context training]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shiju Wang, Yujie Wang, Ao Sun, Fangcheng Fu, Zijian Zhu, Bin Cui, Xu Han, Kaisheng Ma"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tsinghua University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.21275v1",children:"http://arxiv.org/pdf/2509.21275v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes Elastic Pipeline Parallelism (EPP) that adaptively combines token-level and batch-level pipeline parallelism to optimize long-context LLM training. The authors implement InfiniPipe system with workload-balanced sequence processing and co-optimized pipeline scheduling with adaptive checkpointing. Experiments show InfiniPipe achieves 1.69x speedup over state-of-the-art systems for long-context training."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] RecIS: Sparse to Dense, A Unified Training Framework for Recommendation\nModels"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [recommendation systems, sparse-dense training, PyTorch framework, system optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Hua Zong, Qingtao Zeng, Zhengxiong Zhou, Zhihua Han, Zhensong Yan, Mingjie Liu, Hechen Sun, Jiawei Liu, Yiwen Hu, Qi Wang, YiHan Xian, Wenjie Guo, Houyuan Xiang, Zhiyuan Zeng, Xiangrong Sheng, Bencheng Yan, Nan Hu, Yuheng Huang, Jinqing Lian, Ziru Xu, Yan Zhang, Ju Huang, Siran Yang, Huimin Yi, Jiamang Wang, Pengjie Wang, Han Zhu, Jian Wu, Dan Ou, Jian Xu, Haihong Tang, Yuning Jiang, Bo Zheng, Lin Qu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Alibaba"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.20883v1",children:"http://arxiv.org/pdf/2509.20883v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," RecIS proposes a unified sparse-dense training framework for recommendation models based on PyTorch, optimizing sparse components for efficiency while leveraging existing dense optimizations. The framework supports industrial-scale recommendation training integrated with large models. It demonstrates superior performance over TensorFlow-based approaches and is deployed in Alibaba for various recommendation tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [AI cluster scheduling, GPU resource management, unified training and inference scheduling, performance metrics]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Lingling Zeng, Gen Zhang, Jialin Peng, Xiang Xu, Yuan Xu, Lijun Ma"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," ZTE Corporation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.01256v1",children:"http://arxiv.org/pdf/2510.01256v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Kant introduces a unified scheduling system for large-scale AI clusters using strategies like Backfill and Enhanced Binpack to optimize GPU allocation. The system significantly improves resource utilization and reduces fragmentation in distributed training. It has been successfully deployed in multiple data centers, supporting efficient LLM training and inference workloads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\nServing and Fast Scaling"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [multi-SLO scheduling, dynamic scaling, P/D disaggregated architectures, KV cache transfer, device-to-device weight transfer]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zahra Yousefijamarani, Xinglu Wang, Qian Wang, Morgan Lindsay Heisler, Taha Shabani, Niloofar Gholipour, Parham Yassini, Hong Chang, Kan Chen, Qiantao Zhang, Xiaolong Bai, Jiannan Wang, Ying Xiong, Yong Zhang, Zhenan Fan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Huawei Technologies"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2508.15919v2",children:"http://arxiv.org/pdf/2508.15919v2"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," HyperFlexis introduces a unified LLM serving system with multi-SLO-aware scheduling and fast scaling optimizations, including D2D weight transfer to reduce loading overhead. It achieves higher SLO attainment, lower latency, and cost efficiency compared to state-of-the-art baselines. The system supports both collocated and disaggregated prefill/decode architectures for improved performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient\nMultimodal Inference on Battery-Powered Small Devices"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [multimodal models, hardware-software co-design, edge computing, energy efficiency, memory optimization, dynamic scheduling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yilong Li, Shuai Zhang, Yijing Zeng, Hao Zhang, Xinmiao Xiong, Jingyu Liu, Pan Hu, Suman Banerjee"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Wisconsin \u2013 Madison"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.05109v1",children:"http://arxiv.org/pdf/2510.05109v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," NANOMIND presents a hardware-software co-design framework that breaks multimodal models into modular components and schedules them across heterogeneous accelerators. The system achieves significant energy and memory efficiency improvements through token-aware buffer management and optimized computation kernels. This enables battery-powered devices to run large multimodal models entirely on-device for extended periods without network connectivity."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Integrating and Characterizing HPC Task Runtime Systems for hybrid\nAI-HPC workloads"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [HPC, task runtime systems, AI-HPC workloads, RADICAL-Pilot, Flux, Dragon, performance optimization, resource management]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Andre Merzky, Mikhail Titov, Matteo Turilli, Shantenu Jha"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," RADICAL-Computing Inc., Brookhaven National Laboratory, Rutgers University, Princeton Plasma Physics Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.20819v1",children:"http://arxiv.org/pdf/2509.20819v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper integrates RADICAL-Pilot with Flux and Dragon runtime systems to manage hybrid AI-HPC workloads. Performance evaluation shows RP+Flux sustains 930 tasks/s and RP+Flux+Dragon exceeds 1,500 tasks/s, significantly outperforming traditional srun/Slurm. The approach reduces makespan by 30-60% and increases throughput over four times for drug discovery campaigns."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient\nMethod for Solving Large-Scale Linear Optimization Problem"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [in-memory computing, RRAM, linear optimization, primal-dual hybrid gradient, distributed computing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Huynh Q. N. Vo, Md Tawsif Rahman Chowdhury, Paritosh Ramanan, Gozde Tutuncuoglu, Junchi Yang, Feng Qiu, Murat Yildirim"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," IBM Research"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.21137v1",children:"http://arxiv.org/pdf/2509.21137v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents a distributed in-memory primal-dual hybrid gradient method specifically designed for RRAM arrays, minimizing write cycles and incorporating robustness against device non-idealities. The approach uses a symmetric block-matrix formulation to unify operations across distributed crossbars and integrates physics-based simulation for realistic evaluation. Benchmarking shows the RRAM-based solver achieves comparable accuracy to GPU-accelerated solvers while reducing energy consumption and latency by up to three orders of magnitude."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Go With The Flow: Churn-Tolerant Decentralized Training of Large\nLanguage Models"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [decentralized training, churn tolerance, flow optimization, pipeline parallelism, heterogeneous clients]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nikolay Blagoev, Bart Cox, J\xe9r\xe9mie Decouchant, Lydia Y. Chen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universit\xe9 de Neuch\xe2tel, Delft University of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.21221v1",children:"http://arxiv.org/pdf/2509.21221v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," GWTF introduces a decentralized flow algorithm for efficient LLM training across volunteer clients, addressing node churn and network instabilities. It optimizes routing to maximize microbatch processing with minimal delay. Experiments show up to 45% training time reduction in heterogeneous, high-churn environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] SuperOffload: Unleashing the Power of Large-Scale LLM Training on\nSuperchips"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [Superchips, offloading, heterogeneous architecture, Grace CPU, Hopper GPU, NVLink-C2C, adaptive weight offloading, bucketization repartitioning, speculative execution, Adam optimizer optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xinyu Lian, Masahiro Tanaka, Olatunji Ruwase, Minjia Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Illinois Urbana-Champaign (SSAIL Lab), Anyscale, Snowflake"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.21271v1",children:"http://arxiv.org/pdf/2509.21271v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," SuperOffload introduces a novel offloading system optimized for Superchip architecture, combining techniques like adaptive weight offloading and Superchip-aware casting. It achieves up to 2.5x throughput improvement over state-of-the-art systems, enabling efficient training of large models up to 25B parameters on a single Superchip. The system also scales effectively with parallelism methods, supporting training of 13B models with 1M token sequences on multiple Superchips."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-26",children:"2025-09-26"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] The AI_INFN Platform: Artificial Intelligence Development in the Cloud"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [Kubernetes platform, GPU acceleration, distributed computing, workflow management, cloud-native solutions, INFN Cloud]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Lucio Anderlini, Giulio Bianchini, Diego Ciangottini, Stefano Dal Pra, Diego Michelotto, Rosa Petrini, Daniele Spiga"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Istituto Nazionale di Fisica Nucleare (INFN)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.22117v1",children:"http://arxiv.org/pdf/2509.22117v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents a Kubernetes-based platform for developing GPU-powered data analysis workflows that can scale across heterogeneous distributed computing resources. It leverages cloud-native solutions and offloading mechanisms to manage workflows across different resource providers including WLCG sites and supercomputers. The platform aims to effectively share hardware accelerators while supporting diverse research activities within INFN use cases."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Code once, Run Green: Automated Green Code Translation in Serverless\nComputing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [serverless computing, energy efficiency, code translation, large language models, green computing]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Sebastian Werner, Mathis K\xe4hler, Alireza Hakamian"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Hamburg, Technische Universit\xe4t Berlin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.22068v1",children:"http://arxiv.org/pdf/2509.22068v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes ReFaaS, a system that uses LLMs to automatically translate serverless functions into more energy-efficient programming languages. The translated functions can reduce invocation energy by up to 70%, achieving net energy savings after 3,000-5,000 invocations. However, the approach faces challenges with function suitability and varying amortization thresholds."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code\nGeneration Using LLMs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [multi-agent systems, code generation, HPC, auto-tuning, prompt refinement, GPU programming]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shun-ichiro Hayashi, Koki Morita, Daichi Mukunoki, Tetsuya Hoshino, Takahiro Katagiri"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nagoya University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.00031v1",children:"http://arxiv.org/pdf/2510.00031v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," VibeCodeHPC uses multi-agent LLMs with four specialized roles (Project Manager, System Engineer, Programmer, Continuous Delivery) for iterative prompt refinement to automatically tune HPC code. In a case study converting CPU-based matrix multiplication to CUDA GPU code, the multi-agent approach achieved higher-quality code generation per unit time compared to solo-agent configuration and more effectively identified requirement violations. The system demonstrates improved efficiency in HPC code optimization through dynamic agent deployment and activity monitoring."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Agora: Bridging the GPU Cloud Resource-Price Disconnect"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [GPU cloud pricing, resource allocation, memory bandwidth, feature-based pricing, market efficiency]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ian McDougall, Noah Scott, Joon Huh, Kirthevasan Kandasamy, Karthikeyan Sankaralingam"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Wisconsin-Madison"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.05111v1",children:"http://arxiv.org/pdf/2510.05111v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Agora, a feature-based pricing framework that links cloud GPU costs directly to resource consumption like memory bandwidth. It introduces a practical system architecture and shows that fine-grained sampling enables nearly ideal pricing with minimal revenue loss. The approach creates a more transparent and efficient market for GPU cloud resources by addressing the performance-price disconnect in bandwidth-bound workloads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Efficient Fine-Grained GPU Performance Modeling for Distributed Deep\nLearning of LLM"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [performance modeling, distributed training, GPU, parallelism strategies, computational primitives]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Biyao Zhang, Mingkai Zheng, Debargha Ganguly, Xuecen Zhang, Vikash Singh, Vipin Chaudhary, Zhao Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Case Western Reserve University, Rutgers University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.22832v1",children:"http://arxiv.org/pdf/2509.22832v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a fine-grained GPU performance modeling framework that decomposes LLMs into computational primitives and uses operator-level analysis with lightweight sampling. The method achieves low prediction errors (4.98% on A100, 9.38% on GH200) for models up to 20B parameters across 128 GPUs. Crucially, the framework runs entirely on CPUs, enabling rapid hardware configuration exploration without expensive cluster experiments."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-27",children:"2025-09-27"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Scaling LLM Test-Time Compute with Mobile NPU on Smartphones"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [mobile NPU, test-time scaling, quantization optimization, edge AI, model acceleration]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zixu Hao, Jianyu Wei, Tuowei Wang, Minxing Huang, Huiqiang Jiang, Shiqi Jiang, Ting Cao, Ju Ren"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Tsinghua University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.23324v1",children:"http://arxiv.org/pdf/2509.23324v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes leveraging underutilized mobile NPU compute for parallel test-time scaling of smaller LLMs through hardware-aware tile quantization and LUT-based operation replacements. The implemented system achieves up to 19x speedup for GEMM operations and enables smaller models to match larger model accuracy. This approach establishes a new performance-cost Pareto frontier for mobile LLM deployment."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured\nCompression"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [Mixture-of-Experts, dynamic clustering, structured compression, hierarchical routing, parameter reduction, communication optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Peijun Zhu, Ning Yang, Jiayu Wei, Jinghang Wu, Haijun Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The primary research institution appears to be the organization(s) associated with authors Peijun Zhu, Jiayu Wei, Jinghang Wu (likely from institution 1), Ning Yang (likely from institution 2), and Haijun Zhang (from institution 3)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.02345v1",children:"http://arxiv.org/pdf/2510.02345v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces a unified framework combining dynamic expert clustering and structured compression to address the MoE LLM trilemma. The method uses online clustering with fused similarity metrics and decomposes expert weights into shared bases with low-rank residuals, enabling hierarchical routing. Results show 80% parameter reduction, 10-20% throughput improvement, and 3x lower load variance while maintaining model quality comparable to standard MoE models."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Towards Quantum-Ready Blockchain Fraud Detection via Ensemble Graph\nNeural Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models inference], [blockchain fraud detection, graph neural networks, ensemble learning, quantum-ready systems, anti-money laundering]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," M. Z. Haider, Tayyaba Noreen, M. Salman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Universit\xe9 du Qu\xe9bec, SZABIST University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.23101v1",children:"http://arxiv.org/pdf/2509.23101v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes an ensemble framework combining Graph Convolutional Networks, Graph Attention Networks, and Graph Isomorphism Networks for blockchain fraud detection. This approach achieves high recall of illicit transactions with low false positive rates on the Elliptic dataset. The architecture is designed to be quantum-ready for future integration with quantum computing technologies."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Memory Efficient and Staleness Free Pipeline Parallel DNN Training\nFramework with Improved Convergence Speed"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [Other models training], [pipeline parallelism, memory efficiency, convergence speed]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ankita Dutta, Nabendu Chaki, Rajat K. De"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Indian Statistical Institute, University of Calcutta"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.23241v1",children:"http://arxiv.org/pdf/2509.23241v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces V-TiMePReSt and I-TiMePReSt, two pipeline parallel DNN training frameworks that address staleness and memory efficiency. V-TiMePReSt eliminates staleness by using latest weights, while I-TiMePReSt computes intermediate weights to balance memory usage and convergence. Experimental results show both frameworks improve training efficiency with better staleness management and convergence performance."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"2025-09-28",children:"2025-09-28"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] Disaggregated Prefill and Decoding Inference System for Large Language\nModel Serving on Multi-Vendor GPUs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM inference], [heterogeneous GPUs, prefill-decode disaggregation, multi-vendor systems, joint optimization algorithm, resource utilization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Xing Chen, Rong Shi, Lu Zhao, Lingbin Wang, Xiao Jin, Yueqiang Chen, Hongfeng Sun"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," ZTE Corporation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.17542v2",children:"http://arxiv.org/pdf/2509.17542v2"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a P-D disaggregated inference system for LLMs using heterogeneous multi-vendor GPUs, designing a compatible transmission module and joint optimization algorithm. Experimental results show the system effectively handles hybrid inference across different GPU vendors while optimizing deployment solutions. This approach improves resource utilization and reduces costs compared to homogeneous GPU systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous\nRetraining Alignment"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [scheduling], [LLM serving, continuous retraining, edge computing, iteration-level scheduling, GPU resource management, SLO-aware optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yufei Li, Yu Fu, Yue Dong, Cong Liu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Texas at Dallas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2510.03283v1",children:"http://arxiv.org/pdf/2510.03283v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," MACE proposes a hybrid LLM serving system that co-locates inference and fine-tuning with iteration-level scheduling and intelligent memory management. It balances inference latency and model accuracy by dynamically allocating GPU cycles based on update importance. Evaluation shows MACE reduces inference latency by up to 63% while maintaining high GPU utilization and throughput under resource constraints."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv2509] AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on\nHeterogeneous Models"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [LLM training], [pipeline parallelism, model partition, workload scheduling, performance optimization, heterogeneous models]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jihu Guo, Tenghui Ma, Wei Gao, Peng Sun, Jiaxing Li, Xun Chen, Yuyang Jin, Dahua Lin"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Fudan University, Shanghai AI Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"http://arxiv.org/pdf/2509.23722v1",children:"http://arxiv.org/pdf/2509.23722v1"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," AdaPtis proposes an adaptive pipeline parallelism system that jointly optimizes model partition, placement, and scheduling using a performance model. It introduces a unified pipeline executor to support diverse pipeline strategies. Experiments show 1.42x average speedup over Megatron-LM across various LLM architectures and scales."]}),"\n"]}),"\n"]}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(h,{...n})}):h(n)}}}]);