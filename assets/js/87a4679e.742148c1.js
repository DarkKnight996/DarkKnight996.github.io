"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[372],{54:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20260105-20260111","title":"20260105-20260111","description":"2026-01-05","source":"@site/docs/daily/20260105-20260111.md","sourceDirName":"daily","slug":"/daily/20260105-20260111","permalink":"/daily/20260105-20260111","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1767671781000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251229-20260104","permalink":"/daily/20251229-20260104"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=n(4848),t=n(8453);const a={},o="20260105-20260111",l={},c=[{value:"2026-01-05",id:"2026-01-05",level:2},{value:"2026-01-06",id:"2026-01-06",level:2}];function h(e){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"20260105-20260111",children:"20260105-20260111"})}),"\n",(0,s.jsx)(i.h2,{id:"2026-01-05",children:"2026-01-05"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"cs.DC total: 9"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260105] From Consensus to Chaos: A Vulnerability Assessment of the RAFT Algorithm"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [distributed consensus], [RAFT, message replay attacks, message forgery, cryptography, authenticated message verification, freshness check]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Tamer Afifi, Abdelfatah Hegazy, Ehab Abousaif"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Arab Academy for Science, Technology & Maritime"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00273",children:"https://arxiv.org/pdf/2601.00273"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper conducts a security analysis of the RAFT distributed consensus algorithm, identifying vulnerabilities to replay and forgery attacks. It proposes a novel security enhancement framework using cryptography, authenticated message verification, and freshness checks to protect against these threats and build more resilient systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260105] Federated Customization of Large Models: Approaches, Experiments, and Insights"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm training], [federated learning, prefix-tuning, fine-tuning, prompt engineering, knowledge distillation, retrieval-augmented generation]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Yuchuan Ye, Ming Ding, Youjia Chen, Peng Cheng, Dusit Niyato"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Fuzhou University, CSIRO Data61, La Trobe University, Nanyang Technological University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00526",children:"https://arxiv.org/pdf/2601.00526"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper explores federated customization of large models, reviewing techniques like fine-tuning and prefix-tuning within a federated learning framework. It experimentally validates federated prefix-tuning, showing it achieves performance close to centralized approaches with competitive efficiency and robustness."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260105] Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [cloud computing], [benchmarking, cost-performance analysis, scalability, response latency, throughput, API endpoints]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Ravi Teja Pagidoju"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Campbellsville University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00530",children:"https://arxiv.org/pdf/2601.00530"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents a systematic, code-driven benchmarking methodology to compare the performance and cost of deploying retail Point-of-Sale workloads on Google Cloud Platform and Microsoft Azure using free-tier resources. The main conclusion is that GCP offers 23.0% faster response times, while Azure demonstrates 71.9% higher cost efficiency for steady-state operations."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260105] Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [secure multi-party computation], [affine masking, consensus locking, step checksums, data checksums, IND-CPA security]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Prajwal Panth, Sahaj Raj Malla"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," KIIT University, Kathmandu University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00418",children:"https://arxiv.org/pdf/2601.00418"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, which uses per-client affine masking and sequential consensus locking for secure multi-client data aggregation with verifiable integrity. It demonstrates linear scalability up to 500 clients, achieves 100% malicious deviation detection, and significantly reduces computational overhead compared to MPC and HE baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260105] Impact of Clustering on the Observability and Controllability of Complex Networks"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [network theory], [structured systems theory, Monte-Carlo simulations, clustering, scale-free networks, graph theory]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Mohammadreza Doostmohammadian, Hamid R. Rabiee"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Semnan University, Sharif University of Technology"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00221",children:"https://arxiv.org/pdf/2601.00221"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper uses structured systems theory and Monte-Carlo simulations to study how clustering affects the observability and controllability of scale-free networks. It concludes that densely clustered networks require fewer driver and observer nodes for control and observation, offering practical insights for optimizing network design in resource-constrained applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260105] Word Frequency Counting Based on Serverless MapReduce"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [serverless computing, mapreduce, function as a service (faas), word frequency counting, optimization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Hanzhe Li, Bingchen Lin, Mengyuan Xu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Xi\u2019an Jiaotong University, Chongqing University of Education, Qilu Institute of Technology"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00380",children:"https://arxiv.org/pdf/2601.00380"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes using a serverless MapReduce model on a Function-as-a-Service platform to optimize word frequency counting tasks. It focuses on determining the optimal number of map and reduce functions to minimize execution time and improve efficiency. The experiments show that increasing the number of functions improves performance, providing a method to find optimized configurations for such tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260105] Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm inference], [time-warp emulation, CUDA API interception, discrete-event simulation, virtual time coordination]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Amey Agrawal, Mayank Yadav, Sukrit Kumar, Anirudha Agrawal, Garv Ghai, Souradeep Bera, Elton Pinto, Sirish Gambhira, Mohammad Adain, Kasra Sohrab, Chus Antonanzas, Alexey Tumanov"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Georgia Institute of Technology"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00397",children:"https://arxiv.org/pdf/2601.00397"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper presents Revati, a GPU-free time-warp emulator for LLM serving that directly executes real serving system code by intercepting CUDA calls and performing virtual time jumps instead of running actual GPU kernels. It introduces a coordination protocol to synchronize these time jumps across distributed processes. The system achieves less than 5% prediction error and runs 5-17x faster than real GPU execution on frameworks like vLLM and SGLang."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260105] FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-Cloud Collaborative LLM Speculative Decoding"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm inference], [speculative decoding, edge-cloud collaboration, shared-backbone architecture, channel-aware adaptive speculation]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Yuchen Li, Rui Kong, Zhonghao Lyu, Qiyang Li, Xinran Chen, Hengyi Cai, Lingyong Yan, Shuaiqiang Wang, Jiashu Zhao, Guangxu Zhu, Linghe Kong, Guihai Chen, Haoyi Xiong, Dawei Yin"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Baidu Inc., Shanghai Jiao Tong University, KTH Royal Institute of Technology, Wilfrid Laurier University, Shenzhen Research Institute of Big Data"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00644",children:"https://arxiv.org/pdf/2601.00644"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes FlexSpec, a framework for edge-cloud collaborative inference that uses a static, shared-backbone draft model at the edge to work with evolving target models in the cloud, eliminating the need for frequent model synchronization. It also introduces a channel-aware mechanism to dynamically adjust the draft length based on network conditions. Experiments show that FlexSpec improves inference efficiency over conventional speculative decoding approaches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260105] Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [multi-agent systems, language model agents, bio-inspired self-healing, distributed computing continuum, fault diagnosis, resource reconfiguration]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Alaa Saleh, Praveen Kumar Donta, Roberto Morabito, Sasu Tarkoma, Anders Lindgren, Qiyang Zhang, Schahram Dustdar Susanna Pirttikangas, Lauri Lov\xe9n"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Oulu, Stockholm University, EURECOM, University of Helsinki, RISE Research Institutes of Sweden, Lule\xe5 University of Technology, Peking University, TU Wien"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00339",children:"https://arxiv.org/pdf/2601.00339"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces ReCiSt, a bio-inspired self-healing framework for resilient distributed computing systems. It uses language model-powered agents across four computational layers to autonomously isolate faults, diagnose causes, and reconfigure resources. The evaluation demonstrates the framework's ability to perform self-healing within tens of seconds with low CPU overhead."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 15'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00186",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] Online Finetuning Decision Transformers with Pure RL Gradients ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00167",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] Can Optimal Transport Improve Federated Inverse Reinforcement Learning? ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00309",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00737",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] Reinforcement Learning with Function Approximation for Non-Markov Processes ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00151",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00607",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00245",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00538",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00116",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00423",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00677",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00257",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] Reinforcement learning with timed constraints for robotics motion planning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00087",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00728",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] ARISE: Adaptive Reinforcement Integrated with Swarm Exploration ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00693",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 1'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv260105] Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00282",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"2026-01-06",children:"2026-01-06"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"cs.DC total: 19"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] pMSz: A Distributed Parallel Algorithm for Correcting Extrema and Morse Smale Segmentations in Lossy Compression"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [high-performance computing], [distributed parallel algorithm, Morse-Smale segmentation, lossy compression, integral paths, steepest ascending directions, GPU]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Yuxiao Li, Mingze Xia, Xin Liang, Bei Wang, Robert Underwood, Sheng Di, Hemant Sharma, Dishant Beniwal, Franck Cappello, Hanqi Guo"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," The Ohio State University, Argonne National Laboratory, Oregon State University, University of Utah"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01787",children:"https://arxiv.org/pdf/2601.01787"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces pMSz, a distributed parallel algorithm that corrects topological distortions in lossy-compressed scientific data by preserving steepest ascending and descending directions instead of explicitly computing integral paths. This approach minimizes communication and achieves high parallel efficiency, scaling effectively to 128 GPUs on the Perlmutter supercomputer."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm inference], [Singleton Weight Sharing, Topological Synapse, KV-cache sparsification, witness complex, hybrid landmarking, Referential Injection]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Jorge L. Ruiz Williams"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Warp Research"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01298",children:"https://arxiv.org/pdf/2601.01298"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper introduces Warp Cortex, an asynchronous architecture for multi-agent LLMs that decouples agent logic from memory to enable massive scaling. Its core methods include Singleton Weight Sharing and a Topological Synapse for KV-cache sparsification, reducing memory complexity. The authors demonstrate the system can support 100 concurrent agents on a single consumer GPU, with a theoretical capacity for over 1,000 agents."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm inference], [KV cache, cross-stage relay-race inference, sequence-aware trigger, affinity-aware router, memory-aware expander]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Jiarui Wang, Huichao Chai, Yuanhang Zhang, Zongjin Zhou, Wei Guo, Xingkun Yang, Qiang Tang, Bo Pan, Jiawei Zhu, Ke Cheng, Yuting Yan, Shulan Wang, Yingjie Zhu, Zhengfan Yuan, Jiaqi Huang, Yuhan Zhang, Xiaosong Sun, Zhinan Zhang, Hong Zhu, Yongsheng Zhang, Tiantian Dong, Zhong Xiao, Deliang Liu, Chengzhou Lu, Yuan Sun, Zhiyuan Chen, Xinming Han, Zaizhu Liu, Yaoyuan Wang, Ziyang Zhang, Yong Liu, Jinxin Xu, Yajing Sun, Zhoujun Yu, Wenting Zhou, Qidong Zhang, Zhengyong Zhang, Zhonghai Gu, Yibo Jin, Yongxiang Feng, Pengfei Zuo"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Huawei Technologies Co., Ltd."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01712",children:"https://arxiv.org/pdf/2601.01712"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," RelayGR is a production system that scales long-sequence generative recommendation by pre-inferring and caching user-behavior prefixes in HBM across pipeline stages, enabling reuse during ranking. It uses a sequence-aware trigger, affinity-aware router, and memory-aware expander to manage cache footprint and locality. The system allows for longer sequences and significantly improves throughput while meeting strict latency SLOs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] A Multi-Port Concurrent Communication Model for handling Compute Intensive Tasks on Distributed Satellite System Constellations"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [distributed satellite systems], [divisible load theory, multi-port concurrent communication, admission control, inter-satellite links, load allocation]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Bharadwaj Veeravalli"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," National University of Singapore"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01031",children:"https://arxiv.org/pdf/2601.01031"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper develops a Multi-Port Concurrent Communication Divisible Load Theory (MPCC-DLT) framework for scheduling compute-intensive tasks across distributed satellite constellations. It provides closed-form solutions for optimal load distribution and analyzes the trade-offs between computation and communication overhead. The main conclusion is that while highly distributable tasks benefit significantly, communication-heavy tasks show diminishing returns, and the extended framework with admission control enables practical, deadline-aware operation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] DiT-HC: Enabling Efficient Training of Visual Generation Model DiT on HPC-oriented CPU Cluster"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [diffusion training], [communication-free tensor parallelism, AutoMem, HCOps, custom MPI backend, vector and matrix acceleration units]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Jinxiao Zhang, Yunpu Xu, Xiyong Wu, Runmin Dong, Shenggan Cheng, Yi Zhao, Mengxuan Chen, Qinrui Zheng, Jianting Liu, Haohuan Fu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Tsinghua University, Sun Yat-sen University, National University of Singapore, National Supercomputing Center in Shenzhen"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01500",children:"https://arxiv.org/pdf/2601.01500"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," DiT-HC introduces a system for training the DiT generative model on HPC CPU clusters using techniques like communication-free tensor parallelism, optimized kernels, and a custom MPI backend. It achieves significant speedups and high weak scaling efficiency, demonstrating the feasibility of large-scale generative model training on CPU-based supercomputers."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] Performance and Security Aware Distributed Service Placement in Fog Computing"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [deep reinforcement learning, long short-term memory networks, prioritized experience replay, off-policy correction, multi-objective optimization, security score hierarchy]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Mohammad Goudarzi, Arash Shaghaghi, Zhiyu Wang, Rajkumar Buyya"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Monash University, The University of New South Wales"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01125",children:"https://arxiv.org/pdf/2601.01125"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a Security and Performance-Aware Distributed Deep Reinforcement Learning (SPA-DDRL) framework for service placement in Fog computing, which jointly optimizes latency and security using a distributed broker-learner architecture. The method integrates LSTM networks, Prioritized Experience Replay, and off-policy correction. Experiments show it improves service response time by 16.3% and converges 33% faster than baseline approaches while maintaining security compliance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] Communication-Efficient Federated AUC Maximization with Cyclic Client Participation"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [federated learning, AUC maximization, cyclic client participation, minimax optimization, Polyak-\u0141ojasiewicz condition, communication complexity]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Umesh Vangapally, Wenhan Wu, Chen Chen, Zhishuai Guo"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Northern Illinois University, University of North Carolina at Charlotte, University of Central Florida"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01649",children:"https://arxiv.org/pdf/2601.01649"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes communication-efficient algorithms for federated AUC maximization under cyclic client participation, addressing the challenge of partial client availability in real-world federated learning. It analyzes two settings: one with a squared surrogate loss and one with general pairwise losses, establishing improved communication and iteration complexities, especially under the Polyak-\u0141ojasiewicz condition. Experiments on tasks like image classification and fraud detection demonstrate the methods' superior efficiency and effectiveness."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] FFCz: Fast Fourier Correction for Spectrum-Preserving Lossy Compression of Scientific Data"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [scientific data compression], [fast Fourier correction, error-bounded lossy compression, GPU parallelism, SZ3, ZFP, SPERR]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Congrong Ren, Robert Underwood, Sheng Di, Emrecan Kutay, Zarija Lukic, Aylin Yener, Franck Cappello, Hanqi Guo"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," The Ohio State University, Argonne National Laboratory, Lawrence Berkeley National Laboratory"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01596",children:"https://arxiv.org/pdf/2601.01596"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces FFCz, a fast Fourier correction algorithm that modifies the error from existing lossy compressors to preserve both spatial and frequency-domain accuracy in scientific data. It achieves this by iteratively projecting the spatial error vector to satisfy user-defined bounds in both domains, accelerated by GPU parallelism. The method is validated on datasets from cosmology, combustion, and X-ray diffraction, effectively maintaining critical spectral features."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] Making MoE based LLM inference resilient with Tarragon"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [mixture-of-experts, KV cache checkpointing, shadow experts, reconfigurable datapath, asynchronous recovery]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Songyu Zhang, Aaron Tam, Myungjin Lee, Shixiong Qi, K. K. Ramakrishnan"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of California, Riverside, Cisco Research, University of Kentucky"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01310",children:"https://arxiv.org/pdf/2601.01310"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper presents Tarragon, a resilient inference framework for Mixture-of-Experts LLMs. It confines failures to individual workers by using a reconfigurable datapath and self-healing mechanisms like incremental KV cache checkpointing and shadow experts. The evaluation shows it reduces failure-induced stalls by over 160x compared to prior systems while maintaining performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] OrchestrRL: Dynamic Compute and Network Orchestration for Disaggregated RL"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [post-training], [disaggregated RL, adaptive compute scheduler, reconfigurable hybrid optical-electrical fabric, RFabric, parallelism switching, RLSim]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Xin Tan, Yicheng Feng, Yu Zhou, Yimin Jiang, Yibo Zhu, Hong Xu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," The Chinese University of Hong Kong, StepFun"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01209",children:"https://arxiv.org/pdf/2601.01209"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces OrchestrRL, a framework that dynamically orchestrates compute and network resources for disaggregated reinforcement learning. It co-designs an adaptive compute scheduler and a reconfigurable network fabric (RFabric) to address bottlenecks in generation and dynamic traffic patterns. The evaluation shows OrchestrRL improves throughput by up to 1.40x and RFabric offers better performance-cost efficiency than static networks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] Cutting Quantum Circuits Beyond Qubits"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [quantum computing], [circuit cutting, qudit decomposition, Gell-Mann matrices, distributed quantum computing, memory reduction]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Manav Seksaria, Anil Prabhakar"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Indian Institute of Technology Madras"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02064",children:"https://arxiv.org/pdf/2601.02064"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper extends quantum circuit cutting to mixed-dimensional qudit registers by decomposing non-local interactions using tensor products of generalized Gell-Mann matrices. This enables simulation of high-dimensional circuits on disconnected hardware fragments, achieving exact state reconstruction and reducing memory usage significantly, as demonstrated in an 8-particle system."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [federated learning, personalized federated learning, cosine sparsification, parameter packing, dual-weighted aggregation, sparse updates, Non-IID data]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Qiantao Yang, Liquan Chen, Mingfu Xue, Songze Li"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Southeast University, Purple Mountain Laboratories, East China Normal University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01840",children:"https://arxiv.org/pdf/2601.01840"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes FedCSPACK, a personalized federated learning method that uses cosine sparsification for parameter packing and a dual-weighted aggregation mechanism to reduce communication costs and mitigate data heterogeneity. It concludes that this approach effectively improves communication and computational efficiency while maintaining high model accuracy across heterogeneous datasets."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] Vouchsafe: A Zero-Infrastructure Capability Graph Model for Offline Identity and Trust"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [identity and trust systems], [Ed25519, SHA-256, JSON Web Tokens, Zero-Infrastructure Capability Graph, offline verification]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Jay Kuri"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Ionzero Inc."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02254",children:"https://arxiv.org/pdf/2601.02254"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces Vouchsafe, a system that implements a Zero-Infrastructure Capability Graph model for offline identity and trust. It uses self-contained, signed statements with standard cryptographic primitives like Ed25519 and JWTs, enabling trust verification without any online infrastructure. The main conclusion is that a practical, offline-verifiable trust substrate can be built today using only locally available cryptographic data."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] Deciding Serializability in Network Systems"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [formal verification], [Petri net reachability, semilinear-set compression, Presburger-formula manipulation, network-system abstraction]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Guy Amir, Mark Barbone, Nicolas Amat, Jules Jacobs"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Cornell University, ONERA, Jane Street Capital"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02251",children:"https://arxiv.org/pdf/2601.02251"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper presents the SER modeling language and an automated decision procedure for verifying serializability in concurrent programs. The method compiles programs to a network-system abstraction and reduces the verification problem to a Petri net reachability query, using optimizations like slicing and semilinear-set compression. The authors conclude that their framework can successfully handle models of real-world systems like stateful firewalls and BGP routers."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] Benchmarking Quantum Data Center Architectures: A Performance and Scalability Perspective"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [quantum computing systems], [quantum data center, distributed quantum computing, entanglement generation, Bell State Measurement, teleportation, QFly, BCube, Clos, Fat-Tree]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Shahrooz Pouryousef, Eneet Kaur, Hassan Shapourian, Don Towsley, Ramana Kompella, Reza Nejabati"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Cisco Research, University of Massachusetts Amherst"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01353",children:"https://arxiv.org/pdf/2601.01353"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper benchmarks four quantum data center architectures (QFly, BCube, Clos, Fat-Tree) by analyzing their performance under quantum-specific constraints like entanglement generation delays and resource contention. The study concludes that distributed quantum performance is shaped by a complex interaction of topology, scheduling policies, and physical-layer parameters."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] SuperSFL: Resource-Heterogeneous Federated Split Learning with Weight-Sharing Super-Networks"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [federated split learning, weight-sharing super-network, three-phase gradient fusion, resource-aware subnetwork, gradient fusion]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Abdullah Al Asif, Sixing Yu, Juan Pablo Munoz, Arya Mazaheri, Ali Jannesari"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Iowa State University, Intel Corporation, Technical University of Darmstadt"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02092",children:"https://arxiv.org/pdf/2601.02092"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes SuperSFL, a federated split learning framework that uses a weight-sharing super-network to generate client-specific subnetworks for heterogeneous edge devices and employs a Three-Phase Gradient Fusion mechanism for optimization. It demonstrates faster convergence, lower communication cost, and improved energy efficiency compared to baseline methods, making it a practical solution for heterogeneous edge environments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm training], [placement semantics, data parallelism, tensor parallelism, pipeline parallelism, ZeRO, FSDP, memory consumption, communication volume]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Deep Pankajbhai Mehta"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Adobe Inc."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02311",children:"https://arxiv.org/pdf/2601.02311"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"}),' The paper introduces a systematic framework called "placement semantics" to analyze distributed deep learning parallelism strategies by specifying how training states are placed across devices. From this specification alone, it can derive key performance metrics like memory use and communication cost, unifying strategies like ZeRO and pipeline parallelism. The main conclusion is that this framework provides a theoretical foundation to predict and compare strategy behavior without implementation details, matching published empirical results.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] Bringing computation to the data: A MOEA-driven approach for optimising data processing in the context of the SKA and SRCNet"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [Multi-Objective Evolutionary Algorithms (MOEAs), Function-as-a-Service (FaaS), in-situ processing, distributed computing, data-intensive workflows]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Manuel Parra-Roy\xf3n, \xc1lvaro Rodr\xedguez-Gallardo, Susana S\xe1nchez-Exp\xf3sito, Laura Darriba-Pol, Jes\xfas S\xe1nchez-Casta\xf1eda, M. \xc1ngeles Mendoza, Juli\xe1n Garrido, Javier Mold\xf3n, Lourdes Verdes-Montenegro"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Instituto de Astrof\xedsica de Andaluc\xeda, IAA-CSIC"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01980",children:"https://arxiv.org/pdf/2601.01980"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a framework that integrates Function-as-a-Service (FaaS) with a decision-making entity based on Multi-Objective Evolutionary Algorithms (MOEAs) to optimize data processing workflows for the SKA telescope. The approach moves computation closer to the data to overcome network bottlenecks, using MOEAs to find execution plans that balance time and energy costs. The conclusion is that this provides a baseline for efficient, cost-aware computation-to-data strategies within the SKA Regional Centres Network."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv260106] BigSUMO: A Scalable Framework for Big Data Traffic Analytics and Parallel Simulation"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [others], [parallel simulation, SUMO, loop detector data, probe trajectory data, descriptive analytics, prescriptive analytics, interruption detection]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Rahul Sengupta, Nooshin Yousefzadeh, Manav Sanghvi, Yash Ranjan, Anand Rangarajan, Sanjay Ranka, Yashaswi Karnati, Jeremy Dilmore, Tushar Patel, Ryan Casburn"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Florida, NVIDIA Corp., FDOT"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02286",children:"https://arxiv.org/pdf/2601.02286"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"}),' The paper presents BigSUMO, a scalable open-source framework that ingests traffic data like loop detector and probe trajectories to perform descriptive analytics and interruption detection, then uses the SUMO microsimulator for parallel prescriptive simulations of "what-if" scenarios. The main conclusion is that this end-to-end, modular system provides a cost-effective and deployable tool for traffic management and optimization in smart cities.']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 22'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00868",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Horizon Reduction as Information Loss in Offline Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00831",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] PyBatchRender: A Python Library for Batched 3D Rendering at Up to One Million FPS ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01288",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01785",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01665",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01562",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Dichotomous Diffusion Policy Optimization ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00898",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01577",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01800",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism and Comprehensive AI Psychological Counselor ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01802",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Moments Matter",":Stabilizing"," Policy Optimization using Return Distributions ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01803",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01904",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Distorted Distributional Policy Evaluation for Offline Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01917",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] GDRO: Group-level Reward Post-training Suitable for Diffusion Models ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02036",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02061",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02075",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02151",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02196",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02201",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02204",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02256",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Reinforcement Learning for Option Hedging: Static Implied-Volatility Fit versus Shortfall-Aware Performance ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01709",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 17'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01129",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] UltraEval-Audio: A Unified Framework for Comprehensive Evaluation of Audio Foundation Models ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01373",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Digital Twin-Driven Communication-Efficient Federated Anomaly Detection for Industrial IoT ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01701",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Accelerating Decentralized Optimization via Overlapping Local Steps ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01493",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Accelerated Full Waveform Inversion by Deep Compressed Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01268",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01513",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Efficient Cover Construction for Ball Mapper via Accelerated Range Queries ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01405",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Generating Diverse TSP Tours via a Combination of Graph Pointer Network and Dispersion ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01132",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Promptable Foundation Models for SAR Remote Sensing: Adapting the Segment Anything Model for Snow Avalanche Segmentation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01213",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Adaptive Hybrid Optimizer based Framework for Lumpy Skin Disease Identification ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01807",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Yukthi Opus: A Multi-Chain Hybrid Metaheuristic for Large-Scale NP-Hard Optimization ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01832",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02071",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02213",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02253",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Physically-Constrained Autoencoder-Assisted Bayesian Optimization for Refinement of High-Dimensional Defect-Sensitive Single Crystalline Structure ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.00855",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Learning Relationship between Quantum Walks and Underdamped Langevin Dynamics ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.01589",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv260106] Predicting Early and Complete Drug Release from Long-Acting Injectables Using Explainable Machine Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2601.02265",children:"link"})]}),"\n"]})]})}function d(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>o});var r=n(6540);const s={},t=r.createContext(s);function a(e){const i=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(t.Provider,{value:i},e.children)}}}]);