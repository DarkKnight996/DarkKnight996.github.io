"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[546],{3086:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251124-20251130","title":"20251124-20251130","description":"2025-11-24","source":"@site/docs/daily/20251124-20251130.md","sourceDirName":"daily","slug":"/daily/20251124-20251130","permalink":"/daily/20251124-20251130","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1763952360000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251117-20251123","permalink":"/daily/20251117-20251123"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=n(4848),t=n(8453);const a={},l="20251124-20251130",o={},c=[{value:"2025-11-24",id:"2025-11-24",level:2}];function h(e){const i={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"20251124-20251130",children:"20251124-20251130"})}),"\n",(0,s.jsx)(i.h2,{id:"2025-11-24",children:"2025-11-24"}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"cs.DC total: 4"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251124] Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [sys], [cloud computing], [Stochastic Reward Nets, anomaly detection, precision, recall, inspection frequency]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Gabriel Job Antunes Grabher, Fumio Machida, Thomas Ropars"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Universit\xe9 Grenoble-Alpes, University of Tsukuba"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17119",children:"https://arxiv.org/pdf/2511.17119"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper uses Stochastic Reward Nets to model cloud services with performance anomaly detection. The study analyzes how detector characteristics like precision, recall, and inspection frequency affect latency and resource consumption. The main finding is that high precision alone suffices for good performance-cost trade-off with frequent detection, while recall becomes more important with infrequent detection."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251124] Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [GPU kernels], [multi-agent systems, LLM-based optimization, PyTorch inference, explore-exploit tradeoff, error-fixing agents]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Kirill Nagaitsev, Luka Grbcic, Samuel Williams, Costin Iancu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Northwestern University, Lawrence Berkeley National Laboratory"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.16964",children:"https://arxiv.org/pdf/2511.16964"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes using LLM-based multi-agent systems to optimize PyTorch inference performance on GPUs. The research shows that exploit-heavy strategies combined with error-fixing agents achieve the best results, with the optimal implementation delivering an average 2.88x speedup on H100 GPUs across diverse machine learning tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251124] MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm training], [token scheduling, expert parallelism, micro-batch optimization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Chenqi Zhao, Wenfei Wu, Linhai Song, Yuchen Xu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Peking University, Institute of Computing Technology, Chinese Academy of Sciences"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.16947",children:"https://arxiv.org/pdf/2511.16947"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," The paper proposes MicroEP, a parallelization strategy that achieves fine-grained load balancing in Mixture-of-Experts systems through token scheduling across GPUs. Experimental results show that their MicroMoE system improves training throughput by up to 47.6% compared to state-of-the-art systems while maintaining optimal load balance among GPUs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv251124] Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [llm training], [mixture-of-experts, transformer, context parallelism, compressed convolutional attention, fault-tolerance, checkpoint-reshaping, microbenchmarks]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Quentin Anthony, Yury Tokpanov, Skyler Szot, Srivatsan Rajagopal, Praneeth Medepalli, Rishi Iyer, Vasu Shyam, Anna Golubeva, Ansh Chaurasia, Xiao Yang, Tomas Figliolia, Robert Washbourne, Drew Thorstensen, Amartey Pearson, Zack Grossbart, Jason van Patten, Emad Barsoum, Zhenyu Gu, Yao Fu, Beren Millidge"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Zyphra, IBM, AMD"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17127",children:"https://arxiv.org/pdf/2511.17127"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents the first large-scale mixture-of-experts pretraining study on pure AMD hardware using MI300X GPUs with Pollara interconnect. The authors developed ZAYA1-base model with optimized transformer sizing rules and system design including fault-tolerance mechanisms and detailed training recipes. The results demonstrate that AMD's hardware, networking, and software stack are mature enough for competitive large-scale pretraining, achieving performance comparable to leading base models."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 11'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17435",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] CroTad: A Contrastive Reinforcement Learning Framework for Online Trajectory Anomaly Detection ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.16929",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Dissecting Quantum Reinforcement Learning: A Systematic Evaluation of Key Components ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17112",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.16916",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Convergence and stability of Q-learning in Hierarchical Reinforcement Learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17351",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Predicting Talent Breakout Rate using Twitter and TV data ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.16905",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] MIR: Efficient Exploration in Episodic Multi-Agent Reinforcement Learning via Mutual Intrinsic Reward ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17165",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17171",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17367",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Intrinsic preservation of plasticity in continual quantum learning ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17228",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17489",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 8'})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17123",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] BITS for GAPS: Bayesian Information-Theoretic Sampling for hierarchical GAussian Process Surrogates ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.16815",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Generating transition states of chemical reactions via distance-geometry-based flow matching ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17229",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Stable Coresets via Posterior Sampling: Aligning Induced and Full Loss Landscapes ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17399",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Mesh RAG: Retrieval Augmentation for Autoregressive Mesh Generation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.16807",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.16757",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.16681",children:"link"})]}),"\n",(0,s.jsxs)(i.li,{children:["[arXiv251124] Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions in Image Generation ",(0,s.jsx)(i.a,{href:"https://arxiv.org/pdf/2511.17031",children:"link"})]}),"\n"]})]})}function d(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>l});var r=n(6540);const s={},t=r.createContext(s);function a(e){const i=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(t.Provider,{value:i},e.children)}}}]);