"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[369],{7238:(n,i,e)=>{e.r(i),e.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251020-20251026","title":"20251020-20251026","description":"2025-10-21","source":"@site/docs/daily/20251020-20251026.md","sourceDirName":"daily","slug":"/daily/20251020-20251026","permalink":"/daily/20251020-20251026","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1761311518000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251013-20251019","permalink":"/daily/20251013-20251019"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=e(4848),t=e(8453);const a={},o=void 0,l={},c=[{value:"2025-10-21",id:"2025-10-21",level:2},{value:"2025-10-22",id:"2025-10-22",level:2},{value:"2025-10-23",id:"2025-10-23",level:2}];function h(n){const i={a:"a",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.h2,{id:"2025-10-21",children:"2025-10-21"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long\nContext Training"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM training], [distributed training, dynamic sparse attention, ultra-long context, computational efficiency]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Microsoft Research"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.18830v1",children:"http://arxiv.org/pdf/2510.18830v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," MTraining introduces a distributed methodology using dynamic sparse attention with three key components to enable efficient LLM training with ultra-long contexts. It successfully expanded Qwen2.5-3B's context window from 32K to 512K tokens on 32 A100 GPUs. The approach achieves up to 6x higher training throughput while maintaining model accuracy across various benchmarks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [scheduling], [LLM inference, edge computing, SLO-driven scheduling, dynamic rate control, utility maximization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Pan Zhou, Yiming Lei, Ling Liu, Xiaoqiong Xu, Ying Cai, Daji Ergu, Hongfang Yu, Yueyue Dai"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Southwest Minzu University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.18544v1",children:"http://arxiv.org/pdf/2510.18544v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," SLICE proposes an SLO-driven scheduling framework combining utility-maximizing request scheduling with dynamic iterative generation rate control for LLM inference on edge devices. It significantly improves SLO attainment compared to state-of-the-art systems like Orca and FastServe. Experimental results show up to 35x higher SLO attainment and 3.4x faster task completion times."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] Comparative analysis of large data processing in Apache Spark using\nJava, Python and Scala"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models inference], [Apache Spark, programming languages, performance comparison, data processing, ETL workflows, Apache Iceberg]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Ivan Borodii, Illia Fedorovych, Halyna Osukhivska, Diana Velychko, Roman Butsii"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Ternopil Ivan Puluj National Technical University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.19012v1",children:"http://arxiv.org/pdf/2510.19012v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This study compares Apache Spark's performance across Java, Python and Scala by executing ETL operations including data loading from CSV files, transformation and loading into Apache Iceberg tables. Results show Python performs best with small datasets (5MB), while Scala excels in complex operations combining multiple files. The programming language choice significantly impacts Spark processing efficiency, with optimal selection depending on data size and operation complexity."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] A Distributed Framework for Causal Modeling of Performance Variability\nin GPU Traces"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [trace analysis], [GPU traces, performance variability, causal modeling, distributed framework, HPC]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Ankur Lahiry, Ayush Pokharel, Banooqa Banday, Seth Ockerman, Amal Gueroudji, Mohammad Zaeed, Tanzima Z. Islam, Line Pouchard"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Texas State University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.18300v1",children:"http://arxiv.org/pdf/2510.18300v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents a distributed framework that partitions and processes GPU trace data concurrently using causal graph methods and parallel coordinating charts. The approach exposes performance variability and dependencies across execution flows in HPC systems. Experimental results show a 67% improvement in scalability for analyzing multiple traces independently."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] Tokencake: A KV-Cache-centric Serving Framework for LLM-based\nMulti-Agent Applications"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [KV-Cache optimization, multi-agent systems, GPU memory management, scheduling algorithms]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Zhuohang Bian, Feiyang Wu, Teng Ma, Youwei Zhuo"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Beihang University, Peking University, Alibaba"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.18586v1",children:"http://arxiv.org/pdf/2510.18586v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," Tokencake introduces a KV-Cache-centric serving framework that co-optimizes scheduling and memory management for LLM-based multi-agent applications. It uses dynamic memory partitioning and proactive offload mechanisms to handle space contention and time underutilization. Evaluation shows it reduces latency by over 47.06% and improves GPU memory utilization by up to 16.9% compared to vLLM."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"2025-10-22",children:"2025-10-22"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] Serverless GPU Architecture for Enterprise HR Analytics: A\nProduction-Scale BDaaS Implementation"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models inference], [serverless GPU, TabNet, enterprise analytics, compliance, cost efficiency, interpretability]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Guilin Zhang, Wulan Guo, Ziqi Tan, Srinivas Vippagunta, Suchitra Raman, Shreeshankar Chatterjee, Ju Lin, Shang Liu, Mary Schladenhauffen, Jeffrey Luo, Hailong Jiang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," George Washington University and Workday, Inc."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.19689v1",children:"http://arxiv.org/pdf/2510.19689v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents a serverless GPU architecture integrating TabNet for enterprise HR analytics, achieving significant improvements in throughput and latency while maintaining compliance. The design combines GPU acceleration with serverless elasticity to reduce costs and ensure interpretability. Results show 4.5x higher throughput and 90% lower cost per inference compared to Spark baselines, with minimal compliance overhead."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] Enabling Reconfiguration-Communication Overlap for Collective\nCommunication in Optical Networks"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [optical networks, collective communication, network reconfiguration, distributed machine learning]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Changbo Wu, Zhuolong Yu, Gongming Zhao, Hongli Xu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of Science and Technology of China"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.19322v1",children:"http://arxiv.org/pdf/2510.19322v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," SWOT introduces a demand-aware optical network framework that enables intra-collective reconfiguration and overlaps optical switch reconfigurations with ongoing transmissions. This approach dynamically aligns network resources with collective communication traffic patterns in distributed machine learning. Simulation results demonstrate significant performance improvements over traditional one-shot reconfiguration strategies."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via\nHybrid Expert/Data Transmission"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM training], [mixture-of-experts, expert parallelism, cross-datacenter training, communication optimization, hybrid transmission]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Weihao Yang, Hao Huang, Donglei Wu, Ningke Li, Yanqi Pan, Qiyang Zheng, Wen Xia, Shiyi Li, Qiang Wang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Harbin Institute of Technology, Shenzhen"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.19470v1",children:"http://arxiv.org/pdf/2510.19470v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," HybridEP proposes a framework that dynamically transforms expert placement and uses hybrid expert/data transmission to reduce communication overhead in cross-datacenter MoE training. It employs domain-based partitioning and parameter-efficient migration guided by a stream-based model. Experimental results show HybridEP achieves up to 5.6x speedup over existing systems under constrained bandwidth."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] RLBoost: Harvesting Preemptible Resources for Cost-Efficient\nReinforcement Learning on LLMs"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM training], [reinforcement learning, preemptible resources, cost efficiency, rollout optimization, cluster scheduling, resource utilization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Yongji Wu, Xueshen Liu, Haizhong Zheng, Juncheng Gu, Beidi Chen, Z. Morley Mao, Arvind Krishnamurthy, Ion Stoica"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," UC Berkeley, Google, CMU, University of Michigan"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.19225v1",children:"http://arxiv.org/pdf/2510.19225v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," RLBoost introduces a hybrid architecture that efficiently utilizes preemptible GPU resources for RL training on LLMs through adaptive rollout offload, pull-based weight transfer, and token-level response migration. This approach significantly improves training throughput by 1.51x-1.97x while reducing costs by 28%-49% compared to using only on-demand resources. The system effectively addresses resource under-utilization in RL workflows by leveraging the stateless nature of rollout stages."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework\nFor Controllable Lyric Translation"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models training], [lyric translation, curriculum learning, reinforcement learning, unsupervised learning, controllable translation]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Le Ren, Xiangjian Zeng, Qingqiang Wu, Ruoxuan Liang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Xiamen University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.19967v1",children:"http://arxiv.org/pdf/2510.19967v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," LyriCAR proposes a difficulty-aware curriculum reinforcement learning framework for controllable lyric translation, using adaptive curriculum strategies to guide training with progressively complex challenges. The method achieves state-of-the-art performance in EN-ZH lyric translation while reducing training steps by nearly 40%. Experimental results demonstrate superior performance across both standard translation metrics and multi-dimensional reward scores."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] Next-Generation Event-Driven Architectures: Performance, Scalability,\nand Intelligent Orchestration Across Messaging Frameworks"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [scheduling], [event-driven architectures, messaging frameworks, AI-enhanced orchestration, performance benchmarking, reinforcement learning, predictive scaling]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Jahidul Arafat, Fariha Tasmin, Sanjaya Poudel"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Auburn University"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.04404v2",children:"http://arxiv.org/pdf/2510.04404v2"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces AIEO, an AI-enhanced event orchestration system using machine learning for predictive scaling and reinforcement learning for dynamic resource allocation. It comprehensively benchmarks 12 messaging frameworks across different workloads and demonstrates significant improvements in latency, resource utilization, and cost optimization. The research provides standardized benchmarking methodologies and intelligent orchestration solutions for next-generation distributed systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] AdaSPEC: Selective Knowledge Distillation for Efficient Speculative\nDecoders"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [speculative decoding, knowledge distillation, token filtering, model alignment, efficiency optimization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Yuezhou Hu, Jiaxin Guo, Xinyu Feng, Tuo Zhao"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," University of California, Berkeley, Tsinghua University, Georgia Institute of Technology"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.19779v1",children:"http://arxiv.org/pdf/2510.19779v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," AdaSPEC introduces selective token filtering during knowledge distillation to improve speculative decoding efficiency. By filtering out difficult-to-fit tokens using a reference model, it enhances draft-target model alignment on simpler tokens. The method achieves up to 15% higher token acceptance rates than DistillSpec while maintaining generation quality across various tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] Serverless GPU Architecture for Enterprise HR Analytics: A\nProduction-Scale BDaaS Implementation"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models inference], [serverless GPU, TabNet, enterprise analytics, compliance, HR analytics, BDaaS, interpretability]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Guilin Zhang, Wulan Guo, Ziqi Tan, Srinivas Vippagunta, Suchitra Raman, Shreeshankar Chatterjee, Ju Lin, Shang Liu, Mary Schladenhauffen, Jeffrey Luo, Hailong Jiang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," George Washington University and Workday, Inc."]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.19689v1",children:"http://arxiv.org/pdf/2510.19689v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents a serverless GPU architecture integrated with TabNet for enterprise HR analytics, achieving significant improvements in throughput, latency, and cost compared to Spark baselines. The design ensures compliance through feature-mask interpretability while maintaining performance under load. The implementation provides a practical blueprint for secure and efficient analytics in regulated environments."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"2025-10-23",children:"2025-10-23"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for\nScalable LLM Training"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM training], [ZeRO parallelism, asynchronous scheduling, hierarchical sharding, communication optimization, memory efficiency]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Huawei Bai, Yifan Huang, Wenqi Shi, Ansheng You, Feifan Shao, Tengfei Han, Minghui Yu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Unable to determine from provided information"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.20111v1",children:"http://arxiv.org/pdf/2510.20111v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," AsyncHZP introduces hierarchical parameter sharding and multi-stream asynchronous scheduling to optimize memory usage and overlap communication with computation. It achieves superior performance over traditional parallelism methods while maintaining training stability. The approach demonstrates effectiveness across both dense and MoE models without requiring complex tuning."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] Collective Communication for 100k+ GPUs"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM training], [collective communication, NCCLX framework, large-scale GPU clusters, communication efficiency]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Min Si, Pavan Balaji, Yongzhou Chen, Ching-Hsiang Chu, Adi Gangidi, Saif Hasan, Subodh Iyengar, Dan Johnson, Bingzhe Liu, Jingliang Ren, Ashmitha Jeevaraj Shetty, Greg Steinbrecher, Xinfeng Xie, Yulun Wang, Bruce Wu, Jingyi Yang, Mingran Yang, Minlan Yu, Cen Zhao, Wes Bland, Denis Boyda, Suman Gumudavelli, Cristian Lumezanu, Rui Miao, Zhe Qu, Venkat Ramesh, Maxim Samoylov, Jan Seidel, Feng Tian, Qiye Tan, Shuqiang Zhang, Yimeng Zhao, Shengbao Zheng, Art Zhu, Hongyi Zeng"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Meta"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.20171v1",children:"http://arxiv.org/pdf/2510.20171v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper presents the NCCLX collective communication framework developed by Meta to optimize performance for LLM workloads across 100,000+ GPUs. The framework addresses throughput and latency limitations in traditional communication methods at extreme scales. Empirical evaluation on the Llama4 model demonstrates substantial improvements in communication efficiency for large-scale training and inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] Black Box Absorption: LLMs Undermining Innovative Ideas"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [black box absorption, innovation economics, idea safety, governance, engineering agenda]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Wenjun Cao"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Unable to determine from provided information"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.20612v1",children:"http://arxiv.org/pdf/2510.20612v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper identifies and formalizes the concept of Black Box Absorption, where LLMs internalize user-contributed innovations through opaque architectures. It introduces idea units and idea safety frameworks to analyze this phenomenon. The authors propose governance and engineering solutions to protect creator contributions and ensure equitable innovation ecosystems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [kernels], [GPU hash tables, concurrent data structures, performance optimization, benchmarking framework, lock-free queries]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Hunter McCoy, Prashant Pandey"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Nvidia"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2509.16407v2",children:"http://arxiv.org/pdf/2509.16407v2"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," WarpSpeed implements eight state-of-the-art GPU hash table designs with optimization techniques including fingerprint-based metadata and specialized GPU instructions. The library provides a unified benchmarking framework and demonstrates real-world impact through integration into downstream applications. The research offers new insights into concurrent GPU hash table design and practical guidance for developing efficient data structures on modern GPUs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] Symbolic Regression and Differentiable Fits in Beyond the Standard Model\nPhysics"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models inference], [symbolic regression, particle physics, supersymmetry, differentiable methods, neural networks]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Shehu AbdusSalam, Steven Abel, Deaglan Bartlett, Miguel Crispim Rom\xe3o"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Physics research institution (likely CERN or university physics department)"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.20453v1",children:"http://arxiv.org/pdf/2510.20453v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper applies symbolic regression to analyze Beyond Standard Model physics, specifically the Constrained Minimal Supersymmetric Standard Model. The method generates accurate symbolic expressions for physical observables like Higgs mass and dark matter density. Results show symbolic regression produces robust global fits comparable to conventional methods and outperforms neural networks when data isn't focused on promising regions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] Morpheus: Lightweight RTT Prediction for Performance-Aware Load\nBalancing"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [RTT prediction, load balancing, Kubernetes, performance optimization, resource management]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Panagiotis Giannakopoulos, Bart van Knippenberg, Kishor Chandra Joshi, Nicola Calabretta, George Exarchakos"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Based on the content mentioning Kubernetes-managed GPU cluster and performance optimization in edge/cloud environments, likely from institutions like Google, Microsoft Research, or academic institutions with strong systems research groups such as UC Berkeley, MIT, or Stanford"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.20506v1",children:"http://arxiv.org/pdf/2510.20506v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper develops lightweight RTT predictors using time-series monitoring data from Kubernetes clusters to enable performance-aware load balancing. The predictors achieve 95% accuracy with minimal overhead by leveraging highly correlated metrics. Results demonstrate significant reduction in application latency and resource waste through predictive routing decisions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] FLAS: a combination of proactive and reactive auto-scaling architecture\nfor distributed services"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [auto-scaling, cloud computing, distributed systems, performance optimization, SLA compliance]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," V\xedctor Ramp\xe9rez, Javier Soriano, David Lizcano, Juan A. Lara"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Based on the technical content about distributed systems and cloud infrastructure, likely from a computer science department at a research university or cloud computing research lab (specific institution cannot be determined from provided text)"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.20388v1",children:"http://arxiv.org/pdf/2510.20388v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," FLAS introduces a hybrid auto-scaling architecture combining proactive forecasting of high-level metrics with reactive contingency mechanisms for distributed services. The system uses predictive modeling to anticipate SLA parameter changes and resource metric estimation to reduce instrumentation needs. Evaluation shows the approach maintains performance requirements over 99% of the time across various test scenarios including worst-case conditions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] GPU-Accelerated Primal Heuristics for Mixed Integer Programming"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [kernels], [GPU acceleration, mixed integer programming, primal heuristics, feasibility pump, feasibility jump, fix-and-propagate]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Akif \xc7\xf6rd\xfck, Piotr Sielski, Alice Boucher, Kumar Aatish"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Based on the technical focus on GPU-accelerated optimization algorithms, likely from institutions like NVIDIA, MIT, or ETH Zurich (cannot determine precisely without author information)"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.20499v1",children:"http://arxiv.org/pdf/2510.20499v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces GPU-accelerated primal heuristics for mixed integer programming, combining techniques like Feasibility Pump and Fix-and-Propagate with GPU-parallelized components. The approach uses a GPU-accelerated PDLP as an approximate LP solver and a probing cache for faster operations. Experimental results show significant improvements in solution feasibility and objective quality on MIPLIB2017 benchmarks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] RailS: Load Balancing for All-to-All Communication in Distributed\nMixture-of-Experts Training"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM training], [load balancing, all-to-all communication, Mixture-of-Experts, distributed training, network topology]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Heng Xu, Zhiwei Yu, Chengze Du, Ying Zhou, Letian Li, Haojie Wang, Weiqiang Cheng, Jialong Li"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Based on the technical focus on distributed MoE training and network optimization, likely from institutions like Meta AI, Google Research, or academic labs specializing in distributed systems (e.g., Stanford, MIT, CMU)"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.19262v2",children:"http://arxiv.org/pdf/2510.19262v2"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," RailS introduces a distributed load-balancing framework that leverages Rail topology symmetry and local LPT scheduling to optimize all-to-all communication in MoE training. It activates parallel rails for multipath transmission and topology-aware traffic distribution. The method improves bandwidth utilization by 20%-78% and reduces iteration time by 17%-78% in MoE workloads like Mixtral."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] Transferable Graph Learning for Transmission Congestion Management via\nBusbar Splitting"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models inference], [graph neural networks, power systems, topology optimization, congestion management, transfer learning]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Ali Rajaei, Peter Palensky, Jochen L. Cremer"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Based on the technical content and application domain, likely from power engineering and computer science research groups at institutions like ETH Zurich, MIT, or technical universities with strong power systems and machine learning programs"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.20591v1",children:"http://arxiv.org/pdf/2510.20591v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper proposes a graph neural network approach for network topology optimization via busbar splitting to manage transmission congestion. The method uses heterogeneous edge-aware message passing to predict effective splitting actions, achieving significant speed-up while maintaining solution quality. Results demonstrate AC-feasible solutions within one minute on large-scale systems with improved transferability across different grid topologies."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] FlashMP: Fast Discrete Transform-Based Solver for Preconditioning\nMaxwell's Equations on GPUs"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [kernels], [GPU computing, preconditioning, Maxwell's equations, discrete transforms, domain decomposition, electromagnetic simulations]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Haoyuan Zhang, Yaqian Gao, Xinxin Zhang, Jialin Li, Runfeng Jin, Yidong Chen, Feng Zhang, Wu Yuan, Wenpeng Ma, Shan Liang, Jian Zhang, Zhonghua Lu"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Based on the technical content and GPU focus, likely from a research institution with HPC expertise such as a national laboratory or university computing department"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2508.07193v2",children:"http://arxiv.org/pdf/2508.07193v2"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," FlashMP introduces a novel GPU-accelerated preconditioning system using discrete transforms for solving Maxwell's equations, achieving significant speedups over existing methods. The approach employs domain decomposition for multi-GPU scalability and demonstrates up to 16x reduction in iteration counts and 2.5-4.9x speedups compared to state-of-the-art libraries. Weak scalability tests show high parallel efficiency of 84.1% on large GPU clusters."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] Improving Model Representation and Reducing KV Cache via Skip\nConnections with First Value Heads"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [LLM inference], [KV cache reduction, skip connections, model representation, transformer optimization, mesa-optimization]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Zhoutong Wu, Yuan Zhang, Yiming Dong, Chenheng Zhang, Cong Fang, Kun Yuan, Zhouchen Lin"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Unknown (insufficient information to determine)"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.16807v2",children:"http://arxiv.org/pdf/2510.16807v2"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," SkipV1Former introduces skip connections from the first layer's Value heads to deeper layers, reusing half of them to reduce KV cache by approximately 25% while improving perplexity. This method enhances model representation by restoring compressed information and accelerates implicit mesa-optimization in auto-regressive tasks. It also enables efficient uptraining of existing models and combines with other attention methods for further performance gains and KV cache savings."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] Competition is the key: A Game Theoretic Causal Discovery Approach"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models training], [causal discovery, reinforcement learning, game theory, finite-sample guarantees, DDQN, GES, GraN-DAG]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Amartya Roy, Souvik Chakraborty"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Based on the technical content and methodology, likely from a machine learning research group at institutions like MIT, Stanford, CMU, or Google Research"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.20106v1",children:"http://arxiv.org/pdf/2510.20106v1"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper introduces a game-theoretic reinforcement learning framework for causal discovery where a DDQN agent competes against baseline methods. The approach provides provable guarantees including never performing worse than opponents and accelerated convergence. Experimental results show improved performance over existing methods while maintaining theoretical safety and scalability to large graphs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:["\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.strong,{children:"[arXiv2510] A Survey on Cache Methods in Diffusion Models: Toward Efficient\nMulti-Modal Generation"})}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"tags:"})," [mlsys], [Other models inference], [diffusion models, caching methods, efficient inference, multi-modal generation, computational redundancy, training-free acceleration]"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"authors:"})," Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"institution:"})," Based on the technical focus on diffusion models and efficient inference methods, likely from AI research institutions such as Google Research, Meta AI, or academic institutions like Stanford/CMU (specific institution cannot be determined from provided content)"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"link:"})," ",(0,s.jsx)(i.a,{href:"http://arxiv.org/pdf/2510.19755v2",children:"http://arxiv.org/pdf/2510.19755v2"})]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Simple LLM Summary:"})," This paper surveys Diffusion Caching methods that identify and reuse computational redundancies in diffusion models through feature-level cross-step reuse and inter-layer scheduling. The approach evolves from static reuse to dynamic prediction, enabling training-free acceleration without quality degradation. This paradigm provides an efficient inference framework for real-time multimodal applications while maintaining compatibility with other acceleration techniques."]}),"\n"]}),"\n"]}),"\n"]})]})}function d(n={}){const{wrapper:i}={...(0,t.R)(),...n.components};return i?(0,s.jsx)(i,{...n,children:(0,s.jsx)(h,{...n})}):h(n)}},8453:(n,i,e)=>{e.d(i,{R:()=>a,x:()=>o});var r=e(6540);const s={},t=r.createContext(s);function a(n){const i=r.useContext(t);return r.useMemo(function(){return"function"==typeof n?n(i):{...i,...n}},[i,n])}function o(n){let i;return i=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),r.createElement(t.Provider,{value:i},n.children)}}}]);