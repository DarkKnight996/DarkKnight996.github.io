"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[556],{1405:(i,e,n)=>{n.r(e),n.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20260209-20260215","title":"20260209-20260215","description":"2026-02-09","source":"@site/docs/daily/20260209-20260215.md","sourceDirName":"daily","slug":"/daily/20260209-20260215","permalink":"/daily/20260209-20260215","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1770608194000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20260202-20260208","permalink":"/daily/20260202-20260208"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=n(4848),a=n(8453);const t={},o="20260209-20260215",l={},c=[{value:"2026-02-09",id:"2026-02-09",level:2}];function h(i){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...i.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20260209-20260215",children:"20260209-20260215"})}),"\n",(0,s.jsx)(e.h2,{id:"2026-02-09",children:"2026-02-09"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 21"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] Computationally Efficient Laplacian CL-colME"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [decentralized learning, collaborative mean estimation, consensus algorithms, Laplacian-based consensus, graph-based methods]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nikola Stankovic"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," IEEE"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06070",children:"https://arxiv.org/pdf/2602.06070"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes CL-colME, a Laplacian-based consensus variant of collaborative mean estimation that avoids computationally expensive normalization processes. The method maintains the convergence and accuracy of the previous C-colME framework while improving computational efficiency in decentralized, heterogeneous networks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] Experimental Analysis of Server-Side Caching for Web Performance"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [web performance optimization], [server-side caching, in-memory cache, response time, time-to-live, performance evaluation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Mohammad Umar, Bharat Tripathi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Allenhouse Business School"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06074",children:"https://arxiv.org/pdf/2602.06074"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper experimentally compares the performance of a web server with and without simple in-memory caching using a fixed time-to-live. The results show that server-side caching significantly reduces response times, demonstrating its effectiveness for small-scale applications and educational use cases."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [kernel-level optimization, load-balanced execution groups, I/O-aware grouping, KV cache reorganization, FlashAttention]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Rui Ning, Wei Zhang, Fan Lai"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nanjing University, University of Illinois Urbana-Champaign"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06072",children:"https://arxiv.org/pdf/2602.06072"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," PackInfer is a kernel-level attention framework that improves batched LLM inference by packing heterogeneous requests into load-balanced execution groups and reorganizing KV caches into group-contiguous layouts. This approach reduces computation and I/O imbalance, leading to reduced latency and increased throughput compared to FlashAttention."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [reinforcement learning, iterative scheduling, Markov decision process, resource investment problem, decomposition, reconfiguration]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yi-Xiang Hu, Yuke Wang, Feng Wu, Zirui Huang, Shuli Zeng, Xiang-Yang Li"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Science and Technology of China"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06064",children:"https://arxiv.org/pdf/2602.06064"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces iScheduler, a framework that uses reinforcement learning to iteratively schedule tasks by decomposing large resource investment problems into subproblems modeled as a Markov decision process. It achieves fast schedule generation and supports efficient updates by reusing unaffected schedules. Experiments show it reduces time to feasibility by up to 43x while maintaining competitive resource costs compared to commercial solvers."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [GPU kernels], [BlockPerm-SJLT, sketch-kernel co-design, sparse sketching, oblivious subspace embedding, CUDA kernel, memory bandwidth]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Rajat Vadiraj Dwaraknath, Sungyoon Kim, Mert Pilanci"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Stanford University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06071",children:"https://arxiv.org/pdf/2602.06071"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces FlashSketch, a method that co-designs a new sparse sketch family (BlockPerm-SJLT) with an optimized CUDA kernel to improve GPU efficiency. This approach explicitly trades off sketching robustness with hardware performance. The result is a significant speedup (1.7x geomean) over prior state-of-the-art GPU sketches across various benchmarks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [scaling laws, heterogeneous orchestration, hardware-aware routing, progressive sample multiplexing, cost models, Intelligence Per Watt, Energy-Coverage Efficiency, Price-Power-Performance]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Satyam Kumar, Saurabh Jha"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Not explicitly provided in the given text."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06057",children:"https://arxiv.org/pdf/2602.06057"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces QEIL, a framework that uses inference-time scaling laws and heterogeneous hardware orchestration (across CPU, GPU, NPU) to optimize LLM inference on edge devices. It demonstrates that this approach achieves significant improvements in coverage, energy efficiency, latency, and cost compared to homogeneous methods, establishing it as an optimal strategy for energy-constrained edge AI systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [post-training], [hybrid quantization, structural pruning, Fisher Information Matrix, sensitivity-aware pruning, post-training quantization, model compression]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Dinesh Gopalan, Ratul Ali"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," AMD, Jahangirnagar University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06069",children:"https://arxiv.org/pdf/2602.06069"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces HQP, a sensitivity-aware hybrid framework that combines structural pruning using a dynamic weight sensitivity metric from an efficient Fisher Information Matrix approximation, followed by conditional 8-bit post-training quantization. It achieves up to 3.12\xd7 inference speedup and 55% model size reduction while limiting accuracy drop to under 1.5% on edge platforms like NVIDIA Jetson. The method outperforms single-objective compression techniques, providing a hardware-agnostic solution for ultra-low-latency edge AI inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [matrix-based optimizers, data parallelism, tensor parallelism, asynchronous compute, load balancing, Shampoo, Muon, SOAP, ZeRO]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Liangyu Wang, Siqi Zhang, Junjie Wang, Yiming Dong, Bo Zheng, Zihan Qiu, Shengkun Tang, Di Wang, Rui Men, Dayiheng Liu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," KAUST, Alibaba Group, Peking University, MBZUAI"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06079",children:"https://arxiv.org/pdf/2602.06079"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes Canzona, a framework that decouples logical optimizer assignment from physical parameter distribution to efficiently run matrix-based optimizers (like Shampoo) in distributed LLM training. It introduces load-balanced partitioning for Data Parallelism and an asynchronous compute pipeline for Tensor Parallelism. Evaluations show it preserves parallel architecture efficiency, achieving a 1.57x end-to-end speedup and reducing optimizer step latency by 5.8x."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] Mapping Gemma3 onto an Edge Dataflow Architecture"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [dequantization engine, tiled matrix multiplication, FlowQKV, FusedDQP, FlowKV, Q4NX quantization, dataflow architecture]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Shouyu Du, Miaoxiang Yu, Zhiheng Ni, Jillian Cai, Qing Yang, Tao Wei, Zhenyu Xu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Clemson University, University of Rhode Island"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06063",children:"https://arxiv.org/pdf/2602.06063"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents the first end-to-end deployment of the Gemma3 family of models on an AMD Ryzen AI NPU, introducing hardware-aware techniques like FlowQKV for prefill and FusedDQP for decoding, along with a custom 4-bit quantization format. The methods achieve significant speed and power efficiency improvements over iGPU and CPU baselines. The work demonstrates that modern NPUs can enable practical, low-power LLM and VLM inference at the edge and provides a blueprint for mapping transformers onto tiled dataflow accelerators."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [memory-centric benchmark, mobile GUI agents, LLM-as-judge, pass@k, cross-session learning, cross-temporal retention, cross-spatial retention, Progressive Scrutiny]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Guangyi Liu, Pengxiang Zhao, Yaozhen Liang, Qinyi Luo, Shunye Tang, Yuxiang Chai, Weifeng Lin, Han Xiao, WenHao Wang, Siheng Chen, Zhengxi Lu, Gao Wu, Hao Wang, Liang Liu, Yong Liu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Zhejiang University, Nankai University, The Chinese University of Hong Kong, Shanghai Jiao Tong University, vivo AI Lab"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06075",children:"https://arxiv.org/pdf/2602.06075"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces MemGUI-Bench, a comprehensive benchmark and evaluation pipeline designed to assess the memory capabilities of mobile GUI agents. The method includes a memory taxonomy, 128 memory-challenging tasks, and an automated evaluator (MemGUI-Eval) using staged LLM-as-judge and Progressive Scrutiny. The main conclusion is that current state-of-the-art agents exhibit significant memory deficits, with the benchmark identifying five distinct failure modes and synthesizing actionable design implications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] LAAFD: LLM-based Agents for Accelerated FPGA Design"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [high-level synthesis, FPGA, agentic workflow, pipelining, vectorization, dataflow partitioning, co-simulation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Maxim Moraru, Kamalavasan Kamalakkannan, Jered Dominguez-Trujillo, Patrick Diehl, Atanu Barai, Julien Loiseau, Zachary Kent Baker, Howard Pritchard, Galen M Shipman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Los Alamos National Laboratory"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06085",children:"https://arxiv.org/pdf/2602.06085"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces LAAFD, an agentic workflow that uses large language models to automatically translate general-purpose C++ code into optimized FPGA kernels for Vitis HLS. The system leverages HLS co-simulation and synthesis feedback to iteratively apply hardware optimizations like pipelining and dataflow. The results show that LAAFD achieves performance comparable to hand-tuned baselines, significantly lowering the expertise barrier for FPGA acceleration."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] BouquetFL: Emulating diverse participant hardware in Federated Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, hardware emulation, resource restriction, GPU constraints, CPU throttling, memory constraints, heterogeneous clients]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Arno Geimer"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Not specified (author email domain not provided)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06498",children:"https://arxiv.org/pdf/2602.06498"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," BouquetFL is a framework that simulates heterogeneous client hardware in federated learning by programmatically restricting CPU, memory, and GPU resources on a single machine. It enables controlled experimentation under realistic hardware diversity without requiring multiple physical devices. The tool addresses a methodological gap in FL research by providing an accessible way to study system heterogeneity, bringing experimental practice closer to practical deployment conditions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] AdFL: In-Browser Federated Learning for Online Advertisement"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [federated learning, differential privacy, in-browser training, ad viewability prediction]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ahmad Alemari, Pritam Sen, Cristian Borcea"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," New Jersey Institute of Technology, Jazan University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06336",children:"https://arxiv.org/pdf/2602.06336"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes AdFL, a federated learning framework that runs directly in users' web browsers to learn ad preferences without sharing raw data. It demonstrates the framework's feasibility for real-time in-browser training and shows that an ad viewability prediction model built on it achieves high performance, which is maintained even when differential privacy is applied for enhanced security."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [ZeRO-3, host memory caching, parameter-efficient fine-tuning, all-gather, data parallel]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gyeongseo Park, Eungyeong Lee, Song-woo Sok, Myung-Hoon Cha, Kwangwon Koh, Baik-Song An, Hongyeon Kim, Ki-Dong Kang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Electronics and Telecommunications Research Institute (ETRI)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06499",children:"https://arxiv.org/pdf/2602.06499"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes FCDP, a method that caches forward-pass parameters in host memory to reuse them during the backward pass, eliminating redundant inter-node communication while preserving a minimal GPU memory footprint. It achieves up to 100x higher throughput than ZeRO-3 on commodity hardware by leveraging host memory as a fast cache layer instead of an overflow tier."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] Degradation of Feature Space in Continual Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [ai], [continual learning], [isotropic regularization, contrastive learning, rehearsal strategies, catastrophic forgetting, feature space geometry]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chiara Lanza, Roberto Pereira, Marco Miozzo, Eduard Angelats, Paolo Dini"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," CTTC (Centre Tecnol\xf2gic de Telecomunicacions de Catalunya)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06586",children:"https://arxiv.org/pdf/2602.06586"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper investigates whether enforcing isotropy in the feature space can improve continual learning by mitigating catastrophic forgetting. Using contrastive continual learning techniques with rehearsal on CIFAR datasets, the authors find that isotropic regularization actually degrades model performance. The conclusion is that isotropy, beneficial in centralized training, is not a suitable inductive bias for non-stationary continual learning scenarios."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [reinforcement learning, serverless computing, autoscaling, OpenFaaS, farm skeleton, Gymnasium]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Lanpei Li, Massimo Coppola, Malio Li, Valerio Besozzi, Jack Bell, Vincenzo Lomonaco"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," National Research Council of Italy (CNR), University of Pisa, LUISS University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06555",children:"https://arxiv.org/pdf/2602.06555"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a framework using Reinforcement Learning (RL) to dynamically manage the autoscaling of a parallel task farm skeleton on a serverless platform (OpenFaaS). It evaluates RL-based policies against a reactive baseline, finding that AI-driven management better handles platform-specific limitations, improving Quality of Service (QoS) while maintaining efficient resource usage."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [dual-mapping scheduling, cache-affinity scheduling, load-balancing scheduling, KV cache reuse, SLO-aware routing, hotspot-aware rebalancing, dual-hash-ring scaling]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ying Yuan, Pengfei Zuo, Bo Wang, Zhangyu Chen, Zhipeng Tan, Zhou Yu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Huazhong University of Science and Technology, Huawei"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06502",children:"https://arxiv.org/pdf/2602.06502"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes DualMap, a dual-mapping scheduling strategy for distributed LLM serving that maps each request to two candidate instances using independent hash functions to intelligently balance KV cache reuse and load distribution. It incorporates SLO-aware routing, hotspot-aware rebalancing, and lightweight scaling to handle dynamic workloads. Experiments show DualMap improves effective request capacity by up to 2.25x under the same TTFT SLO constraints compared to state-of-the-art methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] Wonderboom -- Efficient, and Censorship-Resilient Signature Aggregation for Million Scale Consensus"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [blockchain consensus], [signature aggregation, censorship-resilient, Byzantine Fault Tolerant, quorum attestation, validator set, simulation]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zeta Avarikioti, Ray Neiheiser, Krzysztof Pietrzak, Michelle X. Yeo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," TU Wien, Institute of Science and Technology Austria, Nanyang Technological University, Aarhus University, Common Prefix"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06655",children:"https://arxiv.org/pdf/2602.06655"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces Wonderboom, a new protocol designed to efficiently aggregate signatures from millions of validators in Ethereum's consensus mechanism. It achieves this up to 32 times faster than the current state-of-the-art while providing stronger security guarantees against censorship and stake-shifting attacks. The authors implement a simulation tool to demonstrate that Wonderboom can handle over 2 million signatures within a single Ethereum slot."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] Same Engine, Multiple Gears: Parallelizing Fixpoint Iteration at Different Granularities (Extended Version)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [static analysis], [fixpoint iteration, parallelization, top-down solver, task granularity, immediate approach, independent approach, publish/subscribe]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ali Rasim Kocal, Michael Schwarz, Simmo Saan, Helmut Seidl"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Technical University of Munich, National University of Singapore, University of Tartu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06680",children:"https://arxiv.org/pdf/2602.06680"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a parallel fixpoint engine for static analysis that is parametric in task granularity, enabling it to operate at different levels of parallelism. It implements two parallelization philosophies\u2014immediate and independent\u2014within the Goblint framework. The results demonstrate the approach's effectiveness in reducing analysis times for large real-world programs."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [concurrent logic programming], [Grassroots Logic Programs, multiagent transition systems, operational semantics, peer-to-peer, deterministic semantics]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ehud Shapiro"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," London School of Economics, Weizmann Institute of Science"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06934",children:"https://arxiv.org/pdf/2602.06934"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper presents deterministic operational semantics (dGLP and madGLP) to facilitate the implementation of Grassroots Logic Programs, a concurrent logic programming language for peer-to-peer systems. The semantics were used as formal specifications for an AI to generate implementations in Dart for workstations and smartphones. The main conclusion is that the developed mathematical specifications enabled correct, implementation-ready semantics, though the development process required iterative refinement across mathematical, informal, and code layers."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv260209] Distributed Knowledge in Simplicial Models"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed computing], [simplicial complexes, epistemic logic, Kripke models, distributed knowledge, majority consensus]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," \xc9ric Goubault, J\xe9r\xe9my Ledent, Sergio Rajsbaum"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," LIX, CNRS, \xc9cole Polytechnique, Institut Polytechnique de Paris; Universit\xe9 Paris Cit\xe9, CNRS, IRIF; Instituto de Matem\xe1ticas, UNAM"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06945",children:"https://arxiv.org/pdf/2602.06945"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces simplicial complexes as models for multi-agent epistemic logic, shifting the focus from possible worlds to agents' local views. It connects this topological approach to distributed computing, showing how distributed knowledge relates to solving the majority consensus task. The work describes the specific distributed knowledge used when the task is solvable and presents a logical obstruction when it is not."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 30'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Transformer-Based Reinforcement Learning for Autonomous Orbital Collision Avoidance in Partially Observable Environments ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06088",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06107",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Self-Improving World Modelling with Latent Actions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06130",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Flow Matching for Offline Reinforcement Learning with Discrete Actions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06138",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Learning Rate Scaling across LoRA Ranks and Transfer to Full Finetuning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06204",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Online Adaptive Reinforcement Learning with Echo State Networks for Non-Stationary Dynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06326",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06359",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06394",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06440",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Prism: Spectral Parameter Sharing for Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06476",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06485",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Adaptive Uncertainty-Aware Tree Search for Robust Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06493",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Progress Constraints for Reinforcement Learning in Behavior Trees ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06525",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Dynamics-Aligned Shared Hypernetworks for Zero-Shot Actuator Inversion ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06550",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06554",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06566",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Sample-Efficient Policy Space Response Oracles with Joint Experience Best Response ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06599",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] The hidden risks of temporal resampling in clinical reinforcement learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06603",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06643",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06669",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06717",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06746",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Soft Forward-Backward Representations for Zero-shot Reinforcement Learning with General Utilities ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06769",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06795",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06825",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] A first realization of reinforcement learning-based closed-loop EEG-TMS ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06907",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Continuous-time reinforcement learning: ellipticity enables model-free value function approximation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06930",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06939",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Optimal Derivative Feedback Control for an Active Magnetic Levitation System: An Experimental Study on Data-Driven Approaches ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06944",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06960",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 16'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Analyzing Diffusion and Autoregressive Vision Language Models in Multimodal Embedding Space ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06056",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Compressing LLMs with MoP: Mixture of Pruners ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06127",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06161",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] To 2:4 Sparsity and Beyond: Neuron-level Activation Function to Accelerate LLM Pre-Training ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06183",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Accelerating Vision Transformers on Brain Processing Unit ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06300",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Beyond Code Contributions: How Network Position, Temporal Bursts, and Code Review Activities Shape Contributor Influence in Large-Scale Open Source Ecosystems ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06426",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Principle-Evolvable Scientific Discovery via Uncertainty Minimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06448",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06706",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06718",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06825",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Are Deep Learning Based Hybrid PDE Solvers Reliable? Why Training Paradigms and Update Strategies Matter ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06842",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06850",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06855",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] When RL Meets Adaptive Speculative Training: A Unified Training-Serving System ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06932",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06949",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv260209] InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2602.06960",children:"link"})]}),"\n"]})]})}function d(i={}){const{wrapper:e}={...(0,a.R)(),...i.components};return e?(0,s.jsx)(e,{...i,children:(0,s.jsx)(h,{...i})}):h(i)}},8453:(i,e,n)=>{n.d(e,{R:()=>t,x:()=>o});var r=n(6540);const s={},a=r.createContext(s);function t(i){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof i?i(e):{...e,...i}},[e,i])}function o(i){let e;return e=i.disableParentContext?"function"==typeof i.components?i.components(s):i.components||s:t(i.components),r.createElement(a.Provider,{value:e},i.children)}}}]);