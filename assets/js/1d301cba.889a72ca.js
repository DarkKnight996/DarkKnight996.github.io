"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[478],{8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>l});var s=i(6540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}},9750:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"daily/20260126-20260201","title":"20260126-20260201","description":"2026-01-26","source":"@site/docs/daily/20260126-20260201.md","sourceDirName":"daily","slug":"/daily/20260126-20260201","permalink":"/daily/20260126-20260201","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1769396796000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20260119-20260125","permalink":"/daily/20260119-20260125"},"next":{"title":"Paper","permalink":"/category/paper"}}');var r=i(4848),t=i(8453);const a={},l="20260126-20260201",o={},c=[{value:"2026-01-26",id:"2026-01-26",level:2}];function h(e){const n={a:"a",annotation:"annotation",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mi:"mi",mrow:"mrow",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"20260126-20260201",children:"20260126-20260201"})}),"\n",(0,r.jsx)(n.h2,{id:"2026-01-26",children:"2026-01-26"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"cs.DC total: 6"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260126] Consensus In Asynchrony"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [distributed consensus], [events-based synchronisation, vector agreement, fault-tolerant consensus, FLP impossibility, binary agreement, deterministic consensus]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Ivan Klianev"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Transactum Pty Ltd"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16460",children:"https://arxiv.org/pdf/2601.16460"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a deterministic fault-tolerant consensus algorithm for asynchronous systems using events-based synchronization, which achieves vector agreement. It argues that the classic FLP impossibility result relies on implicit assumptions and presents experimental evidence against one of them, suggesting consensus in asynchrony is possible."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260126] Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [space filling curves, communication-avoiding algorithms, generalized Hilbert curves, cache blocking, data locality]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Evangelos Georganas, Alexander Heinecke, Pradeep Dubey"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Intel Corporation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16294",children:"https://arxiv.org/pdf/2601.16294"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a matrix multiplication method using space filling curves (specifically generalized Hilbert curves) to partition computation, achieving platform- and shape-oblivious high data locality. It extends this approach with communication-avoiding algorithms to minimize data movement, resulting in compact code that outperforms vendor libraries by up to 2x on various CPU platforms."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260126] Artifact for Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [sys], [cloud-native systems], [Kubernetes, Kepler, cAdvisor, additive energy model, service-level energy measurement]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Julian Legler"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Berlin"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16635",children:"https://arxiv.org/pdf/2601.16635"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents GOXN, an experimentation engine for measuring the energy consumption of Kubernetes-based microservices, using an additive model to derive service-level energy from container-level metrics. The main conclusion is that excluding network and storage energy can lead to significant underestimation of consumption, especially for auxiliary services, and that high tracing loads shift energy dominance towards network and storage components."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260126] W4A16 Mixed-Precision Matrix Multiplication on Decoupled Architecture: Kernel Design and Memory Bottleneck Analysis for Ascend NPUs"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [weight-only quantization, W4A16, mixed-precision matrix multiplication, kernel design, Split-K parallelization, on-the-fly dequantization, memory bottleneck analysis]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Yuanhong He, Peiyu Niu, Jun Chen, Chenchen Zhang, Chao Yang"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," PKU-Changsha Institute of Computing and Digital Economy, Peking University"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16536",children:"https://arxiv.org/pdf/2601.16536"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents a custom W4A16 matrix multiplication kernel for Huawei Ascend 910 NPUs, using vector cores for dequantization and cube cores with Split-K parallelization to optimize performance. It finds that memory transfer, not dequantization computation, is the primary bottleneck, achieving up to 1.48x speedup over FP16 baseline in LLM decoding scenarios."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260126] GPU-Accelerated Selected Basis Diagonalization with Thrust for SQD-based Algorithms"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [Selected Basis Diagonalization, Sample-based Quantum Diagonalization, GPU acceleration, Thrust library, Davidson method, parallel algorithms]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Jun Doi, Tomonori Shirakawa, Yukio Kawashima, Seiji Yunoki, Hiroshi Horii"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," IBM Quantum, IBM Research - Tokyo, RIKEN, Center for Computational Science"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16637",children:"https://arxiv.org/pdf/2601.16637"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a GPU-accelerated implementation of Selected Basis Diagonalization (SBD) for Sample-based Quantum Diagonalization (SQD) algorithms using the Thrust library. The method restructures key computational components into fine-grained, data-parallel primitives with GPU-friendly data layouts. The results show a speedup of up to ~40x over CPU execution, demonstrating that GPU-native primitives provide a high-performance foundation for accelerating quantum-classical workflows."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"[arXiv260126] DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [checkpointing, state providers, asynchronous snapshots, hybrid parallelism, data parallelism, tensor parallelism, pipeline parallelism, ZeRO, FSDP]"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"authors:"})," Avinash Maurya, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"institution:"})," Argonne National Laboratory, Rochester Institute of Technology"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"link:"})," ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16956",children:"https://arxiv.org/pdf/2601.16956"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper introduces DataStates-LLM, a checkpointing architecture that uses composable State Providers to decouple state abstraction from data movement, enabling lazy, non-blocking asynchronous snapshots. It addresses the "3D heterogeneity" of distributed model states to reduce serialization and I/O bottlenecks. The evaluation shows it achieves up to 4x higher checkpointing throughput and reduces end-to-end training time by up to 2.2x compared to state-of-the-art solutions.']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 10'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] Towards a Theoretical Understanding to the Generalization of RLHF ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16403",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] Boosting Deep Reinforcement Learning with Semantic Knowledge for Robotic Manipulators ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16866",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] Sim-to-Real Transfer via a Style-Identified Cycle Consistent Generative Adversarial Network: Zero-Shot Deployment on Robotic Manipulators through Visual Domain Adaptation ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16677",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16486",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16906",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16399",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] Reasoning Promotes Robustness in Theory of Mind Tasks ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16853",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] Reinforcement Learning-Based Energy-Aware Coverage Path Planning for Precision Agriculture ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16405",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] Endless Terminals: Scaling RL Environments for Terminal Agents ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16443",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] LongCat-Flash-Thinking-2601 Technical Report ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16725",children:"link"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 6'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] kNN-Graph: An adaptive graph model for ",(0,r.jsxs)(n.span,{className:"katex",children:[(0,r.jsx)(n.span,{className:"katex-mathml",children:(0,r.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(n.semantics,{children:[(0,r.jsx)(n.mrow,{children:(0,r.jsx)(n.mi,{children:"k"})}),(0,r.jsx)(n.annotation,{encoding:"application/x-tex",children:"k"})]})})}),(0,r.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,r.jsxs)(n.span,{className:"base",children:[(0,r.jsx)(n.span,{className:"strut",style:{height:"0.6944em"}}),(0,r.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.03148em"},children:"k"})]})})]}),"-nearest neighbors ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16509",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] E2Former-V2: On-the-Fly Equivariant Attention with Linear Activation Memory ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16622",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] DSGym: A Holistic Framework for Evaluating and Training Data Science Agents ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16344",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] Bayesian Experimental Design for Model Discrepancy Calibration: A Rivalry between Kullback--Leibler Divergence and Wasserstein Distance ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16425",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] Auto-Regressive Masked Diffusion Models ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16971",children:"link"})]}),"\n",(0,r.jsxs)(n.li,{children:["[arXiv260126] Active learning for photonics ",(0,r.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16287",children:"link"})]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}}}]);