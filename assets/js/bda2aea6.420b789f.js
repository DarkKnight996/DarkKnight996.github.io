"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[619],{6624:(i,e,n)=>{n.r(e),n.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251222-20251228","title":"20251222-20251228","description":"2025-12-22","source":"@site/docs/daily/20251222-20251228.md","sourceDirName":"daily","slug":"/daily/20251222-20251228","permalink":"/daily/20251222-20251228","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1766372006000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251215-20251221","permalink":"/daily/20251215-20251221"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=n(4848),t=n(8453);const a={},o="20251222-20251228",l={},c=[{value:"2025-12-22",id:"2025-12-22",level:2}];function d(i){const e={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...i.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20251222-20251228",children:"20251222-20251228"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-12-22",children:"2025-12-22"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 13"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Sedna: Sharding transactions in multiple concurrent proposer blockchains"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [blockchain consensus], [verifiable rateless coding, transaction sharding, multi-proposer consensus, until-decode privacy]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Alejandro Ranchal-Pedrosa, Benjamin Marsh, Lefteris Kokoris-Kogias, Alberto Sonnino"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Sei Labs, Mysten Labs, University of Portsmouth, University College London"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17045",children:"https://arxiv.org/pdf/2512.17045"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," Sedna is a user-facing protocol that uses verifiable, rateless coding to shard transactions across multiple concurrent proposers in a blockchain, replacing naive replication. It guarantees liveness and until-decode privacy, reducing MEV exposure and approaching the information-theoretic lower bound for bandwidth overhead, yielding a 2\u20133x efficiency improvement. The protocol requires no consensus modifications, enabling incremental deployment."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [dimensionality reduction, Byzantine-robust aggregation, privacy-preserving federated learning, secure multi-party computation, adaptive tuning]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Baolei Zhang, Minghong Fang, Zhuqing Liu, Biao Yi, Peizhao Zhou, Yuan Wang, Tong Li, Zheli Liu"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nankai University, University of Louisville, University of North Texas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17254",children:"https://arxiv.org/pdf/2512.17254"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes ABBR, a practical framework for federated learning that integrates privacy-preserving and Byzantine-robust defenses. It uses dimensionality reduction to speed up secure computations for model filtering and an adaptive tuning strategy to mitigate the impact of undetected malicious models. The framework demonstrates significantly faster performance with minimal overhead while maintaining strong resilience against attacks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Dion2: A Simple Method to Shrink Matrix in Muon"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [Muon optimizer, orthonormalization, matrix shrinking, sampling, Newton-Schulz iterations, FSDP]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kwangjun Ahn, Noah Amsel, John Langford"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Microsoft Research, AI Frontiers, NYU"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16928",children:"https://arxiv.org/pdf/2512.16928"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces Dion2, a simple method to improve the scalability of the Muon optimizer by reducing the computational cost of its orthonormalization step. It works by sampling a fraction of rows or columns at each iteration for orthonormalization, making the update sparse. The method maintains update quality close to full Muon while significantly reducing time per step, as demonstrated in large-scale model training."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Democratizing Scalable Cloud Applications: Transactional Stateful Functions on Streaming Dataflows"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [distributed systems, cloud computing], [streaming dataflow, stateful functions, serializable transactions, fault-tolerance, serverless, Apache Flink, Stateflow, Styx]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Kyriakos Psarakis"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Unknown (Institution not provided in the given text)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17429",children:"https://arxiv.org/pdf/2512.17429"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This thesis proposes using the streaming dataflow execution model to simplify building scalable cloud applications. It introduces Stateflow, a high-level programming model, and Styx, a distributed engine that provides deterministic, serializable transactions with strong fault tolerance. The work concludes that this approach democratizes development by improving programmability and performance over existing systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Scalable Distributed Vector Search via Accuracy Preserving Index Construction"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [SPIRE, hierarchical vector index, partition granularity, accuracy-preserving recursive construction, approximate nearest neighbor search, distributed index]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yuming Xu, Qianxi Zhang, Qi Chen, Baotong Lu, Menghao Li, Philip Adams, Mingqin Li, Zengzhong Li, Jing Liu, Cheng Li, Fan Yang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Science and Technology of China, Microsoft Research, Shopify, Microsoft AI and Research"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17264",children:"https://arxiv.org/pdf/2512.17264"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces SPIRE, a scalable distributed vector index system designed for Approximate Nearest Neighbor Search (ANNS). Its core method involves identifying a balanced partition granularity to avoid read-cost explosion and using an accuracy-preserving recursive construction to build a multi-level index. The main conclusion is that SPIRE achieves high scalability and up to 9.64x higher throughput than state-of-the-art systems in experiments with billions of vectors."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] The HEAL Data Platform"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [data platform], [Gen3 platform, federated system, cloud-based, FAIR principles, mesh architecture, persistent identifiers, metadata services, APIs]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Brienna M. Larrick, L. Philip Schumm, Mingfei Shao, Craig Barnes, Anthony Juehne, Hara Prasad Juvvla, Michael B. Kranz, Michael Lukowski, Clint Malson, Jessica N. Mazerik, Christopher G. Meyer, Jawad Qureshi, Erin Spaniol, Andrea Tentner, Alexander VanTol, Peter Vassilatos, Sara Volk de Garcia, Robert L. Grossman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Chicago"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17506",children:"https://arxiv.org/pdf/2512.17506"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper presents the HEAL Data Platform, a cloud-based federated system built on the open-source Gen3 platform to serve as a single point of search, discovery, and analysis for data from the NIH HEAL Initiative. It interoperates with multiple data repositories using framework services for authentication, metadata, and persistent identifiers. The platform maximizes data value by ensuring data are Findable, Accessible, Interoperable, and Reusable (FAIR), facilitating secondary analysis."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [Logit-Aware Activation Budgeting, Phase-Multiplexed Scheduler, Head-Centric Sparse Attention, parallel decoding, memory footprint optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Jiakun Fan, Yanglin Zhang, Xiangchen Li, Dimitrios S. Nikolopoulos"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Virginia Tech"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17077",children:"https://arxiv.org/pdf/2512.17077"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper introduces dLLM-Serve, a serving system that addresses the memory footprint crisis in Diffusion LLMs by co-optimizing memory, scheduling, and generation quality. It proposes techniques like Logit-Aware Activation Budgeting and a Phase-Multiplexed Scheduler to manage resource oscillation and improve efficiency. The system significantly improves throughput and reduces latency across different hardware, establishing a blueprint for scalable dLLM inference."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [real-time systems], [fixed-priority scheduling, EDF, DAG task models, events executor, LIFO queue]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Oren Bell, Harun Teper, Mario G\xfcnzel, Chris Gill, Jian-Jia Chen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Washington University in St Louis, TU Dortmund University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16926",children:"https://arxiv.org/pdf/2512.16926"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel scheduling approach for ROS2 applications by using an events executor to implement fixed-job-level-priority schedulers for arbitrary Directed Acyclic Graph (DAG) tasks on uniprocessor systems. The method abstracts ROS2 applications as forests of trees and maps them to traditional real-time DAG models, requiring a special LIFO-ordered events queue. The authors conclude that their implementation generates schedules equivalent to a conventional fixed-priority DAG scheduler, helping to bridge the gap between real-time systems theory and ROS2 scheduling."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [C++, MPI, OpenMP, parallel programming, code generation, scalability, ChatGPT, Claude, LLaMA]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Patrick Diehl, Noujoud Nader, Deepti Gupta"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Los Alamos National Laboratory, Louisiana State University, Texas A&M University-Central Texas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17023",children:"https://arxiv.org/pdf/2512.17023"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper systematically evaluates large language models (LLMs) like ChatGPT, Claude, and LLaMA on generating correct and scalable parallel C++ code using MPI and OpenMP for Mandelbrot set computation. The method involves compiling and executing the generated programs to assess correctness, robustness, and performance. The main conclusion is that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance in this HPC code generation task."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [adaptive graph pruning, Spatio-Temporal Graph Neural Networks (ST-GNNs), Federated Learning (FL), Gossip Learning, Sudden Event Prediction Accuracy (SEPA), online semi-decentralized training]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ivan Kralj, Lodovico Giaretta, Gordan Je\u017ei\u0107, Ivana Podnar \u017darko, \u0160ar\u016bnas Girdzijauskas"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Zagreb, RISE Research Institutes of Sweden, KTH Royal Institute of Technology"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17352",children:"https://arxiv.org/pdf/2512.17352"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes an adaptive graph pruning algorithm for Spatio-Temporal Graph Neural Networks (ST-GNNs) to reduce communication overhead in online semi-decentralized traffic prediction systems. It also introduces a novel event-focused metric called SEPA to better evaluate responsiveness to sudden traffic changes. The experiments demonstrate that the method significantly lowers communication costs without compromising prediction accuracy or responsiveness to critical traffic events."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Torrent: A Distributed DMA for Efficient and Flexible Point-to-Multipoint Data Movement"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [distributed dma, chainwrite, network-on-chip, point-to-multipoint, scheduling algorithms]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Yunhao Deng, Fanchen Kong, Xiaoling Yi, Ryan Antonio, Marian Verhelst"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," KU Leuven"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17589",children:"https://arxiv.org/pdf/2512.17589"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper introduces Torrent, a distributed DMA architecture that performs efficient point-to-multipoint data transfers by forming logical data chains (Chainwrite) over a standard Network-on-Chip without modifying its hardware or protocol. It uses scheduling algorithms to optimize chain order and demonstrates significant performance improvements, achieving up to 7.88x speedup over unicast with minimal area and power overhead."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [multi-modal inference], [GPU-internal scheduling, resource sharing, collaborative multi-GPU video decoding, logically decoupled execution, inter-stage blocking elimination]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Wuhan University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17574",children:"https://arxiv.org/pdf/2512.17574"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes FlashCodec and UnifiedServe, a framework that optimizes multi-stage MLLM inference by accelerating video decoding and enabling resource sharing between vision and LLM stages. This approach reduces latency and increases throughput by eliminating inter-stage blocking and improving GPU utilization. The system achieves significantly higher throughput and can serve more requests compared to existing state-of-the-art systems."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251222] Asymptotic behaviour of galactic small-scale dynamos at modest magnetic Prandtl number"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [astrophysics], [Pencil Code, Astaroth, GPU acceleration, magnetohydrodynamics (MHD), supernova-driven dynamo, magnetic Prandtl number]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Frederick A. Gent, Mordecai-Mark Mac Low, Maarit J. Korpi-Lagg, Touko Puro, Matthias Reinhardt"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nordita, KTH Royal Institute of Technology and Stockholm University, Aalto University, Newcastle University, American Museum of Natural History"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17885",children:"https://arxiv.org/pdf/2512.17885"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper uses high-resolution GPU-accelerated simulations with the Pencil Code and Astaroth to model a supernova-driven galactic dynamo. The main finding is that the strength of the turbulent magnetic field from the small-scale dynamo reaches an asymptotic limit at a modest magnetic Prandtl number of only a few hundred, which is far below physical interstellar values. This asymptotic behavior allows the model's characteristics to be incorporated into larger-scale galactic simulations."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 22'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17034",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17213",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17194",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17091",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17180",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17444",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Value Under Ignorance in Universal Artificial Intelligence ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17086",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Reinforcement Learning for Self-Improving Agent with Skill Library ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17102",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Understanding Generalization in Role-Playing Models via Information Theory ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17270",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] A Theoretical Analysis of State Similarity Between Markov Decision Processes ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17265",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.16969",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Large Language Models as Pok\xe9mon Battle Agents: Strategic Play and Content Generation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17308",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17008",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17043",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Learning Safe Autonomous Driving Policies Using Predictive Safety Representations ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17586",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] SCOPE: Sequential Causal Optimization of Process Interventions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17629",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Trust-Region Adaptive Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17636",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] About Time: Model-free Reinforcement Learning with Timed Reward Machines ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17637",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17846",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17853",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17899",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] HydroGym: A Reinforcement Learning Platform for Fluid Dynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17534",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 8'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17267",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17281",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17461",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17091",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17137",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17273",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17299",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251222] Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2512.17659",children:"link"})]}),"\n"]})]})}function h(i={}){const{wrapper:e}={...(0,t.R)(),...i.components};return e?(0,s.jsx)(e,{...i,children:(0,s.jsx)(d,{...i})}):d(i)}},8453:(i,e,n)=>{n.d(e,{R:()=>a,x:()=>o});var r=n(6540);const s={},t=r.createContext(s);function a(i){const e=r.useContext(t);return r.useMemo(function(){return"function"==typeof i?i(e):{...e,...i}},[e,i])}function o(i){let e;return e=i.disableParentContext?"function"==typeof i.components?i.components(s):i.components||s:a(i.components),r.createElement(t.Provider,{value:e},i.children)}}}]);