"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[174],{8266:(i,e,n)=>{n.r(e),n.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20251103-20251109","title":"20251103-20251109","description":"2025-11-03","source":"@site/docs/daily/20251103-20251109.md","sourceDirName":"daily","slug":"/daily/20251103-20251109","permalink":"/daily/20251103-20251109","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1762136932000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20251027-20251102","permalink":"/daily/20251027-20251102"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=n(4848),t=n(8453);const a={},l="20251103-20251109",o={},c=[{value:"2025-11-03",id:"2025-11-03",level:2}];function h(i){const e={a:"a",annotation:"annotation",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mi:"mi",mrow:"mrow",msub:"msub",mtext:"mtext",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,t.R)(),...i.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"20251103-20251109",children:"20251103-20251109"})}),"\n",(0,s.jsx)(e.h2,{id:"2025-11-03",children:"2025-11-03"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"cs.DC total: 8"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm inference], [adaptive expert prefetching, cache-aware routing, hybrid cross-layer prediction, runtime statistics, memory coordination]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Zixu Shen, Kexin Chu, Yifan Zhang, Dawei Xiang, Runxin Wu, Wei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Connecticut"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26730",children:"https://arxiv.org/pdf/2510.26730"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ExpertFlow introduces an adaptive runtime system that combines expert prefetching and cache-aware routing to optimize MoE inference. It dynamically adjusts prediction horizons using runtime statistics and hybrid prediction schemes to reduce parameter transfer latency. The system reduces model stall time to less than 0.1% of baseline while operating under strict memory constraints."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] Wireless Sensor Networks as Parallel and Distributed Hardware Platform for Artificial Neural Networks"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [wireless sensor networks, parallel distributed processing, artificial neural networks, real-time computation, large-scale problems]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Gursel Serpen"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," The University of Toledo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26492",children:"https://arxiv.org/pdf/2510.26492"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes using wireless sensor networks as a massively parallel and distributed hardware platform to implement artificial neural network algorithms. The approach enables real-time computation of large-scale problems by leveraging hundreds of thousands of processing nodes with onboard processing and wireless communication capabilities. This implementation could revolutionize computing by making it possible to solve very large-scale scientific and engineering problems in real time."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [hardware telemetry, unsupervised learning, anomaly detection, hardware-centric approach]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Ziji Chen, Steven Chien, Peng Qian, Noa Zilberman"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," University of Oxford"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26008",children:"https://arxiv.org/pdf/2510.26008"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes a hardware-centric anomaly detection system that uses low-level hardware telemetry and unsupervised learning to identify performance issues in ML infrastructure. This approach requires no workload knowledge and relies solely on hardware signals accessible to cloud operators. The method successfully detected network and configuration issues, accelerating the DeepSeek model by 5.97% in experiments."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] Foundations of Fiat-Denominated Loans Collateralized by Cryptocurrencies"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [decentralized finance protocols], [trusted arbitration, game-theoretical analysis, subgame perfect equilibrium, limited-custodial protocols]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Pavel Hub\xe1\u010dek, Jan V\xe1clavek, Michelle Yeo"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Institute of Mathematics, Czech Academy of Sciences, Charles University, Firefish, National University of Singapore"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25878",children:"https://arxiv.org/pdf/2510.25878"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes limited-custodial protocols for fiat-denominated loans collateralized by cryptocurrencies that rely on trusted arbitration. The authors provide a game-theoretical analysis showing these protocols achieve secure lending mechanisms. The work establishes foundations for cryptocurrency-backed lending while highlighting future research directions in decentralized finance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [llm training], [speculative decoding, reinforcement learning, knowledge distillation, dynamic configuration tuning, reward-weighted updates]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Qiaoling Chen, Zijun Liu, Peng Sun, Shenggui Li, Guoteng Wang, Ziming Liu, Yonggang Wen, Siyuan Feng, Tianwei Zhang"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Nanyang Technological University, Shanghai Qiji Zhifeng Co., Ltd., Tsinghua University, National University of Singapore, Shanghai Innovation Institute"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26475",children:"https://arxiv.org/pdf/2510.26475"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," ReSpec optimizes speculative decoding for reinforcement learning systems by dynamically tuning configurations, evolving drafters via knowledge distillation, and weighting updates with rollout rewards. The system achieves up to 4.5\xd7 speedup while maintaining reward convergence and training stability, providing an efficient solution for RL-based LLM adaptation."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [others], [over-the-air federated learning, stochastic gradient descent, successive convex approximation, power-control design, bias-variance trade-off]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Muhammad Faraz Ul Abrar, Nicol\xf2 Michelusi"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Arizona State University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26722",children:"https://arxiv.org/pdf/2510.26722"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper proposes a novel over-the-air federated learning approach that allows structured model bias to reduce update variance under heterogeneous wireless conditions. The method uses a successive convex approximation algorithm for joint power-control optimization requiring only statistical channel state information. Experiments show this approach accelerates convergence and improves generalization compared to prior OTA-FL baselines by optimizing the bias-variance trade-off."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [gradient sparsification, all-reduce compatible compression, error feedback, top-k selection]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Chuyan Chen, Chenyang Ma, Zhangxin Li, Yutong He, Yanjie Dong, Kun Yuan"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," Peking University, Shenzhen MSU-BIT University"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26709",children:"https://arxiv.org/pdf/2510.26709"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," The paper proposes ARC-Top-K, an All-Reduce compatible gradient compressor that aligns sparsity patterns across nodes using gradient sketches to enable efficient communication. This method preserves globally significant gradient information while being provably contractive and compatible with momentum error feedback. Empirical results show it matches Top-K accuracy while reducing training time by up to 60.7%, combining Rand-K's robustness with Top-K's performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"[arXiv251103] Environmental Impact of CI/CD Pipelines"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"tags:"})," [sys], [cloud computing sustainability], [carbon footprint analysis, water footprint analysis, GitHub Actions, Cloud Carbon Footprint framework, computational resource optimization]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"authors:"})," Nuno Saavedra, Alexandra Mendes, Jo\xe3o F. Ferreira"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"institution:"})," INESC-ID, University of Lisbon, INESC TEC, University of Porto"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"link:"})," ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26413",children:"https://arxiv.org/pdf/2510.26413"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simple LLM Summary:"})," This paper analyzes the environmental impact of GitHub Actions CI/CD pipelines using the Cloud Carbon Footprint framework on a dataset of over 2.2 million workflow runs. The study reveals substantial carbon and water footprints, estimating 456.9 MTCO2e and 5,738.2 kiloliters respectively in the most likely scenario. The authors recommend mitigation strategies including deploying runners in environmentally favorable regions and reducing wasted computational resources."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 32'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26184",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Think Outside the Policy: In-Context Steered Policy Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26519",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] InputDSA: Demixing then Comparing Recurrent and Externally Driven Dynamics ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25943",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Approximating Human Preferences Using a Multi-Judge Learned System ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25884",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] PORTool: Tool-Use LLM Training with Rewarded Tree ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26020",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Estimating cognitive biases with attention-aware inverse planning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25951",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25867",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy Evaluation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26026",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] ",(0,s.jsxs)(e.span,{className:"katex",children:[(0,s.jsx)(e.span,{className:"katex-mathml",children:(0,s.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(e.semantics,{children:[(0,s.jsx)(e.mrow,{children:(0,s.jsxs)(e.msub,{children:[(0,s.jsx)(e.mi,{children:"\u03c0"}),(0,s.jsx)(e.mtext,{mathvariant:"monospace",children:"RL"})]})}),(0,s.jsx)(e.annotation,{encoding:"application/x-tex",children:"\u03c0_\\texttt{RL}"})]})})}),(0,s.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(e.span,{className:"base",children:[(0,s.jsx)(e.span,{className:"strut",style:{height:"0.5806em",verticalAlign:"-0.15em"}}),(0,s.jsxs)(e.span,{className:"mord",children:[(0,s.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c0"}),(0,s.jsx)(e.span,{className:"msupsub",children:(0,s.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,s.jsxs)(e.span,{className:"vlist-r",children:[(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.2778em"},children:(0,s.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"},children:[(0,s.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsx)(e.span,{className:"mord text mtight",children:(0,s.jsx)(e.span,{className:"mord texttt mtight",children:"RL"})})})]})}),(0,s.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,s.jsx)(e.span,{className:"vlist-r",children:(0,s.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,s.jsx)(e.span,{})})})]})})]})]})})]}),": Online RL Fine-tuning for Flow-based Vision-Language-Action Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25889",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] EgoExo-Con: Exploring View-Invariant Video Temporal Understanding ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26113",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26089",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] InfoFlow: Reinforcing Search Agent Via Reward Density Optimization ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26575",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Offline Clustering of Preference Learning with Active-data Augmentation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26301",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Learning to Manage Investment Portfolios beyond Simple Utility Functions ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26165",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26098",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25929",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26109",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Kimi Linear: An Expressive, Efficient Attention Architecture ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26692",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26347",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Action-Driven Processes for Continuous-Time Control ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26672",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26389",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25801",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Human-in-the-loop Online Rejection Sampling for Robotic Manipulation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26406",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Data-Efficient RLVR via Off-Policy Influence Guidance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26491",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26143",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] The Era of Agentic Organization: Learning to Organize with Language Models ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26658",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25992",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25796",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Graph-Enhanced Policy Optimization in LLM Agent Training ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26270",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] A General Incentives-Based Framework for Fairness in Multi-agent Resource Allocation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26740",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26167",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Defeating the Training-Inference Mismatch via FP16 ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26788",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 9'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Polybasic Speculative Decoding Through a Theoretical Perspective ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26527",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25904",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25808",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] FlowQ-Net: A Generative Framework for Automated Quantum Circuit Design ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26688",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Risks and Opportunities in Human-Machine Teaming in Operationalizing Machine Learning Target Variables ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25974",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] Data-Efficient RLVR via Off-Policy Influence Guidance ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26491",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25979",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers? ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.25791",children:"link"})]}),"\n",(0,s.jsxs)(e.li,{children:["[arXiv251103] The FM Agent ",(0,s.jsx)(e.a,{href:"https://arxiv.org/pdf/2510.26144",children:"link"})]}),"\n"]})]})}function d(i={}){const{wrapper:e}={...(0,t.R)(),...i.components};return e?(0,s.jsx)(e,{...i,children:(0,s.jsx)(h,{...i})}):h(i)}},8453:(i,e,n)=>{n.d(e,{R:()=>a,x:()=>l});var r=n(6540);const s={},t=r.createContext(s);function a(i){const e=r.useContext(t);return r.useMemo(function(){return"function"==typeof i?i(e):{...e,...i}},[e,i])}function l(i){let e;return e=i.disableParentContext?"function"==typeof i.components?i.components(s):i.components||s:a(i.components),r.createElement(t.Provider,{value:e},i.children)}}}]);