"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[478],{8453:(i,n,e)=>{e.d(n,{R:()=>t,x:()=>o});var r=e(6540);const s={},a=r.createContext(s);function t(i){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof i?i(n):{...n,...i}},[n,i])}function o(i){let n;return n=i.disableParentContext?"function"==typeof i.components?i.components(s):i.components||s:t(i.components),r.createElement(a.Provider,{value:n},i.children)}},9750:(i,n,e)=>{e.r(n),e.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"daily/20260126-20260201","title":"20260126-20260201","description":"2026-01-26","source":"@site/docs/daily/20260126-20260201.md","sourceDirName":"daily","slug":"/daily/20260126-20260201","permalink":"/daily/20260126-20260201","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1769486775000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"20260119-20260125","permalink":"/daily/20260119-20260125"},"next":{"title":"Paper","permalink":"/category/paper"}}');var s=e(4848),a=e(8453);const t={},o="20260126-20260201",l={},c=[{value:"2026-01-26",id:"2026-01-26",level:2},{value:"2026-01-27",id:"2026-01-27",level:2}];function h(i){const n={a:"a",annotation:"annotation",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mi:"mi",mn:"mn",mrow:"mrow",msup:"msup",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...i.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"20260126-20260201",children:"20260126-20260201"})}),"\n",(0,s.jsx)(n.h2,{id:"2026-01-26",children:"2026-01-26"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"cs.DC total: 6"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260126] Consensus In Asynchrony"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [distributed consensus], [events-based synchronisation, vector agreement, fault-tolerant consensus, FLP impossibility, binary agreement, deterministic consensus]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ivan Klianev"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Transactum Pty Ltd"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16460",children:"https://arxiv.org/pdf/2601.16460"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a deterministic fault-tolerant consensus algorithm for asynchronous systems using events-based synchronization, which achieves vector agreement. It argues that the classic FLP impossibility result relies on implicit assumptions and presents experimental evidence against one of them, suggesting consensus in asynchrony is possible."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260126] Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [space filling curves, communication-avoiding algorithms, generalized Hilbert curves, cache blocking, data locality]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Evangelos Georganas, Alexander Heinecke, Pradeep Dubey"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Intel Corporation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16294",children:"https://arxiv.org/pdf/2601.16294"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a matrix multiplication method using space filling curves (specifically generalized Hilbert curves) to partition computation, achieving platform- and shape-oblivious high data locality. It extends this approach with communication-avoiding algorithms to minimize data movement, resulting in compact code that outperforms vendor libraries by up to 2x on various CPU platforms."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260126] Artifact for Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [cloud-native systems], [Kubernetes, Kepler, cAdvisor, additive energy model, service-level energy measurement]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Julian Legler"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Berlin"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16635",children:"https://arxiv.org/pdf/2601.16635"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents GOXN, an experimentation engine for measuring the energy consumption of Kubernetes-based microservices, using an additive model to derive service-level energy from container-level metrics. The main conclusion is that excluding network and storage energy can lead to significant underestimation of consumption, especially for auxiliary services, and that high tracing loads shift energy dominance towards network and storage components."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260126] W4A16 Mixed-Precision Matrix Multiplication on Decoupled Architecture: Kernel Design and Memory Bottleneck Analysis for Ascend NPUs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [weight-only quantization, W4A16, mixed-precision matrix multiplication, kernel design, Split-K parallelization, on-the-fly dequantization, memory bottleneck analysis]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Yuanhong He, Peiyu Niu, Jun Chen, Chenchen Zhang, Chao Yang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," PKU-Changsha Institute of Computing and Digital Economy, Peking University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16536",children:"https://arxiv.org/pdf/2601.16536"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents a custom W4A16 matrix multiplication kernel for Huawei Ascend 910 NPUs, using vector cores for dequantization and cube cores with Split-K parallelization to optimize performance. It finds that memory transfer, not dequantization computation, is the primary bottleneck, achieving up to 1.48x speedup over FP16 baseline in LLM decoding scenarios."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260126] GPU-Accelerated Selected Basis Diagonalization with Thrust for SQD-based Algorithms"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [GPU kernels], [Selected Basis Diagonalization, Sample-based Quantum Diagonalization, GPU acceleration, Thrust library, Davidson method, parallel algorithms]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Jun Doi, Tomonori Shirakawa, Yukio Kawashima, Seiji Yunoki, Hiroshi Horii"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," IBM Quantum, IBM Research - Tokyo, RIKEN, Center for Computational Science"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16637",children:"https://arxiv.org/pdf/2601.16637"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a GPU-accelerated implementation of Selected Basis Diagonalization (SBD) for Sample-based Quantum Diagonalization (SQD) algorithms using the Thrust library. The method restructures key computational components into fine-grained, data-parallel primitives with GPU-friendly data layouts. The results show a speedup of up to ~40x over CPU execution, demonstrating that GPU-native primitives provide a high-performance foundation for accelerating quantum-classical workflows."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260126] DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [checkpointing, state providers, asynchronous snapshots, hybrid parallelism, data parallelism, tensor parallelism, pipeline parallelism, ZeRO, FSDP]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Avinash Maurya, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Argonne National Laboratory, Rochester Institute of Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16956",children:"https://arxiv.org/pdf/2601.16956"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper introduces DataStates-LLM, a checkpointing architecture that uses composable State Providers to decouple state abstraction from data movement, enabling lazy, non-blocking asynchronous snapshots. It addresses the "3D heterogeneity" of distributed model states to reduce serialization and I/O bottlenecks. The evaluation shows it achieves up to 4x higher checkpointing throughput and reduces end-to-end training time by up to 2.2x compared to state-of-the-art solutions.']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 10'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Towards a Theoretical Understanding to the Generalization of RLHF ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16403",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Boosting Deep Reinforcement Learning with Semantic Knowledge for Robotic Manipulators ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16866",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Sim-to-Real Transfer via a Style-Identified Cycle Consistent Generative Adversarial Network: Zero-Shot Deployment on Robotic Manipulators through Visual Domain Adaptation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16677",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16486",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16906",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16399",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Reasoning Promotes Robustness in Theory of Mind Tasks ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16853",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Reinforcement Learning-Based Energy-Aware Coverage Path Planning for Precision Agriculture ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16405",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Endless Terminals: Scaling RL Environments for Terminal Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16443",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] LongCat-Flash-Thinking-2601 Technical Report ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16725",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 6'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] kNN-Graph: An adaptive graph model for ",(0,s.jsxs)(n.span,{className:"katex",children:[(0,s.jsx)(n.span,{className:"katex-mathml",children:(0,s.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(n.semantics,{children:[(0,s.jsx)(n.mrow,{children:(0,s.jsx)(n.mi,{children:"k"})}),(0,s.jsx)(n.annotation,{encoding:"application/x-tex",children:"k"})]})})}),(0,s.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(n.span,{className:"base",children:[(0,s.jsx)(n.span,{className:"strut",style:{height:"0.6944em"}}),(0,s.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.03148em"},children:"k"})]})})]}),"-nearest neighbors ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16509",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] E2Former-V2: On-the-Fly Equivariant Attention with Linear Activation Memory ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16622",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] DSGym: A Holistic Framework for Evaluating and Training Data Science Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16344",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Bayesian Experimental Design for Model Discrepancy Calibration: A Rivalry between Kullback--Leibler Divergence and Wasserstein Distance ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16425",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Auto-Regressive Masked Diffusion Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16971",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260126] Active learning for photonics ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16287",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"2026-01-27",children:"2026-01-27"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"cs.DC total: 24"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Context Lake: A System Class Defined by Decision Coherence"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [decision coherence, transactional consistency, semantic operations, operational envelopes, composition impossibility theorem]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Xiaowei Jiang"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Tacnode"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17019",children:"https://arxiv.org/pdf/2601.17019"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"}),' This paper introduces the "Decision Coherence Law" and argues that existing data systems are inadequate for AI agents making concurrent, irreversible decisions. It proposes a new system class called "Context Lake," which requires semantic operations, transactional consistency, and bounded operational envelopes to ensure correctness in collective AI systems.']}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Athena: Synergizing Data Prefetching and Off-Chip Prediction via Online Reinforcement Learning"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [reinforcement learning, data prefetching, off-chip prediction, cache management, coordination policy]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Rahul Bera, Zhenrong Lang, Caroline Hengartner, Konstantinos Kanellopoulos, Rakesh Kumar, Mohammad Sadrosadati, Onur Mutlu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," ETH Z\xfcrich, NTNU"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17615",children:"https://arxiv.org/pdf/2601.17615"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces Athena, a framework that uses online reinforcement learning to autonomously coordinate data prefetchers and an off-chip predictor in high-performance processors. It demonstrates that Athena consistently outperforms prior coordination policies across diverse workloads and system configurations by learning to synergize these techniques effectively."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [federated learning, decentralized federated learning, peer-to-peer, knowledge distillation, parameter-efficient fine-tuning, contextual bandit, LinUCB]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Inderjeet Singh, Eleonore Vissol-Gaudin, Andikan Otung, Motoyoshi Sekiya"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Fujitsu Research of Europe"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17133",children:"https://arxiv.org/pdf/2601.17133"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces KNEXA-FL, a framework for orchestrated decentralization in federated learning. It uses a central matchmaker to intelligently pair heterogeneous LLM agents for peer-to-peer knowledge exchange via secure distillation, which significantly improves performance and stability compared to random collaboration or centralized baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Scaling All-to-all Operations Across Emerging Many-Core Supercomputers"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [HPC communication algorithms], [all-to-all, MPI, locality-aware, hierarchical algorithms, Sapphire Rapids, NUMA]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Shannon Kinkead, Jackson Wesley, Whit Schonbein, David DeBonis, Matthew G. F. Dosanjh, Amanda Bienz"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Sandia National Laboratories, University of New Mexico, Los Alamos National Laboratory"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17606",children:"https://arxiv.org/pdf/2601.17606"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces novel locality-aware and hierarchical algorithms for optimizing MPI all-to-all collective operations on emerging many-core supercomputers. It presents a performance analysis comparing these new algorithms against existing implementations. The novel algorithms achieve up to 3x speedup over the system MPI on state-of-the-art Sapphire Rapids systems at 32 nodes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Lightspeed Data Compute for the Space Era"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [distributed computing], [MapReduce, inter-satellite laser links, distance-aware routing, bipartite match scheduling, LEO satellite mesh]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Thomas Sandholm, Bernardo A. Huberman, Klas Segeljakt, Paris Carbone"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," RISE - Research Institutes of Sweden, KTH Royal Institute of Technology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17589",children:"https://arxiv.org/pdf/2601.17589"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes SpaceCoMP, a MapReduce-inspired processing model for Low Earth Orbit satellite networks. It uses inter-satellite laser links for cooperative in-orbit data processing and a scheduling strategy to minimize aggregation costs. The method demonstrates significant improvements in efficiency, showing that orbital mesh networks can enable faster data processing in space."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Conduit: Programmer-Transparent Near-Data Processing Using Multiple Compute-Capable Resources in Solid State Drives"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [near-data processing, in-storage processing, in-flash processing, vectorization, SIMD, instruction-granularity offloading, cost function, SSD simulator]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Rakesh Nadig, Vamanan Arulchelvan, Mayank Kabra, Harshita Gupta, Rahul Bera, Nika Mansouri Ghiasi, Nanditha Rao, Qingcai Jiang, Andreas Kosmas Kakolyris, Yu Liang, Mohammad Sadrosadati, Onur Mutlu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," ETH Z\xfcrich, Inria Paris"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17633",children:"https://arxiv.org/pdf/2601.17633"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," Conduit is a programmer-transparent near-data processing framework for SSDs that uses compile-time vectorization and runtime instruction-granularity offloading to leverage multiple heterogeneous compute resources within an SSD. It outperforms prior offloading techniques by 1.8x and reduces energy consumption by 46% across data-intensive workloads."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Push Down Optimization for Distributed Multi Cloud Data Integration"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [cloud data integration], [push-down optimization, ETL, multi-cloud, data federation, SQL engines, Redshift, BigQuery]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ravi Kiran Kodali, Vinoth Punniyamoorthy, Akash Kumar Agarwal, Bikesh Kumar, Balakrishna Pothineni, Aswathnarayan Muthukrishnan Kirubakaran, Sumit Saha, Nachiappan Chockalingam"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Cognizant Technology Solutions, Albertsons Companies, East West Bank"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17546",children:"https://arxiv.org/pdf/2601.17546"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper examines the feasibility and strategies of push-down optimization for ETL pipelines in multi-cloud environments, evaluating techniques like localized push-down and data federation. It concludes that these approaches, demonstrated in a case study across Redshift and BigQuery, can reduce cross-cloud data traffic, lower runtime, and improve cost efficiency for distributed data integration."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] A Unified Approach to Concurrent, Parallel Map-Reduce in R using Futures"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [parallel computing], [map-reduce, futures, R programming, transpilation, concurrent processing]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Henrik Bengtsson"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Not explicitly stated; author email domain not provided. Likely independent or academic affiliation based on context."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17578",children:"https://arxiv.org/pdf/2601.17578"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper introduces the futurize package for R, which provides a unified function to automatically transpile sequential map-reduce code into parallel equivalents using the future ecosystem. This allows users to parallelize existing code with minimal changes, abstracting away the complexities of diverse parallel APIs. The main conclusion is that this approach simplifies parallel computing in R by separating the declaration of what to parallelize from the choice of how to execute it."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Communication-Avoiding Linear Algebraic Kernel K-Means on GPUs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [distributed linear algebra, 1.5D algorithm, communication-avoiding, multi-GPU, kernel k-means]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Julian Bellavita, Matthew Rubino, Nakul Iyer, Andrew Chang, Aditya Devarakonda, Flavio Vella, Giulia Guidi"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Cornell University, Meta, Wake Forest University, University of Trento, Epic Systems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17136",children:"https://arxiv.org/pdf/2601.17136"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces distributed-memory parallel algorithms for large-scale Kernel K-means clustering on multi-GPU systems, mapping its expensive computations onto communication-efficient distributed linear algebra primitives. The core innovation is a 1.5D partitioning scheme that enables highly scalable clustering of million-scale datasets. The results show that the 1.5D algorithm achieves substantial performance improvements, scaling to data one to two orders of magnitude larger than previous single-GPU methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Decentralized Multi-Agent Swarms for Autonomous Grid Security in Industrial IoT: A Consensus-based Approach"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [fault-tolerance], [decentralized multi-agent swarm, consensus-based threat validation, peer-to-peer protocol, edge computing, swarm intelligence, byzantine fault-tolerant consensus]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Samaresh Kumar Singh, Joyjit Roy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," IEEE (affiliation inferred from membership; no specific academic/research institution provided)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17303",children:"https://arxiv.org/pdf/2601.17303"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper proposes a Decentralized Multi-Agent Swarm (DMAS) architecture using AI agents at edge gateways and a Consensus-based Threat Validation (CVT) protocol for Industrial IoT security. The method enables cooperative, real-time threat detection without a central cloud, achieving sub-millisecond response times and high accuracy in experiments. The results show the approach significantly outperforms centralized and edge-based baselines in speed, accuracy, and bandwidth reduction."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Kareus: Joint Reduction of Dynamic and Static Energy in Large Model Training"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm training], [kernel scheduling, frequency scaling, multi-pass multi-objective optimization, dynamic energy, static energy]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ruofan Wu, Jae-Won Chung, Mosharaf Chowdhury"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Michigan"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17654",children:"https://arxiv.org/pdf/2601.17654"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," Kareus is a training system that jointly optimizes dynamic and static energy consumption by decomposing the problem into local subproblems and using a multi-pass multi-objective algorithm to find optimal kernel execution schedules and GPU frequencies. It demonstrates that fine-grained kernel scheduling and frequency scaling interdependently impact energy use, enabling reductions in training energy by up to 28.3% at the same time or training time by up to 27.5% at the same energy compared to prior work."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] CondenseGraph: Communication-Efficient Distributed GNN Training via On-the-Fly Graph Condensation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [graph condensation, error feedback, distributed training, communication compression]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Zizhao Zhang, Yihan Xue, Haotian Zhu, Sijia Li, Zhijun Wang, Yujie Xiao"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," University of Michigan, University of Southern California, New York University, Rice University, University of California, Berkeley"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17774",children:"https://arxiv.org/pdf/2601.17774"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes CondenseGraph, a framework that reduces communication overhead in distributed GNN training by dynamically compressing boundary node features into compact super nodes. It uses a gradient-based error feedback mechanism to compensate for information loss. Experiments show it reduces communication volume by 40-60% while maintaining accuracy comparable to full-precision baselines."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Multi-core & GPU-based Balanced Butterfly Counting in Signed Bipartite Graphs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [graph algorithms], [vertex-level parallelism, tile-based parallel approach, dynamic scheduling, wedge-based counting]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Mekala Kiran, Apurba Das, Suman Banerjee, Tathagata Ray"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," BITS Pilani Hyderabad Campus, Indian Institute of Technology Jammu"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17707",children:"https://arxiv.org/pdf/2601.17707"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents highly parallel algorithms for balanced butterfly counting in signed bipartite graphs, including a multi-core CPU method (M-BBC) and two GPU-based methods (G-BBC and G-BBC++). The core methods employ fine-grained vertex-level parallelism and tile-based approaches with dynamic scheduling to accelerate computation. The experimental results show substantial speedups over the sequential baseline, demonstrating the scalability and efficiency of the proposed parallel algorithms for large-scale graph analysis."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] On the Extension of Private Distributed Matrix Multiplication Schemes to the Grid Partition"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [distributed computing, coding theory], [polynomial codes, private distributed matrix multiplication, grid partitioning, outer product partitioning, inner product partitioning, collusion parameter]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Christoph Hofmeister, Razane Tajeddine, Antonia Wachter-Zeh, Rawad Bitar"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Technical University of Munich (TUM), American University of Beirut (AUB)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17834",children:"https://arxiv.org/pdf/2601.17834"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes extension operations to adapt existing polynomial codes designed for outer product partitioning to the more general grid partitioning for private distributed matrix multiplication. It shows these extensions improve performance for certain parameters but also impose combinatorial constraints, and introduces a new grid partitioning scheme that bypasses these constraints to outperform existing methods."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [others], [Action Authorization Boundary, Canonical Action Representation, deterministic authorization, non-bypassable enforcement, provenance logging]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Amjad Fatmi"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," The Faramesh Labs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17744",children:"https://arxiv.org/pdf/2601.17744"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces Faramesh, a protocol-agnostic execution control plane that establishes a mandatory Action Authorization Boundary (AAB) to enforce deterministic, non-bypassable authorization for actions proposed by autonomous agents before they are executed. Its core method involves canonicalizing agent intent and providing fail-closed, replayable decision artifacts. The main conclusion is that such an architectural boundary is a necessary structural guarantee for safe, auditable, and trustworthy deployment of autonomous agents in real-world systems, distinct from observability-only or protocol-based solutions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] LLM-42: Enabling Determinism in LLM Inference with Verified Speculation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [speculative decoding, dynamic batching, verify-rollback loop, batch-invariant computation, GPU kernels, floating-point non-associativity]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Raja Gond, Aditya K Kamath, Arkaprava Basu, Ramachandran Ramjee, Ashish Panwar"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Microsoft Research, University of Washington, Indian Institute of Science"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17768",children:"https://arxiv.org/pdf/2601.17768"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents LLM-42, a scheduling-based method that enables deterministic LLM inference by using a speculative fast path with dynamic batching and a lightweight verify-rollback loop to check and correct for non-determinism. This approach reuses existing kernels and incurs overhead only for the portion of the workload requiring determinism, avoiding the throughput degradation of disabling dynamic batching or the fixed overhead of batch-invariant kernels."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] An MLIR Lowering Pipeline for Stencils at Wafer-Scale"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [cluster infrastructure], [MLIR, stencil computations, domain-specific languages, CSL, xDSL, compiler pipeline, asynchronous execution model]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Nicolai Stawinoga, David Katz, Anton Lydike, Justs Zarins, Nick Brown, George Bisbas, Tobias Grosser"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Berlin, The University of Edinburgh, Imperial College London, University of Cambridge"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17754",children:"https://arxiv.org/pdf/2601.17754"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents an MLIR-based compiler pipeline that automatically transforms stencil-based kernels into optimized CSL code for the Cerebras Wafer-Scale Engine (WSE), bridging the semantic gap between mathematical problem descriptions and the WSE's asynchronous execution model. The approach achieves performance comparable to or better than manually optimized code on WSE2 and WSE3, and significantly outperforms traditional GPU and CPU-based supercomputers without requiring application-level code changes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] A Universal Load Balancing Principle and Its Application to Large Language Model Serving"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [mlsys], [llm inference], [load balancing, barrier synchronization, integer optimization, data-parallel decoding, straggler mitigation]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Zixi Chen, Tianci Bu, Chendong Song, Xin Lu, Yinyu Ye, Zijie Zhou"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," HKUST, National University of Defense Technology, Peking University, Stanford University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17855",children:"https://arxiv.org/pdf/2601.17855"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes a universal load-balancing principle formulated as a step-wise finite-horizon integer optimization to address the straggler problem in barrier-synchronized systems like LLM serving. It demonstrates that this method significantly reduces idle time, improves throughput and latency, and lowers energy consumption for data-parallel decoding."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] MultiChain Blockchain Data Provenance for Deterministic Stream Processing with Kafka Streams: A Weather Data Case Study"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [streaming data systems], [MultiChain blockchain, Kafka Streams, Merkle Trees, data provenance, deterministic serialization, cryptographic anchoring]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Niaz Mohammad Ramaki, Florian Schintke"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Technische Universit\xe4t Berlin, Zuse Institute Berlin"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18011",children:"https://arxiv.org/pdf/2601.18011"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a blockchain-based architecture using MultiChain and Kafka Streams to ensure deterministic reproducibility and auditability in stream processing. The method involves canonicalizing, deduplicating, and aggregating weather data, then storing Merkle roots and Kafka offset boundaries on-chain as cryptographic checkpoints. The evaluation with real weather station data shows the system provides linear verification cost, deterministic reproducibility, and scalable off-chain storage with satisfactory transaction throughput."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Types for Grassroots Logic Programs"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [ai], [programming languages], [Grassroots Logic Programs, type system, moded paths, concurrent logic programming, regular sets, well-typing, covariance, contravariance]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Ehud Shapiro"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," London School of Economics, Weizmann Institute of Science"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17957",children:"https://arxiv.org/pdf/2601.17957"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper introduces a type system for Grassroots Logic Programs (GLP), a concurrent logic programming language, by defining types as regular sets of moded paths that capture communication directionality. It provides a syntactic definition of well-typing and proves a semantic characterization linking it to covariance and contravariance conditions. The type system was implemented in Dart with AI assistance to help ensure correctness when programming complex communication modalities."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] On the Bandwidth Consumption of Blockchains"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [blockchain networking], [bandwidth measurement, transport protocol, node role segregation, block propagation, polling, WebSockets]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Andrei Lebedev, Vincent Gramoli"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," The University of Sydney, Redbelly Network"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18400",children:"https://arxiv.org/pdf/2601.18400"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper conducts the first empirical comparison of bandwidth consumption by measuring the network traffic of five blockchain protocols (Algorand, Aptos, Avalanche, Redbelly, Solana). It concludes that the transport protocol and block propagation strategy are the main factors impacting network traffic, and that segregating node roles helps reduce traffic."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] An Initial Evaluation of Distributed Graph Algorithms using NWGraph and HPX"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [distributed graph processing], [NWGraph, HPX, asynchronous many-task model, Breadth-First Search, PageRank, Boost Graph Library]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Karame Mohammadiporshokooh, Panagiotis Syskakis, Hartmut Kaiser"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Louisiana State University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18158",children:"https://arxiv.org/pdf/2601.18158"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," This paper presents a distributed graph processing framework by integrating the NWGraph library with the HPX runtime system, leveraging its asynchronous many-task model to reduce synchronization overhead. The evaluation shows that the approach achieves better performance than the distributed Boost Graph Library for Breadth-First Search, but not yet for PageRank, highlighting the promise and ongoing challenges of applying task-based runtimes to graph algorithms."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] Rhea: Detecting Privilege-Escalated Evasive Ransomware Attacks Using Format-Aware Validation in the Cloud"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [ransomware detection], [format-aware validation, mutation snapshots, cloud-offloaded defense, file format specification, privilege-escalated evasive ransomware (PEER)]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Beom Heyn Kim, Seok Min Hong, Mohammad Mannan"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," Hanyang University, Concordia University"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18216",children:"https://arxiv.org/pdf/2601.18216"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper presents Rhea, a cloud-offloaded ransomware defense system that uses format-aware validation to detect evasive attacks by checking the syntactic and semantic correctness of file formats in replicated data snapshots. It concludes that this method significantly outperforms existing I/O-pattern and statistical content-based approaches against modern, privilege-escalated ransomware."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"[arXiv260127] An Adaptive Purification Controller for Quantum Networks: Dynamic Protocol Selection and Multipartite Distillation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tags:"})," [sys], [quantum networks], [adaptive purification controller, dynamic programming, Pareto pruning, entanglement distillation, BBPSSW, DEJMPS, goodput optimization]"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"authors:"})," Pranav Kulkarni, Leo S\xfcnkel, Michael K\xf6lle"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"institution:"})," AIGNOSCO GmbH, LMU Munich"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"link:"})," ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18351",children:"https://arxiv.org/pdf/2601.18351"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simple LLM Summary:"})," The paper proposes an Adaptive Purification Controller (APC) that uses dynamic programming with Pareto pruning to autonomously select and sequence entanglement purification protocols in a quantum network. It demonstrates that this adaptive approach eliminates fidelity cliffs of static protocols and prevents resource wastage, achieving real-time decision-making with millisecond latencies."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "reinforcement learning" total: 46'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Breaking Task Impasses Quickly: Adaptive Neuro-Symbolic Learning for Open-World Robotics ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.16985",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Cognitive Platform Engineering for Autonomous Cloud Operations ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17542",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Deep Intrinsic Surprise-Regularized Control (DISRC): A Biologically Inspired Mechanism for Efficient Deep Q-Learning in Sparse Environments ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17598",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Quantum-Inspired Episode Selection for Monte Carlo Reinforcement Learning via QUBO Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17570",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Multi-Agent Deep Reinforcement Learning Under Constrained Communications ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17069",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Structure-Aware NL-to-SQL for SFC Provisioning via AST-Masking Empowered Language Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17295",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17223",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17329",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Beyond Instrumental and Substitutive Paradigms: Introducing Machine Culture as an Emergent Phenomenon in Large Language Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17096",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17275",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Retell, Reward, Repeat: Reinforcement Learning for Narrative Theory-Informed Story Generation ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17226",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Embodiment-Induced Coordination Regimes in Tabular Multi-Agent Q-Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17454",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17678",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17687",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17699",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17920",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] SD-E",(0,s.jsxs)(n.span,{className:"katex",children:[(0,s.jsx)(n.span,{className:"katex-mathml",children:(0,s.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(n.semantics,{children:[(0,s.jsx)(n.mrow,{children:(0,s.jsxs)(n.msup,{children:[(0,s.jsx)(n.mrow,{}),(0,s.jsx)(n.mn,{children:"2"})]})}),(0,s.jsx)(n.annotation,{encoding:"application/x-tex",children:"^2"})]})})}),(0,s.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(n.span,{className:"base",children:[(0,s.jsx)(n.span,{className:"strut",style:{height:"0.8141em"}}),(0,s.jsxs)(n.span,{className:"mord",children:[(0,s.jsx)(n.span,{}),(0,s.jsx)(n.span,{className:"msupsub",children:(0,s.jsx)(n.span,{className:"vlist-t",children:(0,s.jsx)(n.span,{className:"vlist-r",children:(0,s.jsx)(n.span,{className:"vlist",style:{height:"0.8141em"},children:(0,s.jsxs)(n.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,s.jsx)(n.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(n.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsx)(n.span,{className:"mord mtight",children:"2"})})]})})})})})]})]})})]}),": Semantic Exploration for Reasoning Under Token Budgets ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17982",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18107",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18142",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18150",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18207",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18217",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18225",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18292",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18296",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18381",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] daVinci-Dev: Agent-native Mid-training for Software Engineering ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18418",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18467",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18479",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18510",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18580",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18586",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18626",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18631",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18681",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18706",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18730",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18734",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18751",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18771",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18778",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18779",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18783",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18795",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Exact Minimum-Volume Confidence Set Intersection for Multinomial Outcomes ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18145",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Emergent Cooperation in Quantum Multi-Agent Reinforcement Learning Using Communication ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18419",children:"link"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:'cs.AI/cs.LG contains "accelerate" total: 17'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Unrolled Neural Networks for Constrained Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17274",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Cognitive Platform Engineering for Autonomous Cloud Operations ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17542",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17364",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Accelerated Sinkhorn Algorithms for Partial Optimal Transport ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17196",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] PALMA: A Lightweight Tropical Algebra Library for ARM-Based Embedded Systems ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17028",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17426",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17687",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17868",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17917",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Multimodal Machine Learning for Soft High-k Elastomers under Data Scarcity ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18032",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18067",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18150",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Frequency-Based Hyperparameter Selection in Games ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18409",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Gradient Regularized Natural Gradients ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18420",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] Physics-Informed Uncertainty Enables Reliable AI-driven Design ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.18638",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] EveNet: A Foundation Model for Particle Collision Data Analysis ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17126",children:"link"})]}),"\n",(0,s.jsxs)(n.li,{children:["[arXiv260127] GenAI-Net: A Generative AI Framework for Automated Biomolecular Network Design ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2601.17582",children:"link"})]}),"\n"]})]})}function d(i={}){const{wrapper:n}={...(0,a.R)(),...i.components};return n?(0,s.jsx)(n,{...i,children:(0,s.jsx)(h,{...i})}):h(i)}}}]);